# 贝叶斯决策理论

决策理论

决策论Decision theory + 概率论Probability theory

探讨了如何在包含不确定性的环境中做出最优决策
概率知识+对决策带来的损失的认识→最优决策

**模型比较理论**
最大似然：最符合观测数据的（即P(D | h) 最大的）最有优势
奥卡姆剃刀：P(h) 较大的模型有较大的优势

掷一个硬币，观察到的是“正”，根据最大似然估计的精神，我们应该猜测这枚硬币掷出“正”的概率是1，因为这个才是能最大化P(D | h) 的那个猜测

如果平面上有N 个点，近似构成一条直线，但绝不精确地位于一条直线上。这时我们既可以用直线来拟合（模型1），也可以用二阶多项式（模型2）拟合，也可以用三阶多项式（模型3），特别地，用N-1 阶多项式便能够保证肯定能完美通过N 个数据点。那么，这些可能的模型之中到底哪个是最靠谱的呢？
奥卡姆剃刀：越是高阶的多项式越是不常见

P(d1|h+) * P(d2|d1, h+) * P(d3|d2,d1, h+) * ..
假设di 与di-1 是完全条件无关的（朴素贝叶斯假设特征之间是独立，互不影响）
简化为P(d1|h+) * P(d2|h+) * P(d3|h+) * ..

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/bayes1.png)

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/bayes2.png)

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/bayes3.png)

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/bayes4.png)

![image](https://github.com/LinglingGreat/Quote/raw/master/img/ML/bayes5.png) 

为了避免其他属性携带的信息被训练集中未出现的属性值"抹去'，在估计概率值时通常要进行"平滑" (smoothing) ，常用"拉普拉斯修正" (Laplacian correction）。拉普拉斯修正实质上假设了属性值与类别均匀分布。

在现实任务中朴素贝叶斯分类器有多种使用方式.例如，若任务对预测速度要求较高，则对给定训练集，可将朴素贝叶斯分类器涉及的所有概率估值事先计算好存储起来，这样在进行预测时只需"查表"即可进行判别;若任务数据更替频繁，则可采用"懒惰学习" (lazy learning) 方式，先不进行任何训练，待收到预测请求时再根据当前数据集进行概率估值;若数据不断增加，则可在现有估值基础上，仅对新增样本的属性值所涉及的概率估值进行计数修正即可，实现增量学习.
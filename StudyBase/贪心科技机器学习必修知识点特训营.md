https://tx.greedyai.com/my/course/150

Bagging vs Boosting

- Bagging: Leverages unstable base learners that are weak
  because of **overfitting**。每个人都是专家，都做得很好，容易过拟合。Bagging降低风险。股票投资不要都放到一个篮子里。list of experts

- Boosting: Leverage stable base learners that are
  weak because of **underfitting**。一般人，不是专家，容易欠拟合。三个臭皮匠胜过诸葛亮。list of weak learner(比随机猜测好)

随机森林是并行式训练。

Boosting是串行式训练。

(x, 真实值)——Model 1——预测值1

(x, 真实值-预测值1=残差1)——Model 2——预测值2

(x, 残差1-预测值2=残差2)——Model 3——预测值3

Model可以是任何的模型，但是一般使用树模型

例子：预测年龄

![1611675680964](img/1611675680964.png)

![1611675753131](img/1611675753131.png)

XGBoost会比较关注残差更大的那些样本。

![1611675794303](img/1611675794303.png)

![1611675896525](img/1611675896525.png)

为什么XGBoost这么火爆？

- 速度快，效果好（Speed and Performance）

- 核心算法可以并行化

- 大多数情况下比其它算法表现好

- 大量的可调节的参数

XGBoost学习路径

- 如何构造目标函数
- 如何近似目标函数
- 如何引入树到目标函数，改造目标函数
- 如何使用贪心算法构造树

使用多棵树预测

![1611754728199](img/1611754728199.png)

![1611754851549](img/1611754851549.png)

第二项类似于正则，防止过拟合。可以是树的深度、叶子结点的权重（即预测值）、叶子结点的个数等

第一项是损失函数。

![1611755123352](img/1611755123352.png)

如何去训练模型

![1611755147830](img/1611755147830.png)

t棵树。第一棵树是0，认为是Base case。预测值是combined prediction，已经把之前的树的预测值加上了。

![1611755315794](img/1611755315794.png)

前t-1棵树的复杂度已知，所以变成了constant。

使用泰勒级数近似目标函数

![1611756275857](img/1611756275857.png)

函数有二阶导数的条件下才能展开成上式。

得到新的目标函数

![1611756828257](img/1611756828257.png)

g, h是很容易计算的，但f(x)如何去表示呢？因为它是一棵树，如果表示成函数的形式呢？

重新定义一棵树

![1611757051623](img/1611757051623.png)

T：叶子结点的数量

w：权重向量

q(x)：输入object，输出index(叶子结点的index)

树的复杂度

![1611757337439](img/1611757337439.png)

新的目标函数

![1611757859705](img/1611757859705.png)

![1611758016987](img/1611758016987.png)

f(x)=ax^2+bx+c的最小值，x=-b/2a

上述是树的形状已知的情况下的最小值

举例：

![1611758341509](img/1611758341509.png)

如何寻找树的形状？

最简单的是暴力搜索法。列出所有可能的树，计算出每棵树的最小的目标函数值，从中选出最小的目标函数值，就找到树的形状了。

![1611758460291](img/1611758460291.png)但是我们无法列出所有可能的树，而且非常耗时。

![1611758580435](img/1611758580435.png)

想想决策树的构造过程，Information gain，这里用Obj

注意每个结点是独立的。每个结点的Obj可以分开求最小值

$\gamma=\gamma (T+1)-\gamma T$

寻找最好的Split

![1611760200562](img/1611760200562.png)



XGBoost的参数

- max_depth：最大深度
- subsample

涉及PAC Learning，VC Dimension



**为什么需要Attention**

NMT(Neural Machine Translation)

输入到Encoder(比如RNN)得到Context，再到Decoder得到输出

context是这种模型的瓶颈，这种模型处理长句子变得颇具挑战性。

2015年提出Attention，Attention技术极大地提高了机器翻译系统的质量，使得模型可以根据需要集中于输入序列的相关部分。

不同点：

- encoder传播了很多数据给decoder。相比较于以前只传递encoding部分的最后的Hidden state，encoder传递了所有的hidden states 到decoder
- 为每一个hidden state产生不同的权重，每一个hidden state分别乘以softmaxed的权重，然后求和

![1611847711278](img/1611847711278.png)

h_t是decoder在t时刻的hidden state

h_s是encoder之前的hidden state，s代表输入的单词数



## Transformer

在论文中被提出，利用Self Attention，去掉RNN，使得训练更快。

Encoder包括Self-Attention和Feed Forward

Decoder包括Self-Attention, Encoder-decoder Attention和Feed Forward

![1612277745084](img/1612277745084.png)

q, k, v 文档检索。K是问题，V是答案。Q是用户的提问。Q和K计算相关度。

多头注意力机制

![1612278838921](img/1612278838921.png)

![1612278997097](img/1612278997097.png)



Positional Encoding

- 用于表达不同单词在输入句子中的位置顺序
- 为了表达位置顺序，transformer为每一个输入的单词的embedding添加了一个位置向量
- 直觉是将位置向量添加到embedding以后，计算attention的时候，这些位置向量提供了不同embedding之间的距离信息。

![1612279345225](img/1612279345225.png)

每一行是一个embedding。黄色的是1，越往下的颜色值越小。



Residual, Layer Normalization

![1612279640698](img/1612279640698.png)

![1612279739515](img/1612279739515.png)

Residual, Normalization在每一层都使用。



Decoder

Decoder的第二个多头注意力的V是来自自己的，Q和K是来自Encoder的输出。

![1612279897424](img/1612279897424.png)

损失函数

![1612280092936](img/1612280092936.png)

训练小技巧

![1612280132775](img/1612280132775.png)

http://nlp.seas.harvard.edu/2018/04/03/attention.html

## Bert

Contextual Embedding

- Word2Vec等embedding忽略了context
- ELMo考虑了context，但是由于其使用bidirectional-LSTM，无法避免梯度消失(gradient vanishing)的问题
- 是否可以利用Transformer的思路构造contextual embedding，Transformer的问题在于其为forward language model，而构造contextual embedding需要both forward和backward

![1612796831936](img/1612796831936.png)

![1612796852065](img/1612796852065.png)

训练：

- Masked Language Model。mask 15%的单词，让模型预测masked的单词，或者随机替换一个单词，让模型预测此位置正确的单词
- Two-sentence Tasks。

选择哪一层的embedding，视具体任务而定

## 基于知识图谱的推荐

水波网络(Ripple Network)

![1612794769697](img/1612794769697.png)

n-hop：可以理解为扩散到第几层时的节点数

Ripple Set：理解为三元组

学习出用户和item的embedding

排序：CTR：$\hat y_{uv}=\sigma(u^Tv)$   点还是不点的概率

召回：获取了user_Embedding与Item_Embedding：向量化检索最相似的TopN

在召回阶段用的多。

![1612795693821](img/1612795693821.png)

随机初始化embedding

![1612796227578](img/1612796227578.png)

v和h的维度相同。

水波网络问题

- 最终效果强依赖于物品图谱的构建质量
- 物品图谱的大小
- 物品图谱的更新频率








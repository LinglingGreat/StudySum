{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: \n",
    "1. sigmoid\n",
    "2. (梯度检查)\n",
    "3. 双层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 实现sigmoid激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "sigmoid(z) = \\frac{1}{1+e^{-z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 sigmoid求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "s = sigmoid(z)\\\\\n",
    "s^{'} = s \\cdot (1-s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from q1_softmax import softmax\n",
    "from q2_gradcheck import gradcheck_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(s):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    s -- A scalar or numpy array.\n",
    "\n",
    "    Return:\n",
    "    ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "    ds = s * (1 - s)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    print (\"Running basic tests...\")\n",
    "    x = np.array([[1, 2], [-1, -2]])\n",
    "    f = sigmoid(x)\n",
    "    g = sigmoid_grad(f)\n",
    "    print (f)\n",
    "    f_ans = np.array([\n",
    "        [0.73105858, 0.88079708],\n",
    "        [0.26894142, 0.11920292]])\n",
    "    assert np.allclose(f, f_ans, rtol=1e-05, atol=1e-06)\n",
    "    print (g)\n",
    "    g_ans = np.array([\n",
    "        [0.19661193, 0.10499359],\n",
    "        [0.19661193, 0.10499359]])\n",
    "    assert np.allclose(g, g_ans, rtol=1e-05, atol=1e-06)\n",
    "    print (\"You should verify these results by hand!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "[[0.73105858 0.88079708]\n",
      " [0.26894142 0.11920292]]\n",
      "[[0.19661193 0.10499359]\n",
      " [0.19661193 0.10499359]]\n",
      "You should verify these results by hand!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoid_basic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.双层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_prop(data, labels, params, dimensions):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation for a two-layer sigmoidal network\n",
    "\n",
    "    Compute the forward propagation and for the cross entropy cost,\n",
    "    and backward propagation for the gradients for all parameters.\n",
    "\n",
    "    Arguments:\n",
    "    data -- N x Dx matrix, where each row is a training example.\n",
    "    labels -- N x Dy matrix, where each row is a one-hot vector.\n",
    "    params -- Model parameters, these are unpacked for you.\n",
    "    dimensions -- A tuple of input dimension, number of hidden units\n",
    "                  and output dimension\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    ### Unpack network parameters (do not modify)\n",
    "    ofs = 0\n",
    "    Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2])\n",
    "\n",
    "    W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H))\n",
    "    ofs += Dx * H\n",
    "    b1 = np.reshape(params[ofs:ofs + H], (1, H))\n",
    "    ofs += H\n",
    "    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))\n",
    "    ofs += H * Dy\n",
    "    b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy))\n",
    "    \"\"\"\n",
    "    W1  # shape = (样本, 节点)  (20, 5)\n",
    "    b1  # shape = (1, 节点)     (1, 5)\n",
    "    W2  # shape = (节点， 特征) (5, 10)\n",
    "    b2  # shape = (1, 类别)     (1, 10)\n",
    "    \"\"\"\n",
    "    ###################################################\n",
    "    \"\"\"\n",
    "    W1  # shape = (节点, )  (10, 5)\n",
    "    b1  # shape = (1, 节点)     (1, 5)\n",
    "    W2  # shape = (节点， 特征) (5, 10)\n",
    "    b2  # shape = (1, 类别)     (1, 10)\n",
    "    \"\"\"\n",
    "    ###################################################\n",
    "    N = data.shape[0]\n",
    "    #样本 = 20， 特征=10，节点 = 5， 类别 = 10\n",
    "    #forward propagation                   \n",
    "    A0 = data                             # shape = (样本，特征) (20, 10)\n",
    "    Z1 = np.dot(A0, W1) + b1              # shape = (样本, 节点) (20, 5)\n",
    "    A1 = sigmoid(Z1)                      # shape = (样本, 节点) (20, 5)\n",
    "    Z2 = np.dot(A1, W2) + b2              # shape = (样本，类别) (20, 10)\n",
    "    A2 = softmax(Z2)                      # shape = (样本，类别) (20, 10)\n",
    "\n",
    "    #compute the cost\n",
    "    target = np.argmax(labels, axis=1)\n",
    "    cost_each = -np.log(A2[range(N), target]).reshape(-1, 1)   #shape = (样本, 1), 计算li\n",
    "    cost = np.mean(cost_each, axis=0)        #shape =  (1,), 计算L\n",
    "\n",
    "    #backward propagation\n",
    "    dZ2 = (A2 - labels) / N                     # shape = (样本，类别) (20, 10)\n",
    "    dW2 = np.dot(A1.T, dZ2)                     # shape = (节点， 特征) (5, 10)\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True)    # shape = (1, 类别)  (1, 10)\n",
    "\n",
    "    dZ1 = np.dot(dZ2, W2.T) * sigmoid_grad(A1)  # shape = (1, 类别)  (1, 10)\n",
    "    dW1 = np.dot(A0.T, dZ1)                     # shape = (样本, 节点)(20, 5)\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True)    # shape = (1, 节点)   (1, 5)\n",
    "\n",
    "    ###################################################\n",
    "    \n",
    "    ###################################################\n",
    "    \n",
    "    ### Stack gradients (do not modify)\n",
    "    grad = np.concatenate((dW1.flatten(), db1.flatten(),\n",
    "        dW2.flatten(), db2.flatten()))\n",
    "\n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check():\n",
    "    \"\"\"\n",
    "    Set up fake data and parameters for the neural network, and test using\n",
    "    gradcheck.\n",
    "    \"\"\"\n",
    "    print (\"Running sanity check...\")\n",
    "\n",
    "    N = 20\n",
    "    dimensions = [10, 5, 10]\n",
    "    data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
    "    labels = np.zeros((N, dimensions[2]))\n",
    "    for i in range(N):\n",
    "        labels[i, random.randint(0,dimensions[2]-1)] = 1\n",
    "\n",
    "    params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (\n",
    "        dimensions[1] + 1) * dimensions[2], )\n",
    "\n",
    "    gradcheck_naive(lambda params:\n",
    "        forward_backward_prop(data, labels, params, dimensions), params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity check...\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "sanity_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############初始化#############\n",
    "np.random.seed(1)\n",
    "N = 20\n",
    "dimensions = [10, 5, 10]\n",
    "data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
    "labels = np.zeros((N, dimensions[2]))\n",
    "for i in range(N):\n",
    "    labels[i, random.randint(0,dimensions[2]-1)] = 1\n",
    "\n",
    "params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (\n",
    "    dimensions[1] + 1) * dimensions[2], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################参数初始化########################\n",
    "ofs = 0\n",
    "Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2])\n",
    "\n",
    "W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H))\n",
    "ofs += Dx * H\n",
    "b1 = np.reshape(params[ofs:ofs + H], (1, H))\n",
    "ofs += H\n",
    "W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))\n",
    "ofs += H * Dy\n",
    "b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy))\n",
    "#########################参数初始化########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "W1  # shape = (节点, )  (10, 5)\n",
    "b1  # shape = (1, 节点)     (1, 5)\n",
    "W2  # shape = (节点， 特征) (5, 10)\n",
    "b2  # shape = (1, 类别)     (1, 10)\n",
    "\"\"\"\n",
    "###################################################\n",
    "N = data.shape[0]\n",
    "#样本 = 20， 特征=10，节点 = 5， 类别 = 10\n",
    "#forward propagation                   \n",
    "A0 = data                             # shape = (样本，特征) (20, 10)\n",
    "Z1 = np.dot(A0, W1) + b1              # shape = (样本, 节点) (20, 5)\n",
    "A1 = sigmoid(Z1)                      # shape = (样本, 节点) (20, 5)\n",
    "Z2 = np.dot(A1, W2) + b2              # shape = (样本，类别) (20, 10)\n",
    "A2 = softmax(Z2)                      # shape = (样本，类别) (20, 10)\n",
    "\n",
    "#compute the cost\n",
    "target = np.argmax(labels, axis=1)\n",
    "cost_each = -np.log(A2[range(N), target]).reshape(-1, 1)   #shape = (样本, 1), 计算li\n",
    "cost = np.mean(cost_each, axis=0)        #shape =  (1,), 计算L\n",
    "\n",
    "#backward propagation\n",
    "dZ2 = (A2 - labels) / N                     # shape = (样本，类别) (20, 10)\n",
    "dW2 = np.dot(A1.T, dZ2)                     # shape = (节点， 特征) (5, 10)\n",
    "db2 = np.sum(dZ2, axis=0, keepdims=True)    # shape = (1, 类别)  (1, 10)\n",
    "\n",
    "dZ1 = np.dot(dZ2, W2.T) * sigmoid_grad(A1)  # shape = (1, 类别)  (1, 10)\n",
    "dW1 = np.dot(A0.T, dZ1)                     # shape = (样本, 节点)(20, 5)\n",
    "db1 = np.sum(dZ1, axis=0, keepdims=True)    # shape = (1, 节点)   (1, 5)\n",
    "\n",
    "###################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 『NLP直播课』Day 5：情感分析预训练模型SKEP\n",
    "\n",
    "本项目将详细全面介绍情感分析任务的两种子任务，句子级情感分析和目标级情感分析。\n",
    "\n",
    "同时演示如何使用情感分析预训练模型SKEP完成以上两种任务，详细介绍预训练模型SKEP及其在 PaddleNLP 的使用方式。\n",
    "\n",
    "本项目主要包括“任务介绍”、“情感分析预训练模型SKEP”、“句子级情感分析”、“目标级情感分析”等四个部分。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp -i https://pypi.org/simple "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  Part A. 情感分析任务\n",
    "\n",
    "众所周知，人类自然语言中包含了丰富的情感色彩：表达人的情绪（如悲伤、快乐）、表达人的心情（如倦怠、忧郁）、表达人的喜好（如喜欢、讨厌）、表达人的个性特征和表达人的立场等等。情感分析在商品喜好、消费决策、舆情分析等场景中均有应用。利用机器自动分析这些情感倾向，不但有助于帮助企业了解消费者对其产品的感受，为产品改进提供依据；同时还有助于企业分析商业伙伴们的态度，以便更好地进行商业决策。\n",
    "\n",
    "被人们所熟知的情感分析任务是将一段文本分类，如分为情感极性为**正向**、**负向**、**其他**的三分类问题：\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/b630901b397e4e7a8e78ab1d306dfa1fc070d91015a64ef0b8d590aaa8cfde14\" width=\"600\" ></center>\n",
    "<br><center>情感分析任务</center></br>\n",
    "\n",
    "- **正向：** 表示正面积极的情感，如高兴，幸福，惊喜，期待等。\n",
    "- **负向：** 表示负面消极的情感，如难过，伤心，愤怒，惊恐等。\n",
    "- **其他：** 其他类型的情感。\n",
    "\n",
    "实际上，以上熟悉的情感分析任务是**句子级情感分析任务**。\n",
    "\n",
    "\n",
    "情感分析任务还可以进一步分为**句子级情感分析**、**目标级情感分析**等任务。在下面章节将会详细介绍两种任务及其应用场景。\n",
    "\n",
    "\n",
    "## Part B. 情感分析预训练模型SKEP\n",
    "\n",
    "近年来，大量的研究表明基于大型语料库的预训练模型（Pretrained Models, PTM）可以学习通用的语言表示，有利于下游NLP任务，同时能够避免从零开始训练模型。随着计算能力的发展，深度模型的出现（即 Transformer）和训练技巧的增强使得 PTM 不断发展，由浅变深。\n",
    "\n",
    "情感预训练模型SKEP（Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis）。SKEP利用情感知识增强预训练模型， 在14项中英情感分析典型任务上全面超越SOTA，此工作已经被ACL 2020录用。SKEP是百度研究团队提出的基于情感知识增强的情感预训练算法，此算法采用无监督方法自动挖掘情感知识，然后利用情感知识构建预训练目标，从而让机器学会理解情感语义。SKEP为各类情感分析任务提供统一且强大的情感语义表示。\n",
    "\n",
    "**论文地址**：https://arxiv.org/abs/2005.05635\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://paddlenlp.bj.bcebos.com/models/transformers/skep/skep.png\" width=\"80%\" height=\"60%\"> <br />\n",
    "</p>\n",
    "\n",
    "百度研究团队在三个典型情感分析任务，句子级情感分类（Sentence-level Sentiment Classification），评价目标级情感分类（Aspect-level Sentiment Classification）、观点抽取（Opinion Role Labeling），共计14个中英文数据上进一步验证了情感预训练模型SKEP的效果。\n",
    "\n",
    "具体实验效果参考：https://github.com/baidu/Senta#skep\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part C 句子级情感分析 & 目标级情感分析\n",
    "\n",
    "### Part C.1 句子级情感分析\n",
    "\n",
    "\n",
    "对给定的一段文本进行情感极性分类，常用于影评分析、网络论坛舆情分析等场景。如:\n",
    "\n",
    "```text\n",
    "选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般\t1\n",
    "15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜欢数字小键盘，输数字特方便，样子也很美观，做工也相当不错\t1\n",
    "房间太小。其他的都一般。。。。。。。。。\t0\n",
    "```\n",
    "\n",
    "其中`1`表示正向情感，`0`表示负向情感。\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/4aae00a800ae4831b6811b669f7461d8482344b183454d8fb7d37c83defb9567\" width=\"550\" ></center>\n",
    "<br><center>句子级情感分析任务</center></br>\n",
    "\n",
    "\n",
    "#### 常用数据集\n",
    "\n",
    "ChnSenticorp数据集是公开中文情感分析常用数据集， 其为2分类数据集。PaddleNLP已经内置该数据集，一键即可加载。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import load_dataset\n",
    "\n",
    "train_ds, dev_ds, test_ds = load_dataset(\"chnsenticorp\", splits=[\"train\", \"dev\", \"test\"])\n",
    "\n",
    "print(train_ds[0])\n",
    "print(train_ds[1])\n",
    "print(train_ds[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "### SKEP模型加载\n",
    "\n",
    "PaddleNLP已经实现了SKEP预训练模型，可以通过一行代码实现SKEP加载。\n",
    "\n",
    "句子级情感分析模型是SKEP fine-tune 文本分类常用模型`SkepForSequenceClassification`。其首先通过SKEP提取句子语义特征，之后将语义特征进行分类。\n",
    "\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/fc21e1201154451a80f32e0daa5fa84386c1b12e4b3244e387ae0b177c1dc963)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer\n",
    "\n",
    "# 指定模型名称，一键加载模型\n",
    "model = SkepForSequenceClassification.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\", num_classes=len(train_ds.label_list))\n",
    "# 同样地，通过指定模型名称一键加载对应的Tokenizer，用于处理文本数据，如切分token，转token_id等。\n",
    "tokenizer = SkepTokenizer.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "`SkepForSequenceClassification`可用于句子级情感分析和目标级情感分析任务。其通过预训练模型SKEP获取输入文本的表示，之后将文本表示进行分类。\n",
    "\n",
    "* `pretrained_model_name_or_path`：模型名称。支持\"skep_ernie_1.0_large_ch\"，\"skep_ernie_2.0_large_en\"。\n",
    "\t- \"skep_ernie_1.0_large_ch\"：是SKEP模型在预训练ernie_1.0_large_ch基础之上在海量中文数据上继续预训练得到的中文预训练模型；\n",
    "    - \"skep_ernie_2.0_large_en\"：是SKEP模型在预训练ernie_2.0_large_en基础之上在海量英文数据上继续预训练得到的英文预训练模型；\n",
    "    \n",
    "* `num_classes`: 数据集分类类别数。\n",
    "\n",
    "\n",
    "关于SKEP模型实现详细信息参考：https://github.com/PaddlePaddle/PaddleNLP/tree/develop/paddlenlp/transformers/skep\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据处理\n",
    "\n",
    "同样地，我们需要将原始ChnSentiCorp数据处理成模型可以读入的数据格式。\n",
    "\n",
    "SKEP模型对中文文本处理按照字粒度进行处理，我们可以使用PaddleNLP内置的`SkepTokenizer`完成一键式处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "\n",
    "from utils import create_dataloader\n",
    "\n",
    "def convert_example(example,\n",
    "                    tokenizer,\n",
    "                    max_seq_length=512,\n",
    "                    is_test=False):\n",
    "    \"\"\"\n",
    "    Builds model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
    "    by concatenating and adding special tokens. And creates a mask from the two sequences passed \n",
    "    to be used in a sequence-pair classification task.\n",
    "        \n",
    "    A skep_ernie_1.0_large_ch/skep_ernie_2.0_large_en sequence has the following format:\n",
    "    ::\n",
    "        - single sequence: ``[CLS] X [SEP]``\n",
    "        - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n",
    "\n",
    "    A skep_ernie_1.0_large_ch/skep_ernie_2.0_large_en sequence pair mask has the following format:\n",
    "    ::\n",
    "\n",
    "        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
    "        | first sequence    | second sequence |\n",
    "\n",
    "    If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n",
    "\n",
    "\n",
    "    Args:\n",
    "        example(obj:`list[str]`): List of input data, containing text and label if it have label.\n",
    "        tokenizer(obj:`PretrainedTokenizer`): This tokenizer inherits from :class:`~paddlenlp.transformers.PretrainedTokenizer` \n",
    "            which contains most of the methods. Users should refer to the superclass for more information regarding methods.\n",
    "        max_seq_len(obj:`int`): The maximum total input sequence length after tokenization. \n",
    "            Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "        is_test(obj:`False`, defaults to `False`): Whether the example contains label or not.\n",
    "\n",
    "    Returns:\n",
    "        input_ids(obj:`list[int]`): The list of token ids.\n",
    "        token_type_ids(obj: `list[int]`): List of sequence pair mask.\n",
    "        label(obj:`int`, optional): The input label if not is_test.\n",
    "    \"\"\"\n",
    "    # 将原数据处理成model可读入的格式，enocded_inputs是一个dict，包含input_ids、token_type_ids等字段\n",
    "    encoded_inputs = tokenizer(\n",
    "        text=example[\"text\"], max_seq_len=max_seq_length)\n",
    "\n",
    "    # input_ids：对文本切分token后，在词汇表中对应的token id\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    # token_type_ids：当前token属于句子1还是句子2，即上述图中表达的segment ids\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        # label：情感极性类别\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        # qid：每条数据的编号\n",
    "        qid = np.array([example[\"qid\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 批量数据大小\n",
    "batch_size = 32\n",
    "# 文本序列最大长度\n",
    "max_seq_length = 128\n",
    "\n",
    "# 将数据处理成模型可读入的数据格式\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length)\n",
    "\n",
    "# 将数据组成批量式数据，如\n",
    "# 将不同长度的文本序列padding到批量式数据中最大长度\n",
    "# 将每条数据label堆叠在一起\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack()  # labels\n",
    "): [data for data in fn(samples)]\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "dev_data_loader = create_dataloader(\n",
    "    dev_ds,\n",
    "    mode='dev',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型训练和评估\n",
    "\n",
    "\n",
    "定义损失函数、优化器以及评价指标后，即可开始训练。\n",
    "\n",
    "\n",
    "**推荐超参设置：**\n",
    "\n",
    "* `max_seq_length=256`\n",
    "* `batch_size=48`\n",
    "* `learning_rate=2e-5`\n",
    "* `epochs=10`\n",
    "\n",
    "实际运行时可以根据显存大小调整batch_size和max_seq_length大小。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from utils import evaluate\n",
    "\n",
    "# 训练轮次\n",
    "epochs = 1\n",
    "# 训练过程中保存模型参数的文件夹\n",
    "ckpt_dir = \"skep_ckpt\"\n",
    "# len(train_data_loader)一轮训练所需要的step数\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "\n",
    "# Adam优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=2e-5,\n",
    "    parameters=model.parameters())\n",
    "# 交叉熵损失函数\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "# accuracy评价指标\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 开启训练\n",
    "global_step = 0\n",
    "tic_train = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        # 喂数据给model\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        # 计算损失函数值\n",
    "        loss = criterion(logits, labels)\n",
    "        # 预测分类概率值\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        # 计算acc\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0:\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, acc,\n",
    "                    10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "        \n",
    "        # 反向梯度回传，更新参数\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        if global_step % 100 == 0:\n",
    "            save_dir = os.path.join(ckpt_dir, \"model_%d\" % global_step)\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            # 评估当前训练的模型\n",
    "            evaluate(model, criterion, metric, dev_data_loader)\n",
    "            # 保存当前模型参数等\n",
    "            model.save_pretrained(save_dir)\n",
    "            # 保存tokenizer的词表等\n",
    "            tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 预测提交结果\n",
    "\n",
    "\n",
    "使用训练得到的模型还可以对文本进行情感预测。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import paddle\n",
    "\n",
    "# 处理测试集数据\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    is_test=True)\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment\n",
    "    Stack() # qid\n",
    "): [data for data in fn(samples)]\n",
    "test_data_loader = create_dataloader(\n",
    "    test_ds,\n",
    "    mode='test',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 根据实际运行情况，更换加载的参数路径\n",
    "params_path = 'skep_ckp/model_500/model_state.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_map = {0: '0', 1: '1'}\n",
    "results = []\n",
    "# 切换model模型为评估模式，关闭dropout等随机因素\n",
    "model.eval()\n",
    "for batch in test_data_loader:\n",
    "    input_ids, token_type_ids, qids = batch\n",
    "    # 喂数据给模型\n",
    "    logits = model(input_ids, token_type_ids)\n",
    "    # 预测分类\n",
    "    probs = F.softmax(logits, axis=-1)\n",
    "    idx = paddle.argmax(probs, axis=1).numpy()\n",
    "    idx = idx.tolist()\n",
    "    labels = [label_map[i] for i in idx]\n",
    "    qids = qids.numpy().tolist()\n",
    "    results.extend(zip(qids, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res_dir = \"./results\"\n",
    "if not os.path.exists(res_dir):\n",
    "    os.makedirs(res_dir)\n",
    "# 写入预测结果\n",
    "with open(os.path.join(res_dir, \"ChnSentiCorp.tsv\"), 'w', encoding=\"utf8\") as f:\n",
    "    f.write(\"index\\tprediction\\n\")\n",
    "    for qid, label in results:\n",
    "        f.write(str(qid[0])+\"\\t\"+label+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Part C.2 目标级情感分析\n",
    "\n",
    "在电商产品分析场景下，除了分析整体商品的情感极性外，还细化到以商品具体的“方面”为分析主体进行情感分析（aspect-level），如下、：\n",
    "\n",
    "* 这个薯片口味有点咸，太辣了，不过口感很脆。\n",
    "\n",
    "关于薯片的**口味方面**是一个负向评价（咸，太辣），然而对于**口感方面**却是一个正向评价（很脆）。\n",
    "\n",
    "* 我很喜欢夏威夷，就是这边的海鲜太贵了。\n",
    "\n",
    "关于**夏威夷**是一个正向评价（喜欢），然而对于**夏威夷的海鲜**却是一个负向评价（价格太贵）。\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/052d46409ba3451693a718552b968d188fa4677235bc43ddbc15fe11ad3b57b1\" width=\"600\" ></center>\n",
    "<br><center>目标级情感分析任务</center></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 常用数据集\n",
    "\n",
    "[千言数据集](https://www.luge.ai/)已提供了许多任务常用数据集。\n",
    "其中情感分析数据集下载链接：https://aistudio.baidu.com/aistudio/competition/detail/50/?isFromLUGE=TRUE\n",
    "\n",
    "SE-ABSA16_PHNS数据集是关于手机的目标级情感分析数据集。PaddleNLP已经内置了该数据集，加载方式，如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds, test_ds = load_dataset(\"seabsa16\", \"phns\", splits=[\"train\", \"test\"])\n",
    "\n",
    "print(train_ds[0])\n",
    "print(train_ds[1])\n",
    "print(train_ds[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### SKEP模型加载\n",
    "\n",
    "目标级情感分析模型同样使用`SkepForSequenceClassification`模型，但目标级情感分析模型的输入不单单是一个句子，而是句对。一个句子描述“评价对象方面（aspect）”，另一个句子描述\"对该方面的评论\"。如下图所示。\n",
    "\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/1a4b76447dae404caa3bf123ea28e375179cb09a02de4bef8a2f172edc6e3c8f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 指定模型名称一键加载模型\n",
    "model = SkepForSequenceClassification.from_pretrained(\n",
    "    'skep_ernie_1.0_large_ch', num_classes=len(train_ds.label_list))\n",
    "# 指定模型名称一键加载tokenizer\n",
    "tokenizer = SkepTokenizer.from_pretrained('skep_ernie_1.0_large_ch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据处理\n",
    "\n",
    "同样地，我们需要将原始SE_ABSA16_PHNS数据处理成模型可以读入的数据格式。\n",
    "\n",
    "SKEP模型对中文文本处理按照字粒度进行处理，我们可以使用PaddleNLP内置的`SkepTokenizer`完成一键式处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "\n",
    "\n",
    "def convert_example(example,\n",
    "                    tokenizer,\n",
    "                    max_seq_length=512,\n",
    "                    is_test=False,\n",
    "                    dataset_name=\"chnsenticorp\"):\n",
    "    \"\"\"\n",
    "    Builds model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
    "    by concatenating and adding special tokens. And creates a mask from the two sequences passed \n",
    "    to be used in a sequence-pair classification task.\n",
    "        \n",
    "    A skep_ernie_1.0_large_ch/skep_ernie_2.0_large_en sequence has the following format:\n",
    "    ::\n",
    "        - single sequence: ``[CLS] X [SEP]``\n",
    "        - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n",
    "\n",
    "    A skep_ernie_1.0_large_ch/skep_ernie_2.0_large_en sequence pair mask has the following format:\n",
    "    ::\n",
    "\n",
    "        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
    "        | first sequence    | second sequence |\n",
    "\n",
    "    If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n",
    "    \n",
    "    note: There is no need token type ids for skep_roberta_large_ch model.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        example(obj:`list[str]`): List of input data, containing text and label if it have label.\n",
    "        tokenizer(obj:`PretrainedTokenizer`): This tokenizer inherits from :class:`~paddlenlp.transformers.PretrainedTokenizer` \n",
    "            which contains most of the methods. Users should refer to the superclass for more information regarding methods.\n",
    "        max_seq_len(obj:`int`): The maximum total input sequence length after tokenization. \n",
    "            Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "        is_test(obj:`False`, defaults to `False`): Whether the example contains label or not.\n",
    "        dataset_name((obj:`str`, defaults to \"chnsenticorp\"): The dataset name, \"chnsenticorp\" or \"sst-2\".\n",
    "\n",
    "    Returns:\n",
    "        input_ids(obj:`list[int]`): The list of token ids.\n",
    "        token_type_ids(obj: `list[int]`): List of sequence pair mask.\n",
    "        label(obj:`numpy.array`, data type of int64, optional): The input label if not is_test.\n",
    "    \"\"\"\n",
    "    encoded_inputs = tokenizer(\n",
    "        text=example[\"text\"],\n",
    "        text_pair=example[\"text_pair\"],\n",
    "        max_seq_len=max_seq_length)\n",
    "\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        return input_ids, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 处理的最大文本序列长度\n",
    "max_seq_length=256\n",
    "# 批量数据大小\n",
    "batch_size=16\n",
    "\n",
    "# 将数据处理成model可读入的数据格式\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length)\n",
    "# 将数据组成批量式数据，如\n",
    "# 将不同长度的文本序列padding到批量式数据中最大长度\n",
    "# 将每条数据label堆叠在一起\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack(dtype=\"int64\")  # labels\n",
    "): [data for data in fn(samples)]\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型训练\n",
    "\n",
    "定义损失函数、优化器以及评价指标后，即可开始训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 训练轮次\n",
    "epochs = 3\n",
    "# 总共需要训练的step数\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "# 优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=5e-5,\n",
    "    parameters=model.parameters())\n",
    "# 交叉熵损失\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "# Accuracy评价指标\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 开启训练\n",
    "ckpt_dir = \"skep_aspect\"\n",
    "global_step = 0\n",
    "tic_train = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        # 喂数据给model\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        # 计算损失函数值\n",
    "        loss = criterion(logits, labels)\n",
    "        # 预测分类概率\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        # 计算acc\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0:\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, acc,\n",
    "                    10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "        \n",
    "        # 反向梯度回传，更新参数\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        if global_step % 100 == 0:\n",
    "            \n",
    "            save_dir = os.path.join(ckpt_dir, \"model_%d\" % global_step)\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            # 保存模型参数\n",
    "            model.save_pretrained(save_dir)\n",
    "            # 保存tokenizer的词表等\n",
    "            tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 预测提交结果\n",
    "\n",
    "使用训练得到的模型还可以对评价对象进行情感预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@paddle.no_grad()\n",
    "def predict(model, data_loader, label_map):\n",
    "    \"\"\"\n",
    "    Given a prediction dataset, it gives the prediction results.\n",
    "\n",
    "    Args:\n",
    "        model(obj:`paddle.nn.Layer`): A model to classify texts.\n",
    "        data_loader(obj:`paddle.io.DataLoader`): The dataset loader which generates batches.\n",
    "        label_map(obj:`dict`): The label id (key) to label str (value) map.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\n",
    "        idx = idx.tolist()\n",
    "        labels = [label_map[i] for i in idx]\n",
    "        results.extend(labels)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 处理测试集数据\n",
    "label_map = {0: '0', 1: '1'}\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    is_test=True)\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "): [data for data in fn(samples)]\n",
    "test_data_loader = create_dataloader(\n",
    "    test_ds,\n",
    "    mode='test',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 根据实际运行情况，更换加载的参数路径\n",
    "params_path = 'skep_ckpt/model_900/model_state.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)\n",
    "\n",
    "results = predict(model, test_data_loader, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 写入预测结果\n",
    "with open(os.path.join(\"results\", \"SE-ABSA16_PHNS.tsv\"), 'w', encoding=\"utf8\") as f:\n",
    "    f.write(\"index\\tprediction\\n\")\n",
    "    for idx, label in enumerate(results):\n",
    "        f.write(str(idx)+\"\\t\"+label+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "将预测文件结果压缩至zip文件，提交[千言比赛网站](https://aistudio.baidu.com/aistudio/competition/detail/50/?isFromLUGE=TRUE)\n",
    "\n",
    "**NOTE:** results文件夹中NLPCC14-SC.tsv、SE-ABSA16_CAME.tsv、COTE_BD.tsv、COTE_MFW.tsv、COTE_DP.tsv等文件是为了顺利提交，补齐的文件。\n",
    "其结果还有待提高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#将预测文件结果压缩至zip文件，提交\n",
    "!zip -r results.zip results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "以上实现基于PaddleNLP，开源不易，希望大家多多支持~ \n",
    "\n",
    "**记得给[PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)点个小小的Star⭐，及时跟踪最新消息和功能哦**\n",
    "\n",
    "GitHub地址：[https://github.com/PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

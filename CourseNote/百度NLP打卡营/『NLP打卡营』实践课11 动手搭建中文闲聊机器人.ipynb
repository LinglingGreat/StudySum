{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 使用PaddleNLP生成式API搭建一个聊天机器人\n",
    "\n",
    "近年来，人机对话系统受到了学术界和产业界的广泛关注并取得了不错的发展。开放域对话系统旨在建立一个开放域的多轮对话系统，使得机器可以流畅自然地与人进行语言交互，既可以进行日常问候类的闲聊，又可以完成特定功能，以使得开放域对话系统具有实际应用价值。\n",
    "\n",
    "本实例将重点介绍PaddleNLP内置的**生成式API**的功能和用法，并使用PaddleNLP内置的**plato-mini模型**和配套的**生成式API**实现一个简单的闲聊机器人。\n",
    "\n",
    "## 环境要求\n",
    "\n",
    "* PaddlePaddle\n",
    "\n",
    "   本项目依赖于 PaddlePaddle 2.0 及以上版本，请参考 [安装指南](https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/install/pip/windows-pip.html) 进行安装\n",
    "\n",
    "* PaddleNLP \n",
    "\n",
    "   ```shell\n",
    "   pip install --upgrade paddlenlp -i https://pypi.org/simple\n",
    "   ```\n",
    "* sentencepiece \n",
    "\n",
    "   ```shell\n",
    "   pip install --upgrade sentencepiece -i https://pypi.org/simple\n",
    "   ```\n",
    "\n",
    "* Python\n",
    "\n",
    "    Python的版本要求 3.6+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "AI Studio平台后续会默认安装PaddleNLP，在此之前可使用如下命令安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade paddlenlp -i https://pypi.org/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/cd/82/04e9aaf603fdbaecb4323b9e723f13c92c245f6ab2902195c53987848c78/pip-21.1.2-py3-none-any.whl (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6MB 15.0MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 19.2.3\n",
      "    Uninstalling pip-19.2.3:\n",
      "      Successfully uninstalled pip-19.2.3\n",
      "Successfully installed pip-21.1.2\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (0.1.85)\n",
      "Collecting sentencepiece\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 15.4 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "  Attempting uninstall: sentencepiece\n",
      "    Found existing installation: sentencepiece 0.1.85\n",
      "    Uninstalling sentencepiece-0.1.85:\n",
      "      Successfully uninstalled sentencepiece-0.1.85\n",
      "Successfully installed sentencepiece-0.1.95\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\r\n",
    "!pip install --upgrade sentencepiece "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 生成式API\n",
    "\n",
    "**PaddleNLP针对生成式任务提供了`generate()`函数，内嵌于PaddleNLP所有的生成式模型。支持Greedy Search、Beam Search和Sampling解码策略，用户只需指定解码策略以及相应的参数即可完成预测解码，得到生成的sequence的token ids以及概率得分。**\n",
    "\n",
    "下面给大家展示一个GPT模型使用生成API的例子："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. 加载 `paddlenlp.transformers.GPTChineseTokenizer`用于数据处理\n",
    "\n",
    "文本数据在输入预训练模型之前，需要经过数据处理转化为Feature。这一过程通常包括分词，token to id，add special token等步骤。  \n",
    "\n",
    "**PaddleNLP对于各种预训练模型已经内置了相应的tokenizer**，指定想要使用的模型名字即可加载对应的tokenizer。\n",
    "\n",
    "调用`GPTChineseTokenizer`的`__call__`方法即可将我们说的话转为模型可接受的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Calling GPTChineseTokenizer.from_pretrained() with a model identifier or the path to a directory instead. The supported model identifiers are as follows: dict_keys(['gpt-cpm-large-cn'])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0d2f85b841a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 设置想要使用模型的名称\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gpt-cpm-small-cn-distill'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPTChineseTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlenlp/transformers/tokenizer_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *args, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m                     \u001b[0;34m\"path to a directory instead. The supported model \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m                     \"identifiers are as follows: {}\".format(\n\u001b[0;32m--> 338\u001b[0;31m                         cls.__name__, cls.pretrained_init_configuration.keys()))\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mdefault_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_HOME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Calling GPTChineseTokenizer.from_pretrained() with a model identifier or the path to a directory instead. The supported model identifiers are as follows: dict_keys(['gpt-cpm-large-cn'])"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import GPTChineseTokenizer\r\n",
    "\r\n",
    "# 设置想要使用模型的名称\r\n",
    "model_name = 'gpt-cpm-small-cn-distill'\r\n",
    "tokenizer = GPTChineseTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\r\n",
    "\r\n",
    "user_input = \"花间一壶酒，独酌无相亲。举杯邀明月，\"\r\n",
    "\r\n",
    "# 将文本转为ids\r\n",
    "input_ids = tokenizer(user_input)['input_ids']\r\n",
    "print(input_ids)\r\n",
    "\r\n",
    "# 将转换好的id转为tensor\r\n",
    "input_ids = paddle.to_tensor(input_ids, dtype='int64').unsqueeze(0)\r\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. 使用PaddleNLP一键加载预训练模型\n",
    "\n",
    "**PaddleNLP提供了GPT,UnifiedTransformer等中文预训练模型，可以通过预训练模型名称完成一键加载。**\n",
    "\n",
    "GPT以Transformer Decoder的编码器为网络基本组件，采用单向注意力机制，适用于长文本生成任务。\n",
    "\n",
    "PaddleNLP目前提供多种中英文GPT预训练模型，我们这次用的是一个小的中文GPT预训练模型。其他预训练模型请参考[模型列表](https://paddlenlp.readthedocs.io/zh/latest/model_zoo/transformers.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import GPTLMHeadModel\r\n",
    "\r\n",
    "# 一键加载中文GPT模型\r\n",
    "model = GPTLMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 调用生成API升成文本\r\n",
    "ids, scores = model.generate(\r\n",
    "                input_ids=input_ids,\r\n",
    "                max_length=16,\r\n",
    "                min_length=1,\r\n",
    "                decode_strategy='greedy_search')\r\n",
    "\r\n",
    "print(ids)\r\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generated_ids = ids[0].numpy().tolist()\r\n",
    "\r\n",
    "# 使用tokenizer将生成的id转为文本\r\n",
    "generated_text = tokenizer.convert_ids_to_string(generated_ids)\r\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可以看到生成的效果还不错，生成式API的用法也是非常的简便。\n",
    "\n",
    "下面我们来展示一下如何使用UnifiedTransformer模型和生成式API完成闲聊对话。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. 加载 `paddlenlp.transformers.UnifiedTransformerTokenizer`用于数据处理\n",
    "\n",
    "`UnifiedTransformerTokenizer`的调用方式与GPT相同，但数据处理的API略有不同。\n",
    "\n",
    "调用`UnifiedTransformerTokenizer`的`dialogue_encode`方法即可将我们说的话转为模型可接受的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import UnifiedTransformerTokenizer\r\n",
    "\r\n",
    "# 设置想要使用模型的名称\r\n",
    "model_name = 'plato-mini'\r\n",
    "tokenizer = UnifiedTransformerTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_input = ['你好啊，你今年多大了']\r\n",
    "\r\n",
    "# 调用dialogue_encode方法生成输入\r\n",
    "encoded_input = tokenizer.dialogue_encode(\r\n",
    "                    user_input,\r\n",
    "                    add_start_token_as_response=True,\r\n",
    "                    return_tensors=True,\r\n",
    "                    is_split_into_words=False)\r\n",
    "\r\n",
    "print(encoded_input.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "`dialogue_encode`的详细用法，请参考[dialogue_encode](https://paddlenlp.readthedocs.io/zh/latest/source/paddlenlp.transformers.unified_transformer.tokenizer.html#paddlenlp.transformers.unified_transformer.tokenizer.UnifiedTransformerTokenizer.dialogue_encode)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. 使用PaddleNLP一键加载预训练模型\n",
    "\n",
    "与GPT相同，我们可以一键调用UnifiedTransformer预训练模型。\n",
    "\n",
    "[UnifiedTransformer](https://github.com/PaddlePaddle/Knover/tree/luge-dialogue/luge-dialogue)以Transformer的编码器为网络基本组件，采用灵活的注意力机制，十分适合文本生成任务，并在模型输入中加入了标识不同对话技能的special token，使得模型能同时支持闲聊对话、推荐对话和知识对话。\n",
    "\n",
    "PaddleNLP目前为UnifiedTransformer提供了三个中文预训练模型：\n",
    "- `unified_transformer-12L-cn` 该预训练模型是在大规模中文会话数据集上训练得到的\n",
    "- `unified_transformer-12L-cn-luge` 该预训练模型是`unified_transformer-12L-cn`在千言对话数据集上进行微调得到的。\n",
    "- `plato-mini` 该模型使用了十亿级别的中文闲聊对话数据进行预训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import UnifiedTransformerLMHeadModel\n",
    "\n",
    "model = UnifiedTransformerLMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "下一步我们将处理好的输入传入generate函数，并配置解码策略。\n",
    "\n",
    "这里我们使用的是TopK加sampling的解码策略。即从概率最大的k个结果中按概率进行采样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids, scores = model.generate(\n",
    "                input_ids=encoded_input['input_ids'],\n",
    "                token_type_ids=encoded_input['token_type_ids'],\n",
    "                position_ids=encoded_input['position_ids'],\n",
    "                attention_mask=encoded_input['attention_mask'],\n",
    "                max_length=64,\n",
    "                min_length=1,\n",
    "                decode_strategy='sampling',\n",
    "                top_k=5,\n",
    "                num_return_sequences=20)\n",
    "\n",
    "print(ids)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import select_response\r\n",
    "\r\n",
    "# 简单根据概率选取最佳回复\r\n",
    "result = select_response(ids, scores, tokenizer, keep_space=False, num_return_sequences=20)\r\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "关于生成式API更详细的用法，请参考[generate](https://paddlenlp.readthedocs.io/zh/latest/source/paddlenlp.transformers.generation_utils.html#paddlenlp.transformers.generation_utils.GenerationMixin.generate)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 下面我们去终端里试试多轮对话吧！\n",
    "\n",
    "PaddleNLP的example中提供了搭建完整对话系统的代码（[人机交互](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/dialogue/unified_transformer#%E4%BA%BA%E6%9C%BA%E4%BA%A4%E4%BA%92)）。我们可以去终端里尝试一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Paddle+Wechaty\n",
    "\n",
    "开头我们展示的聊天机器人就是使用PaddleHub和Wechaty搭建的。大家可以参照这个教程自己尝试[paddlehub-wechaty-demo](https://github.com/KPatr1ck/paddlehub-wechaty-demo)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "以上内容实现基于PaddleNLP，开源不易，希望大家多多支持~ \n",
    "**记得给[PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)点个小小的Star⭐**\n",
    "\n",
    "GitHub地址：[https://github.com/PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/a0e8ca7743ea4fe9aa741682a63e767f8c48dc55981f4e44a40e0e00d3ab369e)\n",
    "\n",
    "**更多使用方法可参考PaddleNLP教程**\n",
    "\n",
    "- [使用seq2vec模块进行句子情感分类](https://aistudio.baidu.com/aistudio/projectdetail/1283423)\n",
    "- [使用预训练模型ERNIE优化情感分析](https://aistudio.baidu.com/aistudio/projectdetail/1294333)\n",
    "- [使用BiGRU-CRF模型完成快递单信息抽取](https://aistudio.baidu.com/aistudio/projectdetail/1317771)\n",
    "- [使用预训练模型ERNIE优化快递单信息抽取](https://aistudio.baidu.com/aistudio/projectdetail/1329361)\n",
    "- [使用Seq2Seq模型完成自动对联](https://aistudio.baidu.com/aistudio/projectdetail/1321118)\n",
    "- [使用预训练模型ERNIE-GEN自动写诗](https://aistudio.baidu.com/aistudio/projectdetail/1339888)\n",
    "- [使用TCN网络完成新冠疫情病例数预测](https://aistudio.baidu.com/aistudio/projectdetail/1290873)\n",
    "- [自定义数据集实现文本多分类任务](https://aistudio.baidu.com/aistudio/projectdetail/1468469)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 加入交流群，一起学习吧\n",
    "\n",
    "现在就加入PaddleNLP的QQ技术交流群，一起交流NLP技术吧！\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/d953727af0c24a7c806ab529495f0904f22f809961be420b8c88cdf59b837394\" width=\"200\" height=\"250\" >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

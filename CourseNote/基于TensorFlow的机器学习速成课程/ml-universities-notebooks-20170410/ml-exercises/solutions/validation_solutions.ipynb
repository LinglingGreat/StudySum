{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation - Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from datalab import bigquery as bq\n",
    "from IPython import display\n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn import learn_io\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "\n",
    "california_housing_dataframe = pd.read_csv(\"https://storage.googleapis.com/ml_universities/california_housing_train.csv\", sep=\",\")\n",
    "\n",
    "california_housing_dataframe = california_housing_dataframe.reindex(\n",
    "    np.random.permutation(california_housing_dataframe.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_features(california_housing_dataframe):\n",
    "  \"\"\"Prepares input features from California housing data set.\n",
    "\n",
    "  Args:\n",
    "    california_housing_dataframe: A Pandas DataFrame expected to contain data\n",
    "      from the California housing data set.\n",
    "  Returns:\n",
    "    A DataFrame that contains the features to be used for the model, including\n",
    "    synthetic features.\n",
    "  \"\"\"\n",
    "  selected_features = california_housing_dataframe[\n",
    "    [\"latitude\",\n",
    "     \"longitude\",\n",
    "     \"housing_median_age\",\n",
    "     \"total_rooms\",\n",
    "     \"total_bedrooms\",\n",
    "     \"population\",\n",
    "     \"households\",\n",
    "     \"median_income\"]]\n",
    "  processed_features = selected_features.copy()\n",
    "  # Create a synthetic feature.\n",
    "  processed_features[\"rooms_per_person\"] = (\n",
    "    california_housing_dataframe[\"total_rooms\"] /\n",
    "    california_housing_dataframe[\"population\"])\n",
    "  return processed_features\n",
    "\n",
    "def preprocess_targets(california_housing_dataframe):\n",
    "  \"\"\"Prepares target features (i.e., labels) from California housing data set.\n",
    "\n",
    "  Args:\n",
    "    california_housing_dataframe: A Pandas DataFrame expected to contain data\n",
    "      from the California housing data set.\n",
    "  Returns:\n",
    "    A DataFrame that contains the target feature.\n",
    "  \"\"\"\n",
    "  output_targets = pd.DataFrame()\n",
    "  # Scale the target to be in units of thousands of dollars.\n",
    "  output_targets[\"median_house_value\"] = (\n",
    "    california_housing_dataframe[\"median_house_value\"] / 1000.0)\n",
    "  return output_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_examples = preprocess_features(california_housing_dataframe.head(12000))\n",
    "training_targets = preprocess_targets(california_housing_dataframe.head(12000))\n",
    "validation_examples = preprocess_features(california_housing_dataframe.tail(5000))\n",
    "validation_targets = preprocess_targets(california_housing_dataframe.tail(5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to Examine the data - sanity check\n",
    "Sanity checking data means checking it against some previous level of expectations.\n",
    "\n",
    "For some values, like `median_house_value`, we can check to see if these values fall within reasonable ranges (keeping in mind this was 1990 data — not today!).\n",
    "\n",
    "For other values, like `latitude` and `longitude`, we can do a quick check to see if these line up with expected values from a quick Google search.\n",
    "\n",
    "But if you look closely, you might see some oddities. For example, `median_income` is on a scale from about 3 to 15. Not at all clear what this scale refers to — looks like maybe some log scale? It's not documented anywhere; all we can assume is that higher is more.\n",
    "\n",
    "Also, the maximum median_house_value is capped at 500,001. This looks like an artificial cap of some kind.\n",
    "\n",
    "Finally, our `rooms_per_person` feature is generally on a sane scale, with a 75<sup>th</sup> percentile value of about 2. But there are some very large values, like 18 or 55, which may show some amount of corruption in the data.\n",
    "\n",
    "We'll use these features as given for now. But hopefully these kinds of examples can help to build a little intuition about how to sanity check data that comes to you from an unknown source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to Examine the data - latitude and longitude\n",
    "Looking at the tables of summary stats above, it's easy to wonder how anyone would do a useful sanity check. What's the right 75<sup>th</sup> percentile value for total_rooms per city block?\n",
    "\n",
    "The key thing to notice is that for any given feature or column, the distribution of values between the train and validation splits should be roughly equal.\n",
    "\n",
    "The fact that this is not the case is a real worry, and shows that we likely have a fault in the way that our train and validation split was created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to Task 1\n",
    "Take a look at how the data is randomized when it's read in.\n",
    "\n",
    "If we didn't randomize the data properly before creating training and validation splits, then we'd be in trouble if the data was given to us in some sorted order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to Task 2\n",
    "This is just one possible setting; there may be other combinations of parameter settings that also give good results. Note that in general, this exercise isn't about finding the One Best setting, but rather about getting the experience of exploring parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    learning_rate,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets):\n",
    "  \"\"\"Trains a linear regression model of one feature.\n",
    "  \n",
    "  In addition to training, this function also prints training progress information,\n",
    "  as well as a plot of the training and validation loss over time.\n",
    "  \n",
    "  Args:\n",
    "    learning_rate: A `float`, the learning rate.\n",
    "    steps: A non-zero `int`, the total number of training steps. A training step\n",
    "      consists of a forward and backward pass using a single batch.\n",
    "    batch_size: A non-zero `int`, the batch size.\n",
    "    training_examples: A `DataFrame` containing one or more columns from\n",
    "      `california_housing_dataframe` to use as input features for training.\n",
    "    training_targets: A `DataFrame` containing exactly one column from\n",
    "      `california_housing_dataframe` to use as target for training.\n",
    "    validation_examples: A `DataFrame` containing one or more columns from\n",
    "      `california_housing_dataframe` to use as input features for validation.\n",
    "    validation_targets: A `DataFrame` containing exactly one column from\n",
    "      `california_housing_dataframe` to use as target for validation.\n",
    "      \n",
    "  Returns:\n",
    "    A `LinearRegressor` object trained on the training data.\n",
    "  \"\"\"\n",
    "\n",
    "  periods = 10\n",
    "  steps_per_period = steps / periods\n",
    "\n",
    "  # Create input functions\n",
    "  training_input_fn = learn_io.pandas_input_fn(\n",
    "     x=training_examples, y=training_targets[\"median_house_value\"],\n",
    "     num_epochs=None, batch_size=batch_size)\n",
    "  predict_training_input_fn = learn_io.pandas_input_fn(\n",
    "     x=training_examples, y=training_targets[\"median_house_value\"],\n",
    "     num_epochs=1, shuffle=False)\n",
    "  predict_validation_input_fn = learn_io.pandas_input_fn(\n",
    "      x=validation_examples, y=validation_targets[\"median_house_value\"],\n",
    "      num_epochs=1, shuffle=False)\n",
    "\n",
    "  # Create a linear regressor object.\n",
    "  feature_columns = set([tf.contrib.layers.real_valued_column(my_feature) for my_feature in training_examples])\n",
    "  linear_regressor = tf.contrib.learn.LinearRegressor(\n",
    "      feature_columns=feature_columns,\n",
    "      optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate),\n",
    "      gradient_clip_norm=5.0\n",
    "  )\n",
    "\n",
    "  # Train the model, but do so inside a loop so that we can periodically assess\n",
    "  # loss metrics.\n",
    "  print \"Training model...\"\n",
    "  print \"RMSE (on training data):\"\n",
    "  training_rmse = []\n",
    "  validation_rmse = []\n",
    "  for period in range (0, periods):\n",
    "    # Train the model, starting from the prior state.\n",
    "    linear_regressor.fit(\n",
    "        input_fn=training_input_fn,\n",
    "        steps=steps_per_period,\n",
    "    )\n",
    "    # Take a break and compute predictions.\n",
    "    training_predictions = list(linear_regressor.predict(input_fn=predict_training_input_fn))\n",
    "    validation_predictions = list(linear_regressor.predict(input_fn=predict_validation_input_fn))\n",
    "    # Compute training and validation loss.\n",
    "    training_root_mean_squared_error = math.sqrt(\n",
    "        metrics.mean_squared_error(training_predictions, training_targets))\n",
    "    validation_root_mean_squared_error = math.sqrt(\n",
    "        metrics.mean_squared_error(validation_predictions, validation_targets))\n",
    "    # Occasionally print the current loss.\n",
    "    print \"  period %02d : %0.2f\" % (period, training_root_mean_squared_error)\n",
    "    # Add the loss metrics from this period to our list.\n",
    "    training_rmse.append(training_root_mean_squared_error)\n",
    "    validation_rmse.append(validation_root_mean_squared_error)\n",
    "  print \"Model training finished.\"\n",
    "\n",
    "  # Output a graph of loss metrics over periods.\n",
    "  plt.ylabel(\"RMSE\")\n",
    "  plt.xlabel(\"Periods\")\n",
    "  plt.title(\"Root Mean Squared Error vs. Periods\")\n",
    "  plt.tight_layout()\n",
    "  plt.plot(training_rmse, label=\"training\")\n",
    "  plt.plot(validation_rmse, label=\"validation\")\n",
    "  plt.legend()\n",
    "\n",
    "  return linear_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linear_regressor = train_model(\n",
    "    learning_rate=0.00003,\n",
    "    steps=500,\n",
    "    batch_size=5,\n",
    "    training_examples=training_examples,\n",
    "    training_targets=training_targets,\n",
    "    validation_examples=validation_examples,\n",
    "    validation_targets=validation_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to Task 3\n",
    "Similarly to what the loading code does, we just need to load the appropriate data file, preprocess it and use `predict` and `mean_squared_error`.\n",
    "\n",
    "Note that we don't have to randomize the test data, since we will use all records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "california_housing_test_data = pd.read_csv(\"https://storage.googleapis.com/ml_universities/california_housing_test.csv\", sep=\",\")\n",
    "\n",
    "test_examples = preprocess_features(california_housing_test_data)\n",
    "test_targets = preprocess_targets(california_housing_test_data)\n",
    "\n",
    "predict_test_input_fn = learn_io.pandas_input_fn(\n",
    "      x=test_examples, y=test_targets[\"median_house_value\"],\n",
    "      num_epochs=1, shuffle=False)\n",
    "\n",
    "test_predictions = list(linear_regressor.predict(input_fn=predict_test_input_fn))\n",
    "root_mean_squared_error = math.sqrt(\n",
    "    metrics.mean_squared_error(test_predictions, test_targets))\n",
    "\n",
    "print \"Final RMSE (on test data): %0.2f\" % root_mean_squared_error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mPa95uXvcpcn"
   },
   "source": [
    "# Classifying Handwritten Digits with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://www.tensorflow.org/versions/r0.11/images/MNIST.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c7HLCm66Cs2p"
   },
   "source": [
    "In this exercise, we will be classifying hand-written digits using the classic MNIST data set. Our goal is to map each input image to the correct numeric digit.  We will create a NN with a few hidden layers and a Softmax layer at the top to select the winning class.\n",
    "\n",
    "First, let's import TensorFlow and other utilities, and load in the data set. Note that this data is a sample of the original MNIST training data; we've taken 20000 rows at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/ml_universities/mnist_train_small.csv -O /tmp/mnist_train_small.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "test": {
      "output": "ignore",
      "timeout": 600
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "4LJ4SD8BWHeh"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import math\n",
    "\n",
    "from IPython import display\n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "\n",
    "mnist_dataframe = pd.read_csv(\n",
    "  io.open(\"/tmp/mnist_train_small.csv\", \"r\"),\n",
    "  sep=\",\",\n",
    "  header=None)\n",
    "\n",
    "mnist_dataframe = mnist_dataframe.reindex(np.random.permutation(mnist_dataframe.index))\n",
    "mnist_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kg0-25p2mOi0"
   },
   "source": [
    "The first column contains the class label. The remaining columns contain the feature values, one per pixel for the `28Ã—28=784` pixel values.  Most of these 784 pixel values are zero; you may want to take a minute to confirm that they're not *all* zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PQ7vuOwRCsZ1"
   },
   "source": [
    "![img](https://www.tensorflow.org/versions/r0.11/images/MNIST-Matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These examples are relatively low-resolution, high contrast images of handwritten numbers.  The ten digits `0-9` are each represented, with a unique class label for each possible digit.  Thus, this is a multi-class classification problem with 10 classes.\n",
    "\n",
    "Now, let's parse out the labels and features and look at a few examples as a sanity check. Note the use of `loc` which allows us to pull out columns based on original location, since we don't have a header row in this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "test": {
      "output": "ignore",
      "timeout": 600
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "JfFWWvMWDFrR"
   },
   "outputs": [],
   "source": [
    "def parse_labels_and_features(dataset):\n",
    "  \"\"\"Extracts labels and features.\n",
    "  \n",
    "  This is a good place to scale or transform the features if needed.\n",
    "  \n",
    "  Args:\n",
    "    dataset: A Pandas `Dataframe`, containing the label on the first column and\n",
    "      monochrome pixel values on the remaining columns, in row major order.\n",
    "  Returns:\n",
    "    A `tuple` `(labels, features)`:\n",
    "      labels: A Pandas `Series`.\n",
    "      features: A Pandas `DataFrame`.\n",
    "  \"\"\"\n",
    "  labels = dataset[0]\n",
    "\n",
    "  # DataFrame.loc index ranges are inclusive at both ends.\n",
    "  features = dataset.loc[:,1:784]\n",
    "  # Scale the data to [0, 1] by dividing out the max value, 255.\n",
    "  features = features / 255\n",
    "\n",
    "  return labels, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_targets, training_examples = parse_labels_and_features(mnist_dataframe.head(15000))\n",
    "training_examples.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_targets, validation_examples = parse_labels_and_features(mnist_dataframe.tail(5000))\n",
    "validation_examples.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a random example and its corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rand_example = np.random.choice(training_examples.index)\n",
    "_, ax = plt.subplots()\n",
    "ax.matshow(training_examples.ix[rand_example].values.reshape(28, 28))\n",
    "ax.set_title(\"Label: %i\" % training_targets.ix[rand_example])\n",
    "ax.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ScmYX7xdZMXE"
   },
   "source": [
    "### Task 1: Build a linear model for MNIST.\n",
    "\n",
    "First, let's create a baseline model to compare against. The `LinearClassifier` provides a set of *k* one-vs-all classifiers, one for each of the *k* classes.\n",
    "\n",
    "You'll notice that in addition to reporting accuracy, and plotting log-loss over time, we also show a [**confusion matrix**](https://en.wikipedia.org/wiki/Confusion_matrix).  The confusion matrix shows which classes were mis-classified as other classes.  Which digits get confused for each other?\n",
    "\n",
    "Also note that we track the model's error using the `log_loss` function. This is not to be confused with the loss function internal to `LinearClassifier` that is used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_training_input_fn(features, labels, batch_size):\n",
    "  \"\"\"A custom input_fn for sending mnist data to the estimator for training.\n",
    "\n",
    "  Args:\n",
    "    features: The training features.\n",
    "    labels: The training labels.\n",
    "    batch_size: Batch size to use during training.\n",
    "\n",
    "  Returns:\n",
    "    A function that returns batches of training features and labels during\n",
    "    training.\n",
    "  \"\"\"\n",
    "  def _input_fn():\n",
    "    raw_features = tf.constant(features.values)\n",
    "    raw_targets = tf.constant(labels.values)\n",
    "    dataset_size = len(features)\n",
    "\n",
    "    return tf.train.shuffle_batch(\n",
    "        [raw_features, raw_targets],\n",
    "        batch_size=batch_size,\n",
    "        enqueue_many=True,\n",
    "        capacity=2 * dataset_size,  # Must be greater than min_after_dequeue.\n",
    "        min_after_dequeue=dataset_size)  # Important to ensure uniform randomness.\n",
    "\n",
    "  return _input_fn\n",
    "\n",
    "def create_predict_input_fn(features, labels):\n",
    "  \"\"\"A custom input_fn for sending mnist data to the estimator for predictions.\n",
    "\n",
    "  Args:\n",
    "    features: The features to base predictions on.\n",
    "    labels: The labels of the prediction examples.\n",
    "\n",
    "  Returns:\n",
    "    A function that returns features and labels for predictions.\n",
    "  \"\"\"\n",
    "  def _input_fn():\n",
    "    raw_features = tf.constant(features.values)\n",
    "    raw_targets = tf.constant(labels.values)\n",
    "    return tf.train.limit_epochs(raw_features, 1), raw_targets\n",
    "\n",
    "  return _input_fn\n",
    "\n",
    "def train_linear_classification_model(\n",
    "    learning_rate,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets):\n",
    "  \"\"\"Trains a linear classification model for the MNIST digits dataset.\n",
    "  \n",
    "  In addition to training, this function also prints training progress information,\n",
    "  a plot of the training and validation loss over time, and a confusion\n",
    "  matrix.\n",
    "  \n",
    "  Args:\n",
    "    learning_rate: An `int`, the learning rate to use.\n",
    "    steps: A non-zero `int`, the total number of training steps. A training step\n",
    "      consists of a forward and backward pass using a single batch.\n",
    "    batch_size: A non-zero `int`, the batch size.\n",
    "    training_examples: A `DataFrame` containing the training features.\n",
    "    training_targets: A `DataFrame` containing the training labels.\n",
    "    validation_examples: A `DataFrame` containing the validation features.\n",
    "    validation_targets: A `DataFrame` containing the validation labels.\n",
    "      \n",
    "  Returns:\n",
    "    The trained `LinearClassifier` object.\n",
    "  \"\"\"\n",
    "\n",
    "  periods = 10\n",
    "  steps_per_period = steps / periods\n",
    "  \n",
    "  # Create the input functions.\n",
    "  predict_training_input_fn = create_predict_input_fn(\n",
    "    training_examples, training_targets)\n",
    "  predict_validation_input_fn = create_predict_input_fn(\n",
    "    validation_examples, validation_targets)\n",
    "  training_input_fn = create_training_input_fn(\n",
    "    training_examples, training_targets, batch_size)\n",
    "\n",
    "  # Create a linear classifier object.\n",
    "  feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(\n",
    "      training_examples)\n",
    "  classifier = tf.contrib.learn.LinearClassifier(\n",
    "      feature_columns=feature_columns,\n",
    "      n_classes=10,\n",
    "      optimizer=tf.train.AdagradOptimizer(learning_rate=learning_rate),\n",
    "      gradient_clip_norm=5.0\n",
    "  )\n",
    "\n",
    "  # Train the model, but do so inside a loop so that we can periodically assess\n",
    "  # loss metrics.\n",
    "  print \"Training model...\"\n",
    "  print \"LogLoss error (on validation data):\"\n",
    "  training_errors = []\n",
    "  validation_errors = []\n",
    "  for period in range (0, periods):\n",
    "    # Train the model, starting from the prior state.\n",
    "    classifier.fit(\n",
    "        input_fn=training_input_fn,\n",
    "        steps=steps_per_period\n",
    "    )\n",
    "    # Take a break and compute predictions.\n",
    "    training_predictions = list(classifier.predict_proba(input_fn=predict_training_input_fn))\n",
    "    validation_predictions = list(classifier.predict_proba(input_fn=predict_validation_input_fn))\n",
    "    # Compute training and validation errors.\n",
    "    training_log_loss = metrics.log_loss(training_targets, training_predictions)\n",
    "    validation_log_loss = metrics.log_loss(validation_targets, validation_predictions)\n",
    "    # Occasionally print the current loss.\n",
    "    print \"  period %02d : %0.2f\" % (period, validation_log_loss)\n",
    "    # Add the loss metrics from this period to our list.\n",
    "    training_errors.append(training_log_loss)\n",
    "    validation_errors.append(validation_log_loss)\n",
    "  print \"Model training finished.\"\n",
    "\n",
    "  # Calculate final predictions (not probabilities, as above).\n",
    "  final_predictions = list(classifier.predict(input_fn=predict_validation_input_fn))\n",
    "  accuracy = metrics.accuracy_score(validation_targets, final_predictions)\n",
    "  print \"Final accuracy (on validation data): %0.2f\" % accuracy  \n",
    "\n",
    "  # Output a graph of loss metrics over periods.\n",
    "  plt.ylabel(\"LogLoss\")\n",
    "  plt.xlabel(\"Periods\")\n",
    "  plt.title(\"LogLoss vs. Periods\")\n",
    "  plt.plot(training_errors, label=\"training\")\n",
    "  plt.plot(validation_errors, label=\"validation\")\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "  \n",
    "  # Output a plot of the confusion matrix.\n",
    "  cm = metrics.confusion_matrix(validation_targets, final_predictions)\n",
    "  # Normalize the confusion matrix by row (i.e by the number of samples\n",
    "  # in each class)\n",
    "  cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "  ax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\n",
    "  ax.set_aspect(1)\n",
    "  plt.title(\"Confusion matrix\")\n",
    "  plt.ylabel(\"True label\")\n",
    "  plt.xlabel(\"Predicted label\")\n",
    "  plt.show()\n",
    "\n",
    "  return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spend 5 minutes seeing how well you can do on accuracy with a linear model of this form. For this exercise, limit yourself to experimenting with the hyperparameters for batch size, learning rate and steps.**\n",
    "\n",
    "Stop if you get anything above about 0.9 accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = train_linear_classification_model(\n",
    "    learning_rate=0.02,\n",
    "    steps=100,\n",
    "    batch_size=10,\n",
    "    training_examples=training_examples,\n",
    "    training_targets=training_targets,\n",
    "    validation_examples=validation_examples,\n",
    "    validation_targets=validation_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mk095OfpPdOx"
   },
   "source": [
    "### Task 2: Replace the Linear Classifier with a Neural Network.\n",
    "\n",
    "**Replace the LinearClassifier above with a [DNNClassifier](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/dnn.py#L298) and find a parameter combination that gives 0.95 or better accuracy.**\n",
    "\n",
    "You may wish to experiment with additional regularization methods, such as dropout. These additional regularization methods are documented in the comments for the `DNNClassifier` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Your code here: Replace the linear classifier with a neural network.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a good model, double check that you didn't over-fit the validation set by evaluating on the test data that we'll load below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/ml_universities/mnist_test.csv -O /tmp/mnist_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist_test_dataframe = pd.read_csv(\n",
    "  io.open(\"/tmp/mnist_test.csv\", \"r\"),\n",
    "  sep=\",\",\n",
    "  header=None)\n",
    "\n",
    "test_targets, test_examples = parse_labels_and_features(mnist_test_dataframe)\n",
    "test_examples.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Your code here: Calculate accuracy on the test set.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WX2mQBAEcisO"
   },
   "source": [
    "### Task 3: Visualize the weights of the first hidden layer.\n",
    "\n",
    "Let's take a few minutes to dig into our neural network and see what it has learned by accessing the `weights_` attribute of our model.\n",
    "\n",
    "The input layer of our model has `784` weights corresponding to the `28Ã—28` pixel input images. The first hidden layer will have `784Ã—N` weights where `N` is the number of nodes in that layer. We can turn those weights back into `28Ã—28` images by *reshaping* each of the `N` `1Ã—784` arrays of weights into `N` arrays of size `28Ã—28`.\n",
    "\n",
    "Run the following cell to plot the weights. Note that this cell requires that a `DNNClassifier` called \"classifier\" has already been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "test": {
      "output": "ignore",
      "timeout": 600
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "eUC0Z8nbafgG"
   },
   "outputs": [],
   "source": [
    "weights0 = classifier.weights_[0]\n",
    "\n",
    "print \"weights0 shape:\", weights0.shape\n",
    "\n",
    "num_nodes = weights0.shape[1]\n",
    "num_rows = int(math.ceil(num_nodes / 10.0))\n",
    "fig, axes = plt.subplots(num_rows, 10, figsize=(20, 2 * num_rows))\n",
    "for coef, ax in zip(weights0.T, axes.ravel()):\n",
    "    # Weights in coef is reshaped from 1x784 to 28x28.\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.pink)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kL8MEhNgrx9N"
   },
   "source": [
    "The first hidden layer of the neural network should be modeling some pretty low level features, so visualizing the weights will probably just show some fuzzy blobs or possibly a few parts of digits.  You may also see some neurons that are essentially noise -- these are either unconverged or they are being ignored by higher layers.\n",
    "\n",
    "It can be interesting to stop training at different numbers of iterations and see the effect.\n",
    "\n",
    "**Train the classifier for 10, 100 and respectively 1000 steps. Then run this visualization again.**\n",
    "\n",
    "What differences do you see visually for the different levels of convergence?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "multi-class_classification_of_handwritten_digits.ipynb",
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

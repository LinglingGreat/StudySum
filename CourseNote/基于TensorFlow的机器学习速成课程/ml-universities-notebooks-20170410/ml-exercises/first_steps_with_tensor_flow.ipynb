{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4f3CKqFUqL2-",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# First Steps with Tensorflow\n",
    "This is an introductory notebook, intended to illustrate some fundamental ML concepts using TensorFlow.\n",
    "\n",
    "In this notebook, we'll be creating a linear regression model to predict median housing price, at the granularity of city blocks, based on one input feature. The data is based on 1990 census data from California."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6TjLjL9IU80G"
   },
   "source": [
    "## Set Up\n",
    "In this first cell, we'll load the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from IPython import display\n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn import learn_io, estimator\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ipRyUHjhU80Q"
   },
   "source": [
    "Next, we'll load our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "california_housing_dataframe = pd.read_csv(\"https://storage.googleapis.com/ml_universities/california_housing_train.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vVk_qlG6U80j"
   },
   "source": [
    "We'll randomize the data, just to be sure not to get any pathological ordering effects that might harm the performance of Stochastic Gradient Descent. Additionally, we scale `median_house_value` to be in units of thousands, so it can be learned a little more easily with learning rates in a range that we usually use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "r0eVyguIU80m"
   },
   "outputs": [],
   "source": [
    "california_housing_dataframe = california_housing_dataframe.reindex(\n",
    "    np.random.permutation(california_housing_dataframe.index))\n",
    "california_housing_dataframe[\"median_house_value\"] /= 1000.0\n",
    "california_housing_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HzzlSs3PtTmt",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Examine the data\n",
    "\n",
    "It's a good idea to get to know your data a little bit before you work with it.\n",
    "\n",
    "We'll print out a quick summary of a few useful statistics on each column.\n",
    "\n",
    "This will include things like mean, standard deviation, max, min, and various quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "test": {
      "output": "ignore",
      "timeout": 600
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "gzb10yoVrydW",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "california_housing_dataframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lr6wYl2bt2Ep",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Build the first model\n",
    "\n",
    "In this exercise, we'll be trying to predict `median_house_value`. It will be our label (sometimes also called a target). We'll use `total_rooms` as our input feature.\n",
    "\n",
    "Recall that this data is at the city block level, so these features reflect the total number of rooms in that block, or the total number of people who live on that block, respectively.\n",
    "\n",
    "To train our model, we'll use the [LinearRegressor](https://www.tensorflow.org/versions/master/api_docs/python/contrib.learn.html#LinearRegressor) interface provided by the TensorFlow [contrib.learn](https://www.tensorflow.org/versions/master/tutorials/tflearn/index.html) library. This library takes care of a lot of the plumbing, and exposes a convenient way to interact with data, training, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dx3wFELWU801"
   },
   "source": [
    "First, we define the input feature, the target, and create the `LinearRegressor` object.\n",
    "\n",
    "The GradientDescentOptimizer implements Mini-Batch Stochastic Gradient Descent (SGD), where the size of the mini-batch is given by the `batch_size` parameter. Note the `learning_rate` parameter to the optimizer: it controls the size of the gradient step. We also include a value for `gradient_clip_norm` for safety. This makes sure that gradients are never too huge, which helps avoid pathological cases in gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "ubhtW-NGU802"
   },
   "outputs": [],
   "source": [
    "my_feature = california_housing_dataframe[[\"total_rooms\"]]\n",
    "targets = california_housing_dataframe[\"median_house_value\"]\n",
    "\n",
    "training_input_fn = learn_io.pandas_input_fn(\n",
    "    x=my_feature, y=targets, num_epochs=None, batch_size=1)\n",
    "\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"total_rooms\", dimension=1)]\n",
    "\n",
    "linear_regressor = tf.contrib.learn.LinearRegressor(\n",
    "    feature_columns=feature_columns,\n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.00001),\n",
    "    gradient_clip_norm=5.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yP92XkzhU803"
   },
   "source": [
    "Calling `fit()` on the feature column and targets will train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "5M-Kt6w8U803"
   },
   "outputs": [],
   "source": [
    "_ = linear_regressor.fit(\n",
    "    input_fn=training_input_fn,\n",
    "    steps=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HFJzk7FTU807"
   },
   "source": [
    "Let's make predictions on that training data, to see how well we fit the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "pDIxp6vcU809"
   },
   "outputs": [],
   "source": [
    "prediction_input_fn = learn_io.pandas_input_fn(\n",
    "    x=my_feature, y=targets, num_epochs=1, shuffle=False)\n",
    "\n",
    "predictions = list(linear_regressor.predict(input_fn=prediction_input_fn))\n",
    "mean_squared_error = metrics.mean_squared_error(predictions, targets)\n",
    "print \"Mean Squared Error (on training data): %0.3f\" % mean_squared_error\n",
    "print \"Root Mean Squared Error (on training data): %0.3f\" % math.sqrt(mean_squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AKWstXXPzOVz",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluate the model\n",
    "\n",
    "Okay, training the model was easy!  But is this a good model?  How would you judge how large this error is?\n",
    "\n",
    "The mean squared error can be hard to interpret, so we often look at root mean squared error (RMSE)\n",
    "instead.  RMSE has the nice property that it can be interpreted on the same scale as the original targets.\n",
    "\n",
    "Compare the RMSE with the range between min and max of our targets.  How does the RMSE compare to this range?\n",
    "\n",
    "Can we do better?\n",
    "\n",
    "This is the question that nags every model developer. Let's develop some basic strategies to help give some guidance.\n",
    "\n",
    "The first thing we can do is take a look at how well our predictions match our targets, in terms of overall summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "test": {
      "output": "ignore",
      "timeout": 600
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "941nclxbzqGH",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "calibration_data = pd.DataFrame()\n",
    "calibration_data[\"predictions\"] = pd.Series(predictions)\n",
    "calibration_data[\"targets\"] = pd.Series(targets)\n",
    "calibration_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E2-bf8Hq36y8",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Okay, maybe this information is helpful. How does the mean value compare to the model's RMSE? How about the various quantiles?\n",
    "\n",
    "We can also visualize the data and the line we've learned.  Recall that linear regression on a single feature can be drawn as a line mapping input *x* to output *y*.\n",
    "\n",
    "First, we'll get a uniform random sample of the data.  This is helpful to make the scatter plot readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "SGRIi3mAU81H"
   },
   "outputs": [],
   "source": [
    "sample = california_housing_dataframe.sample(n=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-JwuJBKU81J"
   },
   "source": [
    "Then, plot the line we've learned, drawing from the model's bias term and feature weight, together with the scatter plot. The line will show up red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "test": {
      "output": "ignore",
      "timeout": 600
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "7G12E76-339G",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "x_0 = sample[\"total_rooms\"].min()\n",
    "x_1 = sample[\"total_rooms\"].max()\n",
    "y_0 = linear_regressor.weights_[0] * x_0 + linear_regressor.bias_\n",
    "y_1 = linear_regressor.weights_[0] * x_1 + linear_regressor.bias_\n",
    "plt.plot([x_0, x_1], [y_0, y_1], c='r')\n",
    "plt.ylabel(\"median_house_value\")\n",
    "plt.xlabel(\"total_rooms\")\n",
    "plt.scatter(sample[\"total_rooms\"], sample[\"median_house_value\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t0lRt4USU81L"
   },
   "source": [
    "This initial line looks way off.  See if you can look back at the summary stats and see the same information encoded there.\n",
    "\n",
    "Together, these initial sanity checks suggest we may be able to find a much better line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AZWF67uv0HTG",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tweak the model parameters\n",
    "For this exercise, we've put all the above code in a single function for convenience. You can call the function with different parameters to see the effect.\n",
    "\n",
    "In this function, we'll proceed in 10 evenly divided periods so that we can observe the model improvement at each period.\n",
    "\n",
    "For each period, we'll compute training loss and graph that.  This may help you judge when a model is converged, or if it needs more iterations.\n",
    "\n",
    "We'll also plot values for the feature weight and bias term learned by the model over time.  This is another way to see how things converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": null,
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "wgSMeD5UU81N"
   },
   "outputs": [],
   "source": [
    "def train_model(learning_rate, steps, batch_size, input_feature=\"total_rooms\"):\n",
    "  \"\"\"Trains a linear regression model of one feature.\n",
    "  \n",
    "  Args:\n",
    "    learning_rate: A `float`, the learning rate.\n",
    "    steps: A non-zero `int`, the total number of training steps. A training step\n",
    "      consists of a forward and backward pass using a single batch.\n",
    "    batch_size: A non-zero `int`, the batch size.\n",
    "    input_feature: A `string` specifying a column from `california_housing_dataframe`\n",
    "      to use as input feature.\n",
    "  \"\"\"\n",
    "  \n",
    "  periods = 10\n",
    "  steps_per_period = steps / periods\n",
    "\n",
    "  my_feature = input_feature\n",
    "  my_feature_column = california_housing_dataframe[[my_feature]]\n",
    "  my_label = \"median_house_value\"\n",
    "  targets = california_housing_dataframe[my_label]\n",
    "\n",
    "  # Create feature columns\n",
    "  feature_columns = [tf.contrib.layers.real_valued_column(my_feature, dimension=1)]\n",
    "  \n",
    "  # Create input functions\n",
    "  training_input_fn = learn_io.pandas_input_fn(\n",
    "    x=my_feature_column, y=targets, num_epochs=None, batch_size=batch_size)\n",
    "  prediction_input_fn = learn_io.pandas_input_fn(\n",
    "    x=my_feature_column, y=targets, num_epochs=1, shuffle=False)\n",
    "  \n",
    "  # Create a linear regressor object.\n",
    "  linear_regressor = tf.contrib.learn.LinearRegressor(\n",
    "      feature_columns=feature_columns,\n",
    "      optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate),\n",
    "      gradient_clip_norm=5.0\n",
    "  )\n",
    "\n",
    "  # Set up to plot the state of our model's line each period.\n",
    "  plt.figure(figsize=(15, 6))\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.title(\"Learned Line by Period\")\n",
    "  plt.ylabel(my_label)\n",
    "  plt.xlabel(my_feature)\n",
    "  sample = california_housing_dataframe.sample(n=300)\n",
    "  plt.scatter(sample[my_feature], sample[my_label])\n",
    "  colors = [cm.coolwarm(x) for x in np.linspace(-1, 1, periods)]\n",
    "\n",
    "  # Train the model, but do so inside a loop so that we can periodically assess\n",
    "  # loss metrics.\n",
    "  print \"Training model...\"\n",
    "  print \"RMSE (on training data):\"\n",
    "  root_mean_squared_errors = []\n",
    "  for period in range (0, periods):\n",
    "    # Train the model, starting from the prior state.\n",
    "    linear_regressor.fit(\n",
    "        input_fn=training_input_fn,\n",
    "        steps=steps_per_period\n",
    "    )\n",
    "    # Take a break and compute predictions.\n",
    "    predictions = list(linear_regressor.predict(\n",
    "        input_fn=prediction_input_fn))\n",
    "    # Compute loss.\n",
    "    root_mean_squared_error = math.sqrt(\n",
    "        metrics.mean_squared_error(predictions, targets))\n",
    "    # Occasionally print the current loss.\n",
    "    print \"  period %02d : %0.2f\" % (period, root_mean_squared_error)\n",
    "    # Add the loss metrics from this period to our list.\n",
    "    root_mean_squared_errors.append(root_mean_squared_error)\n",
    "    # Finally, track the weights and biases over time.\n",
    "    # Apply some math to ensure that the data and line are plotted neatly.\n",
    "    y_extents = np.array([0, sample[my_label].max()])\n",
    "    x_extents = (y_extents - linear_regressor.bias_) / linear_regressor.weights_[0]\n",
    "    x_extents = np.maximum(np.minimum(x_extents,\n",
    "                                      sample[my_feature].max()),\n",
    "                           sample[my_feature].min())\n",
    "    y_extents = linear_regressor.weights_[0] * x_extents + linear_regressor.bias_\n",
    "    plt.plot(x_extents, y_extents, color=colors[period]) \n",
    "  print \"Model training finished.\"\n",
    "\n",
    "  # Output a graph of loss metrics over periods.\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.ylabel('RMSE')\n",
    "  plt.xlabel('Periods')\n",
    "  plt.title(\"Root Mean Squared Error vs. Periods\")\n",
    "  plt.tight_layout()\n",
    "  plt.plot(root_mean_squared_errors)\n",
    "\n",
    "  # Output a table with calibration data.\n",
    "  calibration_data = pd.DataFrame()\n",
    "  calibration_data[\"predictions\"] = pd.Series(predictions)\n",
    "  calibration_data[\"targets\"] = pd.Series(targets)\n",
    "  display.display(calibration_data.describe())\n",
    "\n",
    "  print \"Final RMSE (on training data): %0.2f\" % root_mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kg8A4ArBU81Q"
   },
   "source": [
    "### Task 1:  Tweak to try and improve loss and match the target distribution better.\n",
    "\n",
    "**Your goal is to try and get anything below about 180 in RMSE for this portion.**\n",
    "\n",
    "If you haven't gotten 180 or lower RMSE after 5 minutes of trying, check the solution for a possible combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "test": {
      "output": "ignore",
      "timeout": 600
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "UzoZUSdLIolF",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_model(\n",
    "    learning_rate=0.00001,\n",
    "    steps=100,\n",
    "    batch_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QU5sLyYTqzqL",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Is there a standard method for tuning the model?\n",
    "\n",
    "This is a commonly asked question. The short answer is that the effects of different hyperparameters is data dependent.  So there are no hard and fast rules; you'll need to run tests on your data.\n",
    "\n",
    "Here are a few rules of thumb that may help guide you:\n",
    "\n",
    " * Training error should steadily decrease, steeply at first, and should eventually plateau as training converges.\n",
    " * If the training has not converged, try running it for longer.\n",
    " * If the training error decreases too slowly, increasing the learning rate may help it decrease faster.\n",
    "   * But sometimes the exact opposite may happen if the learning rate is too high.\n",
    " * If the training error varies wildly, try decreasing the learning rate.\n",
    "   * Lower learning rate plus larger number of steps or larger batch size is often a good combination.\n",
    " * Very small batch sizes can also cause instability.  First try larger values like 100 or 1000, and decrease until you see degradation.\n",
    "\n",
    "Again, never go strictly by these rules of thumb, because the effects are data depdendent.  Always experiment and verify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GpV-uF_cBCBU",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Task 2: Try a different feature.\n",
    "\n",
    "See if you can do any better by replacing the `total_rooms` feature with the `population` feature.\n",
    "\n",
    "Don't take more than 5 minutes on this portion."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "first_steps_with_tensor_flow.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

## 数学准备

- 雅可比矩阵（Jacobian）：向量对向量的偏导数所构成的矩阵，考虑函数从空间映射到另一个空间($R^n -> R^m$)，则雅可比矩阵形成一个m行n列的矩阵，矩阵元标记为

  $J_{m,n}=\frac{\partial}{\partial x_n}f(x)_m$

- 海森矩阵（Hessian）：一阶导数的雅可比矩阵，因为二阶偏导的连续性，可以交换偏导的先后顺序，所以海森矩阵也是实对称矩阵。

- 方向导数（direction derivative）:某个标量对特定方向d（单位向量）上的导数，度量了该标量沿着该方向的变化率，是一个向量。

- 梯度（Gradient）：变化率最大的方向导数。

- 鞍点（saddle point）：Hessian正定，对应局部极小值，Hessian负定，对应局部最大值，Hessian不定，对应鞍点（注意，这是充分非必要条件）直观来看，鞍点就是一个方向上是极大值，另一方向却是极小值。

- 厄米矩阵（Hermitian）：对称矩阵的复数域扩展，实对称矩阵是厄密矩阵的特例。厄米矩阵可以被对角化。

- 特征值：矩阵做对角化变换前后，特征向量被缩放的比例。特征向量和特征值是矩阵的固有属性。

## 优化算法

### 机器学习为什么要用优化算法

很多情况下，我们无法获得参数的解析表达，需要采用迭代的方式逼近最佳的参数值。

就算我们可以获得解析表达，但是当数据量变得非常庞大的时候，连计算矩阵的逆都会变得非常慢。

## QA

**SGD, BGD, Adadelta, Momentum哪个方法对超参数最不敏感？**

神经网络经典五大超参数:
学习率(Learning Rate)、权值初始化(Weight Initialization)、网络层数(Layers)
单层神经元数(Units)、正则惩罚项（Regularizer|Normalization)

显然在这里超参数指的是事先指定的learningrate，而对超参数不敏感的梯度算法是Adadelta，牛顿法。
Adadelta自适应学习率调整
Adadelta的特点是在下降初期，梯度比较小，这时学习率会比较大，而到了中后期，接近最低点时，梯度较大，这时学习率也会相对减小，放慢速度，以便可以迭代到最低点。
Momentum冲量法
梯度下降法在求解时的难题主要在于解决极小值和鞍点的问题，为了解决这个问题，可以模拟在现实中的惯性。物体有一个初始动量，在平缓的区域或者小的坑里也能继续向前运动试图滚出小坑，在动量变为0的时候停止，表示已经达到最低点。
https://blog.csdn.net/qq_34470213/article/details/79869206

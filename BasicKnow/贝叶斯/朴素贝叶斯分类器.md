朴素贝叶斯(naive Bayes) 法是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入输出的联合概率分布；然后基于此模型，对给定的输入x, 利用贝叶斯定理求出后验概率最大的输出y 。

朴素贝叶斯法实现简单，学习与预测的效率都很高，是一种常用的方法。

缺点：分类的性能不一定很高

**朴素贝叶斯法与贝叶斯估计是不同的概念。**

## 贝叶斯决策理论

决策理论

决策论Decision theory + 概率论Probability theory

探讨了如何在包含不确定性的环境中做出最优决策

概率知识+对决策带来的损失的认识→最优决策

**模型比较理论**

最大似然：最符合观测数据的（即P(D | h) 最大的）最有优势

奥卡姆剃刀：P(h) 较大的模型有较大的优势

掷一个硬币，观察到的是“正”，根据最大似然估计的精神，我们应该猜测这枚硬币掷出“正”的概率是1，因为这个才是能最大化P(D | h) 的那个猜测

如果平面上有N 个点，近似构成一条直线，但绝不精确地位于一条直线上。这时我们既可以用直线来拟合（模型1），也可以用二阶多项式（模型2）拟合，也可以用三阶多项式（模型3），特别地，用N-1 阶多项式便能够保证肯定能完美通过N 个数据点。那么，这些可能的模型之中到底哪个是最靠谱的呢？

奥卡姆剃刀：越是高阶的多项式越是不常见

P(d1|h+) * P(d2|d1, h+) * P(d3|d2,d1, h+) * ..

假设di 与di-1 是完全条件无关的（朴素贝叶斯假设特征之间是独立，互不影响）

简化为P(d1|h+) * P(d2|h+) * P(d3|h+) * ..

![image](img/bayes1.png)

![image](img/bayes2.png)

![image](img/bayes3.png)

![image](img/bayes4.png)

![image](img/bayes5.png) 连续属性下：高斯朴素贝叶斯

为了避免其他属性携带的信息被训练集中未出现的属性值"抹去'，在估计概率值时通常要进行"平滑" (smoothing) ，常用"拉普拉斯修正" (Laplacian correction）。拉普拉斯修正实质上假设了属性值与类别均匀分布。

在现实任务中朴素贝叶斯分类器有多种使用方式.例如，若任务对预测速度要求较高，则对给定训练集，可将朴素贝叶斯分类器涉及的所有概率估值事先计算好存储起来，这样在进行预测时只需"查表"即可进行判别;若任务数据更替频繁，则可采用"懒惰学习" (lazy learning) 方式，先不进行任何训练，待收到预测请求时再根据当前数据集进行概率估值;若数据不断增加，则可在现有估值基础上，仅对新增样本的属性值所涉及的概率估值进行计数修正即可，实现增量学习.

## 朴素贝叶斯法的学习与分类

### 基本方法

![image-20211002100129940](img/image-20211002100129940.png)

![image-20211002100250599](img/image-20211002100250599.png)

预测时

![image-20211002100328222](img/image-20211002100328222.png)

![image-20211002100343254](img/image-20211002100343254.png)

### 后验概率最大化的含义

![image-20211002100417248](img/image-20211002100417248.png)

![image-20211002100527608](img/image-20211002100527608.png)

## 朴素贝叶斯法的参数估计

### 极大似然估计

![image-20211002100750839](img/image-20211002100750839.png)

### 学习与分类算法

![image-20211002100820541](img/image-20211002100820541.png)

![image-20211002100832354](img/image-20211002100832354.png)

### 贝叶斯估计

![image-20211002101505087](img/image-20211002101505087.png)



## 随机

$W^{[1]}$ = np.random.randn((2,2))\*0.01

$b^{[1]}$ = np.zeros((2,1))

$W^{[2]}$ = np.random.randn((1,2))\*0.01

$b^{[2]}$ = 0


如果把所有的w都初始化为0，根据对称性原理，第一隐层上的所有节点做的计算和该层上所有的其他节点都是一样的，无论迭代多少次，每次都在计算同样的函数，无法收敛。

对logistic回归，可以将w初始化为0，因为它的输出层只有一个节点。

如果w很大，根据z=wx+b，z也会很大，那么tanh函数值也会很大，在这些地方，tanh函数的斜率很小，接近于0，会导致梯度下降收敛很慢。所以一般初始化w时会再乘上一个比较小的系数。

## xavier

思想：为了使得网络中信息更好的流动，每一层输出的方差应该尽量相等。

权重方差应满足：$Var[W^i]=2/(n_i+n_{i+1})$

即：2/(输入层神经元个数+输出层神经元个数)

因为[a,b]间的均匀分布的方差为$Var=(b-a)^2/12$，因此Xavier初始化的实现就是下面的均匀分布：

$W\sim U[-\frac{\sqrt{6}}{\sqrt{n_j+n_{j+1}}},\frac{\sqrt{6}}{\sqrt{n_j+n_{j+1}}}]$

https://blog.csdn.net/shuzfan/article/details/51338178


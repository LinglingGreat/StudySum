---

---

# ç¥ç»ç½‘ç»œåŸºç¡€

## What is neural network?

It is a powerful learning algorithm inspired by how the brain works.

Example 1 â€“ single neural network  
Given data about the size of houses on the real estate market and you want to fit a function that will predict their price. It is a linear regression problem because the price as a function of size is a continuous output.  
We know the prices can never be negative so we are creating a function called **Rectified Linear Unit (ReLU)** which starts at zero.  
The input is the size of the house (x)  
The output is the price (y)  
The â€œneuronâ€ implements the function ReLU (blue line)  

Example 2 â€“ Multiple neural network  
The price of a house can be affected by other features such as size, number of bedrooms, zip code and wealth. The role of the neural network is to predicted the price and it will automatically generate the
hidden units. We only need to give the inputs x and the output y.  

## Supervised learning for Neural Network
In supervised learning, we are given a data set and already know what our correct output should look like,having the idea that there is a relationship between the input and the output.  

**Supervised learning problems are categorized into "regression" and "classification" problems. **

In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. 

In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into
discrete categories.

There are different types of neural network, for example **Convolution Neural Network (CNN)** used often for image application and **Recurrent Neural Network (RNN)** used for one-dimensional sequence data such as translating English to Chinses or a temporal component such as text transcript. As for the autonomous driving, it is a hybrid neural network architecture.

**Structured vs unstructured data**  
Structured data refers to things that has a defined meaning such as price, age whereas unstructured data refers to thing like pixel, raw audio, text.

## Why is deep learning taking off?
Deep learning is taking off due to a large amount of data available through the digitization of the society, faster computation and innovation in the development of neural network algorithm.

Two things have to be considered to get to the high level of performance:
1. Being able to train a big enough neural network
2. Huge amount of labeled data

The process of training a neural network is iterative.  
**Ideaâ€”â€”Codeâ€”â€”Experimentâ€”â€”Ideaâ€”â€”Codeâ€”â€”Experiment...  **
It could take a good amount of time to train a neural network, which affects your productivity. Faster computation helps to iterate and improve new algorithm.

## Logistic Regression
Logistic regression is a learning algorithm used in a supervised learning problem when the output ğ‘¦ are all either zero or one. The goal of logistic regression is to minimize the error between its predictions and training data.  
ğºğ‘–ğ‘£ğ‘’ğ‘› ğ‘¥ ,ğ‘¦Ì‚=ğ‘ƒ(ğ‘¦=1|ğ‘¥), where 0 â‰¤ğ‘¦Ì‚â‰¤1  
$\hat{y}^{(ğ‘–)}=ğœ(ğ‘¤^ğ‘‡ğ‘¥^{(ğ‘–)}+ğ‘)$, where $ğœ(ğ‘§^{(ğ‘–)})= \frac{1}{1+ ğ‘’^{âˆ’ğ‘§^{(ğ‘–)}}}$,$ğ‘¥^{(ğ‘–)}$ the i-th training example

Loss (error) function:

$L(\hat{y}^{(ğ‘–)}, y^{(ğ‘–)})= \frac12(\hat{y}^{(i)}âˆ’ğ‘¦^{(i)})^2$ éå‡¸å‡½æ•°
$L(\hat{y}^{(ğ‘–)}, y^{(ğ‘–)})= -(y^{(i)}log(\hat{y}^{(i)})+(1-y{(i)})log(1-\hat{y}^{(i)}))$ å‡¸å‡½æ•°

+ If $ğ‘¦^{(ğ‘–)}$=1: $L(\hat{y}^{(ğ‘–)}, y^{(ğ‘–)})= âˆ’log(\hat{y}^{(i)})$ where $log(\hat{y}^{(i)})$ and $\hat{y}^{(ğ‘–)}$ should be close to 1
+ If $ğ‘¦^{(ğ‘–)}$=0: $L(\hat{y}^{(ğ‘–)}, y^{(ğ‘–)})= âˆ’log(1-\hat{y}^{(i)})$ where $log(1-\hat{y}^{(i)})$ and $\hat{y}^{(ğ‘–)}$ should be close to 0

Cost function
The cost function is the average of the loss function of the entire training set. We are going to find the parameters ğ‘¤ ğ‘ğ‘›ğ‘‘ ğ‘ that minimize the overall cost function. 

$J(w,b)=\frac1m\sum_{i=1}^{m} L(\hat{y}^{(ğ‘–)}, y^{(ğ‘–)})=-\frac1m\sum_{i=1}^{m}[(y^{(i)}log(\hat{y}^{(i)})+(1-y^{(i)})log(1-\hat{y}^{(i)}))]$

## Gradient descent

## Computation Graphæµç¨‹å›¾

é“¾å¼æ³•åˆ™ï¼Œè®¡ç®—Lå¯¹wçš„åå¯¼

## pythonå®ç°

```python
# sigmoid function, np.exp()
import numpy as np
x = np.array([1,2,3])
s = 1 / (1+np.exp(-x))
print("The sigmoid of x is:"+ str(s))

# sigmoid gradient
# sigmoid_derivative(x)=Ïƒ(x)(1-Ïƒ(x))
ds = s*(1-s)
print("sigmoid_derivative(x) = " + str(ds))

# Reshaping arrays
# image2vector
image = np.array([[[ 0.67826139,  0.29380381],
        [ 0.90714982,  0.52835647],
        [ 0.4215251 ,  0.45017551]],

       [[ 0.92814219,  0.96677647],
        [ 0.85304703,  0.52351845],
        [ 0.19981397,  0.27417313]],

       [[ 0.60659855,  0.00533165],
        [ 0.10820313,  0.49978937],
        [ 0.34144279,  0.94630077]]])
v = image.reshape((image.shape[0]*image.shape[1]*image.shape[2], 1))
print ("image2vector(image) = " + str(v))

# Normalizing rows
x = np.array([
    [0, 3, 4],
    [1, 6, 4]])
x_norm = np.linalg.norm(x,axis=1,keepdims=True)
x = x / x_norm
print("normalizeRows(x) = " + str(x))

# broadcasting and softmax function
# çŸ©é˜µçš„æ¯ä¸ªå…ƒç´ = exp(xik)/sum(exp(xij)),å³æ¯ä¸ªå…ƒç´ çš„expé™¤ä»¥è¯¥è¡Œæ‰€æœ‰å…ƒç´ çš„expå€¼ä¹‹å’Œ
x = np.array([
    [9, 2, 5, 0, 0],
    [7, 5, 0, 0 ,0]])
x_exp = np.exp(x)
x_sum = np.sum(x_exp, axis=1, keepdims=True)
s = x_exp / x_sum
print("softmax(x) = " + str(s))
```

```python
# Vectorization
import time

x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]
x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]

### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###
tic = time.process_time()
dot = 0
for i in range(len(x1)):
    dot+= x1[i]*x2[i]
toc = time.process_time()
print ("dot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### CLASSIC OUTER PRODUCT IMPLEMENTATION ###
tic = time.process_time()
outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros
for i in range(len(x1)):
    for j in range(len(x2)):
        outer[i,j] = x1[i]*x2[j]
toc = time.process_time()
print ("outer = " + str(outer) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### CLASSIC ELEMENTWISE IMPLEMENTATION ###
tic = time.process_time()
mul = np.zeros(len(x1))
for i in range(len(x1)):
    mul[i] = x1[i]*x2[i]
toc = time.process_time()
print ("elementwise multiplication = " + str(mul) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###
W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array
tic = time.process_time()
gdot = np.zeros(W.shape[0])
for i in range(W.shape[0]):
    for j in range(len(x1)):
        gdot[i] += W[i,j]*x1[j]
toc = time.process_time()
print ("gdot = " + str(gdot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")
```

```python
x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]
x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]

### VECTORIZED DOT PRODUCT OF VECTORS ###
tic = time.process_time()
dot = np.dot(x1,x2)
toc = time.process_time()
print ("dot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### VECTORIZED OUTER PRODUCT ###
tic = time.process_time()
outer = np.outer(x1,x2)
toc = time.process_time()
print ("outer = " + str(outer) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### VECTORIZED ELEMENTWISE MULTIPLICATION ###
tic = time.process_time()
mul = np.multiply(x1,x2)
toc = time.process_time()
print ("elementwise multiplication = " + str(mul) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")

### VECTORIZED GENERAL DOT PRODUCT ###
tic = time.process_time()
dot = np.dot(W,x1)
toc = time.process_time()
print ("gdot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")
```

```python
# Implement the L1 and L2 loss functions
yhat = np.array([.9, 0.2, 0.1, .4, .9])
y = np.array([1, 0, 0, 1, 1])
L1 = np.sum(abs(y-yhat))
L2 = np.dot(y-yhat,y-yhat)
```

```python
# Logistic Regression with a Neural Network mindset
# Packages
# numpy is the fundamental package for scientific computing with Python.
# h5py is a common package to interact with a dataset that is stored on an H5 file.
# matplotlib is a famous library to plot graphs in Python.
# PIL and scipy are used here to test your model with your own picture at the end.
import numpy as np
import matplotlib.pyplot as plt
import h5py
import scipy
from PIL import Image
from scipy import ndimage
from lr_utils import load_dataset

%matplotlib inline
# assignment2_2
# A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b âˆ—âˆ— c âˆ—âˆ— d, a) is to use:
X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X
```



# æµ…å±‚ç¥ç»ç½‘ç»œ

## Activation functionsæ¿€æ´»å‡½æ•°

sigmoid activation function:  $a=\frac{1}{1+e^{-z}}$

äºŒåˆ†åˆ†ç±»ï¼Œè¾“å‡ºç»“æœæ˜¯0æˆ–è€…1ï¼Œç”¨ä½œè¾“å‡ºå±‚ï¼Œåªæœ‰åŒæ—¶æ»¡è¶³è¿™ä¸‰ä¸ªæ¡ä»¶æ—¶ï¼Œsigmoidå‡½æ•°æ‰ä¼šå¥½è¿‡åˆ«çš„æ¿€æ´»å‡½æ•°ã€‚

tanh activation function: $a = \frac{e^z- e^{-z}} {e^z+ e^{-z}}$

tanhæ¿€æ´»å‡½æ•°æ˜¯å°†sigmoidå‡½æ•°å‘ä¸‹ç§»åŠ¨ï¼Œå®ƒæœ‰ç±»ä¼¼æ•°æ®ä¸­å¿ƒåŒ–çš„æ•ˆæœï¼Œä½¿å¾—æ•°æ®çš„å¹³å‡å€¼æ¥è¿‘0ï¼Œä»è€Œè®©ä¸‹ä¸€å±‚çš„å­¦ä¹ æ›´æ–¹ä¾¿ã€‚å´æ©è¾¾è€å¸ˆè¯´ï¼Œä»–ç°åœ¨å‡ ä¹ä¸ç”¨sigmoidï¼Œå› ä¸ºtanhå‡ ä¹åœ¨ä»»ä½•æ—¶å€™ï¼Œæ•ˆæœéƒ½æ›´å¥½ã€‚

Relu activation function: a = max(0,z)

leaky activation Relu function: a = max(0.01z, z)

why do you need non-linear activation  functions?

å¦‚æœä¸ç”¨éçº¿æ€§çš„æ¿€æ´»å‡½æ•°ï¼Œè€Œç”¨çº¿æ€§çš„æ¿€æ´»å‡½æ•°ï¼Œé‚£ä¹ˆè¾“å‡ºå¯ç›´æ¥è¡¨ç¤ºæˆè¾“å…¥çš„çº¿æ€§å‡½æ•°å½¢å¼ï¼Œæ— éœ€éšè—å±‚ã€‚

==**ä»¥ä¸‹å›¾åƒä½æ·±åº¦ç¥ç»ç½‘ç»œæ¿€æ´»å‡½æ•°çš„å‡½æ•°å›¾åƒ,æœ€æœ‰å¯èƒ½å‘ç”Ÿæ¢¯åº¦æ¶ˆå¤±çš„æ˜¯(sigmoidå‡½æ•°) **==

å¦‚æœæ­¤éƒ¨åˆ†å¤§äº1ï¼Œé‚£ä¹ˆå±‚æ•°å¢å¤šçš„æ—¶å€™ï¼Œæœ€ç»ˆçš„æ±‚å‡ºçš„æ¢¯åº¦æ›´æ–°å°†ä»¥æŒ‡æ•°å½¢å¼å¢åŠ ï¼Œå³å‘ç”Ÿæ¢¯åº¦çˆ†ç‚¸ï¼Œå¦‚æœæ­¤éƒ¨åˆ†å°äº1ï¼Œé‚£ä¹ˆéšç€å±‚æ•°å¢å¤šï¼Œæ±‚å‡ºçš„æ¢¯åº¦æ›´æ–°ä¿¡æ¯å°†ä¼šä»¥æŒ‡æ•°å½¢å¼è¡°å‡ï¼Œå³å‘ç”Ÿäº†æ¢¯åº¦æ¶ˆå¤±

==**å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°æœ‰**==ï¼šçº¿æ€§å‡½æ•°ã€æ–œå¡å‡½æ•°ã€é˜¶è·ƒå‡½æ•°ã€ç¬¦å·å‡½æ•°ã€Sigmoidå‡½æ•°ã€åŒæ›²æ­£åˆ‡å‡½æ•°ã€Softpluså‡½æ•°ã€Softsignå‡½æ•°ã€Reluå‡½æ•°åŠå…¶å˜å½¢ã€Maxoutå‡½æ•°ç­‰ã€‚



![activation](img/activation.png)

## Formulas for computing derivatives

Forward propagation

$Z^{[1]} = W^{[1]}X + b^{[1]}$

$A^{[1]} = g^{[1]}(Z^{[1]}) $

$Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$

$A^{[2]} = g^{[2]}(Z^{[2]}) = \sigma(Z^{[2]}) $



Back propagation

$dz^{[2]} = a^{[2]} - y$                              $dZ^{[2]} = A^{[2]} - Y$,         $Y=[y^{(1)}, y^{(2)}, ..., y^{(m)}]$

$dW^{[2]} = dz^{[2]} a^{[1]^T}$                         $dW^{[2]} = \frac1m dZ^{[2]} A^{[1]T}$

$db^{[2]} = dz^{[2]}$                                     $db^{[2]} = \frac1m np.sum(dZ^{[2]}, axis=1, keepdims=True)$

$dz^{[1]} = W^{[2]T} dz^{[2]} * g^{[1]'}(z^{[1]})$      $dZ^{[1]} = W^{[2]T} dZ^{[2]} * g^{[1]'}(Z^{[1]})$

$dW{[1]} = dz^{[1]} x^T$                             $dW{[1]} = \frac1m dZ^{[1]} X^T$

$db^{[1]} = dz^{[1]}$,                                  $db^{[1]} = \frac1m np.sum(dZ^{[1]}, axis=1, keepdims=True)$

## Random Initializationåˆå§‹åŒ–



$W^{[1]}$ = np.random.randn((2,2))*0.01

$b^{[1]}$ = np.zeros((2,1))

$W^{[2]}$ = np.random.randn((1,2))*0.01

$b^{[2]}$ = 0

å¦‚æœæŠŠæ‰€æœ‰çš„wéƒ½åˆå§‹åŒ–ä¸º0ï¼Œæ ¹æ®å¯¹ç§°æ€§åŸç†ï¼Œç¬¬ä¸€éšå±‚ä¸Šçš„æ‰€æœ‰èŠ‚ç‚¹åšçš„è®¡ç®—å’Œè¯¥å±‚ä¸Šæ‰€æœ‰çš„å…¶ä»–èŠ‚ç‚¹éƒ½æ˜¯ä¸€æ ·çš„ï¼Œæ— è®ºè¿­ä»£å¤šå°‘æ¬¡ï¼Œæ¯æ¬¡éƒ½åœ¨è®¡ç®—åŒæ ·çš„å‡½æ•°ï¼Œæ— æ³•æ”¶æ•›ã€‚

å¯¹logisticå›å½’ï¼Œå¯ä»¥å°†wåˆå§‹åŒ–ä¸º0ï¼Œå› ä¸ºå®ƒçš„è¾“å‡ºå±‚åªæœ‰ä¸€ä¸ªèŠ‚ç‚¹ã€‚

å¦‚æœwå¾ˆå¤§ï¼Œæ ¹æ®z=wx+bï¼Œzä¹Ÿä¼šå¾ˆå¤§ï¼Œé‚£ä¹ˆtanhå‡½æ•°å€¼ä¹Ÿä¼šå¾ˆå¤§ï¼Œåœ¨è¿™äº›åœ°æ–¹ï¼Œtanhå‡½æ•°çš„æ–œç‡å¾ˆå°ï¼Œæ¥è¿‘äº0ï¼Œä¼šå¯¼è‡´æ¢¯åº¦ä¸‹é™æ”¶æ•›å¾ˆæ…¢ã€‚æ‰€ä»¥ä¸€èˆ¬åˆå§‹åŒ–wæ—¶ä¼šå†ä¹˜ä¸Šä¸€ä¸ªæ¯”è¾ƒå°çš„ç³»æ•°ã€‚

==xavier==

æ€æƒ³ï¼šä¸ºäº†ä½¿å¾—ç½‘ç»œä¸­ä¿¡æ¯æ›´å¥½çš„æµåŠ¨ï¼Œæ¯ä¸€å±‚è¾“å‡ºçš„æ–¹å·®åº”è¯¥å°½é‡ç›¸ç­‰ã€‚

æƒé‡æ–¹å·®åº”æ»¡è¶³ï¼š$Var[W^i]=2/(n_i+n_{i+1})$

å³ï¼š2/(è¾“å…¥å±‚ç¥ç»å…ƒä¸ªæ•°+è¾“å‡ºå±‚ç¥ç»å…ƒä¸ªæ•°)

å› ä¸º[a,b]é—´çš„å‡åŒ€åˆ†å¸ƒçš„æ–¹å·®ä¸º$Var=(b-a)^2/12$ï¼Œå› æ­¤Xavieråˆå§‹åŒ–çš„å®ç°å°±æ˜¯ä¸‹é¢çš„å‡åŒ€åˆ†å¸ƒï¼š

$W\sim U[-\frac{\sqrt{6}}{\sqrt{n_j+n_{j+1}}},\frac{\sqrt{6}}{\sqrt{n_j+n_{j+1}}}]$

https://blog.csdn.net/shuzfan/article/details/51338178

# æ·±å±‚ç¥ç»ç½‘ç»œ

##çŸ©é˜µç»´æ•°

$W^{[l]} : (n^{[l]}, n^{[l-1]})$,   $b^{[l]} : (n^{[l]}, 1)$,           $d^W{[l]}: (n^{[l]}, n^{[l-1]}),          db^{[l]}: (n^{[l]}, l)$

$z^{[l]}, a^{[l]}:  (n^{[l]},  1)$

$Z^{[l]}, A^{[l]}:  (n^{[l]},  m)$,         $l=0æ—¶ï¼ŒA^{[0]} = X = (n^{[0]}, m)$

$dZ^{[l]},  d A{[l]}: (n^{[l]}, m)$

## Forward and backward functions

![forward&backward](img/forward&backward.png)

Forward propagation for layer $l$

Input $a^{[l-1]}$

Output $a^{[l-1]}$, cash ($z^{[l]}$)



$z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]},   a^{[l]} = g^{[l]}(z^{[l]})$                  $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]},   A^{[l]} = g^{[l]}(Z^{[l]})$

å¯¹äºæ­£å‘ä¼ æ’­ï¼Œè¦éå†æ¯ä¸€å±‚æ˜¯éœ€è¦æœ‰ä¸€ä¸ªæ˜¾ç¤ºå¾ªç¯çš„ã€‚

Backward propagation for layer $l$

Input $da^{[l]}$

Output $da^{[l-1]}, dW^{[l]}, db^{[l]}$



$dz^{[l]} = da^{[l]} * g^{[l]'} (z^{[l]})$                                       $dZ^{[l]} = dA^{[l]} * g^{[l]'} (Z^{[l]})$

$dW^{[l]} = dz^{[l]} a^{[l-1]}$                                             $dW^{[l]} = \frac1m dZ^{[l]} A^{[l-1]T}$

$db^{[l]} = dz^{[l]}$                                                        $db^{[l]} = \frac1m np.sum(dZ^{[l]}, axis=1, keepdims=True)$

$da^{[l-1]} = W^{[l]T} dz^{[l]}$                                          $dA^{[l-1]} = W^{[l]T} dZ^{[l]}$

$dz^{[l]} = W^{[l+1]T} dz^{[l+1]} * g^{[l]'}(z^{[l]})$

![summary](img/summary.png)

## Parameters vs Hyperparameters

Parameters: $ W^{[1]},  b^{[1]}, W^{[2]}, b^{2]}, W^{[3]}, b^{[3]}$

Hyperparameters:  èƒ½å¤Ÿæ§åˆ¶Wå’Œbçš„å€¼

Learning rate $\alpha$,    # iterations,  # hidden layer L,  # hidden unites $n^{[1]}, n^{[2]},...$,  choice of activation functio

other : Momentum,  minibatch size, regularization parameters

å‚æ•°æ˜¯æŒ‡æ¨¡å‹èƒ½è‡ªå·±è®¡ç®—å‡ºæ¥çš„ï¼Œæ¯”å¦‚wï¼Œbï¼Œaç­‰ï¼›è€Œè¶…å‚æ•°æ˜¯æŒ‡éœ€è¦äººç»éªŒåˆ¤æ–­çš„å‚æ•°ï¼Œæ¯”å¦‚å­¦ä¹ ç‡ï¼Œè¿­ä»£æ¬¡æ•°ï¼Œç½‘ç»œæ¨¡å‹çš„å½¢çŠ¶ï¼ŒåŒ…æ‹¬å±‚æ•°åŠæ¯å±‚æœ‰å¤šå°‘èŠ‚ç‚¹ã€‚














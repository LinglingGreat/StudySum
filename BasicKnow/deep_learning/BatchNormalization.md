## 为什么要用BN

1.在神经网络中, 数据分布对训练会产生影响. 比如某个神经元 x 的值为1, 某个 Weights 的初始值为 0.1, 这样后一层神经元计算结果就是 Wx = 0.1; 又或者 x = 20, 这样 Wx 的结果就为 2. 现在还不能看出什么问题, 但是, 当我们加上一层激励函数, 激活这个 Wx 值的时候, 问题就来了. 如果使用 像 tanh 的激励函数, Wx 的激活值就变成了 ~0.1 和 ~1, 接近于 1 的部已经处在了 激励函数的饱和阶段, 也就是如果 x 无论再怎么扩大, tanh 激励函数输出值也还是 接近1. 换句话说, 神经网络在初始阶段已经不对那些比较大的 x 特征范围 敏感了. 这样很糟糕, 想象我轻轻拍自己的感觉和重重打自己的感觉居然没什么差别, 这就证明我的感官系统失效了. 当然我们是可以用之前提到的对数据做 normalization 预处理, 使得输入的 x 变化范围不会太大, 让输入值经过激励函数的敏感部分. 但刚刚这个不敏感问题不仅仅发生在神经网络的输入层, 而且在隐藏层中也经常会发生.

只是时候 x 换到了隐藏层当中, 我们能不能对隐藏层的输入结果进行像之前那样的normalization 处理呢? 答案是可以的, 因为大牛们发明了一种技术, 叫做 batch normalization, 正是处理这种情况.

2.训练深度神经网络是很复杂的，因为在训练期间，随着前一层的参数发生变化，每层的输入分布都会发生变化。然后，我们的想法是标准化每层的输入，使得它们的平均输出激活为零，标准偏差为 1。这是针对每一层的每个小批量进行的，即仅计算该小批量的均值和方差，然后进行标准化。这有点类似于网络输入的标准化。

这有什么用？我们知道，规范化网络输入有助于它学习。但网络只是一系列层，一个层的输出成为下一层的输入。这意味着我们可以将神经网络中的任何一个层视为后续子网络的第一个层。我们将其视为一系列相互 feed 的神经网络，我们在应用激活函数之前规范化一个层的输出，然后将其 feed 到后面的层（子网络）。

对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。

**防止“梯度弥散”**。


## 怎么做

Batch normalization 也可以被看做一个层面. 在一层层的添加神经网络的时候, 我们先有数据 X, 再添加全连接层, 全连接层的计算结果会经过 激励函数 成为下一层的输入, 接着重复之前的操作. Batch Normalization (BN) 就被添加在每一个全连接和激励函数之间.

![](img/Pasted%20image%2020210930082913.png)

公式的后面还有一个反向操作, 将 normalize 后的数据再扩展和平移. 原来这是为了让神经网络自己去学着使用和修改这个扩展参数 gamma, 和 平移参数 β, 这样神经网络就能自己慢慢琢磨出前面的 normalization 操作到底有没有起到优化的作用, 如果没有起到作用, 我就使用 gamma 和 belt 来抵消一些 normalization 的操作.



## 为什么BN和Dropout不一起用？

dropout在训练和预测时候，期望无偏，但方差不无偏，但bn要根据方差算，这种偏差可能会出问题

因为每次Batch Normalization是在mini-batch上做的，所以每次的均值和标准差是不一样的。从某种意义上说这是在向模型隐藏层训练过程中引入一些noise。这和dropout有相似的地方，dropout的思想也是在训练过程加入一些noise来增强模型的泛化能力

但是Ian 也说BN和Dropout也可以一起用，特别是数据量不够大的时候，所以还是it depends

https://www.youtube.com/watch?v=Xogn6veSyxA&t=10s 这是Ian Goodfellow的Lecture

## 优缺点

**优点**

(1) 减轻了对参数初始化的依赖，这是利于调参的朋友们的。

(2) 训练更快，可以使用更高的学习率。

(3) BN一定程度上增加了泛化能力，dropout等技术可以去掉。

**BN的缺陷**

从上面可以看出，batch normalization依赖于batch的大小，当batch值很小时，计算的均值和方差不稳定。研究表明对于ResNet类模型在ImageNet数据集上，batch从16降低到8时开始有非常明显的性能下降，在训练过程中计算的均值和方差不准确，而在测试的时候使用的就是训练过程中保持下来的均值和方差。

这一个特性，导致batch normalization不适合以下的几种场景。

(1)batch非常小，比如训练资源有限无法应用较大的batch，也比如在线学习等使用单例进行模型参数更新的场景。

(2)rnn，因为它是一个动态的网络结构，同一个batch中训练实例有长有短，导致每一个时间步长必须维持各自的统计量，这使得BN并不能正确的使用。在rnn中，对bn进行改进也非常的困难。不过，困难并不意味着没人做，事实上现在仍然可以使用的，不过这超出了咱们初识境的学习范围。

## 训练与推理时BN中的均值、方差分别是什么？

**训练**时，均值、方差分别是**该批次**内数据相应维度的均值与方差；

**推理**时，均值、方差是**基于所有批次**的期望计算所得



## 参考资料

[深度学习中 Batch Normalization为什么效果好？](https://www.zhihu.com/question/38102762)

[Batch Normalization的作用](https://blog.csdn.net/qican_7/article/details/100780370)

[深入理解BATCH NORMALIZATION 的作用](https://www.cnblogs.com/hoojjack/p/12350707.html)
 
 #td 
- **什么是批量标准化？为什么它能够奏效？** 训练深度神经网络是很复杂的，因为在训练期间，随着前一层的参数发生变化，每层的输入分布都会发生变化。然后，我们的想法是标准化每层的输入，使得它们的平均输出激活为零，标准偏差为 1。这是针对每一层的每个小批量进行的，即仅计算该小批量的均值和方差，然后进行标准化。这有点类似于网络输入的标准化。这有什么用？我们知道，规范化网络输入有助于它学习。但网络只是一系列层，一个层的输出成为下一层的输入。这意味着我们可以将神经网络中的任何一个层视为后续子网络的第一个层。我们将其视为一系列相互 feed 的神经网络，我们在应用激活函数之前规范化一个层的输出，然后将其 feed 到后面的层（子网络）。

为什么BN和Dropout不一起用？

dropout在训练和预测时候，期望无偏，但方差不无偏，但bn要根据方差算，这种偏差可能会出问题

因为每次Batch Normalization是在mini-batch上做的，所以每次的均值和标准差是不一样的。从某种意义上说这是在向模型隐藏层训练过程中引入一些noise。这和dropout有相似的地方，dropout的思想也是在训练过程加入一些noise来增强模型的泛化能力

但是Ian 也说BN和Dropout也可以一起用，特别是数据量不够大的时候，所以还是it depends

https://www.youtube.com/watch?v=Xogn6veSyxA&t=10s 这是Ian Goodfellow的Lecture

为什么要用BN？ #td 

[深度学习中 Batch Normalization为什么效果好？](https://www.zhihu.com/question/38102762)
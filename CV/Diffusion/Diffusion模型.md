![](img/Pasted%20image%2020221115215902.png)

加噪声的过程，加的噪声是越来越多的

![](img/Pasted%20image%2020221203213610.png)

![](img/Pasted%20image%2020221203213726.png)

接下来是逆向过程

![](img/Pasted%20image%2020221203213826.png)

![](img/Pasted%20image%2020221203213851.png)

![](img/Pasted%20image%2020221203213911.png)

Zt其实就是我们要估计的每个时刻的噪声
- 看起来无法直接求解，只能训练一个模型来计算了
- 模型的输入参数有两个，分别是当前时刻的分布和时刻t

![](img/Pasted%20image%2020221203214118.png)



## 扩散模型极简回顾

对于前向过程，宋飏博士在其获得了2021-ICLR-Outstanding-Paper-Award的论文里证明了DDPM里的离散加噪只是连续时间里的随机过程的离散化形式。而对于前向的扩散过程和后向的去噪过程都有相对应的随机微分方程和常微分方程表示。并且**DDPM的优化目标（预测每一步所添加的噪声）实际上可以理解为学习一个当前输入对目标数据分布的最优梯度方向**。这实际上非常符合直觉：即我们对输入所添加的噪声实际上使得输入远离了其原本的数据分布，而学习一个数据空间上最优的梯度方向实际上便等同于往去除噪声的方向去行走。

与宋飏博士同组的宋佳铭博士则在其另一篇工作DDIM里开启了DDPM加速采样的序幕。**在此之前，我们使用DDPM对同一张图像加噪后再去噪我们是无法得到同一张图像**的（除非固定随机数种子）。但在DDIM里宋博士证明了我们可以不止可以在后向去噪时使用确定性的常微分方程来得到确定性的采样结果，我们在前向扩散时也可以通过构造后向的常微分方程的逆过程来得到前向的最终加噪结果（这句话实际是在说如果我们有一条确定性的路径，那么前向和后向过程无非是正着走一遍和反着走一遍而已。）。这个结论使得扩散生成变得高度可控，不用担心对一张图的前后向得到完全不一样的图片使得一系列的调控成为可能。

## 基于迭代去噪过程的图像编辑

#### IVLR：Conditioning Method for Denoising Diffusion Probabilistic Models

这篇论文是直接基于DDPM的工作上展开的受控图像生成。其主要针对的痛点是DDPM的随机性太高无法确定性的生成，导致我们很难控制模型生成带有我们想要的语义信息的图片。其核心思想非常地简洁但精巧：我们的前向和后向是一个等长的过程。其中前向时原数据的信息逐渐丢失（先丢失高频信息再丢失低频信息）而后向时信息逐渐从纯噪声中补全（先补全低频信息再到高频信息）。如果我们记录下前向过程里每一步的噪声图像，将其与后向过程中的噪声图像混合，我们就可以影响后向过程的生成结果（考虑极端情况完全替换后向过程的噪声图像的话则一定可以轻易地回到原图）。而**我们通过影响混合时注入的前向信息的多少，或者后向时注入信息的时间步的多少，可以控制所生成的图与原图的相似程度**。

具体来说，其算法如下：其中是一个**低通滤波器加上一系列降维再升维保持图像维度不变**的过程。

![](img/Pasted%20image%2020221128202530.png)

很明显，我们可以通过控制降维再升维的倍数来控制信息的留存比例。也可以通过噪声在后向过程里添加的时间步多寡来调整控制的强弱。通过原论文里的两张图可以看到，随着压缩倍数的增加，其细节信息的缺失会导致最终生成的结果与原图的语义差别加大。同样的趋势可以在后向去噪时施加影响的终止步数上看到。越早终止施加，则语义差别越大。

![](img/Pasted%20image%2020221128202602.png)

![](img/Pasted%20image%2020221128202613.png)

总结一下这篇论文的优缺点。无需额外训练，需要调控的超参不多，并且直观易懂。但适用场景也比较局限无法局部调整只能全局修改，并且只能保留原有图像的空间布局，无法做到改变姿势角度等变化，且无法精细化地控制生成的图像的性质。

#### SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations

这篇文章和上一篇IVLR是同一时间段的工作，只比其早了四天，来自于宋飏和宋佳铭两位博士的整个组包括导师似乎都挂名了这篇论文。其核心思想同样非常优雅直观：我们在上面提到，扩散的前向过程实际是个图像语义信息不断被噪声掩盖的过程，并且总是从高频信息开始到低频信息。IVLR里实际用了低通滤波和下上采样来抽取出低频信息来影响后向去噪过程。那么我们是否可以**直接省略这一步直接让前向过程不要加噪到纯噪声，而是加噪到中间过程使其保留一些低频信息**不就好了吗？下图直观地体现了这个过程，其中t0就是前向加噪的比例。通过调控信息保留的多少，我们也同样调控了生成与原图之间的相似程度。

![](img/Pasted%20image%2020221128202837.png)

这两篇工作的核心思想非常接近。只是论述方式和实验分析略有不同。

#### RePaint: Inpainting using Denoising Diffusion Probabilistic Models

这篇工作略晚于以上两篇工作，核心思想也非常接近，但略有不同的是针对其只能全局修改的痛点增添了对图像的MASK操作，将任务转变为了图像补全任务，使得局部修改成为可能。

具体来说，论文作者的思路依然是我们在前向扩散时，依然记录每一步的噪声图像。在后向去噪时，我们将未被掩码的区域从前向的记录里抽取出来，而被掩码的区域则由噪声填充，拼合成一张完整的图后我们开始迭代去噪。之后每一步都更新未掩码区域为前向的记录部分，而掩码区域更新为后向去噪的结果。做法如下图所示

![](img/Pasted%20image%2020221128202859.png)

但是这个简单的做法会有一个很大的弊端就是掩码的区域里所有的信息实质上是被全部丢弃的，重新生成的结果往往在局部语义上是自洽的但是没有考虑到全局语义。比如以下这个例子，对比最左边的原图与我们朴素的做法产生的结果我们可以看到，尽管掩码部分重新生成的结果与周围的材质和颜色接近，但在全局语义上这张图明显是不自洽的。

![](img/Pasted%20image%2020221128202917.png)

于是作者提供了一个非常有启发性的洞见：**我们在后向去噪时，考虑的拼合图像里包含了原图前向扩散的静态输出，即使我们在后向去噪时不断地试图生成语义一致的内容，但图像里的未掩码区域都会在下一步被替换成没有考虑后向生成过程的前向扩散输出。导致了语义的不一致性。此外随着去噪过程的逐渐深入，我们的方差也在逐渐减小，使得更正语义变得更为困难。**换句话说，模型需要更多的步骤来更正语义的不一致性。而作者具体的做法则是结合了以上两点洞见。首先我们在每一步去噪的时候，我们重新将去噪后的拼合结果加一次噪声至, 再重复同样的后向去噪步骤。将以上这个过程重复n次，我们就得到了上图里语义一致的输出结果。这个做法分别对应了未掩码区域没有考虑后向生成结果的问题和需要更多步骤生成的问题。

这篇论文最大的出彩之处可能在于以上笔者介绍的精彩洞见（该算法集成在了Huggingface的diffusers库里）。但受限于其框架依然是基于迭代去噪过程的无条件生成的DDPM模型（即不条件于文本图像或类别等信息的生成），其对掩码区域所能做的调控依然非常有限，只能借助于随机性重新生成。下面笔者将介绍基于显式分类器引导的图像生成。

## 基于显式分类器的图像引导生成

在上面关于扩散模型的极简回顾里，笔者提到**扩散模型的优化目标本质上是在数据空间拟合一个前往目标数据分布的最优梯度方向****。那么很自然地，如果我们想要做引导生成，我们可以用贝叶斯定理将基于条件生成的梯度拆解成一个基于显式分类器的梯度和一个常规的无条件生成的梯度。**换句话来说，我们依然可以使用之前DDPM的方式继续训练一个无条件生成模型，我们现在只需要**额外训练一个新的基于噪声输入的分类器就可以了。**

![](img/Pasted%20image%2020221128203039.png)

实际上以上关于显式分类器的想法更早提出于宋飏博士的Diffusion SDE的论文，但只在结尾占了很小的篇幅。目前讨论分类器图像引导生成的主要引用的是以下这篇论文。

#### Diffusion Models Beat GANs on Image Synthesis

这篇论文有数个重要的贡献点，包括对UNet架构的探索，更高的生成质量，显式分类器在DDPM，DDIM的具体算法及其推导，条件引导生成质量和多样性的取舍等。篇幅所限，本笔记只谈论其引导生成的部分。

具体来说，该论文使用UNet里的前半段下采样的部分加了个Attn Pooling的结构来当分类器。训练数据是DDPM在训练时的加噪结果加上一些增广策略。在具体使用梯度引导时，作者发现如果只按1比1的比例增加梯度引导时，引导效果并不好。所以一个直观的想法就是加大引导梯度的强度，使其类别性质更明显。如下图所示，使用比例为一的梯度引导生成威尔士柯基时，效果并不好，类别指定的影响不明显。但当比例放大为十的时候，效果就很直接了。

![](img/Pasted%20image%2020221128203100.png)

当然，随着引导的强度增强，多样性也会受到影响，简单来说观察这个等式。Z是因为取对数梯度引入的一项任意常数，可以取值为我们归一化的常数。那么实际上加大了引导强度后，我们是对一个更加集中的分布（更加临近于众数mode）做逼近。而这样也自然使得生成的质量会更好，但代价是生成多样性的损失。

![](img/Pasted%20image%2020221128203328.png)

这篇论文提出的图像引导生成方法的优点在于提出了一个权衡生成质量和多样性的方案。在当时取得了比GAN更加优异的效果。但是缺点也很明显，因为在噪声图像上的分类判断无法直接复用常见的分类模型，我们必须额外训练一个新的模型。并且只能按照类别生成限制了其使用场景，下面我们看一看基于CLIP的大规模文本图像对齐的预训练模型可以如何极大地扩大其使用场景。

# 基于CLIP模型的多模态图像引导生成

#### More Control for Free! Image Synthesis with Semantic Diffusion Guidance

在上面我们提到，基于显式分类器的引导可以使得图像生成指定的类别的图像。而Semantic Diffusion Guidance(SDG)**这篇论文最大的贡献在于其扩大化了P(y|x)的定义。实际上我们完全可以把分类引导的定义拓展为文字，图像或者多模态的引导。**具体来说，我们可以将分类器重新写成方程

有了以上的定义，现在我们可以借助CLIP模型里文本和图像之间对齐的表征来做一些损失计算了。具体来说，想要使用一个文本来引导图像生成，我们可以每一步都计算现在的图像表征和文本表征的距离，使用方程的梯度来计算缩小这个距离的方向。最简单的方式莫过于余弦距离：

![](img/Pasted%20image%2020221128203405.png)

其中EI代表图像编码器，EL代表文本编码器。

在图像上，我们同样可以故技重施也使用余弦距离，但只使用余弦距离没有考虑到空间信息和风格信息。作者在这里使用了更加具体的损失。一个是考虑空间布局的对应特征图里对应位置的L2范式差，和考虑风格信息的对应特征图的Gram-Matrix（关于Gram-Matrix和风格引导的关系可以参考论文Demystifying Neural Style Transfer）.

![](img/Pasted%20image%2020221128203435.png)

![](img/Pasted%20image%2020221128203442.png)

其中注意上文的图像编码器都是针对噪声输入xt的编码器，所以作者重新训练了CLIP里的图像编码器。其中架构上在BatchNorm里偏置和缩放都改为依赖于时间步t。**但是对于编码器训练的损失作者没有使用l2范式或MSE使噪声图像和干净图像的输出对齐，而是同样使用了CLIP的对比交叉熵损失来做匹配任务。**这点笔者认为是更现实的损失设置，毕竟要求噪声图像里的卷积在漫长的时间步，变化极大的噪声方差里达到相同的编码效果非常不现实。

![](img/Pasted%20image%2020221128203501.png)

上图对比了当时的一些图文引导的效果。我们可以看到同样是图片引导，IVLR的引导只能固定整体的图片布局。而SDG既可以生成更加多样的姿态，也可以保留相应的姿态。而对于文本引导的生成其生成的图像也更为多样。

那么总结一下优缺点，SDG在当时无疑是效果十分惊艳的。但就在十天之后OpenAI发布了GLIDE，使用了下面会提到的隐式分类器引导的图像生成。于是这篇论文背后的技术就彻底地被扫进了历史的尘埃。从中也可以看到这个领域的进展有多么可怕。但尽管如此，这篇论文其中的洞见以及引导生成的方式依然值得我们学习。而这个技术方案最大的缺点依然是基于分类器所需要的额外模型训练的负担。

#### Blended Diffusion for Text-driven Editing of Natural Images

和这篇文章的思路一脉相承的还有Blended Diffusion这篇论文。一样是在DDPM生成的时候做额外的梯度引导，只是额外添加了MASK的操作，使得文本引导可以只针对具体的某个区域更改。

具体来说，作者尝试了两种思路。第一种是掩码区域用CLIP做文本引导，非掩码区域用MSE和LPIPS两样损失来保证非掩码区域不变（MSE是像素和像素间的差别，而LPIPS是块和块间的感知损失）。但这种保证背景不变的方法往往要将非掩码区域的损失加权到将近上万倍才能保持背景大致不变。于是作者就干脆让背景部分保持不变，每一步只用CLIP的引导损失（余弦距离）计算掩码区域的引导梯度。

![](img/Pasted%20image%2020221128203520.png)

具体算法如上图所示。其中值得一提的是**作者其实没有训练一个基于噪声输入的CLIP。**而是在每一步的时候使用从xt预测的噪声来预估干净的图像x0。而因为CLIP计算损失时使用的是预估的X0，其效果可能出现了相应的下降，导致了语义更新的效果不明显。于是作者使用了增广的做法求CLIP损失在增广集上的损失来降低方差，增强引导效果。

笔者认为这篇论文最有意思的地方不在于其对MASK的使用，而是在于其不训练新的CLIP和使用增广数据的平均梯度来引导的做法。

#### DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation

上面两篇文章的做法都是基于CLIP的损失来引导扩散模型的生成，其做法本身不需要对扩散模型做任何微调和修改。但Diffusion-CLIP这篇论文比较有意思的点在于，**与其将我们的梯度应用于中间噪声输出的更新，或许可以试一试直接更新底层的扩散模型使得其生成贴合CLIP的预期**。其中损失函数的定义如下，分为两项。

![](img/Pasted%20image%2020221128203536.png)

第二项id_loss很好理解，主要是我们根据噪声输入xt预测的干净图像x0应该和真正的干净图像一致。是一个L1范式的损失。但第一项L-direction比较有意思。上面我们提到过**余弦损失函数有一些缺点，比如其引导后生成的多样性低，其引导容易效果不明显（类似于对抗攻击）。**而下面定义的这个新损失函数，其中delta_T是原文本和目标文本向量间编码的差，delta_I是原图像和预测图像编码向量的差。用一件去这两个差值的余弦距离就是我们的新损失函数。

![](img/Pasted%20image%2020221128203550.png)

这个损失是由CLIP的作者也是GPT系列的作者Alec-Radford提出来的。**这个损失使用了目标文本间的方向差来指导编码图像间的方向，使得生成的效果多样性得到提升。另一方面也缓解了上面的模式坍塌，引导效果不好的表现。**

除此之外，笔者在文章开头时提到了DDIM可以使得我们对一张图扩散后，再去噪生成时仍能精确复原原图。Diffusion-Clip也使用了类似的技术。但总结来说，这篇论文针对每一个新的性质引导（即新的目标提示y_tar)都需要重新训练一个新的扩散模型使得该技术路线相对其他受控图像生成没有任何优势。

#### Diffusion Models Already Have a Semantic Latent Space

这篇论文是一篇来自2023的ICLR会议投稿论文，目前仍在评审中。标题起得非常引人注目。全文的核心思想承接Diffusion-Clip，依然是使用CLIP的损失对扩散模型进行微调。但不同的是，**作者认为无需微调整个扩散模型，而只需微调扩散模型里UNet架构里最中间的那一层就可以**了。作者称该层是h-space, 也就是题目里的语义潜在空间。除此之外，**论文的亮点还包括将DDIM采样时做扩散引导的整体流程拆成了三个部分**。第一个部分是有引导部分。第二部分是无引导无方差的DDIM采样，第三部分是使用DDPM采样。我们知道DDIM的采样生成质量其实是低于DDPM的（或者说SDE虽然采样速度慢但生成效果比ODE好，这个现象直到最近才被解决，关于DDIM采样质量为何更低有兴趣的读者可以对比笔者的理解在附录[2]下和笔者讨论）。所以这篇论文针对这个现象，将引导拆成的三部分对应了几个问题：首先图像的空间布局整体语义等低频信息是在去噪的早期被决定的，所以引导可以只放在早期。其次我们可以兼顾采样的速度只在生成最终的高频信息时切换回DDPM。同时如果过长的DDPM生成，因为其方差项或者说SDE的随机性存在，过长的第三阶段可能会更改图片的语义信息。所以这个阶段作者经验性地设为了全局去噪长度的四分之一。

这篇文章给出的图片生成效果十分惊艳。但具体仍有几个缺陷。其一是文章最重要的理论theorem 1的推导是基于一个错误的前提，导致整个推导是错误的。其二是关于hspace为何有效为何是UNet的最中间层没有足够说明。其三是该论文的做法依然与Diffusion-Clip一样需要每一个性质训练一个新的模型，哪怕这个模型可迁移性较好，训练成本较低依然难以让人接受。

笔者这篇笔记大体是以时间和技术体系更迭的顺序写的。以上的所有论文都是直接基于无条件生成的语义引导所做的工作。随着新的更强大更便捷的模型如Stable-Diffusion, Imagen等如雨后春笋般涌现，上面的各项工作可能只剩下了借鉴意义。

## 基于隐式分类器的文生图大模型

#### Classifier-Free Diffusion Guidance

上面我们提到了显式分类器需要额外训练一个分类器，并且往往会造成多样性下降的特点。这篇论文最大的贡献在于提出了一个深刻的洞见：**即我们可以用贝叶斯将显式分类器的梯度引导再拆解为两项****其中一个是无条件生成的梯度预估模型（例如常规的DDPM），另一个是基于条件生成的梯度预估模型（条件生成可以建模为UNet+cross-attention）。而我们甚至可以使用同一个模型同时表示两者，区别只在于生成时是否将条件向量置为零即可。**（具体的数学推导可以参照附录[4]）

![](img/Pasted%20image%2020221128203725.png)

一方面这大大减轻了条件生成的训练代价（无需训练额外的分类器，只需要在训练时进行随机drop out condition来同时训练两个目标），另一方面这样的条件生成并不是以一个类似于对抗攻击的方式进行的。而同时，仔细观察我们上面的采样式子，实际上它是两个梯度的差值所形成的。如果有玩过stable-diffusion-webui的读者可能会注意到其中一个feature是negative prompt. 也就是使用者可以指定不想要出现的提示，让生成不会包括该内容。其实现方式实际上就用了上面这个无分类器引导的公式。将无条件生成转为带着你不想要的提示的条件生成。

**无分类器引导可以说是GLIDE/Stable-Diffusion/Imagen的做法的直接奠基工作之一。**在此之前要做到多模态或者说文生图的引导生成，通常大家用的是clip模型，来对生成的图像和文本的距离做一个损失差值，用这个差值来引导多模态生成。但有了classifier-free这篇论文之后，文生图或者图生图都可以用一个模型，以cross-attention的方式条件于该信息来引导生成，并且生成效果更好更精确。这也使得前序的相当多工作，只剩下了借鉴意义。

限于篇幅，在这个章节里笔者将不会对Stable-Diffusion/Imagen/Glide等论文背后的技术做详细解析，感兴趣的可以在附录[6]里查看。

# 在隐式分类器上引导生成过程中的调控生成

梳理了上面的多篇论文后，我们终于在时间上赶上了最新的文生图时代（2022.11）。以下笔者将介绍一些直接基于这个时代的文生图模型如Stable-Diffusion/Imagen做引导生成的论文。其中笔者将其粗略地分为了两类，一类是需要微调，一类是不需要微调的技术路线。他们各有千秋。下面笔者将首先介绍一些谷歌系列的基于Imagen模型所做的引导生成工作。**他们都需要微调且核心思想都非常接近**。注意其中不少做法也可以迁移到SD上。

在前序我们介绍的一系列论文里，我们介绍了几种引导生成的做法。第一种是根据扩散模型迭代去噪的特性，我们在模型的低频细节上继续生成。这种做法虽然能保留大部分几何特征，但是也同样**无法调控几何特征。**举个例子，如果迭代去噪的原图是一张站立的人像，我们无法通过模糊化重新生成来改变他的姿态。我们可以调控加噪的强度，让他随机变成新的图像。但即使是我们加上了语句的引导，我们也无法在引导后保留原目标的特征（如人像的外观）。

从传统的思维去思考如何达成对目标物体的调控的话，可能会涉及到语义分割，再根据条件引导调控所分割的物件的各项性质。但从这篇论文开始起，下列的四篇论文都是一个核心思想。**通过对生成模型的微调，使其将对应物体的视觉信息与一个特殊的字符绑定起来（如同树的视觉外观与树这个字符的绑定一样），之后再将其当作正常的语言字符使用来添加调控信息。**打个比方，将一颗有具体形象的树用字符x来表示之后，用户可以指定“正在燃烧的x”，“发光的x”来使模型对指定形象的树进行修改。

#### Imagic: Text-Based Real Image Editing with Diffusion Models

具体来说，Imagic将概念绑定这件事拆成了三个步骤，对于输入图像x和我们希望生成的目标描述文本text_target来说：

1：我们首先冻结整个模型，使用模型训练时的生成目标来微调text_target的文本表征，使其接近于图像的表征。

2：我们放开整个模型的权重更新，依然使用训练时的生成目标，但这次全模型微调。模型的输入是图像x和我们微调后的文本表征。这一步是因为哪怕我们让目标文本表征和原图的表征接近了，也不能保证我们输入让我们微调后的目标文本表征可以生成我们的原图，所以我们再次将这两个概念一起训练，**使得我们可以使用微调后的目标文本表征生成我们的原图**

3：既然我们已经将原图和微调后的新文本表征绑定起来了，现在我们再使用**原本的目标文本表征**与微调后的文本表征做插值，来对原图像施加影响即可。

这三步有一点绕，但简单来讲可以将微调后的目标文本表征近似当作原图像原生的文本表征，那么最后一步使用目标表征对原生的表征施加影响就非常自然了。

![](img/Pasted%20image%2020221128203747.png)

注意上图里SDEdit无法改变空间布局，而Imagic可以改变狗的姿态动作而不影响背景的特点。

当然，这篇文章也有一些细节的地方需要注意，比如第一步微调时不能过久，使新的文本表征过分远离原本的目标文本表征，这会使得线性插值失效。

#### UniTune: Text-Driven Image Editing by Fine Tuning an Image Generation Model on a Single Image

相比于Imagic的做法，这一篇作者的做法更加偏工程调参和不具有泛化性一些。具体来说依然是概念绑定。作者分为两步：

1：第一步先选定一个低频的词汇或者干脆就是随机组合的字符词如"beikkpic"，这样可以最小化其原有含义的影响。接着对整个模型微调这个低频词和原图像的组合，使其概念捆绑。

2：第二步时则使用微调的模型对“[低频词]+目标引导描述”生成。如"beikkpic a monster"

其中，作者发现如果仅仅是使用简单的文生图策略，目标引导基本失效。于是作者使用了以下一些技巧：**使用前面提到的无（隐式）分类器引导配合一定的权重放大目标引导的影响。**我们可以看到随着权重的增加，影响的效果也在逐渐强化。其次，作者为了保证尽量生成与原图接近的结果，也同样使用了类似上面提到的SDEdit的做法，从原图的中间噪声结果出发重构，而不是从纯噪声出发重构来增强一致性。最工程性的一点是原作者还实验了将生成图和原图做像素间的插值可以提升一些生成图片的质量。

![](img/Pasted%20image%2020221128203805.png)

要说这篇文章的缺点很明显来自两方面，一方面这个文章的做法虽然通过概念绑定使得额外的性质调控成为可能，但其类似于SDEdit的做法也使得其所有结果的几何构图都和原图一致。无法像Imagic一样做到姿态改变等。而第二个缺点则很明显是这篇文章的做法太过经验性，需要不断的调试，泛化性值得怀疑。

[DreamBooth](../DreamBooth/DreamBooth.md)

最后笔者将介绍两篇无需微调的工作。他们同样也可以在Imagen/SD上互相迁移。他们有一个特点就是**无需一个显式的Mask，可以只用文本来生成掩码来找到文本对应的修改位置，再辅以文本调控生成的手段。**

#### DiffEdit: Diffusion-based Semantic Image Editing with Mask Guidance

我理解的之所以DiffEdit会叫这个名字，是因为可以拆成两个部分。Diff + Edit。其中Diff不仅仅指扩散Diffusion还可以指Difference差异。而事实上，这篇论文生成掩码的方式靠的就是两次扩散的差异。

![](img/Pasted%20image%2020221128203855.png)

上面这幅流程图清楚地揭示了整个算法的流程。首先我们将原图片加上随机的噪声到达较为接近纯噪声的某个幅度（如五成至六成附近，这个比例是由实验得出的）。接着我们再分别使用描述原图片的原文本和目标文本所去噪的结果的像素差值来得到一个掩码（比如上图里马形的掩码）。之后我们再使用DDIM的确定性加噪方式，将图片加到一定程度后，使用类似前面我们提到过的方式分别处理掩码区域和背景区域。

当然在生成过程中为了得到稳定的掩码类似我们前序提到过的数据增广的方法也是必不可少，比如极值去除后的多次计算结果的平均后再映射至[0, 1]区间内二值化。同时第二步里的DDIM的前向加噪比例实际上也调控了我们能有多长的区间来影响生成的步骤数。举个例子，如果确定性加噪至百分百，当然影响生成的时间更长，可往往生成的结果的偏差也越大。在下图里可以清晰地看到这样的趋势。

![](img/Pasted%20image%2020221128203910.png)

这篇文章虽然需要更多次的前向和去噪过程，但其不需要额外训练的特点是其一大优势。其次尽管目前市面上大部分的文图工具都配有掩码生成的涂抹工具可以轻松获得图像的掩码，但文字的调控依然在一些复杂的情景上，如要对整棵树的花进行改变的情况下比手动涂抹方式有优势。最后放上一张DiffEdit目前仍有缺陷的调控场景：

![](img/Pasted%20image%2020221128203922.png)

#### Prompt-to-Prompt Image Editing with Cross-Attention Control

这篇文章同样也是2023年ICLR的投稿论文，不同于上一篇我们提到存在不少问题，这篇论文在OpenReview上获得了8-8-8的一致高分。该论文以另一种方式得到掩码来进行精细化地调控。这篇文章的洞见来自于一个重要思考：**即多模态里文生图的文本是如何对生成过程施加影响的？**我们在前面提到过，基于隐式分类器的文图模型是通过训练一个既可以做无条件生成的梯度预估，也可以做条件生成的梯度预估的模型实现的。而其中这个条件交互的方式在Imagen和Stable-Diffusion里都是通过cross-attention实现信息融合的。那么很明显，我们的着眼点也应该在cross-attention上。而作者的洞见则在于：**我们输入的文本和像素之间存在着一个空间对应的关系。通过调控注意力和像素间的映射。我们能够对图像的不同区域实施准确的引导。**这个洞见很好理解，我们知道注意力机制的本质是加权，是通过求向量间（在本场景是多模态的嵌入向量）的距离来得到一组权重的过程。具体来说，在transformer里运用到的scaled-attention计算公式如下：

![](img/Pasted%20image%2020221128203942.png)

其中M就是我们的二维权重矩阵。M_ij代表第j个token对第i个像素的权重。作者称这个矩阵为cross-attention-map。而观察上式可以发现，**图片的空间布局，几何形状等信息实质上高度依赖于该注意力映射矩阵。并且文本和像素组成的空间信息存在明显的对应关系**（如下图里熊这个单词与熊的外形轮廓高度重合）。

![](img/Pasted%20image%2020221128203957.png)

有了以上洞见据此进行图像引导生成就很直观了，作者将其分为三个主要场景：**单词替换**（比如在上图里将熊换成猫则将猫这个token对应的map换成熊的map），**单词增添**（在原有的map上增加新的单词的map），**注意力重加权**（如果想放大或减弱某个词对原图的引导效果则对其map乘上新的权重值，如降低下雪的效果开花的程度等）。

![](img/Pasted%20image%2020221128204008.png)

其中一些值得注意的部分，比如体型相差过大的物体如熊和猫，作者通过调控map注入的步骤的多寡和不同的词注入的时间来松弛map的约束。如果是局部修改，为了保证背景的绝对不变，作者也通过两次不同的cross-attn-map的计算的差值得出一个mask之后再分别引导。方法和上面提到的DiffEdit非常相似。

**参考资料**  

1.  _扩散模型背后的数学：https://zhuanlan.zhihu.com/p/558937247_
    
2.  _DDIM如何加速采样和进行确定性采样：https://zhuanlan.zhihu.com/p/578948889_
    
3.  _扩散模型与能量模型，随机微分方程和常微分方程的关系：https://zhuanlan.zhihu.com/p/576779879_
    
4.  _扩散模型里的显式和隐式分类器引导生成：https://zhuanlan.zhihu.com/p/582880086_
    
5.  _扩散模型在文本生成里的应用：https://zhuanlan.zhihu.com/p/561233665_
    
6.  _Stable-Diffusion和其背后的相关论文详解：https://zhuanlan.zhihu.com/p/572156692_

## 参考资料

[【Diffusion模型】由浅入深了解Diffusion，不仅仅是震撼，感受它带给我们的无限可能！！（超详细的保姆级入门教程）](https://www.bilibili.com/video/BV1ne411u7J6)

[读了14篇论文，终于会拿捏Diffusion了](https://mp.weixin.qq.com/s/brvSAAmhkSKTTOXZqT0HKQ)

[diffusion model最近在图像生成领域大红大紫，如何看待它的风头开始超过GAN？](https://www.zhihu.com/question/536012286/answer/2533146567)




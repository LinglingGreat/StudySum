

传统多模态任务：图文检索，视觉问答，视觉蕴含等

## 文本图像模型结构

![](img/Pasted%20image%2020230205152139.png)

a和c都用了预训练的目标检测器，训练和部署困难。

vision transformer出来之后，ViLT应运而生。其中基于patch的视觉特征和以前的基于bounding box的视觉特征没什么太大的区别，也能拿来做图片分类、目标检测。所以可以用patch embedding代替预训练的目标检测器，降低运算复杂度。

但是这样的embedding太简单了，所以模态融合就变的很重要。模态融合用了transformer encoder。

ViLT的缺点：性能不够高，比不过c，现有的多模态任务中需要更多的视觉能力，视觉模型不够强，应该要比文本模型大。另外训练成本也很高，不亚于c中的。

CLIP（用了对比学习ITC loss）擅长图文匹配，只需要点乘，embedding也可以提前存储。

如果想做一个很好的多模态学习，模型结构应该很像c，视觉模型比文本模型大，模型融合也足够大。CLIP的ITC loss，另外还需要语言模型的MLM loss，ITM（Image Text Matching）Loss

ALBEF就是这样的模型 [ALBEF](../ALBEF/ALBEF.md)

[VLMo](../VLMo/VLMo.md)


## Transformer结构

[BLIP](../BLIP/BLIP.md)

[CoCa](../CoCa/CoCa.md)

[BeiTv3](../BeiT/BeiTv3.md)

## 总结

![](img/Pasted%20image%2020230205175506.png)

language interface: metaLM, PaLi(通过prompt确定做什么任务)

generalist model: unified-IO, uniperceiver, 
generalist model: unified-IO, uniperceiver-MOE

## 参考资料

[【多模态论文串讲·上【论文精读·46】】 ](https://www.bilibili.com/video/BV1Vd4y1v77v)

[多模态论文串讲·下【论文精读·49】](https://www.bilibili.com/video/BV1fA411Z772)




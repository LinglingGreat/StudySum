

## 现有大模型

![](img/Pasted%20image%2020230227103950.png)

[主流大语言模型的技术原理细节](https://mp.weixin.qq.com/s/P1enjLqH-UWNy7uaIviWRA) 1.比较 LLaMA、ChatGLM、Falcon 等大语言模型的细节：tokenizer、位置编码、Layer Normalization、激活函数等。2. 大语言模型的分布式训练技术：数据并行、张量模型并行、流水线并行、3D 并行、零冗余优化器 ZeRO、CPU 卸载技术 ZeRo-offload、混合精度训练、激活重计算技术、Flash Attention、Paged Attention。3. 大语言模型的参数高效微调技术：prompt tuning、prefix tuning、adapter、LLaMA-adapter、 LoRA。

[LLMs](Models/README.md)

## Prompt
- [ ] todo


## Instruction tuning

[Flan-PaLM_T5](Alignment/Flan-PaLM_T5/Flan-PaLM_T5.md)


## CoT

总结：[CoT](CoT/CoT.md)


[Why did all of the public reproduction of GPT-3 fail? In which tasks should we use GPT-3.5/ChatGPT?](https://jingfengyang.github.io/gpt)

## Benchmark

[Benchmark](Benchmark/README.md)

## dataset


## Alignment

[Anthropic 关于 RLHF 的研究](https://mp.weixin.qq.com/s/5LHwam2B4goElW5vPiyp9g)


## 资源列表汇总

【收集了有关大型语言模型中不确定性、可靠性和鲁棒性的资源和论文】: github.com/jxzhangjhu/Awesome-LLM-Uncertainty-Reliability-Robustnes


【'Xwin-LM：旨在开发并开源大型语言模型的对齐技术，包括监督微调(SFT)、奖励模型(RM)、拒绝采样、人类反馈强化学习(RLHF)等】’Xwin-LM: a collection of LLM alignment technologies and models' GitHub: github.com/Xwin-LM/Xwin-LM

[GitHub - MLGroupJLU/LLM-eval-survey: The official GitHub page for the survey paper "A Survey on Evaluation of Large Language Models".](https://github.com/MLGroupJLU/LLM-eval-survey)

[GitHub - RUCAIBox/LLMSurvey: The official GitHub page for the survey paper "A Survey of Large Language Models".](https://github.com/RUCAIBox/LLMSurvey)

[GitHub - AIoT-MLSys-Lab/Efficient-LLMs-Survey: Efficient Large Language Models: A Survey](https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey)

[GitHub - hyintell/awesome-refreshing-llms: EMNLP'23 survey: a curation of awesome papers and resources on refreshing large language models (LLMs) without expensive retraining.](https://github.com/hyintell/awesome-refreshing-llms)

[Hugging Face 年度回顾: 2023, 开源大模型之年](https://huggingface.co/blog/zh/2023-in-llms)

故事生成的系列论文和数据等： https://github.com/yingpengma/Awesome-Story-Generation

[浙大、微软等发布最新综述，深入探索语音对话模型的前沿进展](https://mp.weixin.qq.com/s/49LriNT5aOt6Mm3yfq4RHA) 

## 训练情况对比



baichuan1:

baichuan2: 2.6T tokens

qwen

llama




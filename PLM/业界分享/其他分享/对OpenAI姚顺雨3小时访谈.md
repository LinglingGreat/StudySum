---
title: 对OpenAI姚顺雨3小时访谈
created: 2025-12-27
tags:
  - 播客
---
对OpenAI姚顺雨3小时访谈：6年Agent研究、人与系统、吞噬的边界、既单极又多元的世界 - 张小珺Jùn｜商业访谈录

[微信公众平台](https://mp.weixin.qq.com/s/2sNq-AMGP3CODOvkqxrb8w)

# 第一章 序

## 01 人—“我一直有这个非共识，我想要去做Agent”


2025年4月，OpenAI研究员姚顺雨发布了一篇有名的博文《The Second Half》，宣告AI主线程的游戏已进入下半场。

姚顺雨毕业于清华和普林斯顿大学，博士期间意识到语言是人类发明的最重要的工具，也是最有可能构建通用系统的，于是转向Language Agent研究，至今已6年。

姚顺雨表达了许多此前从未分享过的观点。比如：

- 创业公司最大的机会是：能够设计不同的**interface****（交互方式）。
    
- **最终，借助模型的能力或许能产生****beyond ChatGPT****（超越****ChatGPT****）的交互方式，变成****Super App****（超级应用）。
    
- **我们的想象力仍被以往的交互方式所限制，还有许多尚未诞生的交互方式。这些新的交互方式，会改变我们的世界。**
    
- **OpenAI****可能会成为新世界里非常重要的一环，但这并不代表，这个世界会被这样一个单极系统垄断。如果这样，世界就太灰暗了。
    
- **最终智能的边界，可能不是由一家机构定义，而是由不同****Super App****共同定义的。
    
- **也许，这个世界在变得越来越单极的同时，也在变得越来越多元。**

个人经历：

本科从合肥考到清华，读姚班。在姚班大家会告诉你去美国读PhD，我就去美国读PhD，我在普林斯顿读PhD。读PhD之后很自然，OpenAI是做research（研究）最好的地方，就加入OpenAI——感觉我前28年的人生，非常的乖。

**15-19****年在清华姚班，****19-24****年在****Princeton****，****24****年毕业进****OpenAI

**张小珺：****Agent****或****Language****最吸引你的是什么？

**姚顺雨：**是它的可泛化性（generalizable）。绝大多数事，你都可以用语言表达。

我当时隐隐约约有个直觉：你如果真想去实现AGI（通用人工智能）——那时还没人提“AGI”这个词——**但如果你真的想做一个非常通用的系统（****general system****），你就得去构建一个智能体。**

回头看AI历史，很久很久以前，从Herbert Simon（赫伯特·西蒙）在1960年代开始，大家最早的想法就是要做一个Agent。当时大家的野心很大——想用一个夏天搞定视觉，再用另一个夏天搞定语言，拼在一起，去做一个Agent，他就应该比人还聪明。

但这事太难了。慢慢地，AI变得非常碎片化。大家研究的问题越来越小。比如，有的人研究视觉一小部分问题，有的人研究语言某个子任务，越来越细分，越来越垂直。

但到2015年之后，开始出现Scaling Law（扩展规律），包括很多研究突破，历史上一些关键时刻也在提示我们：也许我们应该从这种“垂直式思维（vertical thinking）”重新回到更“通用式思维（general thinking）”，再去尝试构建真正通用的系统。

**张小珺：当你进入****Agent****系统做研究，要让语言模型真正行动起来，你意识到最重要的几件事是什么？**

**姚顺雨：****第一年最大收获是：要用****GPT****，不要用****BERT****。**

- **BERT：“来自Transformer的双向编码表示”，由Google AI在2018年发布的一种NLP预训练模型。**
    

可能现在很多人不知道BERT，当时语言领域最火的模型叫BERT。想法是：我有一句话，通过某种方式学到这句话的一个表示，通过这个表示做很多下游任务，比如做一些单选题，或者基于选择的任务。

当时95%的人做BERT，只有5%的人做GPT。这也是因为当时NLP的主要任务都是一些：我有一句话，这句话是积极的还是不积极的；我很讨厌这个电影，这是一个负面的句子。都是非常简单的事。这种事BERT确实效果更好。

但你会发现，如果你要做一个language Agent，你需要的不只是选择能力，而是去自由产生新动作的能力。

当然如果你在玩围棋，或者视频游戏，选择有限。如果你玩马里奥兄弟，他就是上、下、左、右。但如果你玩基于语言的游戏，动作是自由的。比如我在这个游戏可以用剑杀怪兽，或者我可以去第三个房间，或者我可以用金色钥匙打开第一个房间的门。BERT永远做不到。

世界的本质就是，你的行为空间是open-ended（开放）的，这种在开放空间决策的能力BERT永远做不到。我发现这个之后，就再也没用过BERT。

**第二个****learning****是：任务或环境非常重要。**

当你有一个非常差的任务，你永远不可能学到非常好的东西。当时有很多人在做：这个句子是正面的还是负面的？a这句话能不能导致b这句话成立？当时这些任务看上去很难，现在看非常简单。

首先你要找一个足够有挑战的任务，这个任务能做出本质的新方法。当时你想去做Agent或语言Agent，实际上没什么选择，只能去做文字游戏。

Zork是个非常经典的文字游戏。你在一个基于文字的世界里，有点像一个互动脚本，可以往下走、往上走，可以去各个房间，可以做各种各样的事。

但你会发现，这个环境还是有很多缺陷，能学到的局限在这个环境，这个环境还是不够大。而且你如果用RL学这个环境，就会像用RL学传统的视频游戏，可以把这个游戏打通关，但对于其他任务没有迁移作用。你可以把围棋下得特别好，但对世界上其他事情没有价值。

我们需要一个更好的环境。

**张小珺：你博士期间的研究工作：语言智能体（****Language Agent****）、****ReAct****（浏览维基百科进行推理）、****Reflextion****（反思）、****Tree of Thoughts****（思维树）、****digital automation****（数字自动化）、****WebShop****（网上购物）****——****这些研究跨度很大，它们的共性问题是什么？你是怎么按着兴趣一步一步延伸的？**

**姚顺雨：**从我的角度，是非常自然的过程。当我意识到环境有问题，我第一个比较重要的工作是WebShop，**首先要解决环境问题。**如果没有一个好的任务或环境，把这个游戏刷得再高，没有意义。

2015年有一个非常好的工作叫World of Bits（比特世界）。当时想法是，我们应该把电脑或互联网作为一个环境，这个环境比游戏更exciting（令人兴奋）。但因为各种技术限制，没有做得特别好。到2021年，我和导师讨论，觉得这时可能是一个自然的时间点重新去做。

我当时也觉得，技术还没完全成熟，大多数人还在研究一些比较标准的任务：a能不能导致b，或者翻译，或者从一篇文章回答问题。那个阶段想做互联网上的Agent，技术还没ready（准备好）。但也正因为技术没成熟，反而是一个好的时间点开始做。到2022年，我们就做了WebShop这个环境。

2022年，GPT-3.5发布，还有后来Chain of Thought（思维链）出现，带来新的方法层面上的机会。我们就做了ReAct这个工作。我现在还是觉得，我自己最喜欢的工作是ReAct。

- **ReAct: Synergizing Reasoning and Acting in Language Models，在语言模型中协同推理与行动，是一种让大语言模型在与外部环境交互时，同时进行“推理”和“行动”的方法框架。**

之后，基于这两个方向：一方面做更多方法（method），一方面做更多任务（task）。

但总体来说，我的研究有两个核心：

- 一是怎么去做一些有价值、和现实世界更相关的任务和环境；
    
- 二是怎么去做一些简单、但又通用的方法。
    

**张小珺：****ReAct****的提出标志了范式的变化吗？**

**姚顺雨：**这需要5年或10年以后再去看。

当时学术界还不太能接受，我去做一个prompting（提示工程），把它作为research（研究）。传统意义上，你需要提出一些fancy（花哨）的东西——需要提出一些数学公式，训练一个模型，证明很多理论，或者做很多工程上的事。但如果你只是去用一个模型，感觉太软了。

不过，当时最有价值的，就是去研究怎么使用模型。如果你想训练模型，会落后OpenAI或这些公司好几年。你做的很有可能几年前别人已经发现了。如果你想做不一样的，可能怎么去使用模型更有价值。

**张小珺：为什么你做这件事情比大部分人都早？**

**姚顺雨：**有幸运的部分，我PhD做的第一个事就是基于语言模型做Agent。当时做的人很少，因为它太难了，或者不是一个共识类的事情。当时共识类任务是做问答，做翻译，或者做一些已经被社区接受的任务。

**我一直有这个非共识：我想要去做****Agent****。**

**另一点是，我一直想做简单且通用的东西。**我不想做一个很复杂、但只能在一个领域奏效的东西。这个方向在传统意义上很难被接受，大家习惯了做AI的方式：把问题不停细分，做很多细分方法。

可能并没有多少人想做一个简单且通用的系统，或者认为这是可能的——尤其20年之内。

# 第二章 系统

  

## 02 **机器的手**—“人最重要的affordance是手，AI呢？”

---

张小珺：今天我们的话题是**Agent****和强化学习，我们很好奇你会怎么定义****Agent****？

**姚顺雨：** 这是一个很好的问题。要结合讨论背景看。

从自然语言处理的角度，Agent是相对于一个只会生成文章或对话的系统而言。它能和外界交互，比如使用计算器、互联网，或调用各种工具。也就是说，不仅能生成内容，还能操作和互动。

但从更广义的AI背景看，Agent是一个非常古老的概念。

**任何能进行自我决策、与环境交互，并试图****optimize reward****（优化奖励）的系统，都可以被称为****Agent****。

从这个角度出发，今天我们讲的Agent更多是指：怎么基于语言模型这样的foundation model（基础模型）去做具备自我决策能力的系统，而不是传统意义上基于规则或仅在某个领域用强化学习（RL，Reinforcement Learning）训练出来的Agent。

因为“Agent”这个词在不同时代有不同定义——你可以说AlphaGo是Agent，也可以说Waymo是Agent，甚至可以说机器人是Agent。这个词的意义很依赖具体情境。

**张小珺：你研究的****“Language Agent”****（语言智能体）和传统****Agent****，存在本质区别吗？

**姚顺雨：****本质区别是可以推理，因为推理才可以泛化。

举个简单的例子，我做ReAct一个很强的动机是：我做完colm，我的第一个工作之后，在思考一个问题——为什么我可以一下子去玩一个新的游戏，但现在这些系统或AI需要几十万步甚至几百万步训练，才能完成类似的事？

我发现，是因为我可以思考。我看到一个全新的环境，会想：这个灯是黑的，那可能有危险，基于常识可能有怪兽；我现在最重要的是点亮灯。基于之前的上下文（Context），灯在我后面，那我应该先向后走。

如果没有这样的思考能力，而是直接从复杂语言去预测“我要往后走”，就很难——没有推理做不到。

**最大区别在于，语言模型提供了一个足够强的先验（****prior****），这个先验让你可以推理，而推理又可以在不同的环境间泛化。

所以核心是推理能力，推理才能带来泛化。

**张小珺：从你的视角看，****Agent****是一个怎样的演变历程？它是怎么一步步发展到今天的？

**姚顺雨：** 我可以说一下自己的理解，可能不完整，或者有一些错误。

最早的AI，我们称为Good Old-Fashioned AI（符号主义AI），想法很简单：我注重的是推理，我怎么想，就把这些规则写出来，让AI也这么做。比如，如果温度高于30度，空调就应该降温。这种基于规则的AI，可以造出很多早期智能体，比如最早的机器人、最早证明数学定理的系统，很多是这么做出来的。

但很快，1980年代，大家发现这个东西有瓶颈——你不管写多少规则，还是很难覆盖这个世界上所有可能发生的情况。

那时符号主义走向极致，大家开始做专家系统：找很多专家，把这世界上所有可能的规则都写下来，是不是就能得到AGI？或者一个通用的、有用的系统？

但后来发现，无论你写多少规则，还是有很多特殊情况无法处理。这些规则只能用于一个任务。比如你写了一个诊断心脏病的系统，写了很多规则，但人千变万化，你没办法处理所有情况，这个系统也没法处理肺病。导致了第一次AI寒冬。

后来我们有了新的神经网络（Neural Network），也就是第二波Agent兴起，标志是Deep Reinforcement Learning（深度强化学习）。典型事件是DeepMind玩视频游戏、做AlphaGo，OpenAI玩机器手、打Dota。

这一波核心是：我有一个虚拟环境，可以无限次尝试，有奖励机制，还有通用网络架构，我就像黑盒一样去学怎么maximize reward（最大化奖励），它就变强了。

这个方向取得了很多成功，最有名的是AlphaGo。但还是有老问题：每做一个新环境，都要做很多Environment-Specific（环境特定）工程。比如做Dota，要调很多参数（parameter tuning），做很多基于这个环境的工程。最大问题是：这些方法没法泛化。

你学了一个围棋Agent，没办法玩别的游戏。你在一个环境里学到的东西，没办法迁移到另一个环境。这肯定是不理想的。而且，如果你所有能解决的问题都在虚拟环境里，或者是像游戏那样可以无限次玩的环境，你就没法找到很好的真实世界应用。

第三波Agent是从大语言模型开始的。我们发现它可以做推理，而基于推理，就能进入一些新的环境，比如编程、互联网、各种数字环境。这些数字环境有一个共性：大多数都是基于语言的，需要推理。

这一次Agent的核心区别有两点：一方面是方法上，我们使用语言模型，用推理去构建能处理各种问题的Agent；另一方面是环境本身也发生了进化，从早期符号主义环境（比如数学定理），到下围棋、打游戏，再到今天互联网、编程、电脑操作这些更接近真实世界的数字环境。

**所以这是两条线：一条是方法线，一条是任务线。**

**大家可能更多注意到方法线，容易忽视任务线。但这两条线是相辅相成的。**

**张小珺：我一直有一个基础疑问。****OpenAI****提出的大模型能力分级从****Level 1****到****Level 5****，很多人都很熟悉了：

- **Level 1** **是聊天机器人（****Chatbot****）**
    
- **Level 2** **是推理者（****Reasoner****）**
    
- **Level 3** **是智能体（****Agent****）**
    
- **Level 4** **是创新者（****Innovator****）**
    
- **Level 5** **是组织者（****Organizer****）**
    

**但这个五级划分的内在逻辑是什么？为什么是先有聊天机器人、推理者，然后才是****Agent****？****Level 4****和****Level 5****又是怎么来的？它们之间是递进关系吗，还是各自独立发展？**

**姚顺雨：** 逻辑是，首先你要有语言的先验知识。基于语言的先验知识，最早能做出来的应用是Chatbot（L1）。接下来，基于语言先验，你需要具备推理能力，这是Reasoner（L2）。

当你既有语言知识，又具备推理能力，才可能进一步做各种Agent（L3），尤其是能泛化的Agent。也就是说，Agent建立在Chatbot和Reasoner能力之上。

很明显，**今天****Agent****发展最关键的两个方向：**

- **一个是让它拥有自己的****reward****（奖励），能自己探索；**
    
- **另一个是****Multi-Agent****（多智能体），让它们之间能形成组织结构。**
    

这两个方向，我觉得是正交，它们可以并行发展。

谁是Level 4，谁是Level 5，我不确定。但这两个事情是显然的下一步。

**张小珺：从****Level 2****到****Level 3****，也就是你做的这一步****——****从训练模型到使用模型，是一个很重要的跨越。**

**姚顺雨：** 或者说，是从单纯做推理，到把推理应用在Agent上，用它去和环境交互。

**张小珺：****Agent****目前有哪些主流架构？形成共识了吗？**

**姚顺雨：** 我的感觉是，大多数时候大家用的还是类似ReAct架构。你能够去推理，然后你可以产生action（行动）。这是最简单的一种形式。但最简单的反而是效果最好的。

当然，基于不同任务，大家会设计很多workflow（工作流）或更specific（特定）的方法。但如果说最通用、适配性最强的方案，我还是觉得是类似ReAct的方法。

**李广密：提升****Agent****能力，你自己最看重的是哪几个关键能力？**

**之前有人提到****Context****（上下文）、****Long-Context Reasoning****（长上下文推理）、****Tool Use****（工具调用）或****Instruction Following****（指令遵循）。你刚才一直强调****Reasoning****（推理），那如果要提升****Agent****的能力，你最在意哪些能力维度？**

**姚顺雨：** 这是个很好的问题。我觉得现在没有一个特别成熟的taxonomy（能力分类体系），或划分系统。每个人都有自己的理解方式。

有些人会按照工具划分，比如coding（编程）能力、上网能力、使用计算机的能力，这是一种划分方法；另一种是按照模型自身的能力划分，比如多模态处理、长上下文处理、推理能力——这两种划分都有道理。

但就我现在看，我最看重的是Context（上下文）处理能力，或Memory（记忆）能力。因为只有在这个基础上，才能进一步实现Lifelong Learning（终身学习）或Online Learning（在线学习）的能力。

**李广密：你刚才一直在提环境，你认为****code****代码是一个实现****AGI****最重要的环境吗？它可以支持多轮的强化学习（****RL****）、提供闭环反馈，也可以验证结果。如果我们在代码这个环境上构建****Agent****，会不会发展更快？**

**姚顺雨：** 毫无疑问，这是最重要的环境之一。

**Code****有点像人的手。**

它某种程度上，是AI最重要的_affordance_（环境给予行动者的可能性）。

对于物理世界，**人最重要的****affordance****是手**——我们围绕它制造各种工具，比如锤子、笔、筷子。**但对****AI****、对****Digital Agent****（数字智能体）来说，最重要的****affordance****可能就是****code****。**

因为其他东西，都是给人定义的。比如网页、小说、视频，是为人类设计的；但code是一个天然就给机器使用的表达形式。

我2022年一直在想：做Coding Agent明明是很重要的事，为什么没人做？

我们当时做了一个工作叫InterCode。大家都在做的是：给一个coding task（编程任务）模型生成一段代码，然后你去evaluate（评估）它。但我们就在想：为什么不把执行结果反馈给模型？

我们可以让它变成一个多轮Agent task（智能体任务），构造成一个环境，而不是单次完成的任务。基于这个，我们后来做了SWE-bench、SWE-Agent。

- **SWE-bench是一个真实世界的软件工程基准，用GitHub上的issue和修复代码来评测模型的代码修复能力。**
    
- **SWE-Agent是一个基于大语言模型的智能体，能在SWE-bench上自主阅读代码库、修改代码并运行测试来解决问题。**
    

  

有时候，很有意思的一点：一个东西明明非常重要，但就是没人做。如果你是一个研究员，觉得你做的事很重要，但别人不觉得、也没人做，并不是坏事——可能它真的很重要，只是大家还没开始。

**李广密：这里有个很强的非共识：有的人觉得****code****是这一轮技术革命最大的价值体现，但也有人觉得可以泛化到更多任务里，在电脑、手机、数字世界中都可以实现，****Agent****操作人能做的****95%****、****99%****任务。**

**你对从****code****到数字世界这一步的跨越，或者它的泛化，是有信心的吗？**

**姚顺雨：** 更广义说，你可以认为API也是code的一部分。任何基于code的接口，都属于code环境的一部分。

有个非常经典的debate（争论）：最终的AGI，是基于API或code的？还是基于GUI（图形界面）？或者是为人定义的前端环境？还是它是一个混合体（mix）？

这个问题有点像：你是想改造你的车让它适应所有路，还是改造所有路让它适应现在的车？

很多时候，现实中并没有现成的API，只有GUI。但你可以人为为它构造一个API。

当然，最终结果很可能是meet in the middle（在中间相遇），两边都会做，而且这个事情可能没那么难。

现在看，让一个Agent既能使用code，又能操作人类界面的screenshot（截屏）、前端，两者兼顾也没那么困难。从这个角度说，让Agent像车一样能适配各种路，比起要改造所有路让它们都有API，要容易很多。

Coding肯定很重要，但如果让Agent也能操作GUI，最终Agent很可能是“什么都能做”的。

  

## 03 任务的设定—“我们对简单任务的robustness没有重视”

---

**张小珺：你****4****月发布博文《****The Second Half****》（下半场），你是怎么想到****the second half****这个****idea****的？受了什么启发吗？**

**姚顺雨：** 我是受邀去斯坦福一门课做talk，当时想，能讲点什么？没法讲太技术，只能讲更哲学的内容，就想到这个话题。

这个想法来自我在OpenAI的工作经验，以及之前做research的感悟。大家过去往往更关注模型训练、方法设计，但我觉得**现在的****bottleneck****（瓶颈）已经转移了：变成怎么去定义好的任务，怎么去定义好的环境。**

**张小珺：现在是处在那个转折点吗？从上半场到下半场。**

**姚顺雨：** 主线正从“上半场”转向“下半场”。我说的主线是基于语言的智能体。当然你也可以说，在Audio（音频）、Multimodal（多模态）、Robot（机器人）这些方向，还有很多未解的问题。

但我觉得，从语言出发，去定义Reasoning（推理）、定义Agent，我们终于有了一个非常general（通用）的方法，而且这个方法是可泛化的——我们实现了一个基点时刻**。**

这带来一个本质变化：以前我面对很多怪兽，需要造出各种不同武器去打它们；现在我有了一把通用武器，比如机关枪，我不需要再为每个怪兽单独造武器。接下来要思考的问题就变成：我该朝哪个方向开枪？

现在方法的问题已基本解决，真正重要的是——我们要用这个通用方法，解决什么问题？

**李广密：怎么设定任务？怎么定义问题？关于这个，你在探索过程中有什么思考吗？**

**姚顺雨：** 不同的人有不同的flavor（风格），我从很早就有一个偏好：我想定义一个基于结果的reward（奖励），而不是基于过程的；而且这个reward应该是基于规则、可计算的，而不是来自人的偏好、模型的偏好，或者一些黑盒指标。

我们做WebShop的时候，最困难的一点是，怎么定义reward。我觉得做任何RL（强化学习）任务最难的不是建环境，而是怎么设计reward。你当然可以把Amazon或Facebook模拟出来，工程上确实很难，但总是可以做。但最难的，是怎么设计一个既有难度，又有实际价值，同时又有一个好的reward的任务。

我希望这个reward是不noisy（不噪声大）的，是可解释的，是白盒的（white-box），不是那种黑盒的东西（black-box）。

事实证明，这也是现在RL成功的关键。像math（数学）和coding（编程）这种任务，之所以能做出来，核心就是：

- Reward**是基于结果，不是基于过程；**
    
- **Reward是白盒的、基于规则的，不是基于人的偏好或模型的偏好。**
    

比如，一个数学题答案是3，它就是3——只要你得出的是3，就是对的；不是3，就是错的。

但如果你reward是基于过程，就会出现hacking（投机取巧）。你去优化人的偏好、模型的偏好，也会出现hacking。比如你生成一段非常优美的代码，但它并不解决实际问题。

我后面做的很多task，也都是用同样的filter（筛选标准）。

比如SWE-bench这类工作：

- 第一，它是结果导向，而不是过程导向；
    
- 第二，它的reward是基于规则、白盒的，而不是来自人或模型的主观偏好。
    

**张小珺：就像上面说的，****OpenAI****有****5****个分级。如果从任务定义出发，是不是也可以做出一套产品能力的分级？随着模型能力溢出，我们开始使用这些能力，****Agent****能力可以怎么分级，你脑海中有没有一个初步的框架？**

**姚顺雨：** 我现在倾向于认为，不同类型应用会带来不同challenge（挑战）。这些挑战是正交的，很难说哪个更难、哪个更简单。

人类也有这个问题——洛克菲勒和爱因斯坦谁更厉害？很难定义；成为一家大公司CEO和成为一个数学家，哪个更难？只是不同的挑战类型。

而对于Agent，另一点是：人觉得很简单或难的事情，对Agent可能不是那样。

人觉得做客服比做软件工程师简单很多，工资也低、文凭要求也低。但现在反而做软件工程对Agent更容易。因为软件工程有更好的环境、更清晰的reward、更大的数据量，等等。但你想做一个特别robust（健壮）或reliable（可靠）的客服，反而更难。它涉及复杂的reliability challenge（可靠性挑战）。

我们当然可以把人类工作分成不同的category（类别）。但对AI来说，人类觉得难或不难的任务划分，不一定直接映射到AI的能力上。

**张小珺：整体来说，什么样的任务适合****Agent****做？什么样的任务适合人和****Agent****一起做？什么样的任务适合人做？**

**姚顺雨：** 我现在感觉任务大概可以分成几类。

一类任务更注重**reliability****（可靠性）**。你做客服，重要的是：100次里你需要99次甚至更多不能出错。你只有85次让用户满意，还有15次不满意，可能被炒鱿鱼。这种任务比较简单，但需要极高稳定性。Agent就需要特别强调reliability。

另一类任务更注重**creativity****（创造力）**。你去证明黎曼猜想，或者写一个复杂程序，或者创作一部文学剧本。这类任务允许你失败很多次，只要有一次做得特别好，就算成功了。这是非常不一样的挑战。

还有一种划分方式是：看任务的深度和广度。

有些任务像Cursor（一个代码编辑工具），是非常短的loop（循环）。我只需要把一个文件改一下，可能3秒就完成。但也有一些任务需要30分钟、3小时，甚至3天。这种任务需要的是Long-Term Memory（长期记忆）的能力。

再比如，从任务的广度看，我只是去解决一个具体bug，这是比较窄的问题。但如果我要从0搭建一个像Windows这样的操作系统，这是一个非常广的任务。你可以说这是一个人能做的事情，一个小组能做的事情，还是一个公司才能做的事情？从这个角度，我们也需要做更多motivation research（动机建模研究）。

**张小珺：哪些任务对于****Agent****是相对更好定义的？从易到难的顺序应该是什么？**

**姚顺雨：** 我们可以平行做很多不同事情。有一个简单的设计评估指标（metric）方法。

在coding任务中，我们传统有一个评估指标叫Pass@k，意思是：你对同一个代码生成任务，最多尝试_k_次，其中起码有一次的成功概率是多少？你可以想象，当这个_k_越来越大，系统被使用的成功概率也会变大。

很多时候做coding相关研究，它会report（报告）的是Pass@100，也就是：同一个任务你跑100次，起码成功一次的概率是多少？

但我们2024年发了一个研究，叫TAU-Bench（Tool-Agent-User Benchmark，工具–智能体–用户基准测试），想法是：对于另一类任务，比如客服，我们需要一个刚好**相**反的指标，我们把这个指标定义为Pass^k。也就是：每一次都成功的概率是多少，或者失败一次的概率是多少？

有些任务我们需要优化的是Pass@k（多次尝试中至少成功一次），而另一些任务，比如客服，我们需要优化的是Pass^k（每次都成功），或者我们最关心的是Pass@1（一次就要成功）。

但是，**现在我们对于简单任务的****robustness****（稳健性）并没有特别重视**——这是因为大家做AI还是在做一些benchmark（基准任务），而不是实际应用。

但如果你接受了这个mindset（思维）转变，很自然你就会意识到：有些应用是需要特别强调robustness的，那你就需要去优化它的robustness。

现在大家还没完全意识到这件事；但我相信，如果大家意识到这个转变，会带来很大进步。

  

## 04 泛化的工具—“语言是人为了泛化而发明出来的工具”


**张小珺：你有一句非常****high level****（抽象）的总结：语言通过智能体中的推理实现了泛化。这里的泛化是一个已经被证实的，还是一种推断？**

**姚顺雨：** 为什么语言非常独特？因为它是人在这个世界完成各种各样事情的工具。

**语言也是人类发明的工具，像火或笔一样。**但它之所以特殊，是因为它是一个帮助你解决任何事情的通用性（general-purpose）或泛化性（generalizable）的工具。

当你学会了这门工具，你可以去做很多新任务。比如你学会了攀岩，它帮不了你完成新任务。但你学会了语言，你可以通过语言和人交流，学习、思考、推理。

2020年以前，大家没把这个事想清楚，觉得语音、文字、图像、视频都是一些数据，没什么区别。但我觉得最大区别是：**语言是人为了实现泛化而发明出来的工具，这一点比其他东西更本质。**

**张小珺：这里说的是语言具有泛化能力，那么强化学习终于具备了泛化能力，这是一种推断还是一种结论？**

**姚顺雨：** 可以说是我个人观点，当然很多人在讨论。泛化与否，本质上是一个spectrum（谱系）问题，是一个相对概念，不是绝对的0和1。

我之所以这么说，是因为在此前，如果你在一个特定环境上训练，模型只能在这个环境表现良好，不能轻易迁移到其他环境。但现在，你在一个环境上训练，模型可以适应更多不同环境，这才是最本质的区别。

DeepSeek大家觉得一个有趣结果是：你在数学和编程领域用强化学习训练模型，但它在创意写作上也变得更强。

这体现了本质区别：AlphaGo只能下围棋，不能下象棋；而现在你学会数学，也能提高创意写作。

**李广密：我读你的文章，印象最深的也是，你提到****RL****终于泛化了，是真的泛化吗？****——****你刚才也说，有很多先验知识已经****train****（训练）到****model****（模型）里头了，有什么迹象让你感觉是真的泛化了，而不是****training data****（训练数据）里面就包含这些数据？**

**姚顺雨：** 对，我觉得是有可能的。如果你的Pre-Training（预训练）已经包含了所有事情，那么 RL（强化学习）只是激发出这些能力的skill（技能）。

事后想起可能是Ilya（OpenAI前首席科学家），还是谁，说过一句话，意思是：M**aybe the ultimate generalization****（也许最终的泛化），就是你去****overfit****（过拟合）现实。**如果你能把剩下的所有事情都做完，那么讨论它是过拟合还是泛化就不重要了。

但我觉得，它还是泛化的。原因是它能够推理。当你能在一个环境学到如何思考的技能，并且这种思考能力能迁移到新环境，这才是泛化的本质原因。

**李广密：训练某一类游戏变强，能泛化到其他游戏也都很强吗？比如，一个模型打**Dota**（多人在线战术竞技游戏）非常强，是不是在所有游戏里都很强？**

**姚顺雨：** 不好说。即使是推理，它在不同环境的泛化能力也可能不一样。比如，基于逻辑的推理，可能从数学到编程的迁移更容易；基于人情世故的推理，可能在另一类任务上迁移得更好。

但重要的是，现在终于有可能出现一个单一模型能够做所有任务。之前认为这不太可能，但现在是有可能的——你可以在很多不同任务上做强化学习，而且它能迁移到更多任务。

当然，如果只考虑任务与任务之间的迁移，迁移程度和任务本身的性质有关系。

**李广密：代码和数学之所以容易泛化，你有想过背后的原因吗？是因为他们有思考过程？**

**姚顺雨：** 只是因为它是最早开始做的。它之所以最早开始做，是因为它相对简单，有一个很好的reward（奖励信号），不需要复杂环境，它本身就是推理。

现在看，很多其他任务也是可泛化的。只是我们一开始做的是这个任务，所以，大家对这个方向的讨论比较多。

  

## 05 奖励的机制—“当AI玩一个语言游戏，要怎么定义内在激励？”

  

**张小珺：基于基础模型往上长，****Agent****生态树在你脑海中，会是一个怎样的结构？**

**姚顺雨：****一个方向是：****fundamental research****（基础研究）怎么演变？或者说，方法怎么演变？**

**另一个方向是：应用，或者它的交互方式（****interaction****）有怎样的演变？**

这两个方向之间有关联。但它们需要不同的人去探索不同的方向。比如Cursor并没有在fundamental research上做什么创新，但做了交互方式上的创新。

在fundamental research上，比较重要的有三方面：

- 一个是**Memory****（记忆），**
    
- **一个是****Intrinsic Reward****（内生奖励机制），**
    
- **还有一个是****Multi-Agent****（多智能体系统）。**
    

这也跟OpenAI提出的Innovator（L4、创新者）和Organization（L5、组织者）框架很像。

你作为一个Innovator，首先你需要一个Long-Term Memory（长期记忆）。

比如，我是Wiles（安德鲁·怀尔斯，数学家），我研究费马大定理，可能花了20年。我就需要一个长期记忆。

我有这个长期记忆还不够，还需要有**内在的**reward。因为在你真正证明那件事之前，没有任何外部奖励**（**Extrinsic Reward**）****——**你没有获奖，没有做成任何“可交付”的事情，也没人给你feedback（反馈）。你需要自己给自己反馈。

这是所有Innovator最重要的。无论你是艺术家、科学家、文学家，还是任何类型的创作者，对吧？

另一方面，作为一个Organization（组织），你需要解决的问题是：Agent和Agent之间怎么协作？怎么让Multi-Agent（多智能体）协作scale（规模化）？

现在的Agent就像一个普通大学生，做一个数字化的实习生。或者说，AGI就是一个普通一本大学生在电脑上能做所有事情的一个能力。

但是，人类社会的边界是什么？这当然覆盖80%或90%的人。但我们最崇拜的人，是哪两种？

- 一种是创造新东西，在认知或审美上开创新领域的人：爱因斯坦、高更、梵高、贝多芬；
    
- 另一种是能创造新组织、伟大组织的人：伊隆·马斯克、乔布斯。
    

很自然，**个体的创造力和组织的协作能力****——****都非常重要。**

**张小珺：为什么****OpenAI****分级的最后一级是组织者（****L5****）？**

**姚顺雨：** 我一开始是认为Innovator（L4）和Organization（L5）是更正交或并列的关系。

我当时在群里问了一个问题：当一个大公司CEO和一个科学家，到底哪一个难？

这个不好说，实现路径有区别。所以，不用太纠结谁是第四级，谁是第五级，都很重要。不一定要先实现哪一个才能实现另一个，可以同时去探索。

**李广密：这中间有几个关键的问题要突破，比如长期记忆，这是短期可预期突破的吗？**

**姚顺雨：** 也许吧。当然也取决于多短期？但我觉得当它足够有价值，它必然会突破——如果你对技术是乐观的。

**李广密：长期记忆，你要展开讲一讲吗？**

**姚顺雨：** 我不知道我能分享多少，但我的信念是——是Utility（效用）的问题。

为什么我们现在的模型，推理很强，考试很强，玩游戏很强；但它还没创造出足够经济价值？——**根本原因是：它没有这些****Context****（上下文）。**

人类社会比较 tricky（复杂微妙）的一点是：当然，我们确实写下了很多东西——我们用文字、Google Doc、Notion，记录了很多东西；但很多Context永远只存在人的大脑，是通过一个分布式的系统来维护。

比如，你老板跟你之间的行为习惯，或者一些很难用语言总结下来的信息。这些Context存在于人的脑海里。人没办法把这些东西全部写下来。

这就导致——人是不可或缺的。

**只有人有这样的能力：进入一个环境，获得这个环境里的****Context****。**

如果这个问题解决了，Utility问题就可以在很大程度被解决。这个世界，大多数人并不是乔布斯，也不是爱因斯坦，只是一个普通人。他的数学推理没有o3强，但他能manage Context（管理上下文）。

他去一个公司7天，除了在文件上看到信息外，脑子里也积累了Context。而这些Context是o3没有的。虽然他没有o3聪明，但因为他拥有Context，他做得比o3好。

**李广密：有可能我们很快就会看到最强的软件工程师，甚至****2027****年看到能操作人类电脑、手机上几乎所有任务和指令的通用****Agent****，你对这一天的想象是怎样的？是过于乐观还是比较合理？**

**姚顺雨：** 现在还没有well-defined（明确定义）。现在模型写代码的能力超过世界上几乎所有人，或者说，它的数学和逻辑推理能力，也比大多数人强。但是，当你说它能不能很好使用环境，关键还是看你让它做什么任务，这个任务能不能被合理定义。

很多时候，人类最难的问题不是推理本身，而是获得完整Context（上下文）。

**现在模型的****bottleneck****（瓶颈）不是缺少推理能力，或者写代码、使用前端的能力，而是缺少一个完整的上下文。**

我不知道这是Intelligence（智能）问题，是产品问题，还是别的什么问题——但如果想让AI真正发挥价值，这个问题必须解决。

**李广密：你刚才提到另一个关键点：模型或****Agent****要有内生奖励系统。今天是不是还没有这样一个系统？如果我们真的要赋予它内生奖励机制，是不是在它持续自主学习中，就可以改动自己的模型权重，从而更聪明？**

**我们离这一步还有多远？**

**姚顺雨：** 我不知道。我觉得会有这一天，但很难预测时间。

当然，它自我提升的方式，也许是改变自己的权重，也许是拥有一个基于语言的长期记忆，也许是一个基于Embedding（向量表示）的长期记忆，或者其他形式的记忆机制。但我相信，它会自我提升。

**李广密：内生奖励，你能讲讲吗？**

**姚顺雨：** 就像我刚刚说的，很多创新者之所以能在没有外在激励的情况下坚持，是因为他有内在的价值观或激励机制。

这个问题，AI和神经科学已经研究多年。婴儿是最典型的例子。他们拥有基于好奇心**或**自我激励的机制。很多婴儿会反复玩一个玩具，用嘴去咬一个东西，或者做一些看似“无意义”的动作。

你说他获得了什么reward吗？他没有升职加薪，没有拿到钱，没有任何外在激励——他只是好奇。他的动机是：“如果我做这个事，我会有什么样的感觉？”如果这个感觉是新的、不同的，他就可以从中学习。

**张小珺：他可以获得安全感。**

**姚顺雨：**对，就是说，好奇心、掌控感、安全感，是一些内在动机。正是这些东西驱动了人去做某些事。否则，很难从纯粹理性角度解释：他为什么要做？

但有意思的是，当人长大之后，会发生重要变化。当你是婴儿，你对世界的理解，是基于视觉、触觉，基于物理世界的。你学习的是，怎么把触觉、听觉、视觉，以及对骨骼系统的控制结合起来。

当你长大之后，你对世界的理解方式变了，变成一个基于语言、推理、文字系统的理解。你开始思考：这个世界是怎么运作的？我怎么才能开一个公司？怎么才能升职？怎么才能做成一些事情？

**你玩的，不再是一个物理游戏，而是一个文字游戏。**

在这个文字游戏里，当然也存在内在激励，但又好像和婴儿时期的好奇驱动不太一样。

这是AI面临的挑战：传统AI，比如玩迷宫、做机器人仿真，它可以定义一些基于世界模型或者模仿婴儿阶段好奇心的内在激励。

**但当****AI****在玩的是一个语言游戏，要怎么定义内在激励？**——这个问题就变得不太一样了。

**张小珺：你在文章也说，我们忽视了任务评估标准的重要性。应该怎么去评估？****——****比如，我们怎么去衡量一个****Agent****？有哪些北极星指标？**

**姚顺雨：**还是要思考怎么去创造更多现实世界的价值。

当然这个事情在不同领域、不同应用下，有非常不同的任务设计、方法和路径。但有一个大趋势是：应该更多去思考实际价值，而不是这些被设计出来、类似考试或游戏的东西。

我们发现，一旦你可以定义一个考试或游戏，离它被解决也不远了。

甚至你可以说，世界之所以难，是因为它不是一个被设计出来的东西。考试和游戏有一个很大特征是：它在被设计的时候，就已经有一个很好的reward或标准答案。

但当你已经有一个非常好的reward或标准答案，再加上现在已经有一个general recipe（通用解法），那这个事情离被解决也不远了。

而真实世界的问题是：它没有标准答案，没有标准的reward function（奖励函数）。很多时候人做事情，也不一定是为了一个理性的reward，但人还是去做了。

**张小珺：它是开放的。**

**姚顺雨：**对，现在主要问题是这个。最大问题不在于，我有没有一个well-defined（明确定义）的答案，而是我怎么找到它。

**张小珺：我们未来还需要更多地推翻各种各样的基本设定吗？**

**姚顺雨：**我觉得需要。

人类一直在做这件事，不是吗？

  

# 第三章 吞噬的边界

  

## 06 双刃剑—“创业公司最大机会是：设计不同的interface”


**张小珺：你知道，应用型创业公司很担心，大模型公司的模型能力溢出，会把他们做的****Agent****吞掉。**

**长期看，****Cursor****这样的公司，壁垒是什么？哪些****Agent****是模型公司必然会做的？哪些有创业公司机会？****——****边界可能在哪？**

**姚顺雨：** 创业公司应该担心的是模型没有溢出能力，这样你就真的什么都做不了了。有溢出能力是个非常好的事情，这几乎意味着你有机会。

**创业公司最大机会是：能设计不同的****interface****（交互方式），或者说人和数字世界交互的方式。**

ChatGPT或所有做模型的公司，都在做类似ChatGPT的产品。ChatGPT的本质是：你是在像和人交互一样去进行和数字世界的交互。

你的Chatbot是像人一样的东西——你和他聊天，给他布置任务，让他帮你做Deep Research（深入研究）或者写代码——交互方式是像人，或者像助手一样的交互方式。

如果你能用模型通用能力，创造不同的交互方式，就能创造巨大的机会。

**最终，可能模型的能力会产生****beyond ChatGPT****（超越** **ChatGPT****）的交互方式，变成****Super App****（超级应用）。**

如果你做旧的interface，你利用这些新的模型，很容易被ChatGPT取代。如果你的交互方式很像ChatGPT，你有什么理由不被ChatGPT取代？如果你做的是新的交互方式，但模型没有继续变好、没有新的溢出能力，也很难做。

**对于创业公司，最好的机会是：你做新的交互方式，并且模型不停有新的溢出能力，让你能够赋能这些新的交互方式****——****两者缺一不可。**

**张小珺：但是****ChatGPT****也可以跟进这个新的交互方式。**

**姚顺雨：**对。**但拥有一个****Super App****对于公司是双刃剑。**

当你已经有了一个交互方式，你必然形成路径依赖。就像2020年Google有无限多资源和钱，有Transformer，但它最自然的想法是：我怎么用这东西提升搜索引擎？

当你有像ChatGPT这样的Super App，很自然你的研究就会center around（围绕）这个Super App，会center around这个交互方式。

你会探索新的产品，但即使是大厂，即使是谷歌，即使是OpenAI，大部分资源还是会围绕你Super App的交互方式——所以，这是创业公司的机会。

**李广密：你刚才提到交互方式，今天还是人跟****code****交互、人跟****text****交互，那人跟****Agent****未来是怎么交互？你感觉****Her****会是一种正确的交互方式吗？如果这种交互奏效，有没有机会****beat****（胜过）****ChatGPT****今天的形态？**

**姚顺雨：** Her是不是还是类似一个Assistant（助手）的形态？只不过它有语音而不是文字？

这是一个显然很有价值的形态，人和人交互已经几千年、几万年、几百万年，这是对人最自然的形态，肯定是最显然的Super App。

但这个生态位，我觉得ChatGPT是站住的。模型公司一开始做的就是这个。

那我觉得不显然的是：我能不能基于不像人的交互方式？

Cursor是很好的例子，创造了一种新的交互。不是像人一样的交互，而是像Copilot（副驾驶）。写代码的时候，它能给你提示或编辑。没有人和人是这样交互的。这是它的价值所在。

Google也是很好的例子。雅虎是一个更像黄页、更让人熟悉的交互。但谷歌是一个让人不熟悉的交互，很奇怪。

Assistant、Her，或者像人一样的交互方式，显然是最重要的交互方式之一，但还是会有足够多的机会，诞生新的交互方式。

**张小珺：你脑海里有没有一些新的交互？非****ChatGPT****在探索的形态，也非传统互联网的交互，在你脑海里有吗？**

**姚顺雨：** Canvas是一个好的尝试，可以基于现在的任务，在线生成最符合情境、个性和任务的前端。这是值得探索的方向，但也很难。

**李广密：在你看来，应用公司的数据飞轮，对他们来说重要吗？或者说，在什么环境下才能形成？**

**我感觉，****Chatbot****产生的是偏好数据，好像没什么数据飞轮；****Code****可能有思考过程的数据，这种思考过程的数据代表一类能力，可能是有用的；像****Canvas****也好，****Artifacts****也好，可能是有思考过程的数据，这类可能有机会形成很强的数据飞轮效应。**

**姚顺雨：** 大多数公司还没有形成数据飞轮；他们依赖模型变好，利用模型变好的溢出能力。

如果你要有数据飞轮，首先你要能自己去训模型，并且能通过交互有很好的reward，使你能把好的数据和不好的数据分开。

比较成功的是Midjourney，有非常清晰的reward——人更喜欢哪张图，这个reward和应用是对齐的，reward做得更好，公司就更成功，模型也更好——一切都对齐。有了这种情况，才能自己训练模型，做数据飞轮。

这个过程必须比较非主线。因为如果很主线，我也可以通过Pre-Training或RL提升能力，靠泛化或其他方式。

总的来说，大部分公司目前还没有形成飞轮。

  

## 07 对Agent创业者的思索—“这世界是相互抄的关系，而不是单向抄的关系”


**李广密：在你看来，****Agent****创业者一定要有研究背景吗？**

**姚顺雨：** 不好说，挺看人的。很难把人简单分成research和非research两类，没那么泾渭分明——人与人之间的差异很大。

可能最重要的一点，还是得找到value（价值）。不管你叫它product-market fit（产品与市场契合）、产品的sense，还是别的——找到真正有价值的东西最重要。技术只是手段，目前最重要的是解决问题，需要找到一个好的问题。

如果你有很强research背景，比如自然语言处理，反而可能是坏事——因为你会对技术太执着，拿着锤子到处找钉子。

Cursor创始人是四个本科生。Perplexity创始人是研究员出身。真的挺看人的，跟你是否做过research，没有那么强相关性。

**张小珺：好的****AI****产品经理应该长什么样？**

**姚顺雨：** 好的AI产品经理就是一个好的产品经理，可以第一性思考。AI是变化很快的，相对不变的是人、人性、人的需求。这变化得更慢。

你能找到一个好的需求，从第一性原理反推：要把它做成，我需要应用什么样的技术？

**张小珺：你怎么看****Manus****、****GensPark****这些产品和他们的创始人？**

**姚顺雨：** 我试过Manus，还没试过GensPark 。Manus挺有意思，给我一些启发。他们产品sense很好，有打磨产品的基因。

**张小珺：这个产品应该是****OpenAI****主线上的产品对吧？**

**姚顺雨：** Emm……You will see。

基于Manus，我再讲一点。传统大家认为发生的事情是：我大厂先做出来一个东西，创业公司就可以开始抄。比如做出ChatGPT，我可以去抄一下ChatGPT，做一个类似的事情。

但现在，似乎反过来也可以成立。可以先小厂做一个事情，它创造出来一个交互的创新或者产品的创新，做模型的公司也可以去借鉴或者应用。

这点还是挺有意思。很多时候大家会说，模型做得越来越好了，是给创业公司做嫁衣了。因为你创造很好的模型，如果没有自己运用特别好，这些创业公司就用好了。

但也可以反过来，如果你创造一个非常好的交互，但没有能力把模型或底层能力做特别好，大公司也可以借鉴你的交互，再加上它的模型能力，做得也特别好。

**这世界是个相互抄的关系，而不是一个单向抄的关系。**

**李广密：如果你是****Manus****创始人、****CEO****，你今天要走向垂直方向吗？**

**姚顺雨：** Manus的一个价值是，它给人非常general（通用）的感觉。但我觉得，**有一个非常通用感觉交互方式的****Agent****，和你有一些****Killer App****（爆款应用），是不矛盾的。**

一个比较理想的情况，你有一个非常通用的交互方式，这个交互方式想象力足够大。比如Cursor，虽然它是IDE（集成开发环境），如果它只做 IDE，想象空间是有上限的，就在IDE里面。但如果你做一个非常general的产品形态，比如Manus，想象空间是很高的。

但并不矛盾的是，你可以有每个阶段的Killer App。比如它做PPT特别好，做Deep Research特别好，或者做其他东西特别好。

iPhone或iPad是非常通用的产品形态，但它一开始，都有一些Killer App支持它有momentum（增长动能）。包括ChatGPT，包括微信，很多伟大产品都这样。

你有一个足够通用、简单，或第一性的交互方式，它有很多想象空间。但你去维护它，或者设计路径的时候，你能有各种各样的应用，使它不停地增长。

**张小珺：你听了我和肖宏（****Manus****创始人）的播客，有什么感觉吗？**

**姚顺雨：** 我觉得挺有意思。印象最深刻的是他说，VC是一个非常贵的融资方式，不是在你不好的时候，而是在你好的时候。他有很多挺不一样的思考问题角度。

**张小珺：****2025****年过年****DeepSeek****全球爆火，这对硅谷的****AI****研究员带来了哪些叙事变化？**

**姚顺雨：** 从OpenAI角度，大家讨论的有几点：

一点是Chain of Thought（思维链）的reveal（展示）。显示出一条长的思维链，似乎很重要，它是产品形态的突破。很多时候，技术积累已经到了，就像洪水已经到达闸口，需要一个时刻“开闸”，让大多数人真正感受这个技术。

我们会说有iPhone moment、ChatGPT Moment，可能有DeepSeek moment。这个moment就是指，一个非常大的交互方式上的冲击，带来了magical（神奇）的体验。

另一点是对开源的重新思考。Sam（OpenAI首席执行官）在他Twitter上讲了很多，说OpenAI过去忽视了这件事，但仔细想一想，它是有价值的，可能应该做。

我们默认认为开源落后于闭源，原因是，它不像 Linux（操作系统），我有1000个人可以每人出一份力，让系统通过分布式变得越来越好。做好一个强模型更像我有20个特别厉害的人，再加上大量资源，就可以做得很好。它需要非常特殊的组织、资源和人才集中。

这种情况下，传统意义上开源的优势没有那么大。比如Facebook在开源上，做得也没有那么好，在美国很多人也习惯性忽视这个路径。

做好开源是一个“很吃亏”的事。你首先要有足够的资源，有很强的人，有很好的组织文化，还要有商业上的justification（正当性）。最好情况是：你是个慈善家，有几百亿美金，你就做这件事造福世界。

这是一个小概率事件，但它发生了，就有这样一个人去做了这样一个事。

DeepSeek在许多方面，组织架构、工程能力、基础设施，确实有值得称道的地方。

**张小珺：有一个****Agent****创业者想问你：****Agent****如何****scale up****？现在的主要瓶颈是算力，****Agent****的****token****用量非常可怕，单个用户消耗可能是****Chatbot****的****500****到****1000****倍，再叠加几百万个用户，成本非常高。这种情况下，****Agent****应该怎么扩展？**

**姚顺雨：****最重要的点是****——****你得先找到一个好的应用。**

Cost（成本）本身不是最大问题，问题是你的成本并不能证明你的performance（性能）或value（价值）是合理的。

如果这是一个很有价值的事，我花500美元，但可以赚1000美元——根本不是问题。这不是technical bottleneck（技术瓶颈），而是product-market fit（产品与市场契合度）的问题。

所以，现在最关键的，是要找到真正有价值的应用。模型的cost会下降，能力会提升，这个方向是确定的。但能不能找到那个有value的点，是最本质的问题。

当然，不同的应用，做法可能会很不一样：

- 如果是一个相对简单的任务，我可以训练一个小模型，让它更快、更便宜、更针对这个任务。
    
- 但如果你要做的是更复杂的事，比如投资、Deep Research，就需要更大的模型，在cost和value之间寻找新的平衡。
    

总的来说，**第一步永远是：找到一个真正有价值的场景。**

一旦你找到它，cost的问题总是有办法解决。

**张小珺：你在****OpenAI****的一个好处是不是，可以很清楚知道哪些是模型公司的主赛道，哪些领域可能是创业公司的机会？**

**姚顺雨：** 每个公司一旦有它的Super App（超级应用），所有事都会围绕Super App。当你有ChatGPT，训练模型的方式、组织架构，都会围绕ChatGPT重构。

如果你做一个和ChatGPT形态很不一样的东西，是会有机会的。

  

## 08 既单极又多元的世界—“这个世界不是单方压倒另一方，双方都有自己的力量”

  

**张小珺：一位****AI****研究者说，他对****Agent****的想象很有限，希望你能对未来的****Agent****畅想一下。你曾经说过，你的终极理想是打造****“****世界上最强的****Agent”****，它会是什么样的？**

**姚顺雨：** 大多数人对AGI的想象就是一个模型，就像这个世界上最聪明的人，他拥有所有知识、能力，比我们都聪明，是最强智能体。

但我现在的感觉是：不同的交互方式下，有不同“好”的定义，有不同“强”的边界。

**最终的智能边界，是由不同的交互方式决定的，而不是由一个****single model****（单一模型）决定。**

想象空间非常大。就像一开始互联网诞生，最早Super App只是把邮件升级成Email，Amazon已经算非常创新的东西了。现在就像那个阶段——**我们的想象力仍被以往的交互方式所限制，还有许多尚未诞生的交互方式。**

这些全新的交互方式，会改变我们的世界。

**张小珺：在你脑海中，最强的****Agent****应该是什么样？**

**姚顺雨：** 对于不同的任务和交互，需要不同的Agent系统去解决。

模型是可以share（共享）的，但如果你讨论的是整个系统，那就不一样了。就像你问，这个世界上最强的互联网网站是什么？最强的互联网公司是什么？很难回答。它是一个multiface（多面向）的系统，有很多不同侧面。

AI可能也会变成这样的结构。OpenAI可能会成为一个类似Google的公司，成为新世界里非常重要的一环——**但这并不代表，这个世界就会被这样一个单极系统垄断。**

**如果真是那样，这个世界就会变得很灰暗。**大多数人也就没什么价值了。

**张小珺：你对未来****Agent****生态的构想会是什么样？现在有点像，当年大家都在创业做****App****的时候，如果再往后推演几年，这个世界会是什么样？**

**姚顺雨：** 很难说。**但肯定会有很多不同的交互方式，创造出不同的系统。**

OpenAI这样的公司，会想继续推进一个中心化的助手系统，有更多环境、更强能力，做更多事情。

也会有不同的生态系统，有不同的交互方式，会训练完全不同的模型。甚至从Pre-Training开始，所需要的能力和很多东西都不同。

**比如，另一种交互方式可能是，我想造一个朋友。**这个朋友不需要数学、物理特别强，数学太强反而不自然。它记忆不一定特别好，会犯错，有感情，也不是特别rational（理性）。但这也是有价值的——可能有人会做这种事。

这类东西很难和ChatGPT比强弱，它们是不同应用，有不同价值。

**也可能出现一个由****Agent****组成的社会。**

为什么这个世界上很多人有价值？不是因为他们的数学或编码能力强，而是因为他们拥有别人没有的信息。

中间商本质是拥有信息差。拥有信息差的人会想维护自己的权利和资源。这样的人会发明出更Multi-Agent（多智能体）或更 Distributed Network（分布式网络）。

在交易世界里，信息很重要，每个人只拥有信息的一小部分，这种情况会出现新的不同形态。可能是Multi-Agent，每个人有自己的Agent，Agent之间可以与百万甚至更多人交换信息，达成交易或某些目的。

根本上，现在非常强的巨头和重要节点，有动力继续推动中心化。但在中心化之外的力量，也有动力做一些非中心化的事情。

**这个世界可能不会是单方压倒另一方，双方都会有自己的力量。**

**而这个世界智能的边界、研究的边界，可能不是由一家机构定义，而是由不同****Super App****共同定义的。**

  

## 09 **环境是记忆层级中最外层的部分**—“这很哲学”


李广密：更关键的是，大模型技术没有垄断性。硅谷头**3-4****家好像都能追到一定的水平。如果****OpenAI****有垄断性，那是比较可怕的。**

**姚顺雨：** 我觉得暂时没有垄断性。但如果你能找到一个产品形态，把研究优势转换成商业优势，就会产生壁垒。

现在对于ChatGPT比较重要的是Memory（记忆）。

这是可能产生壁垒的地方。如果没有Memory，大家拼谁的模型更强。但有了Memory，拼的不仅是谁的模型更强，而是用户用哪个更多、哪个粘性更强。

我积累了更多Context，它能给我更好体验，我就会有粘性——这或许是研究优势转化成商业优势的方式。

**张小珺：最近****ChatGPT****会出现灰色提示词，显示****“****记忆已更新****”****，这个更新的是什么？**

**姚顺雨：** 我最近没怎么用这个功能，但好像做了一些提升。

我怀疑是它产生或者使用记忆的方式变得更好。包括能更有效从很多用户对话中提炼出来，或者retrieve（检索）出更相关的内容。细节我不特别了解。

**李广密：****MCP****（模型上下文协议）本质也是****Memory****吗？因为我的很多****Context****在我的个人软件、企业软件里，****MCP****本质也是****hack****（利用）****Context****的一种方法。**

**姚顺雨：** 某种程度上，是的。从Agent角度看，这个世界有一个Memory Hierarchy（记忆层级）。Memory Hierarchy最外层永远是环境。

有点像你考虑电脑，它有个Memory Hierarchy，从CPU缓存到内存再到硬盘，但最外层的Memory永远是外部环境。比如我插一个U盘、拔一个U盘，或者把东西上传到互联网，或者做个音乐变成光盘。

**前年冬天，我读到冯诺依曼临终前写的一本书，****_The Computer and the Brain_****。最让我印象深刻的一句话是：****Essentially, the Environment is always the most outer part of the Memory Hierarchy.****（基本上，环境永远是记忆层级中最外层的部分。）**

这很哲学。

对于人，你有你的Memory Hierarchy，有Working Memory（工作记忆）、Long-Term Memory（长期记忆）在脑子里，但最外层是你的笔记本、Google Doc、Notion，这些是你最外层Memory Hierarchy的一部分。

- 《计算机与大脑》（_The Computer and the Brain_）是20世纪伟大的数学家约翰·冯·诺依曼于1956年完成的未完成著作。这本书源自他为耶鲁大学西里曼讲座准备的讲稿，探讨了计算机与人脑在信息处理的相似性与差异性。尽管书籍篇幅仅96页，但其深刻的洞察力和前瞻性思考，使它成为计算机科学和神经科学领域的重要经典之一。
    

**李广密：****Long Context****跟****Long-Term Memory****是什么样的关系？**

**姚顺雨：** Long Context是实现Long-Term Memory的一种方式。

如果你能实现1亿或1千亿或无限长的Context，它是实现Long-Term Memory的一种方式。它是一种和人区别很大的方式，但这是有可能的。当然会有很多不同方式，不好说哪种是最好，或者最合适。

**李广密：现在业界实现****Long Context****有****Linear****（线性）方式、****Sparse****（稀疏）方式，或者** **Hybrid****（混合）方式，你有倾向吗？**

**姚顺雨：** 我不想对方法进行评论，但我想对evaluation（评估）和task（任务）进行评论。

起码到去年为止，大家主要还在做所谓Long Range Arena（长距离评估基准），比如hay in the stack——我有一个很长的输入，我在中间插入一句话，比如 “姚顺雨现在在OpenAI”，然后我问你相关问题。

这是一个必要但不充分的任务。你能完成这个任务，是Not Memory Work（非长期记忆任务）中的前置条件，但远不是充分条件。它是必要条件，但现在大家有点陷在这个必要条件，没有创造更难或更有价值的任务，这是个问题。

当没有一个很好的评估方式，很难真正讨论各种方法的好坏。

  

## 10 Chatbot系统会演化成Agent系统—“人和Agent交互的方式是什么样？”
  

**张小珺：对于未来****12****到****24****个月，****Agent****领域有可能发生的事情，你有哪些预测？**

**姚顺雨：****首先，这些模型公司的****Chatbot****系统会演化成一个很自然的****Agent****系统，它是一个很自然的过渡。**

Grok、ChatGPT或Anthropic Cloud，默认的交互方式会是Agentic（智能体式的）交互方式。Chat可能还会保留或作为一个子集，但Agent会成为一个很显然、更重要的交互方式。

会有新的类似Cursor的产品出现，Cursor是在coding和IDE（集成开发环境）环境下做的Copilot（辅助编程助手），但我觉得会有机会做一些新的环境或更大环境下的Copilot。

这两种大的交互方式是互补的，或者说不一样的正交的。

一边是，我有一个基于模型的，可能是一个remote（远程）的Virtual Machine（虚拟机）或者Environment（环境），我在里面做很多事；另一边是，有很多既有的环境，比如既有的软件，或者既有的场景，我把Agent或AI能力引进去。

大趋势可能是，两方面都会往下发展。

**李广密：如果我们想推动****Agentic****能力变得更强，要在哪里做工作？是在****Pre-Training****做工作还是在****RL****做工作？如果我是一个应用创业者，这两个东西是做不了的，最多尝试一些端到端****RL****的过程，对吧？**

**姚顺雨：** 最重要的还是想清楚价值，你应用的价值是什么，痛点是什么，要解决的问题是什么？

虽然你不能做Pre-Training，但更有价值的是：**Agent****和数字世界的交互环境是什么样的？（是基于****MCP****还是****API****，还是别的东西？）人和****Agent****交互的方式是什么样的？**

这两个是你可以去做的，并且它需要很多设计、很多基础设施、很多工程，需要各种各样的东西。现在还远远不够好，有很多进步空间。

**还有另一个很重要的是：怎么构建一个生态系统，或者怎么积累用户的****Context****（上下文）或****Intention****（意图）？这**还有很多可以做的空间。

**李广密：你刚才提到****Agent Infra****（智能体基础设施），如果两年后****Agent****已经大爆发，巨量的****Agents****在数字世界运行，需要重新帮****Agents****设计一套新的数字化系统吗？**

**Agent****需要的虚拟机、电脑、浏览器、搜索的****API****、身份认证、经济系统等等，这套****Infra****是为****Agent****设计的，而不是完全为人设计的？**

**姚顺雨：** 我个人感觉两年以内，这个世界还不会变得这么分布式，还是更偏中心化。就是说，会有一些Super App。

当然现在有很多创业公司，但做得好的就是那么几家。两年内还是会有些Super App，这些Super App会有各自的Infra，有各自的Environment或交互方式。

两个事情都可以做到极致，就是一个是基于用户local（本地）的Digital Environment（数字环境），比如我有个手机，有个电脑，有个软件，我已经在这了，我怎么把它去扩充，怎么把它变得更好？

另一个是从头创造新的Environment，比如我做Deep Research或我做Operator（操作者），我实际上创造一个新的Environment。这两个事都还有很多可做的空间。

**张小珺：两年后呢？**

**姚顺雨：** 这个世界变化很大。有些像科幻的预测、想法或图景。没有人可以预测两年后发生什么。

**张小珺：在你看来，大型科技公司是否应该重新开启****Pre-Training****叙事？（自己从头探索****Pre-Training****）**

**姚顺雨：** 这里面涉及cost和value取舍。现在做的人很少，是因为成本非常大，但带来的additional value（额外价值）没有那么大。

即使你做完Pre-Training，你还需要做Post-Training、RLHF（基于人类反馈的强化学习，Reinforcement Learning with Human Feedback）等一系列工作，才能真正把模型价值释放出来。

但如果有一天，这个世界上存在很多不同的Super App、不同的交互，它们需要不完全相同的模型能力，甚至需要不同的模型，这些差异的价值足够大，能够证明Pre-Training的成本是合理的，那么Pre-Training就是合理的。这最终是value和cost权衡问题。

**李广密：****Pre-Training****和****RL****未来的关系会是怎样的？会不会更多先验知识被放到****Pre-Training****里？**

**姚顺雨：****我一个不成熟的想法是：不同应用需要不同形态的****Agent****，构造方式可能不一样。**

如果我只需要下围棋，我直接做AlphaGo就可以了，不需要Pre-Training，也不需要其他。

如果我有一个非常垂直的场景，这个场景价值足够大，我又有很多数据，可以形成闭环，我也许基于一个主要由RL驱动的系统就能work。

像Google的广告系统或TikTok的推荐系统，有点类似这样的系统——我找到了一个足够封闭的环境，做类似RL的事，就可以带来足够多价值，那这个路径是合理的。

但这个世界上还有很多长尾任务，它们需要泛化，需要构建一个更像人的系统。你虽然不是无所不知，但你可以学习，你可以通过在线学习进入一个新的公司、适应环境、完成新的任务。在这些地方，Pre-Training重要性会更高，因为它带来更强的泛化性。

所以不同应用会有不同技术路线。但技术路线毕竟是工具，只要你的value大于cost，技术上的选择是flexible（灵活）的。

没有哪种技术路线一定会胜出。只要它在经济上成立，就有可能性。

  

# 第四章 人类的全局

  

## 11 **人与系统**—Agent要不要像人？  “是一个效用问题”

  

**张小珺：在你研究****Agent****的过程中，对于人，你有更深的认知吗？怎么看人和****Agent****的同与不同？**

**姚顺雨：**我意识到，人之所以能泛化，是因为人能推理。

这个很有意思。我2018年在MIT Josh Tenenbaum实验室——他是一个认知科学的大佬——我学了很多认知科学的东西。

认知科学，或者计算认知科学，一个核心故事是：我们现在的AI虽然有很多进展，但还有很多问题。我们应该去看看，人有哪些优势，人是怎么做这些事情的，为什么人能把这些事做得更好？比如说，人能够从几个样本中泛化，但机器做不到，为什么？我们要从人身上去寻找这些方法，再把它应用到AI上。

后来我的认知有了变化。我发现，现在真正能奏效的AI系统，跟人还是很不一样。比如Scaling Law、强化学习，还有很多训练策略，它们和人类学习的方法本质是不同的。

我现在觉得，一个更好的方法是：你先去思考人能做什么，而机器现在不能做。这是客观事实。

但你找到差异之后，你可以基于第一性原理去思考，如何解决这个问题。你不一定要依赖“人是怎么解决这个问题的”来解决它。

比如说，人现在能做的事情是什么？我可以进一家公司，在里面工作7天，我能积累公司的Context。即使我不是很聪明，但我依然能完成很多AI做不了的事。这个差异客观存在。那怎么解决？

可能认知科学或神经科学会告诉你：人脑有海马体（Hippocampus），有情节记忆（Episodic Memory），有某种架构或机制。但我觉得，我们不需要完全照搬生物机制。可以从第一性原理出发，设计Long-Term Memory该怎么做。

所以，从人身上可以借鉴的一点：哪些事情是人可以做，而机器目前不能做？这点比较robust（稳固）和客观。但至于“人是怎么做到的”，以及“我们在多大程度上要借鉴这种方式”，这个问题本身更主观、也更 noisy（带噪声）。

神经科学或认知科学也没有100%解答这些问题，只提供了猜想或理论模型。另外，即便被证实，比如人类视觉是目前研究比较透彻的领域之一，人类大脑有六层皮层（cortex），每一层有各种结构和功能。但从这里获得的启发是：我们也许要构建新的神经网络，而不需要照抄那些细节。

**张小珺：比方说，设计****Agent****在什么情况下，需要它越来越像人？什么情况下需要它不像人？**

**姚顺雨：**Again，这是一个Utility Problem（效用问题）。

很多问题上，人的方式并不一定更有价值。比如下围棋、开车。我不知道。大多数人可能开车的方法并不好，也许基于规则有更好的开车方式。但有些事情，人就是做得更好。那你就应该思考，怎么去bridge the gap（弥合这个差距）？

下围棋、打游戏，基于强化学习可以学到和人不一样、甚至更好的方式，就不需要像人。

但如果在一个公司打工，和老板搞好关系，完成各种各样的任务，人就是比AI做得更好，就需要更像人。

**张小珺：你怎么思考人和****Agents****未来的关系？**

**姚顺雨：** 这是一个交互方式的问题。

很有可能有很多Agents，长得并不像人，和它交互的方式并不像人——可能是平台、页面、游戏，或者别的东西。你就不会把它拟人化。当然，肯定会有很多拟人化的Agent。

**李广密：如果****Agent****有了长期记忆，它是不是就是你的朋友了？如果它是你的朋友，人和****Agent****就平等了，是不是我们就要给它发身份证了？**

**姚顺雨：** 发身份证的目的是什么？

**李广密：它作为独立个体跟我们共存。**

**姚顺雨：** 会有可能吧。这些事情最终还是从Utility（效用）出发。

一个事情如果有价值，就会产生。比如，很多人很孤独，他需要一个朋友，技术如果能创造这样的体验，拟人化就是合理存在的未来。

但如果它去做一个平台、一个推荐、一个游戏，这个技术会有很多不同的交互方式，让你感觉它不像一个人，或者你根本感觉不到有区别。你就不会把它看成拟人化。

还是会基于这个事情的经济价值。

**李广密：你提到经济价值。你觉得****AI Agent****跟****Crypto****（加密技术）未来有结合的地方吗？**

**比如，****Crypto****这一套智能合约机制，如果跟****Agent****结合，在未来有没有可能是这样：一个****Agent****帮我完成某个任务，这个任务有一个公允价值计量。任务完成之后，就可以按照智能合约的约定去分配经济利益。**

**这样是有机会探索出一种叫做****value-based****（基于价值的）商业模式。只是说，现在我们还不太能准确衡量这个任务的客观供给价值是多少。**

**姚顺雨：** 我对Crypto了解不多，但可能一个核心问题是：这个技术的演变，会变得更中心化还是去中心化？——两边都有argument（论点）。

中心化论点是：现在这种新的超级公司，OpenAI或Anthropic，它们有可能变成one trillion、ten trillion、hundred trillion（万亿、十万亿、百亿万亿）级别的公司。它们可能会占据绝大多数资源，尤其是算力，也有能力去创造出一个Super App或Super Platform（超级平台），拥有巨大中心化优势。

而去中心化argument（论点）是：每一个个体都可以被赋能。现在人和人之间之所以差距这么大，是因为存在信息差、认知差、智能差。如果智能变得便宜，像电一样，它也可以赋能给大多数人。

这个问题挺有意思的。

我最近的一个思考是这样：我感觉人类社会是一个网络，它有两个重要性质：

- 一个性质是中心化程度，也可以说是资源分配的集中性。我们发现，原始社会是非常平均的社会，但随着技术发展，它变得越来越中心化。你可以用二八定律、马太效应、或whatever来解释这种趋势。
    
- 但还有另一个维度，是你从网络边缘到中心的速度或可能性。
    

过去几百年发生的事情是这样：网络越来越中心化，贫富差距越来越大，二八定律、马太效应更明显；但与此同时，平民或普通人翻身的机会也变多了。

如果是在古代，门阀制度、九品中正制，或者欧洲贵族制度，农民永远是农民。印度种姓制度也一样，有明显的阶级固化。

看起来，技术发展的趋势是两件事同时加剧——一方面，中心化加剧，因为效率这个因素是根本性的；另一方面，创造新东西的机会，起码到目前为止，是越来越多的。

变得更中心化和变得更diverse（多样化），可能并不矛盾。

但未来是不是一定会持续下去，也不好说。

  

## 12 OpenAI的抉择时刻—“如果你没有different bet，很难超越前面的霸主”

  

**张小珺：我想聊聊****OpenAI****。我记得你提到****OpenAI****的几次尝试很有意思。**

**它最初的计划是构建****Gym****，一个用于各种游戏的标准强化学习环境。后来是****World of Bits****和****Universe****项目，试图把整个互联网或计算机交互编程成一个游戏。一旦能把整个数字世界变成一个环境，用聪明的强化学习算法解决它，就拥有了****AGI****。**

**但这套思路并没有奏效。直到****GPT-2****和****GPT-3****出现，人们才意识到，之前缺失的是先验知识。你需要一个强大的语言预训练过程，把一般常识和语言知识提炼进模型中。再通过微调，让它成为一个能浏览网页的或能对话的智能体。**

**你能不能更详细讲讲，****OpenAI****探索过程背后的思路演化？从****Gym****到****Universe****到****GPT****这一整条路径的尝试中，转折点是怎么发生的？**

**姚顺雨：** 这是我自己的总结和揣测。

OpenAI是一个比较bottom-up（自下而上）的公司。在最初7、8年里，它更像是一个research lab（研究实验室），每个人有各种各样的想法，做各种各样的尝试。可能每个人想法都不一样。

但客观看，一开始大家的重点还是聚焦强化学习，当时最火的方向是这个，对吧？

DeepMind大概2015年刚成立，那时AI领域最受关注的公司是DeepMind，它最成功的成果也是强化学习。GPT出现前，最成功的AI项目是AlphaGo。很自然，OpenAI也做强化学习。

**但问题在于，如果你没有一个****different bet****（不同的下注方向），很难超越前面的霸主。**如果OpenAI一直做强化学习，可能很难超过DeepMind。即使你在某些任务上做得比它好，人们提到强化学习，想到的还是DeepMind。

你要想超越之前的霸主，就必须有一个different bet。而GPT是那个不同的赌注——但这个选择在当时是一个非共识的事情。

我可以讲个例子**：我导师是****GPT‑1****第二作者，他在****OpenAI****待了一年，然后去普林斯顿当教授。**他对这件事是有点怀疑的。

他觉得GPT‑1的结果也不是特别好，在排行榜上也不是分数最高，而且训练花了很多算力。当时已经有Scaling Law初步雏形。2017年，Ilya就跟我导师说：”Language is basically solved, and we just need to scale up." 语言模型的问题已经被解决了，现在只需要扩展规模就行了。

但即使你在OpenAI，即使你是GPT作者，你也可能没有形成共识。所以OpenAI当时做的是一个非常反共识的决定。现在已经变成了共识。但接下来，你还需要寻找下一个反共识的方向。

**张小珺：当时其他人对你导师的看法是怎样的？**

**姚顺雨：** 我说实话，当时OpenAI内部绝大多数人也不认为scale-up（扩大模型规模）是最promising（有前景）的方向，我觉得这是有可能的。

Ilya最大贡献并不是他做了GPT‑1，或者他具体参与了什么技术工作；而是，他是那个号召大家all in（全力投入）这个方向的人。

Dario（Anthropic联合创始人兼 CEO，曾是OpenAI研究副总裁）也是。他最大贡献不是提出某个具体技术，而是：作为一个创始人，我敢赌。我敢赌这个方向，把所有钱砸进去。

**李广密：有人愿意去做****GPT‑3****是特别关键的。像****Dario****也好，****Tom Brown****（****Anthropic****联合创始人）也好，他们敢于把****GPT‑3****做出来，这件事让人看到了更大希望，也泛化了。**

**姚顺雨：对，** 当然好处在于，你并不需要所有人达成共识。只需要有足够多人达成共识，就可以把它做出来。

**张小珺：对于****OpenAI****内部来说，强化学习在什么时候开始变得特别重要？**

**姚顺雨：** 强化学习一直很重要。即使我在做GPT的时候，John Schulman（OpenAI联合创始人之一，强化学习领军人物）还是在继续做强化学习。并不是我做了GPT就把强化学习扔掉了。而是公司70%、80%的资源在做强化学习，一些别的东西还在做。

后来证明，ChatGPT成功，强化学习也很关键。没有RLHF，没有Alignment（对齐）技术，它也没办法形成一个产品。

历史并不是说我把强化学习彻底抛弃，转而走另一条路，再返回来走强化学习，而是更soft（柔和）的过程。

**李广密：接下来几年，你预计会有更多****GPT‑3****时刻吗？**

**姚顺雨：** 会有新的scaling dimension（扩展维度）出现。如果你有大量的Memory（记忆），你的test-time compute（测试时计算资源）就会有所增加，可以用新的方式scale（扩展）。

如果你有了Multi-Agent（多智能体系统），那你的test-time compute又会出现另一个新维度去扩展。

我觉得会有新的scale dimension出现，但当你有很多scale dimension，怎么去选择？怎么基于某一个应用去分配不同scale维度的比重？——这会是一个很有意思的问题。

  

## 13 假若你是一个CEO—“首先我肯定会学习”

  

**李广密：顺雨，如果你是一个全球超大互联网或科技公司的****CEO****，今天这个公司还没有自己的模型，没有好的研究文化，甚至没有好的****AI****战略，你作为****CEO****会怎么做？**

**姚顺雨：** **首先，我肯定会学习，我会想弄清楚这个事情到底是什么。如果你作为CEO不懂这个事情，所有事情会变得很难。**

**很多时候，一个公司的bottleneck（瓶颈）就在于，CEO 对这个事理解不够。如果你不理解，去招一些很好的人、做一些事情，你很可能被他们忽悠。所以，首先要自己学习。**

**然后要从创造新的价值来思考问题。毕竟你不是技术专家，而是一个CEO，你有一些场景、一些资源、一些优势。从第一性原理看，一个新的技术产生了，你要思考的是，怎么用这些新技术结合你现在的资源去创造新的价值。**

当然，你可以尝试做一个和当前业务完全不一样、但价值非常大的事情，比如ChatGPT，但对大多数公司来说，即使很有钱、很强，也不一定make sense（合理）。

所以，第一是自己要学习技术；第二是要思考怎么创造新的价值。

**李广密：如果你成为了伯克希尔的****CEO****，未来要拿出****500****亿美金****allocate****（分配）到****AGI****行业，你会怎么****allocate****这笔钱？****——****既能体现回报，也能体现对人类的贡献。**

**姚顺雨：**这是个很好的问题。取决于你有多少精力，或者有多少资源分配颗粒度。

当然现在OpenAI、Anthropic，这些模型层公司，大概率会有更大价值。

还有一类很有价值的，是能积累User Context（用户上下文），或者能构建特殊Environment（环境）的公司。最终如果AI或AGI是一个系统，它需要有Intelligence（智能），需要有Environment，还需要有User Context，或者对用户的理解。

现在有很多User Data（用户数据）或User Context 的公司，有点像发明车之前的煤炭、煤矿，或者像发明汽车之前的石油公司。

从这个角度，微信或大平台，还是一个易守难攻的好平台，它积攒大量的Context。

如果Intelligence是一个可以逐渐民主化、逐渐变得便宜、逐渐普及，拥有这样的平台，拥有这样的Environment，拥有这样的Context，可能会是一个很强的壁垒。它可能还是一个很好的投资。

**李广密：如果你是****Cursor****的****CEO****，你会去做****Pre-Training****的事情吗？**

**姚顺雨：**我肯定会训练模型，或者尝试训练模型，但做不做Pre-Training看情况。

Coding是非常主线的任务，所有大厂都会把模型的coding做好。所有的Pre-Training、Post-Training、RL，都会考虑到这一点。

这个情况下，要不要做可能取决于，首先这些闭源模型做得有多好，其次开源模型做得有多好，中间有多少gap，你能填补多少这样的gap。

但当然，如果你有很多钱，有很多资源，想把这事情做了，也是合理的。

**张小珺：今天顺雨当了很多公司的****CEO****，那我再问一个：如果你是微信的一号位，你会怎么在微信里做****Agent****？**

**姚顺雨：**我可能会不急，先观望观望。

我好像没有理由要急。我会观察，我会学习 AI，会观察有没有什么新的交互方式很有意思。但我不会急着去做很多事——我有易守难攻的地方，为什么要急着进攻？

比较危险的是一个颠覆性的创新。**真正的危险，不是说一个类似于微信的东西打败了微信，而是一个很不一样的东西打败了微信。**

就像微信打败了QQ。当时担心的并不是一个类似QQ的东西打败了QQ，而是一个很不一样的产品去打败这个东西。需要对颠覆性创新有所警惕。

但如果是这些incremental（渐进式的）创新，这种小的创新，早做晚做可能区别没有那么大，也不用太担心。

**李广密：所有人都说微信卡位好，但今天微信还没有很激进地投入，如果未来****Multi-Agents** **、****Long-Term Memory****这些问题解决了，但这个****Agent****系统不长在微信上，是比较恐怖的。原有网络不一定有价值。**

**姚顺雨：**这取决于人类的网络会变成什么样？你会有更多Agent朋友，还是更多人类朋友？或者你有更多Agent职业上的交互，还是有更多人类职业上的交互？

微信上你既有朋友，也有基于职业的交互——比如我要买个东西，我要咨询律师，对吧？

这取决于人类的网络会变成什么样。但总会有一个这样的网络，基于这个网络，肯定会需要有基础设施，需要有平台。

**李广密：怎么保证****AGI****实现之后的安全问题？微信过去还是一个比较负责任、比较安全的平台，那如果未来****power****（能力）很强了，很多坏人来做坏事，甚至颠覆人类，安全问题长期怎么解决？要有****AI****宪法吗？**

**姚顺雨：**安全是很复杂的问题。比如ChatGPT，如果它不安全，产品就失败了，没有商业价值。即使是为了商业价值，它也会重视安全。

但现在的主要分歧是，需不需要产品之外、更意识形态上的安全？这个大家没有定义清楚。

前者容易解决：如果你有一个好的应用，你总会有办法解决安全问题，我相信。至于第二者，会有很大不确定性，我很难评价。

**李广密：你个人会担心****AGI****实现之后的安全问题吗？**

**姚顺雨：**我会担心。但现在最大问题是——AGI还没实现，我们还没创造足够价值。

如果我们还没想清楚，怎么把它变得有价值，就急着把它变得很安全，好像没有意义。

  

## 14 **这个时代，做上限更高的事更好**—“如果敢想、胆子大，就会有好事发生”
  

**张小珺：你作为****AI****研究者，博士期间工作已经获得了很多关注，在你眼中，你做对了什么？**

**姚顺雨：**我想做的就两条线：简单通用的方法、有实际价值的任务。这些任务往往是，如何在真实数字世界创造新的价值。这是一个处女地，是一个巨大的宝藏。我恰好挖掘了一些东西。

需要你想得足够大胆或足够通用吧。

另一个很重要的是：要去看很多东西的交界处。ReAct之所以能做出来，因为我们选了一些自然语言处理的任务，也选了一些游戏的任务，需要把自然语言处理和强化学习的边界打通。但很多人会陷入一个学科内部，就更难去做更通用的东西。

**张小珺：****ReAct****在做的过程中有遇到什么坎吗？**

**姚顺雨：**最难的都是找任务。

大多数好的方法提出，是因为它有一个特定任务，这个特定任务恰好激发出一个通用方法。比如PPO（Proximal Policy Optimization，一种强化学习优化算法）一开始是为了解决一个特定问题；Transformer一开始是为了解决一个特定任务；Attention（注意力机制）受翻译这个任务影响很深。

但我的经历比较特殊，很多时候我是脑子里先想到一个东西，我觉得它很通用、很好。但我要去找一些任务，证明它很通用、很好，或者未来有价值。它现在还没有足够多价值，但你需要先找一些简单任务去证明它有价值。这是很难的。

创业需要product market fit，做research需要method-task fit（方法和任务的匹配）——这是最难的。

**张小珺：你曾经想到最激进的一个任务是怎样的？**

**姚顺雨：**这个时代再激进也不叫激进——Anything is possible。

毕业前我想得多的是，怎么创造一个爱因斯坦？我那时是比较academia（学院派）的人——你在普林斯顿，你的偶像是冯诺依曼、爱因斯坦——很自然，能想到最有意思的任务是，我能不能发现下一个相对论？这毫无疑问能标志，AGI或ASI（超人工智能）实现了。

后来，我到了硅谷，到了加州，进入公司之后，我发现人类的组织也是一个有意思的事情。如果能创造一家新的公司，创造一个one trillion dollar（一万亿美元）、基于Agent的公司，是很有意思的。

**张小珺：为什么是人类的组织也很有意思，而不是人类的产品很有意思？**

**姚顺雨：**产品当然很有意思，但很多组织的方式，就像一个general method（通用方法），能创造很多不一样的伟大的东西。比如股份制、组织架构，它就像非常通用的AI方法一样，创造了很多不一样的伟大的东西。

**张小珺：在你的成长路上，你的****mindset****（思维方式）跟同龄人差不多吗？还是不一样？**

**姚顺雨：**我的路径挺按部就班的，也没有跳级，没有做什么很surprising（让人惊讶）的事情。但我对一个东西的价值，或者taste（品味），有自己的看法。大家往往会倾向于做一个确定性比较高的事情，包括做研究、做公司。

**但我觉得恰好是这个时代，你去做上限更高的事情是更好的。**

因为现在有一个巨大的机会。如果没有这样一个巨大的机会，最佳路径可能是去做_incremental_（渐进式）、确定性强的事情，一步一步地积累。但恰好有一个上限非常高的事情。

如果你敢想，或者你胆子特别大，或者你想象力很丰富，就会有好事发生。

**张小珺：在你成长路上，对你启发大的是什么？是书、电影、音乐？哪些东西塑造了你的****mindset****？**

**姚顺雨：**看书挺有帮助，我是一个喜欢看杂书的人。什么书都看，什么电影都看，什么地方都想去。

我从小是一个比较_general_的人——我想试图变得很通用，试图了解很多不同的学科，做很多不同的事情。

但后来我发现，一个人即使再聪明、再有精力，他能理解的知识或能做的事情，也只是人类社会积累的知识的很小一部分。更好的是，你去创造一个比你更通用、更_general_的事情。

**我好像一直对于通用性，有一种执念或追求。**

**张小珺：通用性意味着什么呢？****——****可以足够简洁？**

**姚顺雨：**我不知道，但我从小就是想学习很多不同学科，都很有意思。

我在姚班很多同学，他们是那种很deep（深度的）、很focus（专注的）同学——我去做竞赛，我就把这个事做到极致，不停刷题，做到世界金牌。

但我好像不是那种性格，我是那种——我会看很多数学，也会看很多历史，会看各种各样乱七八糟的东西。

**张小珺：你会刷竞赛吗？**

**姚顺雨：** 我也搞竞赛，但没有本科同学那么厉害。我是信息学拿了全国银牌。

**张小珺：你是清华的说唱社联合创始人，对吧？我昨天去翻了一下你的网易云音乐。**

**姚顺雨：** 被你找到了？看来你有Deep Research的能力。

**张小珺：你最喜欢的说唱歌手是谁？**

**姚顺雨：** 我有很多喜欢的说唱歌手。说唱很有意思，每个人风格都很不一样，这点是很多人喜欢说唱的原因——你有自己的个性、自己的flow（节奏）、自己对生活的思考，你可以创造不一样的东西。它不一定是最好的，但大家是不一样的，这点很吸引人。

**张小珺：它跟你做****AI****有相似之处吗？**

**姚顺雨：** GPT-3刚出来，大家都觉得很厉害嘛，我想到第一个做的就是，看看能不能生成说唱歌词，并且有内容性。似乎今天还是很难。也许说唱歌手是一个被人们低估的工作。

**张小珺：填词，这不就是****predict next token****（预测下一个词元）在做的事情吗？**

**姚顺雨：** 一个东西好听、flow好、听上去舒服，是很难被量化的reward。很多时候一个东西，比如flow或style，它出现太多了，就不好了。独特反而是好的。真正伟大的说唱歌手，有很多独特的对生活的思考，而AI还没有生活。

**张小珺：有可能有对于智能来说，比语言更本质的存在吗？**

**姚顺雨：** 在特定领域，肯定有比语言更好的表示，比如围棋。

**但语言的诞生，不是为了处理某个特定任务的效率或交流，它为的是打通所有任务或者打通人的认知能力，形成一个通用的表示。**

它并不是为了某个特定任务最优而优化，它在特定任务上有冗余性，但它整体是通用的。

AI当然也可以创造一个新的语言，可能效率更高。但我觉得，最终大概率就是英语。因为人类已经有很强的先验知识，而且人有这样的价值取向或动机，想让机器的语言和人更像。

这样，我们可以更好地理解它、控制它、监控它、改变它、操控它，似乎是个很自然的选择。

**张小珺：你内心的驱动力是什么？你的愿景是什么？你****10****年后想成为谁？**

**姚顺雨：** 用一个非常俗的话说，希望你对这个世界创造一些不同——探索新的、根本性的研究，是一种创造不同的方式；创造一种完全不同的新的产品形态，也是一种创造不同的方式。

如果我现在去做一家类似xAI或Thinking Machine的公司，或者做一个类似Chatbot或Assistant的产品，还是可能赚很多钱，商业上很成功；但如果我做了一个形态很不一样的东西，失败了——我起码探索了不一样东西，会更有意思吧？

我导师令我印象最深的是这样一句话。学术圈经常发生这样的事——你有一个想法，然后别人做了，你会很烦。他说：If someone else can do it, then it's okay to let them do it（如果别人能做，那就让他们去做吧）。

从人类全局的角度，如果这个事情很多人能做，别人做可能是不是也没有什么区别？对这个社会，或者对整体来说，似乎没有什么变化。

当然，有人说这个非常假。最终你会发现，这个世上没有什么事情是不可替代的。相对论即使没有被提出，也会有人提出，没有什么事情是你不在，另一个人不能提出了——但是，我觉得这话还是有道理的。

如果你很清楚看到别人就在做这个事，你可以选择去和他卷。如果你要和他卷，你更有效率，或者你能做得更好，也是合理的。或者，你也可以去做一些不一样的探索。

我觉得，最终你要对这个社会产生价值。

但这个时代很幸运的一点：**这个技术非常通用，这个技术非常伟大，有足够多探索的空间。**

另一点是，我想让生活更有趣，更有意思，更快乐，就去做一些自己喜欢的事情。这很难用语言解释，就是一个taste（品味）或preference（偏好）的问题。

**张小珺：你会考虑创业吗？**

**姚顺雨：** OpenAI大多数人都会考虑创业。现在是非常exciting的时候。已经有很多OpenAI的人出去创业了。我需要去做更有挑战的事情，很自然会去创业。

但还是应该找到一个好的事情。我喜欢把事情想得清楚一点再去做。

**张小珺：我们最后还有几个快问快答。**

**姚顺雨：** 好。

**张小珺：一个全球范围内你喜欢的食物。**

**姚顺雨：** 我喜欢椰子。

**张小珺：一个全球范围内你喜欢的地点。**

**姚顺雨：** 我很喜欢伊斯坦布尔。

**张小珺：一个少有人知道但是必须知道的知识点。**

**姚顺雨：** 我挺建议大家去看《智能简史》这本书。有很多很有意思的知识点。

为什么大多数动物都是左右两侧对称，并且有一个像嘴一样的食物入口，有一个像肛门一样的食物出口？为什么气体是同一个口，而食物和水是两个口？这个很有意思，有些本质原因。

**张小珺：什么本质原因？**

**姚顺雨：** 你会发现，如果你要做navigation（导航），在这个世界中移动，左右对称的结构最优。世界上所有交通工具都是左右对称的。因为你可以一个方向前进后退，另一个方向向左转向右转。它和车和飞机都是左右对称，结构是类似的。

至于食物和气体还有别的原因。

**张小珺：基于你所有读过的书，推荐两本必读书。**

**姚顺雨：**《智能简史》这本书很有意思，是我去年读的。

我会推荐各种各样的自传。传记很有意思，好像你在体验别人的生活。

**张小珺：你心目中影响** **AI** **进程的几篇论文。**

**姚顺雨：** 有很多，我觉得没有最重要——Backprop（反向传播）、Transformer（变换器）、GPT（生成式预训练变换模型）——都是积累的过程，没有一个是最伟大的工作。

**李广密：你会对****Agent****创业者有什么建议吗？

**姚顺雨：** 可能有点老套：想清楚你的价值是什么。技术是工具，理解技术趋势很重要，但创造价值是最重要的——想清楚你为用户带来了什么样的增量价值，这是最主要的。

**张小珺：基于你当下的认知，一个最关键的重要的****bet****是什么？

**姚顺雨：** 就是bet on有different Super App（不同的超级应用）的产品形态，有不同的交互方式。

如果你不相信这一点，世界就变得很灰暗，就是只有OpenAI或者Anthropic有机会。

但如果你相信这一点，就会有很多新的机会。


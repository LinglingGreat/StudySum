---
title: 强化学习大师课-清华叉院助理教授吴翼
created: 2025-08-03
tags:
  - 播客
  - rlhf
---
[一堂「强化学习」大师课｜对谈清华叉院助理教授吴翼 - 42章经 | 小宇宙 - 听播客，上小宇宙](https://www.xiaoyuzhoufm.com/episode/67efcaf5f9578163d601286a)

> 这期国内 RL 领域的专家、清华大学交叉信息研究院助理教授吴翼，来讲讲 RL 的原理到底是啥、RL+LLM 的路径是怎么发展起来的、目前存在哪些非共识、未来还会怎么演变等等。
> 
> 人生就是一个 RL 的过程，区别是 RL 有明确的奖励函数，但是人生没有。可能如吴翼教授所说，我们首先都要以一种「最大熵」的方式去主动和不确定的世界交互，才能找到自己的奖励函数，优化自己的人生曲线。
> 
> 最后，吴翼教授的团队最近开源了一个 RL 框架 AReaL-boba，在 SOTA 7B 上跑出了 AIME24 61.9 的分数，也欢迎大家去 GitHub 关注。
> 
> 【人类博物馆】
> 
> 导游：曲凯，42章经创始人
> 
> 32 号珍藏：吴翼，清华大学交叉信息研究院助理教授，前 OpenAI 研究员。

【时光机】

1:51 到底什么是 RL？
- 传统机器学习的本质是记住大量标注过正确答案的数据对。
- RL 是一套用于解决多步决策问题的算法框架。它要解决的问题没有标准答案，每一步的具体决策也不受约束，但当完成所有决策后，会有一个反馈机制来评判它最终做得好还是不好。
4:25 人生就是一个强化学习的过程
- 其实人生也是一个强化学习的过程，因为你有很多种选择，但没人告诉你通往成功的路具体要怎么走。
- RL 的前提是你知道奖励函数到底是什么，比如打游戏，赢了就是好，但人生并没有这样的标尺。
- 人生有一个很好玩的地方是，你需要花很多时间先探索自己的奖励函数是什么，很多人可能努力了很长时间，最后却发现找错了奖励函数。
6:22 RL 和 LLM 是怎么结合起来的？

最初，RL 和 LLM 并没有什么关联。LLM 的本质是 next token prediction，每次只预测下一个词是什么。当我们给 LLM 看过足够多的数据后，它可能就具备了通用性和泛化能力，可以从中找到规律，预测一句话后面要接什么。用一句俗话来说，就是「熟读唐诗三百首，不会作诗也会吟」。

但这种训练方式有一个缺陷，就是 LLM 不会遵从指令。

这也是为什么 GPT-3 刚发布时，很多人觉得它不好用。举个例子，如果你对 GPT-3 说「解释登月」，你期待它给你介绍一下阿波罗计划、嫦娥计划等等。但可能它见过的数据都长成「解释登月是什么」这样，所以它会回你一个「是什么」。

7:01 强强联手第一步：InstructGPT，实现指令遵从

所以为了解决「指令遵从」的问题，OpenAI 第一次在 InstructGPT 中将 RL 和 LLM 结合了起来，然后发现效果还不错。

我们知道 RL 需要一个明确的任务，一些可用的决策动作，和一个 reward。还用刚刚的例子，OpenAI 的做法是把 RL 的任务定义成「解释登月」这个指令，决策是模型看到这个指令之后所说的每一个词，reward 就是模型最后给出的答案是不是和指令的需求一致。

大家发现没法定义一致，还是需要人来判断模型输出的答案哪个更好，或者写一些好的答案作为范本。最后是用人为定义的这些数据训练出了一个 reward model，也就衍生出了 RLHF (Reinforcement Learning from Human Feedback）的概念。


11:41「慢思考」的需求催生了 RL 的应用

RLHF 到去年为止的主要价值在于让 LLM 好用，好比让一个聪明的清北学生经过实习之后能变成一个很能打的员工。

它不存在 scaling law，也不能让 LLM 本身更聪明，但是 RL 可以。

这是怎么实现的呢？就是模型能遵从指令后，大家又在寻找除了预训练之外，能让模型变得更聪明的第二曲线。最后 Ilya 想到，LLM 在收到一个指令后会立马输出答案，但实际上人类在处理复杂问题之前往往会先思考。那如果能让 LLM 学会「慢思考」，是不是就能变得更聪明？

实现这一点的方法有很多，最终 OpenAI 发现了一种简单有效的方法，就是让模型「多吐点字」。收到指令后没必要着急给出答案，可以先吐 10000 个字再作答。这个吐字的过程就相当于思考，思考时间越长，答案就越准确。

这个过程就叫 inference time scaling。

**曲凯：**为啥 LLM 吐的字越多就越聪明？

**吴翼：**这件事到今天为止还没有一个理论能解释。所以从有「慢思考」的想法到真正实现它，OpenAI 其实花了一年半到两年的时间，这个过程需要极大的坚持。

总之 OpenAI 摸索出这个范式之后，问题就变成了要怎么训练一个能吐那么多话的模型。

这时又轮到 RL 登场了，因为吐字的这套逻辑和打游戏的逻辑非常像，我们只期待模型最后能输出正确答案，中间它吐的那几万个字是什么无所谓。

那新的问题又来了，要怎么评估模型慢思考的过程中产生的这些数据？因为和 RLHF 不同，RLHF 的输出可能只有几百个字，所以我们可以人为标注和排序。但是 RL 的探索过程非常开放，在慢思考时可能会输出几十万个 token，人工标注是做不过来的。

最后 OpenAI 用了一种非常简单的方式来解决这个问题，就是只用有标准答案的问题去训练模型的推理能力。比如解方程，如果正解是 x=3，那模型写 x=3 就正确，写 x=4 就不对。至于它是怎么推出的这个答案，我们不管。

16:10 为什么说 Anthropic RL 做得特别好？

**曲凯：**那如果只看最后的答案对不对，思考过程还会有做得好与不好的区别吗？

**吴翼：**有区别。我觉得 Anthropic 就做得特别好。

因为我们很难限制模型的思考过程，所以模型经常在一些很简单的问题上也要思考很久。比如有人会抱怨，问 DeepSeek「1 + 1 等于几」这么简单的一个问题，它也要想半天。

相比之下，Anthropic 在这件事上就处理得比较好。你问 Claude「1 + 1」它就直接出答案，你问它一个复杂点的问题，比如说算个 24 点，它才想上个 10 秒。

但正是因为我们对模型的思考过程不做限制，所以模型也会衍生出一些很有意思的泛化能力。比如我们问 R1 一个没有标准答案的哲学问题，它想一想也能讲得头头是道。

**曲凯：**这是怎么做到的？

**吴翼：**首先模型训练时本身就会产生一定的泛化能力，但这还不太够。所以当我们用大量有标准答案的理科问题训完模型之后，还会用一些文科训练把它往回掰一掰，不然这个模型就太理性了。

**曲凯：**就有点 Nerdy (笑)。

**吴翼：**是哈哈，所以你看 DeepSeek 的那篇 paper，其实他们最后是把一个 Nerd 模型和一个具备人文属性的 base 模型合了起来，又做了 SFT 和 RLHF，最终才有了比较平衡的 R1。但是怎么合起来保留两者的能力，也是个挑战。


21:17 行业对 RL+LLM 的最优路径形成共识了吗？

**曲凯：**那 R1 之后，现在整个行业对 RL+LLM 的最优路径形成共识了吗？

**吴翼：**国内一些做得比较好的团队，比如豆包和 DeepSeek，肯定还在探索各种新的可能性。其它团队还处在一个追赶的阶段，虽然他们可能也有自己的思考。

而海外的很多团队都有不同的 belief。拿 OpenAI 来说，他们的 RL 已经从纯推理进化到 Agent 范式了。

也就是从 InstructGPT 时传统的单轮交互，跃迁到多轮交互，而且能自由上网，与虚拟世界互动，这是一个非常大的进步。

25:11 RL 起来之后，对 Agent 的影响是什么？

**曲凯：**Agent 这个概念，我记得 23 年初就有了，也出现了 AutoGPT 等等，但如果我们现在回头来看，是不是那个时候还不具备 Agent 的基础？Agent 是从 RL 起来之后才可以开始做了吗？

**吴翼：**Agent 最核心的能力在于对文本之外的世界产生影响。当年的 AutoGPT 和 LangChain 等产品其实已经具备这种能力，但它们实现这一能力主要依赖 Prompt Engineering，你必须把每一步拆解清楚，让它按部就班地照做。而现在有了 RL 之后，像 Operator，Deep research 就可以自主探索，端到端自己完成任务了。

32:11 Intelligence = LLM (理解) × RL (决策)，二者缺一不可

**曲凯：**如果 RL 已经能很好地完成复杂决策了，那能不能抛开 LLM，只用 RL 来做呢？

**吴翼：**OpenAI 很早就试过这么干，但屡试屡败。

第一次是 16 年的一个叫  World of Bits 的项目，他们试图通过 RL 来完成在网页上订机票的任务，失败了。

第二次是在 20 年，他们的机器人团队想通过图像输入来控制机器人整理桌面，又失败了。但这次失败后，另一个团队在 RL 中引入了一个预训练模块，就把这件事给做成了。

于是大家发现，仅仅通过 RL 很难训练出一个通用模型，虽然 RL 有很强的决策能力，但它的理解能力不足，需要有一个经过预训练的模型提供一些基础的理解能力、记忆能力等等。

所以 LLM 和 RL 之间是乘法关系，二者相乘才能实现最后的智能，缺一不可。

**曲凯：**这样捋下来其实很多东西就串起来了，之所以 OpenAI 能把 RL 先做起来，是因为他们一开始就在做 RL，后面把 LLM 和 RL 结合起来是一件很自然的事情。

那如果理解能力是交给了大语言模型或者多模态模型负责，生成能力呢？这个和 RL 有关系吗？

**吴翼：**RL 和生成之间也没有太大关系。另外我一直认为生成容易，理解难。

**曲凯：**我前段时间听到过一个非常反常识的说法：理解一张图片所需的 token 其实比生成要高。

**吴翼：**对，需要的 token 多正是因为理解是一件更难的事情。

举个例子，如果你收到了一个俄文指令，但你根本不会俄文，那你可能自己悟一辈子也悟不会。而如果你已经懂俄文了，那再用俄文培训你做任何事情，用俄文写诗，买票，都不会那么困难。

**曲凯：**所以相当于 LLM 负责理解跟记忆，RL 进来之后给 LLM 加上了长程思维能力，并且负责决策跟执行。那能不能说 RL 和 LLM 放到一起就已经完整了呢？

**吴翼：**我只能说我们现在看到的最好的 Agent 模型确实是这样整合出来的，但我觉得这个范式和 RL 算法都还有很大的发展空间，因为这个路径才刚刚起步，RL 的 scaling law 也才刚刚开始。

34:14 Scaling law 的未来
34:33 Pretraining 的两个发展方向
36:43 RL 还处于早期，进入深水区后可能会走向分化

**曲凯：**具体会怎么发展？现在大家都觉得预训练的 scaling law 已经不太 work 了，RL 会有多大的发展空间？

**吴翼：**这里要稍微更正一下，预训练的 scaling law 并不是不 work 了，只是收益变小了。

现在预训练还有两个重要的方向，一个是数据，比如如果要强化多模态的理解能力，那就需要大量的图文混合数据，而且图文间需要有比较好的逻辑关系。这类高质量数据网上是远远不够的，所以就需要发展合成数据。

另一个方向是把模型做小，尽量蒸馏出一个更小规模、但具备同等能力的模型。因为 RL 模块已经足够复杂，在如此高的复杂度上，LLM 规模越大，训练的不稳定性就越高。很多人总说蒸馏可耻，但实际上如果能做到像 o3-mini 那样的程度，不仅不可耻，反而非常厉害。

对于强化学习来说，首先它的 scaling law 还处于初始阶段，斜率足够高，决策能力还会持续提升。

其次，不同的大模型公司在 RL+LLM 的路径上走向深水区之后，方向上也会出现分化。目前主要有三个大的分支：

1) 编程，典型代表是 Anthropic。

2) Agent，典型代表是 OpenAI。

3) 通用的泛化能力 (比如泛化到哲学、文学等领域)，典型代表是 DeepSeek。

**曲凯：**强化学习做得好与不好，在用户侧的感知主要是什么？

**吴翼：**首先可以判断准确率。不管是做数学题，还是写分析报告，或者是在网站上完成一些操作，准确率都是可以验证的，只不过有的可以通过 benchmark 来验证，有的还是要看人的体感。

此外，多轮交互的体验也特别重要。举个例子，当我们让模型协助 debug 时，它通常会直接开搞。但 Claude 就很不一样，它会问问题，比如你装的环境包是什么之类的，用户体验就非常好。

**曲凯：**那这个训练过程是怎么实现的呢？我感觉预训练的训练过程比较好理解，无非是喂更多的数据，或者做些算法调优，RL 需要做什么？

**吴翼：**对于强化学习来说，基建最最重要，其次是数据，再其次才是算法。

基建可以理解为强化学习的框架，它决定了你的迭代效率。比如我们之前做过一版框架，以前训练 7B 模型需要一周，而迭代了框架版本之后，只需要两天就能把模型训一遍，这样一周就多了三次迭代试错的机会。

数据的作用很好理解，就好比学数学，差的老师只会让学生狂刷小学题，刷了一堆也没用，而好的老师则会针对性地出题，可能只需要两道题，就能让学生掌握某个知识点。

最终的效果是各种因素耦合的结果，而且也存在很大的不确定性。

强化学习很像炼丹，它是一门玄学，你经常会发现，它一直涨势很好，但是某一刻莫名其妙就不涨了，或者说一开始不涨，突然开始猛增。

40:02 大模型团队的组织架构要如何设计？

**曲凯：**大模型现在主要是两条主线，一条是 LLM 加强理解能力，一条是 RL 加强决策能力。那对于大模型团队来说，该怎么设计相应的组织架构呢？

**吴翼：**最好的情况就是这两条线别分得太开，因为 RL+LLM 的范式非常复杂，需要团队中的每个人都有破圈意识，最好什么都懂一点，才不容易出问题。举个例子，一个做 RL 后训练的人，也得懂一些预训练的知识，否则 Ta 没办法判断拿到的预训练模型缺什么能力、少什么数据。

但确实不同范式之间差别比较大，所以可能至少要有一个预训练团队和一个后训练团队。再往下分工，我觉得大致可以通过目标来区分，比如可以有多模态、RLHF 分支等等。

43:21 一个反常识：对 AI 来说，理解比生成更难，token 消耗更大
47:38 现在做 Agent 一定需要一个懂 RL 的人吗？

**曲凯：**现在很多做 Agent 的公司都想配一个懂 RL 的人，你觉得这是必要的吗？

**吴翼：**我觉得很 make sense。虽然现在 RL 的门槛还很高，但一两年内总会降下来，未来很可能大家都需要用 RL 做一些简单的微调，先储备一些人才总是没错的。

**曲凯：**但未来大家真的需要自己做 RL 吗？你看前两年大家都在讨论应用公司到底要不要自己做预训练，现在基本上大家都默认使用开源模型就足够了。那如果过两年开源模型的 RL 水平也跟上来，大家是不是也可以直接用现成的？

**吴翼：**这就是创业公司自己需要思考的问题了。

且不说这件事的答案到底是怎样的，我认为有一点是比较明确的，创业公司不该有终局思维。

相反，创业公司的机会恰恰在终局到来之前。Manus 就是一个特别好的例子，在市场上还没有太多类似产品的时候，他们以最快的速度做出了一个可用的产品，抓住了机会。

现在 AI 发展得这么快，如果你真的去考虑终局，很可能会陷入一种无意义感，觉得世界上没有你的机会。如果你一定要追求笑到最后，那还不如趁早投奔字节（笑）。


49:32 为什么 RL 人才这么稀缺？

**曲凯：**是。那从 Alpha Go 到现在，大家讲 RL 已经快 10 年了，为什么相关的人才还是这么稀缺？

**吴翼：**主要还是门槛太高了。你去看论文引用数，研究 RL 的，会比研究 NLP (自然语言处理)、CV (计算机视觉) 的，少一个数量级。要想把 RL 环境配好、跑完、能复现结果，就会筛掉一大批人，而且 RL 的数学也更麻烦些。

另外相比 NLP、CV 来说，RL 在工业界大规模使用的机会比较少，大家只能在学校里做，没有一个工业级的人才池，所以人才体量也会小很多。

最后就是强化学习对工程的要求非常非常高，所以导致大家也没有好的基建条件去做强化学习。

所以其实我回国之后做了很多开源的工作，就是希望能让更多的人把 RL 用起来。


56:10 RL 目前三大分支：泛化 (DeepSeek)、代码 (Anthropic)、Agent (OpenAI)
58:55 框架对 RL 意味着什么？

**曲凯：**你们最近就联合蚂蚁研究院新发了一个强化学习的开源框架 AReaL-boba。

**吴翼：**对。前面提到过，框架其实就是基建，是 RL 训练和迭代的基础。但开源的 RL 框架本就不多，用起来可能也不太顺手，所以我们就自己搞了一个。

我们的这个框架用起来比较稳定，速度也比较快，把 7B 的模型做到了 SOTA 标准。

**曲凯：**也欢迎大家去 GitHub 上关注这个项目。

那如果国内的公司现在想招或者培养一个 RL 人才，该怎么做？

**吴翼：**这很难讲，因为每个团队的风格和所处阶段都不太一样。如果非要给一条建议，我会觉得「动手能力」和「不给自己设限」这两点很重要，现在开源的项目和相关的资料信息非常多，所以关键还是要看这个人是不是发自内心地想学习，愿不愿意动手去实践。


1:02:51  RL 在海内外进展还有明显差距
1:04:42 想做好 RL，基建≫数据＞算法
1:06:05 研究 RL 收获的一些人生启发

**曲凯：**最后，我想再回到开头你提到的「人生就是一场强化学习」这个点。你研究了这么多年 RL，是不是在自己做决策时也能获得一些参考？有没有什么可以泛化到人生中的经验？

**吴翼：**我有一系列围绕 Diversity-driven RL 的工作，还蛮有参照意义的。

传统的 RL 只在乎结果，所以一旦发现能稳赢的策略之后，就会无限重复这个套路。

但人类不是这样思考的。人虽然也想赢，但会想换个赢法。就比如踢足球，虽然我知道带球单刀特别好进球，但总这么踢我会觉得无聊，就想换头球试一试。

人之所以为人，是因为人都是 Diversity-driven 的，所以才有了截然不同的经历和多姿多彩的人生。

**曲凯：**我们之前录过一期[德扑主题的播客](https://mp.weixin.qq.com/s?__biz=MzIyMDE5OTYyMw==&mid=2651050651&idx=1&sn=074df0ea770615efe7b445a59010b45c&scene=21#wechat_redirect)也聊过类似的一点，就是顶尖的牌手越来越趋同，一个比一个像 AI，然后这个游戏就变得没意思了。

**吴翼：**对。所以我们试着给 AI 强化学习的过程加了一个限制条件，不仅要求结果的质量，还要求结果的多样性——每次都要找到一个新的解决路径。结果模型自然而然地发现了一些很好玩的东西。

映射到人生中，我发现现在很多人都倾向于选择风险最低的路径。我跟学生聊天的时候就发现，很多同学在做升学决策时，考虑问题的角度往往是为了「求稳」，比如有的同学觉得去美国可能会有签证风险，于是就会选择国内保研。

但我觉得人还是要追求「熵值最大化」的生活方式。

**曲凯：**不过强化学习是可以无限试错的，而人生总有一种滋味叫「后悔」。

**吴翼：**是有这个差别，但我觉得还是要勇敢一点，很多人觉得人生只有一次，我觉得不是，人生怎么着也能试错个三四次，尤其 20 岁的年纪，你做任何事情都是对的。

前提是你要敢于选择，多跳出去看看，不要老局限在 local optimum (局部最优解) 里。比如我当年在伯克利读书的时候，如果不是我自己主动地去敲了隔壁另一个导师的门，我可能最终不会拐上强化学习这条路。

**曲凯：**是。最后我还想和你探讨一下奖励函数这件事。你开头说人很可能奋斗了一段时间，却发现最终的 reward 并不是自己想要的。那在 RL 里会遇到类似的问题吗？有可能中途改变奖励函数吗？

**吴翼：**在传统 RL 里不大行，但因为我研究多智能体强化学习和人机交互，经常要面对没有标准问题和清晰目标的情况，所以就需要训练 AI 主动搜集信息、探索正确的奖励函数的能力。

比如我们做过一个能和人一起玩 Overcooked（一个厨房经营游戏）的 AI，在人机合作的过程中，这个 AI 需要猜出人想干嘛，然后和人一起打配合。那在这个过程中 AI 就必须大胆尝试，在尝试中得到各种信号，然后揣摩出人的意图，也就是自己的 reward 到底什么。

人生也是一样，要想找到自己的奖励函数，首先需要多主动探索，先和世界交手个三百回合。


【Reference】

吴翼的 PhD 毕业论文：On Building Generalizable Learning Agents

吴翼获机器学习顶级会议 NIPS2016 最佳论文奖的论文： Value Iteration Network

吴翼提到的他非常喜欢的有关 Diversity-Driven RL 的两篇论文：Iteratively Learn Diverse Strategies with State Distance Information、Discovering Diverse Multi-Agent Strategic Behavior via Reward Randomization

吴翼团队和蚂蚁研究院开源的强化学习训练框架：AReaL-boba

[mp.weixin.qq.com/s/uce0H6JNz4PYzIIn-Xkv\_A](https://mp.weixin.qq.com/s/uce0H6JNz4PYzIIn-Xkv_A)


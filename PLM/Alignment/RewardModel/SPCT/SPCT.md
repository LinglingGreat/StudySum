---
title: ITS_GRM
created: 2025-04-06
tags:
  - reward模型
type: 论文
papername: 
conference: ACL/EMNLP/NAACL/EACL/COLING/SIGIR/AAAI/ICML/ICDM
year: 2025
institution:
  - DeepSeek
  - 清华
---

## 论文基本信息

标题：Inference-Time Scaling for Generalist Reward Modeling

作者：DeepSeek与清华团队

链接：http://arxiv.org/abs/2504.02495

代码：即将开源模型


## 背景

自从DeepSeek-R1发布之后，数学、代码等可进行结果验证领域的RL训练路径似乎比较清晰了，就是采取ORM（outcome reward model）的方式。虽然这不代表PRM（Process Reward Model）方法就不生效了，但ORM的简单性使得结果可验证领域的研究多了许多，效果也提升了一大截。

但随之而来的另一个问题是，结果不可验证（或者说没有标准答案）的领域的RL又该何去何从？这些领域目前还是需要依靠高质量的数据标注（需要制定标准，进行人工标注或者大模型标注），且每个领域都需要单独收集数据，并不具有泛化能力。有什么办法能够解决数据获取、泛化能力这2个难点呢？这篇DeepSeek和清华合作的论文也许能给我们提供一些思路。

论文提出，现有的研究主要集中在特定领域的奖励生成上，而在一般领域的奖励生成更具挑战性。此外，现有研究主要集中在如何通过增加训练计算来提高奖励质量，但很少关注推理时间的可扩展性。因此论文的研究内容包括如何通过增加推理计算来提高通用奖励建模的质量和可扩展性，并提出了一种新的学习方法Self-Principled Critique Tuning（SPCT），以促进GRMs的推理时间可扩展性。

## 预备知识

不同RM方法的比较：

![](img/Pasted%20image%2020250406151247.png)

生成式方法有scalar，semi-scalar, and generative
- scalar：输入query和response，输出标量值作为reward
- semi-scalar：除了输出标量值之外，还会输出评判式文本（critique）
- generative：只生成 critiques 作为文本 reward，可以从中提取 reward 值（比如分数被包含在文本中）。
打分方法有pointwise and pairwise
- pointwise：给每个response一个分数
- pairwise：从给定候选中选出最好的response

关于上述图其实还有很多可以讨论的点，具体可以看这篇文章[精讲（但绝对讲明白）Deepseek的新论文SPCT](https://zhuanlan.zhihu.com/p/1891591607582688267)，写的非常通俗易懂了。

为了研究推理时间可扩展性，论文集中讨论基于采样的方法：给定query和response对，可以生成多个reward，最终reward会综合考虑这些reward的结果（比如多数投票决定，或者取平均值）。

scalar方法不适合此场景，因为每次生成的奖励值都是一样的。pairwise则是输入灵活性不足，不支持单个(query, response)输入。因此论文采取了pointwise GRMs，综合考虑了推理时间可扩展性（可以通过多次采样得到更好的reward）和输入的灵活性（支持输入单个/多个responses来进行打分）。

	为什么没有选择图中的CLoud方法呢？

从效果上来说，下图表明了CLoud方法的拉垮。。**半标量模型除了生成标量奖励外，还会生成文本评论 (critique)。虽然评论内容可能因采样而有所不同，但最终提取的标量奖励的方差可能有限，导致基于采样的投票或平均等推理时扩展方法带来的性能提升不明显。**

这是因为标量奖励通常是基于评论以某种方式（例如，通过一个额外的价值头部 或预定义的规则）提取出来的。如果这个提取过程对评论的细微变化不敏感，或者标量奖励的输出范围有限，那么多次采样的结果仍然会非常接近，导致聚合后的性能提升不大.

![](img/Pasted%20image%2020250406181026.png)



pointwise GRMs的公式定义如下，很清晰了：

![](img/Pasted%20image%2020250406154412.png)

GRM需要在各种不同的领域中生成高质量的奖励，而不仅仅是某个特定领域。因此奖励的标准更加多样化和复杂，并且通常没有明确的参考或标准答案。为此，对于一般领域，论文采用原则来指导奖励生成，而不是人为的规则。LLM 的原则最早在 Constitutional AI 中引入，是指导 LLM 或精选分类器构建安全数据管道的手工标准。根据原则，GRM 的奖励生成更改为

![](img/Pasted%20image%2020250406154902.png)

论文进行了一项初步实验，使用 Reward Bench 的 Chat Hard 子集和 PPE 基准的 IFEval 子集，以检查适当的原则对奖励质量的影响。

测试了不同的LLM自己生成原则，为每个样本生成四次pointwise reward，然后筛选出那些其相应奖励与正确答案一致的原则。和没有原则时候的结果做对比。结果表明，在正确的标准下，适当的原则能更好地指导奖励生成。

![](img/Pasted%20image%2020250406155147.png)

## Self-Principled Critique Tuning (SPCT)

SPCT能够使pointwise GRMs学习生成更具适应性的、高质量的原则，以便有效地指导critique的生成。

SPCT包括两个阶段：（1）rejective fine-tuning作为冷启动，（2）rule-based online RL用于增强一般奖励生成。

前面的内容表明，合适的原则可以在特定标准内指导奖励生成，这对于高质量的奖励至关重要。然而，要大规模生成适用于通用奖励模型的有效原则仍然具有挑战性。为了解决这个挑战，论文建议将原则从理解阶段解放出来，应用于生成阶段，即将原则视为奖励生成的一部分，而不是预处理步骤。公式（2）中，原则是预先定义好的，公式（3）中GRMs会自行生成原则，然后基于原则再生成critiques。

![](img/Pasted%20image%2020250406160430.png)


这种转变使得原则能够基于输入query和response生成，自适应地调整奖励生成过程。通过对通用奖励模型（GRM）的后期训练，可以进一步提高原则和相应批评的质量和细粒度。在大规模生成原则的情况下，GRM有可能在更合理的标准和更细的粒度下输出奖励，这对于推理时的扩展也至关重要。

Rejective Fine-Tuning (Cold Start): 拒绝微调阶段的核心思想是让通用奖励模型（GRM）能够生成格式正确的原则和批评，并适用于各种输入类型。与之前的研究将单一、成对和多响应的奖励模型数据混合在不同格式中的做法不同，论文采用在前面介绍的pointwise GRM，以灵活地为任意数量的响应生成相同格式的奖励。在数据构建方面，除了通用指令数据外，还从奖励模型数据中根据查询和响应的不同数量，使用预训练的GRM采样轨迹。对于每个查询及其对应的响应，采样进行$N_{RFT}$次。拒绝策略也进行了统一，即拒绝那些预测奖励与真实值不一致（不正确）的轨迹，以及所有$N_{RFT}$轨迹均正确的查询和响应（过于简单）。

判断预测奖励Si是否正确的公式如下，其中 ri​ 表示针对查询 x 的第 i 个响应 yi​ 的真实奖励：

![](img/Pasted%20image%2020250406161730.png)

这个公式是说
- 对于单个响应的情况（n=1），如果预测的奖励 S1​ 等于真实的奖励 r1​，则认为预测是正确的。
- 对于多个响应的情况（n≥2），需要确保预测的奖励中的最高值对应的是真实最佳回复。

然而，与之前的研究类似，论文发现预训练的GRM在有限的采样配额内，难以为部分查询和相应响应生成正确的奖励。因此，论文选择性地在GRM的提示中添加$arg⁡max_l⁡\{r_l\}_{l=1}^n$（称为提示采样hinted sampling），以期望预测的奖励与真实值对齐，除此之外还进行非提示采样。对于提示采样，每个查询及其响应仅采样一次，且仅在不正确时才拒绝轨迹。超越之前的研究，论文观察到提示采样的轨迹有时会简化生成的批评，尤其是在推理任务中，这表明在线强化学习对GRM的必要性和潜在好处。

Rule-Based RL：基于规则的在线RL则进一步优化原则和critiques生成。采取GRPO训练方式，基于规则的结果奖励。在rolling out中，GRM会生成原则和critiques，从中抽取出预测的奖励值，和真实值做比较。这里没有format reward，而且会应用更大的 KL 惩罚系数来确保格式并避免严重的偏差。

![](img/Pasted%20image%2020250406162509.png)

其中：
- n 是候选回复的数量。
- Si​ 是模型为第 i 个回复生成的点式奖励（pointwise reward）。
- ri​ 是第 i 个回复的真实奖励标签（ground truth reward）。

这个奖励函数的设计目的是让模型能够准确地区分最佳回复。具体来说：

1. **多回复情况（n≥2）**：
    
    - 如果有多个候选回复，模型需要生成的点式奖励 Si​ 能够正确地识别出最佳回复（即真实奖励最高的回复）。
    - 如果模型成功地将最高奖励分配给了真实最佳回复，则奖励函数返回 1。
    - 否则，返回 −1。
        
2. **单回复情况（n=1）**：
    
    - 如果只有一个候选回复，模型生成的点式奖励 S1​ 需要与真实奖励 r1​ 一致。
    - 如果一致，则奖励函数返回 1；否则返回 −1。

奖励函数鼓励GRM通过在线优化的原则和批评来区分最佳响应，有利于在推理时进行有效的扩展。奖励信号可以无缝地从任何偏好数据集和标记的LLM响应中获得。
## Inference-Time Scaling with SPCT

为了在通用奖励生成中利用更多的推理计算来进一步提升DeepSeek-GRM的性能，论文探索了基于采样的策略，以实现有效的推理时扩展。

1. Voting with Generated Rewards

![](img/Pasted%20image%2020250406163154.png)

由于 Si，j 通常设置在较小的离散范围内，例如 {1， ...， 10}，因此投票过程实际上将奖励空间扩大了 k 倍，并使 GRM 能够生成大量的原则，有利于最终奖励的质量和颗粒度。一个直观的解释是，如果每个原则都可以被视为判断视角的代理，那么更多的原则可能会更准确地反映实际分布，从而导致scaling有效性。值得注意的是，为了避免位置偏差以及为了多样性，响应在采样前会进行随机排序。

2. Meta Reward Modeling Guided Voting

DeepSeek-GRM 的投票过程需要多次采样，并且由于随机性或模型限制，生成的一些原则和批评可能会有偏见或质量低下。因此，我们训练一个 meta RM 来指导投票过程。meta RM 是一个pointwise scalar RM，经过训练以识别 DeepSeek-GRM 生成的原则和批评的正确性，使用二元交叉熵损失，label基于公式（4）获取。

meta RM 输出 k 个采样奖励的 meta 奖励，最终结果由排名前 $k_{meta} ≤ k$ 个 meta 奖励的奖励投票，从而过滤掉低质量的样本。

SPCT方法的总结：

![](img/Pasted%20image%2020250406160219.png)

训练时候通过拒绝采样（过滤掉不正确的/太简单的样本）微调GRM，使其学习正确的输出格式。然后采用在线RL训练进一步提升生成性能。

推理的时候，先生成原则，再生成评判文本，从中抽取出reward。汇总reward有2种方式，1种是投票法：把所有的reward加起来，比较哪个response的reward高；另1种是meta RM方法，先用 meta RM获取前k个更具有可靠性的输出（图中是绿色的第1和第3个输出），把这k个的结果加起来进行比较。
## 实验
![](img/Pasted%20image%2020250406163858.png)

SPCT 提高了 GRM 的一般性奖励生成能力，与标量和半标量 RM 相比，偏差明显减少。

![](img/Pasted%20image%2020250406163922.png)

SPCT 提高了 GRM 的推理时间可扩展性，而 meta RM 总体上进一步提高了扩展性能。

在没有拒绝性采样批评数据的冷启动的情况下，一般指令调整的 GRM 在经历在线 RL （66.1 → 68.7） 后仍然显着提高。此外，非 提示采样似乎比提示采样更重要，这可能是因为提示采样轨迹中出现的shortcut。这表明在线RL的重要性

与以前的工作 （Cao et al.， 2024） 一致，论文确认通用指令数据对于 GRM 的性能至关重要。原则生成对于 DeepSeek-GRM-27B 的贪婪解码和推理时间缩放的性能都至关重要。对于推理时间扩展，meta RM 引导投票在不同的 kmeta 下显示出稳健性。

![](img/Pasted%20image%2020250406164332.png)

使用 32 个 DeepSeek-GRM-27B 样本进行直接投票可以达到与 671B MoE 模型相当的性能，而元 RM 引导投票可以用 8 个样本达到最佳结果，证明了 DeepSeek-GRM-27B 的推理时间缩放与缩放模型大小相比的有效性。此外，论文用包含 300 个样本的下采样测试集对 DeepSeek-R1 进行了测试，发现其性能甚至比 236B MoE RFT 模型差，这表明扩展推理任务的长链思维并不能显著提高通才 RM 的性能。

## 总结

这篇论文主要讲了怎么训练一个更适用于通用场景的奖励模型，通过RFT+在线RL来训练奖励模型学会自己输出评估标准，接着基于评估标准又进一步输出单个/多个回复的奖励的分析过程和结果。此外，在推理阶段还可以通过多次采样的方法让模型生成多个候选的评估标准+分析过程+结果，用投票法或者meta RM方法进一步提升奖励结果的准确性。

之前看DeepSeek-R1的论文，他们有提到非推理场景的奖励模型是生成式奖励，不仅输出最终奖励，还包括导致奖励的推理链。这篇论文虽然没有直接提到思维链，但critiques也算是一种广泛意义上的思维链了，通过不仅仅输出奖励值，还要输出分析过程这种方法可以有效地减少奖励黑客问题，也能一定程度上提升准确率。这种方式相比只输出标量值的奖励模型，会增加挺多训练成本的，毕竟输出critiques的推理时间更长了。而论文中的多次采样增加推理时间扩展性的方法虽然听起来有效，但又再次增加了训练成本。。。不过好处是，小模型增加推理时间扩展性似乎可以比肩更大的奖励模型，这样看来是不是训练成本也不一定增加了呢？
## 参考资料

[精讲（但绝对讲明白）Deepseek的新论文SPCT](https://zhuanlan.zhihu.com/p/1891591607582688267)
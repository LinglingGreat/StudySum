---
title: DPO
created: 2024-06-15
tags:
  - alignment
type: è®ºæ–‡
papername: 
conference: ACL/EMNLP/NAACL/EACL/COLING/SIGIR/AAAI/ICML/ICDM
year: 
institution:
---

## è®ºæ–‡åŸºæœ¬ä¿¡æ¯

æ ‡é¢˜ï¼š

ä½œè€…ï¼š

é“¾æ¥ï¼š

ä»£ç ï¼š

æ¡†æ¶å›¾ï¼š


## æŸå¤±å‡½æ•°
å¯¹äºåŒä¸€ä¸ª propmtï¼Œç»™å®šä¸€ä¸ªå¥½çš„å›ç­”Â ğ‘¦ğ‘¤Â å’Œä¸€ä¸ªä¸å¥½çš„å›ç­”Â ğ‘¦ğ‘™ï¼Œ**é€šè¿‡é™ä½ä¸å¥½å›ç­”è¢«é‡‡æ ·çš„æ¦‚ç‡ï¼Œæå‡å¥½å›ç­”çš„æ¦‚ç‡**ï¼Œä»è€Œè¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚è¿™ä¸ªæ•°æ®å’Œè®­ç»ƒ Reward Model çš„ pair æ•°æ®æ ¼å¼å®Œå…¨ä¸€è‡´ï¼Œéƒ½æ˜¯åŒä¸€ä¸ª prompt å¯¹åº”ä¸¤ä¸ªä¸åŒè´¨é‡çš„ responsesã€‚

![](img/Pasted%20image%2020240615170451.png)

[[æºç ](https://link.zhihu.com/?target=https%3A//github.com/huggingface/trl/blob/main/trl/trainer/dpo_trainer.py)] ä¸­è®¡ç®— loss çš„éƒ¨åˆ†ï¼ˆæœ€ç®€å•çš„sigmoidæŸå¤±å‡½æ•°ï¼‰ï¼š

```python
def dpo_loss(
        self,
        policy_chosen_logps,
        policy_rejected_logps,
        reference_chosen_logps,
        reference_rejected_logps,
    ):
        """Compute the DPO loss for a batch of policy and reference model log probabilities.

        Args:
            policy_chosen_logps: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,)
            policy_rejected_logps: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,)
            reference_chosen_logps: Log probabilities of the reference model for the chosen responses. Shape: (batch_size,)
            reference_rejected_logps: Log probabilities of the reference model for the rejected responses. Shape: (batch_size,)
        """
        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps

        pi_logratios = pi_logratios.to(self.accelerator.device)
        ref_logratios = ref_logratios.to(self.accelerator.device)
        logits = pi_logratios - ref_logratios

        losses = -F.logsigmoid(self.beta * logits)
        return losses
```

rewardsçš„è®¡ç®—æ–¹æ³•ã€‚æ‰€ä»¥DPOçš„lossä¹Ÿå¯ä»¥ç†è§£ä¸ºï¼š

`L_DPOâ€‹=âˆ’log(sigmoid(chosen_rewardsâˆ’rejected_rewards))`

```python
chosen_rewards = self.beta * (policy_chosen_logps - reference_chosen_logps).detach()
        rejected_rewards = self.beta * (policy_rejected_logps - reference_rejected_logps).detach()
```

è®¡ç®—logpsçš„å‡½æ•°

```python
def get_batch_logps(
        self, 
        logits: torch.FloatTensor,
        labels: torch.LongTensor,
        average_log_prob: bool = False,
        label_pad_token_id: int = -100,
        is_encoder_decoder: bool = False,
    ) -> torch.FloatTensor:
        """Compute the log probabilities of the given labels under the given logits.

        Args:
            logits: Logits of the model (unnormalized). Shape: (batch_size, sequence_length, vocab_size)
            labels: Labels for which to compute the log probabilities. Label tokens with a value of label_pad_token_id are ignored. Shape: (batch_size, sequence_length)
            average_log_prob: If True, return the average log probability per (non-masked) token. Otherwise, return the sum of the log probabilities of the (non-masked) tokens.

        Returns:
            A tensor of shape (batch_size,) containing the average/sum log probabilities of the given labels under the given logits.
        """

        if logits.shape[:-1] != labels.shape:
            logger.info(f"logits shape: {logits.shape}; label shape: {labels.shape}")
            raise ValueError("Logits (batch and sequence length dim) and labels must have the same shape.")

        if not is_encoder_decoder:
            labels = labels[:, 1:].clone()
            logits = logits[:, :-1, :]
        loss_mask = labels != label_pad_token_id

        # dummy token; we'll ignore the losses on these tokens later
        labels[labels == label_pad_token_id] = 0

        per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)

        if average_log_prob:
            return (per_token_logps * loss_mask).sum(-1) / loss_mask.sum(-1)
        else:
            return (per_token_logps * loss_mask).sum(-1)
```

### chosen_logitså’Œchosen_logpsçš„å…³ç³»

1.Â **`chosen_logits`**

- **å®šä¹‰**:
    - `chosen_logits`Â æ˜¯æ¨¡å‹å¯¹Â **chosen responses**Â çš„åŸå§‹è¾“å‡ºï¼ˆæœªå½’ä¸€åŒ–çš„ logitsï¼‰ã€‚
    - å½¢çŠ¶ä¸ºÂ `(batch_size, sequence_length, vocab_size)`ï¼Œå…¶ä¸­Â `vocab_size`Â æ˜¯è¯æ±‡è¡¨çš„å¤§å°ã€‚
- **å«ä¹‰**:
    - è¡¨ç¤ºæ¨¡å‹å¯¹æ¯ä¸ª token çš„é¢„æµ‹åˆ†æ•°ï¼ˆlogitsï¼‰ï¼Œå³æœªç»è¿‡ softmax å½’ä¸€åŒ–çš„åŸå§‹åˆ†æ•°ã€‚
    - è¿™äº› logits å¯ä»¥ç”¨äºè¿›ä¸€æ­¥è®¡ç®—æ¦‚ç‡åˆ†å¸ƒæˆ–æŸå¤±å‡½æ•°ã€‚
- **ç”¨é€”**:
    - é€šå¸¸ç”¨äºè®¡ç®—æ¨¡å‹çš„é¢„æµ‹åˆ†å¸ƒæˆ–ä¸å…¶ä»–æ¨¡å‹çš„è¾“å‡ºè¿›è¡Œæ¯”è¾ƒã€‚
        

---

2.Â **`chosen_logps`**

- **å®šä¹‰**:
    - `chosen_logps`Â æ˜¯æ¨¡å‹å¯¹Â **chosen responses**Â çš„å¯¹æ•°æ¦‚ç‡ï¼ˆlog probabilitiesï¼‰ã€‚
    - å½¢çŠ¶ä¸ºÂ `(batch_size, sequence_length)`ï¼Œè¡¨ç¤ºæ¯ä¸ª token çš„å¯¹æ•°æ¦‚ç‡ã€‚
- **å«ä¹‰**:
    - è¡¨ç¤ºæ¨¡å‹å¯¹Â **chosen responses**Â ä¸­æ¯ä¸ª token çš„é¢„æµ‹æ¦‚ç‡çš„å¯¹æ•°å€¼ã€‚
    - è¿™äº›å€¼æ˜¯é€šè¿‡å¯¹Â `chosen_logits`Â è®¡ç®— log-softmax å¾—åˆ°çš„ã€‚
- **ç”¨é€”**:
    - é€šå¸¸ç”¨äºè®¡ç®—æŸå¤±å‡½æ•°ï¼ˆå¦‚è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼‰æˆ–è¯„ä¼°æ¨¡å‹çš„é¢„æµ‹è´¨é‡ã€‚


## æ ¸å¿ƒäº®ç‚¹


## ä¸»è¦æ”¶è·


## å‚è€ƒèµ„æ–™

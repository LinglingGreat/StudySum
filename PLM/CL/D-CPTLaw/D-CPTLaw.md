---
title: D-CPTLaw
created: 2024-08-21
tags:
  - ScalingLaw
  - 预训练
  - 增量预训练
  - 数据配比
type: 论文
papername: 
conference: ACL/EMNLP/NAACL/EACL/COLING/SIGIR/AAAI/ICML/ICDM
year: 2024
institution:
  - 阿里
  - 香港科技大学
  - Waterloo
---

## 论文基本信息

标题：D-CPT Law: Domain-specific Continual Pre-Training Scaling Law for Large Language Models

作者：

链接：

代码：

框架图：


## 背景
通过拟合 D-CPT 定律，可以在有限的实验中使用小规模训练成本轻松预测任意混合比例、模型大小和数据集大小的通用和下游性能。

![](img/Pasted%20image%2020240821192407.png)


## 相关研究



## 核心亮点

![](img/Pasted%20image%2020240821192238.png)



## 实验




## 未来方向



## 主要收获


## 参考资料

---
title: Minimax01
created: 2025-01-18
tags:
  - 大模型
  - 线性注意力
  - 专家模型
type: 论文
papername: 
conference: ACL/EMNLP/NAACL/EACL/COLING/SIGIR/AAAI/ICML/ICDM
year: 2025
institution:
  - minimax
---

## 论文基本信息

标题：MiniMax-01: Scaling Foundation Models with Lightning Attention

作者：

链接：

代码：https://github.com/MiniMax-AI

框架图：


## 背景

![](img/Pasted%20image%2020250118123505.png)

![](img/Pasted%20image%2020250118124825.png)



目的：构建一个与领先商业模型的性能相匹配的模型，同时提供更长一个数量级的上下文窗口。

关键技术：lightning attention，线性注意力变体的 I/O 感知实现。在该架构中，每七个具有lightning attention的 Transnormer 块（Qin 等人，2022a）后面跟着一个具有 Softmax 注意力的 Transformer 块。

我们根据实际约束确定了模型的总参数：具有使用 8-bit量化在8 个 GPU 和 640GB 内存的单台机器上处理超过 100 万个令牌的能力。为了最大化参数和计算能力，我们实施了专家混合 (MoE)（Fedus 等人，2022；Lepikhin 等人，2021）。我们综合考虑训练资源、推理资源和最终模型性能，旨在在三者之间找到更好的平衡。大量的实验引导我们确定了最终的模型规格：4560 亿个参数、459 亿个激活参数和 32 名专家。

我们使用专家并行（EP）和专家张量并行（ETP）在 MoE 中实现全面通信。最大限度地减少与 GPU 间通信相关的开销。

为了促进上下文窗口无限扩展，我们设计了varlen ring attention来减少计算冗余，并设计了Linear Attention Sequence Parallelism（LASP）的改进版本（Sun et al., 2024）以充分利用设备的并行能力。实现了一套专为lightning attention推理而定制的全面 CUDA 内核，在 Nvidia H20 上实现了超过 75% 的模型浮点运算利用率 (MFU)（Chowdhery 等人，2023）。

我们为 MoE 和lightning attention开发了优化的并行策略和高效的计算通信重叠技术。这种方法使我们能够对跨数百万个令牌的上下文中具有数千亿个参数的模型进行有效的训练和推理。 MiniMax-Text-01 的上下文窗口在训练期间可以达到多达 100 万个标记，并在推理过程中以可承受的成本推断到 400 万个标记。

预训练：通过严格的数据清理、基于奖励的质量提升、更好的数据混合策略，打造多样化、高质量的语料库。通过系统的重复感知测试进行验证。

为了充分利用该架构的长上下文能力，我们引入了对超参数的深入分析，并提出了一个三阶段的训练过程，成功地将上下文窗口扩展到一百万个令牌。

在对齐阶段，我们通过精确调整的奖励维度和多阶段训练方法来激励模型的各种能力，特别是在长上下文和现实场景领域。

随后，我们通过集成轻量级 Vision Transformer (ViT)（Dosovitskiy 等人，2021）模块来增强我们的语言模型的视觉功能，从而创建我们的视觉语言模型 MiniMaxVL-01。 MiniMax-VL-01 采用四阶段训练过程，接受了 5120 亿个视觉语言标记的额外训练。训练的最后阶段是专门为了优化用户体验而设计的。
## 相关研究



## 核心亮点


## 模型结构

我们的设计遵循 Transformer 风格的块，每个块都包含一个通道混合器（注意力块）和一个特征混合器（MLP 块）。我们采用两种类型的通道混合器：闪电注意力和 Softmax 注意力。特征混合器是一个包含多个前馈网络 (FFN) 的 MoE。为了确保 MoE 块中的负载平衡，我们提出了一种受 GShard（Lepikhin 等人，2021）启发的新颖负载平衡策略，我们将其称为全局路由器。旨在保持训练稳定性。此外，还集成了 DeepNorm（Wang 等人，2024a）以增强整体性能。

最终的 MiniMax-Text-01 架构以结构化模式集成了线性注意力机制和 softmax 注意力机制。具体来说，具有 softmax 注意力的 Transformer 块位于每 7 个线性注意力 Transnormer 块之后（Qin 等人，2022a），总共有 80 层。每个注意力模块由 64 个头组成，每个头dimension为 128。softmax 注意力层采用组查询注意力（GQA）（Ainslie 等人，2023），组大小为 8。旋转位置嵌入（RoPE）（ Su et al., 2024）应用于注意力头维度的一半，基频设置为 10,000。该模型的hidden size配置为6144，每层包含32个专家，采用top-2路由策略。每个专家内的前馈网络的隐藏维度为 9216。MiniMax-Text-01 总共包含 4560 亿个参数，其中每个处理的令牌激活 459 亿个参数。

![](img/Pasted%20image%2020250118125900.png)

接下来深入研究模型架构的考虑因素，即不同注意力机制的集成、MoE 和线性注意力之间的协同作用、超参数选择背后的基本原理以及基于缩放定律确定模型大小的方法。
### MOE
与dense相比，MoE 提供了增强可扩展性和效率的途径。通常，MoE 是特征混合器层中前馈网络 (FFN) 的替代品，它由多个 FFN 专家组成，其中每个代币都被路由到其中一个或多个专家。具体来说，对于输入标记xt，其对应的输出隐藏状态ht计算为

![](img/Pasted%20image%2020250118130414.png)

其中E代表专家总数，Wg是门的权重，FFNi代表第i个专家，TopK(·)表示保留所有E个专家中前k个分数，而将剩余分数设置为−∞的操作。

基于 MoE 的 LLM 的培训可以分为 token-drop 和 dropless 两种。我们采用 token-drop 策略来提高训练效率。通过这种方法，每个专家都会被分配一个容量限制，指定它可以处理的最大代币数量。一旦达到此容量，路由到该专家的任何额外令牌都将被丢弃。

为了评估 MoE 架构的有效性，我们对具有 70 亿个参数的密集模型和具有 240 亿个参数中的 20 亿个激活参数的 MoE 模型进行了比较研究。结果如图 4 所示，表明在相同计算预算下，MoE 模型在各种基准上的性能显着优于密集模型。

![](img/Pasted%20image%2020250118130706.png)

当扩展到更大的模型时，我们遇到了路由崩溃的挑战，这是由于指定分配的代币集中分布而产生的。为了缓解这个问题，我们将一个简单的全局路由策略纳入 GShard（Lepikhin 等人，2021）辅助损失中，以实现更好的负载平衡。

辅助损失：

![](img/Pasted%20image%2020250118131411.png)

全局路由（Global Router.）：

GPU 内存大小限制了 LLM 训练中的微批量大小，导致各个专家并行 (EP) 组内的令牌分布出现大幅波动。此外，不同 EP 组的代币分配各不相同，可能会导致负载不平衡，其中一个 EP 组中的专家可能超载，而另一组中的专家则未得到充分利用。为了解决这个问题，我们在 EP 组之间实施了全局令牌调度策略。具体来说，我们引入了一个额外的全收集通信步骤，以在跨不同 EP 组调度令牌之前同步等待每个专家处理的令牌数量。在同样的容量限制下，这种全局路由机制可以有效降低整体令牌掉落率，从而保证训练的稳定性。

### Linear Attention

![](img/Pasted%20image%2020250118131750.png)

线性注意力利用“right product kernel trick”将平方计算复杂度转化为线性复杂度，如图 5 所示。n 表示序列长度，d 表示特征维度。以 TransNormer (Qin et al., 2022a) 为例，NormAttention 机制可以写为$O = Norm((QK^⊤)V)$, 线性变体是$O = Norm(Q(K^⊤V))$

线性公式有助于高效的循环预测，训练复杂度为 O(nd^2)。此外，线性注意力确保了 O(d^2) 的恒定计算复杂度，而与序列长度无关。这是通过循环更新 K^⊤V 项来实现的，从而避免了对整个注意力矩阵的重复计算。相比之下，softmax 注意力在推理过程中会产生 O(nd^2) 的复杂度。

在causal language modeling任务中，right product的功效会受到损害，因此需要计算 cumsum（Hua 等人，2022）。这种限制阻碍了高效并行计算的实现。

#### Lightning Attention

闪电注意力 (Qin et al., 2024b,c) 代表了 TransNormer (Qin et al., 2022a) 的 I/O 感知、优化实现。这种方法确定了现有线性注意机制计算效率的主要瓶颈：causal language modeling中固有的缓慢的 cumsum 操作。为了缓解这个问题，Lightning Attention提出了一种新颖的平铺技术，可以有效规避cumsum操作。关键的创新在于将注意力计算战略性地划分为两个不同的部分：块内计算和块间计算。左乘积注意力计算用于块内操作，而右乘积用于块间操作。这种划分至关重要，因为可以显着减小块内的大小，从而确保整体计算复杂度保持线性。

阐明为什么它可以在实践中实现理论上的线性复杂性：

![](img/Pasted%20image%2020250118132802.png)

![](img/Pasted%20image%2020250118133523.png)

#### Effectiveness of Lightning Attention
尽管闪电注意力在小规模实验中表现出了希望和竞争性能，但其在大规模设置下下游任务中的扩展行为和能力仍有待探索。为了缩小差距，我们进行了一系列扩展实验，以评估闪电注意力机制与 softmax 注意力相比的可扩展性，同时验证广泛下游任务的性能。值得注意的是，在我们的实验中，我们观察到闪电注意力表现出有限的检索能力。这一发现启发我们探索一种混合方法（Hybrid-lightning），该方法利用闪电和softmax注意力的优点，通过每隔八层用softmax注意力代替闪电注意力来增强检索性能。

我们遵循 Kaplan 等人建立的 FLOPs 计算方法。 （2020）。为了分析的目的，我们定义了以下变量：l（层数）、d（模型维度）、h（注意力头数量）、b（批量大小）和 n（序列长度）。模型参数和 FLOP 的清单如表 1 所示。

![](img/Pasted%20image%2020250118133734.png)

我们对 Softmax（配备 FlashAttention-2（Dao，2024））、闪电注意力和混合闪电注意力模型进行了不同规模的训练：7000 万、1.6 亿、4.1 亿、10 亿、30 亿和 70 亿个参数。每个模型都在包含多达 3000 亿个标记的数据集上进行训练，上下文长度为 8192。我们的训练方法遵循 Chinchilla 提出的方法（Hoffmann 等人，2022），其中训练损失作为直接指标测试性能。对于每个模型架构和训练序列长度，我们维护了全局统一批量大小为 4M tokens。采用 Adam 优化器，配置学习率为 3e-4，权重衰减为 0.1。由于计算资源有限，所有实验都应用了固定的学习率调度程序。

**scaling law**

我们根据上述设置的实验拟合缩放曲线，其中我们针对不同的计算预算 (C) 更改模型大小 (N) 和数据集大小 (D)，并观察相应的训练损失 (L)测试损失估计器。我们首先遵循 Chinchilla 的方法论，在 L 和 C 之间建立幂律关系（Hoffmann et al., 2022）。使用拟合曲线，我们得出最佳模型大小 Nopt ∝ Ca 和最佳数据集大小 Dopt ∝ Cb 的系数。最初的缩放定律（Kaplan et al., 2020）使用 L(X) = (X0/X)^(α_X) ，而后续研究（Clark et al., 2022；Gao et al., 2024；Henighan et al., 2020； Hoffmann et al., 2022）采用 L(X) = ε + (X0/X)^(α_X) 来更好地拟合，其中 ε 表示不可减少的损失。为了简单起见，我们将这些形式统一为 L(X) = β_X X^(α_X) ，以便于直接比较基于 αX 和 βX 的缩放能力。缩放定律的总结如表2和图6所示。可以直观地理解，在相同的计算预算下，具有闪电注意力的模型往往会使用更多的参数和标记，但与纯softmax模型相比，它们实现了更低的损失注意力。


![](img/Pasted%20image%2020250118133954.png)

**Performance**

我们在图 7 中展示了下游任务的基准测试结果。闪电注意力在大多数下游任务中表现出可比的性能（NIAH 除外）。这表明线性注意力表现出与 Transformer 模型类似的语言建模能力，但在检索任务中存在不足，使其不适合法学硕士。然而，混合闪电注意力不仅匹配甚至超越了softmax注意力的检索和外推能力,非常适合法学硕士的in-context learning。

![](img/Pasted%20image%2020250118134244.png)

**Speed**

我们通过测量每个 GPU 每秒处理的令牌 (TGS) 来评估具有 30 亿个参数的 softmax 注意力、闪电注意力和混合闪电模型的端到端训练速度。为了完整起见，我们还在评估中纳入了流行的线性模型，例如 HGRN2 和 Mamba2。对于速度基准测试，训练上下文长度逐渐增加，直到达到单节点 H800 GPU 上的内存不足限制。如图 8 所示，无论序列长度如何，闪电注意力都能实现恒定的训练速度，并且是唯一优于 FlashAttention2 的线性模型。

![](img/Pasted%20image%2020250118134505.png)

#### Hybrid Architecture
我们对混合架构的初步实验已经取得了有希望的结果，激励我们通过两种变体更深入地挖掘其潜力：hybrid-cosformer2 和hybrid-hgrn2。在hybrid-cosformer2模型中，我们将cosformer2架构中的线性注意力层替换为每八层间隔的softmax注意力层。这种替代策略同样适用于 Hybrid-hgrn2 模型。我们使用一致的设置进行实验来评估这些替代方案的下游性能。我们的研究结果如表 3 所示，表明混合闪电模型实现了最佳性能。

![](img/Pasted%20image%2020250118134914.png)

除了线性模型之外，滑动窗口注意力还可以通过适当调整窗口大小来实现线性计算复杂度。由于它基于 softmax 注意力，因此它可以作为评估线性架构的稳健基线。因此，我们采用了混合窗口方法，每八层将滑动窗口注意力替换为完整的 softmax 注意力。我们评估了从 256 到 1024 的各种 SWA 窗口大小。我们的结果表明，与混合闪电模型相比，较大的窗口大小会导致训练速度变慢。为了在同等速度条件下比较这些模型，我们没有考虑大于 1024 的窗口大小。如表 4 所示，混合闪电模型在所有指标上都优于所有其他模型，特别是在 NIAH 基准测试中表现出色。

![](img/Pasted%20image%2020250118134950.png)

#### Discussion

混合模型在检索和外推任务中不仅匹配而且超越了 softmax 注意力。这个结果有些违反直觉。如何理解？

![](img/Pasted%20image%2020250118135159.png)

### Module Ablations in MoE

基于前面几节的结论，我们进行了另外两组消融实验，以在更大范围内验证 MoE 架构中的模块选择：（1）混合闪电注意力与 Softmax 注意力：验证混合闪电注意力在MoE中的优势。 (2) 前层归一化与后层归一化：在我们的混合架构中，模型的有效深度起着重要作用。因此，我们期望为深度模型找到更好的归一化算法。

Hybrid-lightning Attention versus Softmax Attention.

我们在 MoE 架构中对 softmax 注意力和混合闪电注意力进行了小规模比较分析。具体来说，我们使用具有 280 亿个参数的 MoE 和 50 亿个激活参数，并利用 softmax 注意力作为基础模型。对于基础模型中的每 8 个连续层，我们系统地将前 7 层中的 softmax 注意力替换为闪电注意力。基础模型和修改后的模型都经过 1 万亿个代币的训练。如表 5 所示，结果表明，用闪电注意力替换某些 softmax 注意力层可以提高大多数基准测试的准确性。

Pre Layer Normalization versus Post Layer Normalization.

预层归一化（Baevski 和 Auli，2018；Child 等人，2019；Wang 等人，2019）（PreNorm）在残差连接和注意机制之前应用归一化层，已证明在 LLM 中增强了稳定性和性能。由于PreNorm允许梯度通过残差连接更直接地从输出流向输入，在一定程度上绕过了子层，因此降低了模型的有效深度。相比之下，后层归一化（Wang et al., 2019）（PostNorm）在残差连接和注意机制之后应用归一化，从而保留了模型的有效深度。然而，PostNorm 很容易出现梯度消失和爆炸，这给法学硕士的培训带来了重大挑战。大多数现有的法学硕士主要使用 PreNorm，因为传统 Transformer 架构中更宽和更深的网络之间的性能差异通常可以忽略不计，并且优先考虑训练稳定性。

实验在具有 93 亿个激活参数和总共 600 亿个参数的模型上进行，每个模型由 48 个采用不同归一化方法的层组成。两个模型都使用 5000 亿个代币进行训练。对于 PostNorm，我们利用 DeepNorm (Wang et al., 2024a) 来确保更稳定的训练。如表 5 所示，在所有评估指标中，PostNorm 始终优于 PreNorm。

![](img/Pasted%20image%2020250118135602.png)

### Model Spec

在最终确定模型模块的架构后，下一步需要扩展模型，这需要在各个维度上精心设计模型的超参数。我们的主要目标是在性能和​​推理效率之间取得平衡。与多设备实现相比，单设备推理通过消除跨机器通信开销提供了卓越的效率。因此，我们将模型的总参数限制为 500B，确保与 8 × 80G 配置上的单节点推理兼容，适用于 8 位量化下高达 1M 令牌的序列。鉴于我们有限的训练预算，我们制定以下优化问题来确定最佳参数分配

![](img/Pasted%20image%2020250118135706.png)

通过小规模模型的对比实验，我们首先建立了几个关键变量的最佳范围：（1）softmax和线性注意机制的混合比例； (2)模型架构的深宽比； (3) 线性注意力记忆大小与隐藏大小的比率； (4)激活的FFN与注意力的比例； (5) 利用RoPE进行softmax注意力的维度比例

我们的实验表明，混合架构对层深度特别敏感，更深的模型始终优于较浅的模型。值得注意的是，浅层模型需要更多的 softmax 注意力层才能实现可比较的性能，这凸显了更深层次架构的效率优势。我们还观察到，增加线性注意力内存大小可以显着提高模型性能，并且在一半的 softmax 注意力维度上实现 RoPE 可以在不降低性能的情况下实现长度外推。

基于这些优化的架构变量，我们采用既定的缩放法则（Clark et al., 2022；Hoffmann et al., 2022）来确定最佳模型大小。我们使用 16、32 和 64 名专家，在 5000 亿个代币中训练激活参数范围从 4400 万到 12 亿的模型。然而，我们发现当外推到具有 93 亿个参数的更大模型时，这些方法的预测变得不太可靠。为了解决这个限制并实现更准确的预测，我们提出以下公式：

![](img/Pasted%20image%2020250118135901.png)

基于方程13和14的预测，我们确定了一个具有 459 亿个激活参数和 4560 亿个总参数的候选模型作为最佳配置。

## Computation Optimization

1. 在混合专家 (MoE) 架构的训练过程中减轻全对全 (a2a) 通信开销是一项持续存在的挑战。我们为专家选择的配置，特别是选择大型模型，对 GPU 内存提出了很高的要求。因此，主要挑战在于在内存利用率、计算效率和与所有通信相关的开销之间实现最佳平衡。 
2. 由于我们努力在训练和推理中支持至少 100 万个令牌上下文窗口，因此对于这个庞大的模型来说，在如此广泛的上下文窗口中跨不同 GPU 准确分配令牌变得至关重要。然而，这种必要性不可避免地会带来额外的通信开销。因此，设计策略来最小化这种开销，特别是在我们的混合架构的背景下，提出了重大挑战。 
3. 目前闪电注意力机制的实现是专门针对训练过程进行优化的。然而，在推理场景中，有效管理现实世界的批量输入会出现挑战，这些输入可能包含可变序列长度和包含前缀缓存的特定输入。

这部分内容偏infra，可以直接看论文。

## Pre-Training

### Data

diverse sources including academic literature, books, web content, and programming code.

数据质量增强：卓越的数据质量是大型语言模型的基础。我们实施了复杂的过滤管道，将基于规则的清理和重复数据删除程序与既定实践相结合（Penedo 等人，2023、2024；Rae 等人，2021）。为了在粒度级别评估文档质量，我们利用上一代模型作为奖励标记器（具有 5B 次激活和 60B 总参数的 MoE 模型）。最初，我们评估多个质量维度，包括连贯性、简洁性、教育价值、帮助性、知识丰富性和分类相关性。通过综合分析，我们确定了这些指标之间的显着相关性，并最终关注三个关键维度：知识深度、实用性和分类分布，同时保留其他指标作为次要验证指标。

数据格式优化：网站和书籍中的内容一旦经过适当的提取和清理，自然可以用作高质量的教科书（Gunasekar et al., 2023），而无需进一步格式化。对于对话和问答数据，文本的顺序性质本质上捕获了对话逻辑和问答关系。尽管人类受益于额外的格式（例如 Markdown）以提高可读性和理解性，但我们发现，大量格式实际上会通过引入限制人类对话中存在的自然变化的固定模式来降低数据多样性和质量。最终，为了保持格式泛化能力并适应人类偏好，我们实现了一种嵌套文档格式，其中包含用于对话和 QA 数据的多功能模板，仔细平衡自然理解与跨各种交互模式的结构一致性。

数据混合：我们开发了一种复杂的方法来调整数据分布，利用我们的三个主要质量指标。基于后续部分详细介绍的实验范式，我们发现虽然知识深度和有用性方面的高分内容通常会在能力评估中产生优异的表现，但完全消除低分内容可能会对下游任务绩效产生不利影响。因此，我们实施平衡采样策略，首先在基础语料库中均匀分布，然后调整采样权重以支持高质量内容，同时保持不同类别的充分代表性。

对于标记化，我们采用字节级字节对编码（BPE）（Brown 等人，2020；Shibata 等人，1999），并结合了预标记器方法。我们战略性地对多语言内容进行上采样，以提高相应的压缩效率。生成的词汇表大小设置为 200K 个标记。

#### 数据实验

我们进行数据实验来系统地比较不同模型变体的性能。具体来说，我们将实验制定为统计假设检验，比较基线模型和使用不同数据配置训练的模型之间的评估指标分布。当测试新数据集 D 的有效性时，我们将备择假设表述为 H1：μTD > μTbaseline，其中 μ 表示加权平均性能指标，T 表示测试样本中评估值的分布。

我们精心设计评估标准，以确保获得有意义的见解。我们研究了广泛的多项选择基准，在查询制定中丢弃选择索引，并研究完成的可能性。我们观察样本对数归一化精度 log accnorm2 的分布，定义为

![](img/Pasted%20image%2020250118141725.png)

通过这样的统计设置，我们能够进行功效分析来确定最小测试样本大小，同时将 MDE（最小可检测效应）保持在与我们的训练方差相似的水平，并保证 95% 的置信度和 80% 的决策功效。有了置信方法集，我们对 token 数量和模型大小进行了简单的缩放实验，最终达到了用 40B 个 token 的数据训练 1B 个激活和 8B 个总参数的 MoE 的实验步骤，其中数据混合包括 20B 个网络文档和20B 假设数据。


经验证明，重复数据的合并会对模型的性能和泛化能力产生一些不利影响（Hernandez 等人，2022）。因此，实施重复数据删除策略对于优化 LLM 性能至关重要。最近的研究（Abdin 等人，2024 年；Penedo 等人，2024 年）表明，重复训练高质量文档可以提高下游性能，某些高质量领域被训练最多 50 次，其中重复性被测量通过 MinHash 相似性（Broder，1997；Lee 等人，2022）。然而，我们的实证分析表明，他们的实验范式不足以评估重复的影响，因为数据效率在整个训练过程中并不一致。

为了更好地与完整训练的结果保持一致，我们引入了一种新颖的重复感知实验框架。具体来说，我们首先对数据集执行全局重复数据删除以删除冗余条目。然后，我们对文档进行下采样，以使重复频率与最终训练计划的要求保持一致，同时遵守消融实验的预算限制，这与之前的实验设置不同，之前的实验设置直接采用与中使用的数据分布相同或相似的数据分布。最后的训练阶段。我们的研究结果表明，低质量数据在训练超过两个 epoch 后，性能会大幅下降，而高质量数据可以有效训练最多 4 个 epoch。值得注意的是，从所提出的框架得出的解决方案与使用更多计算资源获得的结果具有更好的一致性。通过仔细控制训练数据的重复和质量，我们实现了更高效、更有效的数据混合，最终带来更好的模型性能。

### 训练策略

![](img/Pasted%20image%2020250118141952.png)

有人认为，在关键批量大小下进行训练可以在训练时间和数据效率之间产生近乎最佳的平衡（Kaplan 等人，2020）。接下来，我们在较小模型的数据上拟合损失与临界批量大小之间的幂律关系，如图 13 所示。当达到相应的损失时，批量大小将加倍。
![](img/Pasted%20image%2020250118142230.png)

![](img/Pasted%20image%2020250118142218.png)

我们逐步将模型的训练上下文长度扩展到 1M 个令牌。由于我们的架构的有效长度外推能力，该模型成功地证明了其在普通 Needle-In-AHaystack 检索任务 (NIAH) 测试 2 中处理最多 4M 个标记的序列的能力，尽管仅在最多 1M 个标记的上下文上进行了训练，如下所示如图 14 所示。

![](img/Pasted%20image%2020250118142446.png)

具体来说，我们采用三阶段训练程序来系统地对不同长度范围内的长上下文数据进行上采样，同时保留关键域的分布特征以保持短上下文评估性能的稳定。训练数据混合、RoPE基频和训练长度的详细信息如表6所示。我们还混合了10%的高质量长上下文问答数据，其长度分布与长上下文预训练数据相似在每个阶段最后 20% 的训练周期中（Parmar et al., 2024）。为了减轻分布变化造成的潜在不稳定性，我们在整个过渡阶段利用特定源权重的线性插值。该方法有利于数据分布向期望的目标分布逐步且受控地演化，从而确保训练稳定性和保持收敛特性。

此外，我们的研究结果表明，NIAH 不足以有效监控模型在整个训练过程中的表现。这主要是因为 NIAH 指标性能很早就达到了峰值，特别是在最初的 128K 训练步骤中。为了解决这个限制，我们使用要求更高的任务来评估模型的中间检查点，这些任务旨在随着训练的进行而增加复杂性。值得注意的是，尽管这些任务的难度不断增加，但我们始终观察到模型性能指标的稳步提高。这种持续的上升轨迹清楚地表明了实施长情境持续预训练的至关重要性和必要性。更多详细信息请参见第 5.7.2 节。

![](img/Pasted%20image%2020250118142529.png)


## Post-training

在本节中，我们提出了一个全面的训练后框架，旨在增强模型的总体性能、长上下文能力和现实世界的适用性。我们的方法首先创建一个多样化的、高质量的提示数据集，并附有一个分层奖励系统，该系统评估多个维度的响应：正确性、真实性、有用性和无害性。训练过程包括监督微调（SFT）、离线和在线强化学习（RL）。通过这些阶段，我们系统地使模型与我们定义的目标保持一致。通过详尽的数据挖掘技术和专门的无害奖励模型确保模型安全。我们引入了一种新颖的多阶段训练方法，该方法显着增强了模型处理扩展上下文的能力，同时保持在较短序列的最佳性能。这种方法产生了一个能够处理复杂的现实场景的强大系统。对学术和内部基准进行的广泛评估表明，我们的模型在所有任务中都实现了最佳性能，同时建立了超长上下文处理的新标准。

### 提示集合

我们广泛的提示集合包含来自不同来源的数百万条多样化、高质量的查询。我们开发了一个标记系统，根据任务类型、知识领域和难度级别对每个提示进行分类。收集过程结合了复杂的过滤机制，以消除冗余提示，同时保持最佳的难度分布。提示集涵盖长上下文、编程、数学、逻辑推理、创意写作、函数调用、常识和安全相关场景等各个领域。

### 奖励模型

我们的奖励模型框架评估四个关键维度的响应，以确保符合我们的核心原则

- 正确性。我们对可以严格验证的回复实施严格的评估系统。对于数学和推理任务，我们利用早期版本的 MiniMax-Text-01 基于答案一致性生成二进制奖励信号。编程解决方案在安全的沙箱环境中经过全面的测试，性能指标来自测试用例的成功率。
- 诚实。我们采用验证管道来评估响应的事实准确性。该过程涉及系统响应采样、语句分解和聚类、众包验证以及使用高级语言模型进行自动比较以生成真实性分数。
- 乐于助人。我们的评估框架通过确定性和概率性方法评估对用户指令的遵守情况。我们实施基于规则的自动化约束验证系统，并辅以对关键指标（包括连贯性、深度、上下文相关性和风格适当性）的人工评估。最终的有用性分数通过加权评分系统结合了多个评估信号。 
- 无害。基于宪法人工智能原则（Bai 等人，2022b），我们制定了包括安全协议、内容适当性和法律合规性的评估标准。我们的评估系统利用经过人工注释验证的精心校准的提示，早期版本的 MiniMax-Text-01 提供标准化的安全评估。

### SFT

我们的 SFT 数据集构建涉及一个多阶段过程，利用通过迭代 SFT 和 RL 循环训练的特定领域专家模型。我们实施拒绝抽样（Bai 等人，2022a；Dubey 等人，2024）来生成专家的高质量响应，在不同温度设置下对每个提示的多个变化进行采样，以选择由奖励层次结构衡量的最佳演示。响应选择过程进一步结合了 n 元语法和语义相似性过滤器，以确保训练数据的最大多样性和质量。

### Reinforcement Learning

离线RL：我们结合了离线 RL 阶段，即直接偏好优化 (DPO)（Rafailov 等人，2023），以优化模型在不同提示分布中的性能，因为它简单且易于长上下文场景的数据构建。我们特别关注与 SFT 阶段使用的提示保持分布一致性的提示。为了评估提示选择的影响，我们使用两个提示类别进行比较实验：经过 SFT 训练的提示和未经 SFT 训练但同源的提示。实证结果表明，经过 SFT 训练的提示与未经训练的提示之间的性能差异可以忽略不计。因此，我们在离线 RL 阶段采用 SFT 训练的模型。实验协议包括为每个提示生成具有不同温度参数的响应，然后使用第 5.2 节中描述的奖励模型进行系统评估。然后，我们确定最佳和最差的响应，以构建 DPO 培训的偏好对。

与离线学习方法相比，在线学习表现出卓越的样本效率和跨领域泛化能力。因此，我们实施在线强化学习来提高模型性能，特别是在数学推理任务中。我们的方法强调及时的多样性，并优先考虑具有中等成功率的提示，以最大限度地提高政策更新期间的信息增益。值得注意的是，我们在在线 RL 期间使用未经 SFT 训练的提示，因为我们的经验观察表明，重复使用前一阶段的提示会导致模型饱和，其特点是响应困惑度降低。我们提出了一种修改后的组相对策略优化（GRPO）（Shao et al., 2024）方法，其中包含以下关键创新

- 重要性采样权重裁剪。传统的 PPO/GRPO 实现采用单侧裁剪（Schulman et al., 2017; Shao et al., 2024），在处理具有较大策略比例和负优势的代币时有时会导致梯度不稳定。为了解决这个问题，我们在损失函数中实现了额外的裁剪，放弃了这种情况，从而有效地调节了重要性采样幅度并减轻了噪声传播。 
- KL 散度优化。由于类似的梯度不稳定问题，我们通过方差-偏差权衡的理论分析重新表述了 KL 散度项，以进一步稳定梯度行为，得到 DKL (θ) = Et [SG(πθ (at |st) − πref (at |st)) log πθ (at |st)]，其中 SG(·) 表示停止梯度算子。该公式保持了策略一致性，同时减少了梯度方差。 
- 平衡优势估计。我们还确保正面和负面示例之间的公平奖励贡献，这在分布偏态的场景中特别有效。这种方法通过调节不同示例组的奖励的绝对大小来保持稳定的训练动态。

## 长文本训练

我们提出了一种系统的多阶段训练方法来增强模型处理扩展上下文的能力，如表1所示。 7. 该方法经过系统设计，旨在优化长序列处理，同时保持传统较短序列的性能效率。在整个训练后阶段，RoPE基频保持在1000万，以确保位置编码的一致性。

Stage I: Initial Short-Context Training. 第一阶段实现 SFT，序列限制为 8,192 个令牌。这一基础阶段建立了处理标准长度查询和响应的基线能力，这构成了大多数实际应用程序。我们在此阶段删除了超过 8,192 个标记的长上下文提示。

Stage II: Extended Context Training. 第二阶段将序列长度显着扩展至 1,032,192 个令牌。此阶段将不同序列长度的训练样本与 50% 长上下文提示结合起来，促进模型全面适应广泛的上下文处理。序列长度的战略扩展是实现强大的长上下文功能的基础。

Stage III: Short-Context Preference Optimization. 在此阶段，我们将序列长度恢复为 8,192 个标记，并实施直接偏好优化 (DPO)。这种校准可确保在传统上下文大小上实现最佳性能，同时保持先前获得的功能。

Stage IV: Long-Context Preference Optimization. 第四阶段重点是通过 DPO 增强长上下文处理能力，包含 1,032,192 个 token 的序列。采用类似于第三阶段的训练协议，具有完全长上下文数据，适用于扩展序列长度。

Stage V: Online Reinforcement Learning. 最后阶段实现短上下文在线强化学习，序列长度为 8,192 个标记。

![](img/Pasted%20image%2020250118143817.png)

## 模型评估

![](img/Pasted%20image%2020250118143918.png)

![](img/Pasted%20image%2020250118144025.png)

![](img/Pasted%20image%2020250118144046.png)



## 对比DeepSeek

|方面|MiniMax-01|DeepSeek-V3|
|---|---|---|
|模型架构|基于线性注意力机制，采用混合架构 (Hybrid-Lightning)，并集成了 MoE 架构。|基于 Transformer 架构，采用 MLA 和 DeepSeekMoE 架构，并引入了辅助损失无关的负载均衡策略。|
|参数规模|4560 亿总参数，459 亿激活参数。|6710 亿总参数，370 亿激活参数。|
|训练数据|14.8 万亿 token，涵盖学术文献、书籍、网络内容和编程代码等。|14.8 万亿 token，涵盖高质量、多样化的文本数据，并优化了数学和编程样本的比例。|
|训练策略|采用三阶段训练方法，将上下文窗口扩展到 100 万 token，并最终外推到 400 万 token。|采用两阶段上下文扩展训练，将上下文窗口从 4K 扩展到 32K，再扩展到 128K。|
|训练成本|未明确说明，但强调其训练效率高。|278.8 万个 H800 GPU 小时，总成本约为 557.6 万美元。|
|多模态能力|MiniMax-VL-01通过集成图像<br><br>编码器和图像适配器，扩展了<br><br>模型的多模态理解能力。|未提及多模态能力。|
|性能表现|在长上下文处理方面表现出色，在 Ruler 和 LongBench-V2 等长上下文基准测试中表现优异。|在大多数基准测试中表现优异尤其是在数学和编码任务上在长上下文理解任务中，DeepSeek-V3也展现出<br><br>强大的能力，例如在 FRAMES 和 LongBench v2 上表现优异。|
|优势|-线性注意力机制和混合架构使其<br><br>在处理超长上下文时更具优势。  <br>- MoE 架构和全局路由策略提高了<br><br>训练效率。  <br>- 变长环注意力和改进的 LASP 算法进一步提升了长上下文处理能力。|- MLA 和 DeepSeekMoE 架构<br><br>在保证高效训练和推理的同时，实现了强大的性能。  <br>- 辅助损失无关的负载均衡策略和多 token 预测训练目标提升了<br><br>模型性能。  <br>- FP8 混合精度训练框架降低了<br><br>训练成本。|
|局限性|- 混合架构中仍保留部分 softmax 注意力层。可能影响长上下文处理<br><br>效率。  <br>- 复杂编程任务的性能有待提升。  <br>- 缺乏对长上下文检索和推理能力<br><br>的更深入评估。|- 推荐的部署单元较大，可能对小型团队造成负担。  <br>- 推理速度仍有提升空间。|


## 主要收获


## 参考资料

---
title: Phi-4
created: 2024-12-25
tags:
  - 大模型
  - 合成数据
type: 论文
papername: 
conference: ACL/EMNLP/NAACL/EACL/COLING/SIGIR/AAAI/ICML/ICDM
year: 2024
institution:
  - 微软
---

## 论文基本信息

标题：Phi-4 Technical Report

作者：

链接：http://arxiv.org/abs/2412.08905

代码：

框架图：


## 简介

 phi-4，14B参数量，**其训练方案主要关注数据质量**。大多数语言模型的预训练主要基于网页内容或代码等**有机数据源**，phi-4在整个训练过程中战略性地整合了**合成数据**。虽然 Phi 系列之前的模型很大程度上提炼了教师模型（特别是 GPT-4）的能力，但 phi-4 在以 STEM 为核心的 QA 能力上大大**超越了其教师模型**。

相比phi-3来说，**phi-4架构的变化很小，但由于改进的数据、训练课程和后训练方法的创新**，phi-4 相对于其大小实现了强劲的性能，尤其是在以推理为中心的基准测试上。

合成数据构成了 phi-4 训练数据的主体，并使用多种技术生成，包括**多代理提示、自我修订工作流程和指令反转**。这些方法能够构建数据集，从而在模型中产生更强的推理和解决问题的能力，解决传统无监督数据集的一些弱点。 phi-4 中的合成数据在后训练中也发挥着至关重要的作用，其中采用**拒绝采样**等技术和**直接偏好优化 (DPO) **的新颖方法来细化模型的输出。

**Phi-4的发展由三个核心支柱指导**： 

1. 预训练和中间训练的合成数据：精心设计高质量的合成数据集，优先考虑推理和问题解决，并仔细生成以确保多样性和相关性。
2. 高质量有机数据的筛选和过滤：精心挑选并过滤了有机数据源，包括网页内容、授权书籍以及代码库，以提取合成数据管道中的种子，鼓励深度推理，并优先考虑教育价值（对模型）。这些种子构成了合成生成管道的基础。为了补充这些合成数据集，也从网络中筛选高质量的数据（在知识和推理方面），直接用于预训练。 
3. 后训练：创建SFT数据集的精炼版本，以及基于关键令牌搜索的新技术来创建DPO数据对。 

![](img/Pasted%20image%2020241225134943.png)
## 数据处理

### 合成数据的目的

**结构化和渐进式学习。** 在有机数据集中，单词之间的关系往往复杂且间接。可能需要许多推理步骤才能将当前的单词连接到下一个，这使得模型难以有效地从下一词预测中学习。相比之下，语言模型生成的每个单词都是由先前的单词定义并预测的，因此更容易让模型遵循由此产生的推理模式。通过这种方式，合成数据可以作为一种“喂食”的形式，以可消化和有进展的方式呈现挑战。

一个简单的例子来说明这一点是，人类撰写的数学问题的解决方案可能会从最终答案开始。这个答案对于人类或LLM来说都太难立即输出了——人类通过非线性编辑产生它，但预训练期望LLM能够学习以线性方式生成它。数学问题的合成解决方案不会遇到这样的障碍。

**与推理上下文对齐。** 合成数据通常更接近我们期望模型生成的输出格式。在这样的数据上进行训练有助于将模型的预训练经验与其在推理期间遇到的情况对齐。这种对齐确保了在生成过程中看到的语境相对于模型所预先训练的数据仍然保持在分布内。

例如，网页论坛的风格与LLM交互非常不同。如果一个事实仅出现在网页论坛数据中，则预训练模型会认为它在生成的聊天中出现的可能性很小。将从网页论坛重写的事实转换为LLM的语言风格，在推理期间使这些事实更易于访问。

**生成phi-4合成数据的方法遵循以下原则**：

1、多样性：数据应全面涵盖每个领域的子主题和技能。这需要从有机来源中精心挑选多样化的种子。

2、精细和复杂性：有效的培训需要精细、非简单的例子，这些例子反映了该领域的复杂性和丰富性。数据必须超越基础知识，包括边缘案例和高级例子。

3、准确性：代码应正确执行，证明应有效，并且解释应遵循已建立的知识等。

4、链接思维：数据应鼓励系统推理，以逐步的方式向模型展示各种问题的解决方案。这有助于复杂任务中产生连贯的结果。

### 预训练和中间训练的合成数据

研究人员创建了**50种广泛类型的人工合成数据集**，每一种都依赖于不同的种子和不同的多阶段的提示程序，并涵盖了各种主题、技能以及交互性质。总共积累了约400B未加权tokens。用于生成phi-4人工合成数据集的新方法：

• **种子选择**：合成数据生成始于来自多个领域的高质量种子。

1. 网页和代码种子：从网页、书籍以及代码库中提取摘录和片段，重点是内容复杂性高、推理深度大且教育价值高的内容。为了确保质量，采用两阶段过滤过程：**首先识别具有强大教育潜力的页面，其次将选定的页面分割成段落，并为每个段落评分以评估其事实性和推理内容。**

2. 问题数据集：从网站、论坛和问答平台收集了一个大型的问题集合。然后使用基于多数的技巧过滤这些问题以平衡难度。具体来说，研究人员为每个问题生成了多个独立的答案，并应用多数投票来评估响应的一致性。**对于所有答案一致（表明该问题是太容易）或完全不一致的回答（表明该问题是太难或含糊不清），删除这些问题。** 这个筛选过程产生了一组挑战模型推理和解决问题能力但仍然可接受的问题的数据集。在拒绝采样基础上，用多数答案代替真实答案。

3. 从多种来源创建问题-答案对：利用语言模型来提取来自有机源（如书籍、科学论文和代码）的问题-答案对。这种方法不是仅识别文本中的显式Q&A对，而是设计了一个用来检测文本中推理链或逻辑过程的管道。**语言模型识别出推理或解决问题过程的关键步骤，并将其重新表述为问题及其相应的答案**。实验表明，在正确的情况下，这种方法可以比在原始内容上的训练更有效（以学术基准和内部基准的改进程度衡量）。

• **重写和增强**：种子通过多步骤提示工作流程转化为合成数据。这包括将给定段落中大部分有用内容重新编写为练习、讨论或结构化推理任务。

• **自我修订**：然后通过一个反馈循环，模型迭代地改进其初始响应，该循环由关注推理和事实准确性的评分标准指导。

• **代码和其他任务的指令逆转**：为了增强模型从指令生成输出的能力，使用了指令逆转技术。例如，将现有代码片段从代码数据集提取出来，并用它来生成包含问题描述或任务提示的相应指令。产生的合成数据对中，指令出现在代码之前。仅保留原始和再生代码之间具有高一致性的数据，以确保指令与输出之间的对齐。这种方法可以推广到其他目标用例。

• **代码和其他科学数据的验证**：当适当的时候，整合测试以验证推理密集型合成数据集。通过执行循环和测试来验证合成代码数据。对于科学数据集，问题是从科学材料中提取出来的，并使用一种设计用于确保高相关性、扎实性和难度平衡的方法。

### 网络和问答数据的整理与筛选

**问答数据集**。通过审查公共网站、依赖现有数据集和获取外部数据集，收集了数百万高质量的有机问题及其解决方案。前模型经验表明，问答数据对诸如数学推理和学术表现等能力具有显著贡献。消融研究显示，有机问题比合成问题更有效。为了扩大有机问题的数据集规模，研究人员使用了几种方法来合成增强该数据集中的问题，将答案替换为合成生成的答案，并采用多数投票以提高准确性。所有收集到的问题和答案都经过彻底的去重处理，确保它们不会与测试集有重叠。

**针对性的高质量网页数据**。为phi-4收集了各种高质量的有机数据源，优先考虑推理密集和细致入微的材料（例如学术论文、教育论坛和编程教程）。除了直接在文本上进行训练外，还使用各种网络来源作为专门合成数据生成管道的种子。研究人员认为干净且正确的自然数据对于填充合成数据至关重要：轻微错误可能导致衍生合成文档的质量严重下降。因此，研究人员在网络数据的完美管理上投入了大量精力。主要的技术和考虑因素：

• 目标收购：包括了大量推理密集型文档的主要存储库，这些文档是公开允许使用的（例如，arXiv、PubMed Central 和 GitHub）或明确许可的（例如，已授权书籍），以达到外部可用语料库通常标准以上的全面性、新颖性和清洁度。

• 网页筛选：为了捕捉信息丰富的网页来源的长尾（例如，论坛、博客、课程材料和特定领域的维基百科），研究人员采取了从大量网页备份中选择少量最高质量文档的方法，使用在约10^6个LLM生成的标注数据上训练的小型非LLM分类器。这种方法倾向于过度强调与STEM相关的关键词，因此研究人员创建了一个专门的管道来放大高质量的非STEM内容（如艺术、历史、旅行、文化和娱乐）。这些主题分类器也是通过蒸馏LLM标注器得到的。最后，研究人员通过检测n-gram统计和压缩比率的异常值来删除损坏文本和二进制文件。

• 多语言数据：我们整合了多语言数据集，以确保模型能够处理各种语言，包括德语、西班牙语、法语、葡萄牙语、意大利语、印地语和日语。这涉及从CommonCrawl 和维基百科中获取并处理高质量的多语言文档。多语言处理管道由基于fastText 的语言识别模型组成，用于将文档分类为176 种语言，然后使用相同的分类器对网页快照进行过滤以筛选质量。请注意，这些分类器是根据多语言LLM生成的标注数据训练而成的。

• 自定义提取和清洗管道：为了确保异构有机数据源之间的充分清洁度和一致性，需要一组定制的启发式方法和解析器。对于每个目标数据源，研究人员构建了自定义管道来处理各种文件格式（例如多文件TeX源、ePub和其他XML类似格式、Microsoft Word文档以及PDF）。对于通用网页数据，构建了一个自定义HTML到文本的提取器，非常小心地保留经常被简单的解析器损坏的脆弱内容（例如，TeX/MathML 方程、代码块、表格和论坛线程结构）。该提取器使用各种信号（例如 HTML 标签名称、CSS 类、内容长度和树深度）来修剪和标准化 DOM 树，以区分样板、广告、方程式和语法荧光笔工件等元素。

### 后训练数据集

后训练数据由以下组成：

• 监督微调（SFT）数据集：使用精心挑选的用户提示，这些提示来自公共可用的数据集和合成生成的数据混合物，生成多个模型响应，并通过基于LLM的评估过程选择最佳选项。

• 直接偏好优化（DPO）：基于拒绝采样和LLM评估生成了DPO对，其中一部分是根据创建关键令牌对的方法。

## 预训练细节

phi-4 模型基于仅解码器的transformer架构，具有 14B 参数和默认上下文长度为 4096。在中期训练期间将其扩展到 16K 上下文长度。该架构与 phi-3-medium 架构非常相似，只是现在使用 tiktoken 分词器（以更好地支持多语言）以及包含未使用的令牌的填充词汇表大小为 100,352，并且对 4K 上下文长度进行全注意力处理，而不是像 phi-3-medium 中那样使用滑动窗口。

该模型使用线性warm-up和峰值学习率（0.0003）的衰减策略以及恒定权重衰减（0.1），global batch size=5760，在大约 10 T 的令牌上进行了预训练。通过从较短的运行中进行插值来调整训练超参数，并进一步通过压力测试学习率warm-up阶段以确保稳定性，然后是更短的中期训练阶段将原始上下文长度从 4k 增加到 16k。

![](img/Pasted%20image%2020241225143504.png)

### 预训练数据组成

phi-3 模型家族使用了两阶段策略进行训练。大部分的训练令牌在第一阶段中被使用，该阶段主要由过滤后的网页数据组成。第二阶段则使用了一种混合数据集进行训练，其中主要是合成令牌，并且为超滤和推理密集型网页数据分配了一个较小的比例。随着我们合成数据大小和复杂性的增加，我们观察到使用非合成令牌对 phi-3 家族模型大小的好处略有下降。我们注意到两个关键观察结果。 
- **Web 数据集在推理密集型的基准测试方面显示出微小的优势。合成数据训练更多轮次相比添加新网络数据可以带来更好的性能。**
- **仅使用合成数据训练的模型在知识密集型基准测试中表现不佳，并且幻觉增加。**

实验如图所示：

![](img/Pasted%20image%2020241225144447.png)

### 数据混合

为了设计给定训练令牌预算的预训练数据混合，研究人员搜索来自不同来源的各种令牌分配，即：1）合成、2）网页重写（分为推理和知识密集部分）、3）过滤后的网页、4）目标收购和有机数据（例如学术数据、书籍和论坛），以及5）代码数据。

研究人员使用了较短的令牌范围（1T个令牌）来推导数据混合。这些消融试验依赖于已建立的结果：**（1）短期训练与长期训练之间的高相关性，直到数据源过拟合饱和阈值为止。（2）在给定足够大的数据混合距离时，7B和14B模型在不同数据混合上的性能之间存在较高的秩相关性。**

这里重点介绍了一些对数据构成具有最佳洞察力的消融。具体来说，研究人员将目标收购和代码类别中的令牌比例冻结，并改变合成、网页和网页重写簇中的令牌比例。

表4总结了精心挑选的消融实验。在三个类别之间均匀分配令牌是不理想的，因为合成数据的质量更高，并且唯一显示从网络数据受益明显的基准测试是TQA。表格第2行和第3行中的合成偏重变体比所选的最终数据混合稍好一点，但研究人员决定包括目标领域和重知识的过滤后的网页数据，以提高知识水平（知识benchmark），平衡模型所有能力。

![](img/Pasted%20image%2020241225145136.png)

![](img/Pasted%20image%2020241225145147.png)

研究人员还注意到，随着模型进入训练后阶段，所选数据混合和重合成的数据之间的差距很大程度上缩小了。预训练数据混合的端到端优化，同时考虑后训练的影响，是未来一个有趣的研究领域。
### 中间训练细节

研究人员尝试使用天然具有长上下文的数据源，并与人工创建的长上下文数据进行比较，在后者中样本被填充以填补序列。观察到**前者在更长上下文的任务上表现更好**。

受此启发，研究人员进一步过滤高质量的非合成数据集（即学术、书籍和代码数据），以分离上下文超过 8K 的样本。然后将长度为 16K 或更高的数据子集加权。研究人员也创建了满足> 4K 序列要求的新合成数据集。**最终的数据混合包括 30％ 新筛选出的更长上下文数据以及 70％ 来自预训练阶段的recall令牌**。为了适应更长的上下文，研究人员将rope位置编码的基本频率增加到 250K 。与预训练阶段相比，将最大学习率降低十倍，并总共对 250B 个令牌进行训练。 

![](img/Pasted%20image%2020241225145657.png)


## 后训练

chatml格式，SFT一轮，关键令牌搜索方法的数据集上DPO一轮，偏好对数据集上DPO一轮。

SFT：学习率为 10^-6 ，对来自不同领域（包括数学、编码、推理、对话、模型身份和安全）的高质量数据进行微调。 还添加了针对 40 种语言的多语种数据。 总共使用约8B tokens的数据。

![](img/Pasted%20image%2020241225150317.png)

DPO数据涵盖了聊天格式数据、推理以及Responsible AI (RAI) 数据，从而在数学、编码、推理、稳健性和安全性方面改进了模型。

在SFT 模型上进行了两轮DPO 。Pivotal Token Search（PTS），用于第一轮DPO 的对齐生成。

第二轮(由法官指导的DPO)，收集了大约85万对数据。提示来自各种公开可用的指令微调数据集，并且还包括与安全性和Responsible AI (RAI)相关的提示。然后从GPT-4o、GPT-4t以及我们的模型中生成响应。从这些响应中创建各种组合的DPO对，并使用GPT-4o作为裁判来为给定的一组标记正负。对于一组响应，每个助手响应基于准确性、风格和细节获得一个分数。将具有更高准确度或总体（平均值为准确性、风格和细节）得分的响应标记为正面响应。

这两个阶段也包括少量用于安全性和幻觉抑制的数据。 

### 关键令牌搜索

考虑一个生成模型，它对给定提示产生一个响应。对于每个产生的词，这对应于模型响应的一个前缀，可以考虑该前缀下模型答案正确的条件概率以及相对于该词的概率增量（换句话说，**在生产那个词之前和之后正确性的概率差异**）。通常情况下，整体正确性高度依赖于成功地生成少量关键词。例如，我们可以看到在图3中，当模型输出数学解决方案时，“negative”的采样关键词导致从可能失败到很可能成功的转变；而再次采样的词“(a”则可能导致再次失败的风险。我们把这些词称为关键词，因为它们对解决方案的过程有显著的影响。 

![](img/Pasted%20image%2020241225150940.png)

如图所示，在许多概率远低于"negative"的0.31的令牌上，这将导致梯度稀释关键令牌信号的噪声。更糟糕的是，由于其低概率为0.12，缺乏稳健性的令牌（a）会获得强烈的正向学习信号。

此外，直觉表明，在两篇文本实质上偏离彼此时，比较它们的个体下一个令牌对数概率（如在DPO中所做）并不十分有意义。相反，当这两篇文本开始从彼此分离时的第一个令牌发出信号更有意义。 

为了缓解这些影响，我们采用一种称为Pivotal Token Search (PTS) 的方法，**找到关键Token (Pivotal Token)，然后只对Pivotal Token去施加Loss**.

对于Query Q，completion tokens Tfull = t1, t2, . . ., 找到对p(success ∣ t1, . . . , ti)概率有显著影响的关键token。从Q + t1, . . . , ti采样completions来估计概率，用一个称为oracle系统来检测准确性（比如数学问题有标准答案，编码问题可以用测试套件）。如图4所示，Subdivide过程递归地将序列分成片段ti, ..., tj，直到每个片段的$∣ p(success ∣ t_1, . . . , t_{i−1}) − p(success ∣ t_1, . . . , t_j) ∣$的概率达到阈值p(gap)以下（不是关键token，没必要再分割），或者该片段只是单个token（无法再分割了）。导致成功概率急剧变化的token被视为关键。

**然后将Q + t1, . . . , ti−1作为query, 将使得p(success ∣ t1, . . . , ti−1, t(acc/rej))增加或减少的单个token t(acc)和t(rej)分别作为chosen和rejected，构造DPO数据对。**

PTS 的二分搜索算法并不保证总是找到所有关键标记，但它只找到关键标记，并且如果在解决方案过程中成功概率接近单调，则它会找到所有关键标记。

![](img/Pasted%20image%2020241225152002.png)

![](img/Pasted%20image%2020241225151816.png)

针对那些正确答案容易获取的任务，使用PTS生成偏好数据，例如数学、各种形式的问题回答和编码。为了提高样本效率，将目标问题过滤为仅包含0.2≤p（成功）≤0.8，因为对于非常容易或困难的任务，关键令牌很少见。 

如图所示，数学问题回答的例子显示了关键令牌往往不是实际错误，而是驱动模型走下更不利路径的选择。在这里，两边分别乘以分母与直接交叉相乘同样有效，但对于模型来说，后者更稳健。通过针对此类选择生成DPO数据，PTS可以帮助phi-4在它特别擅长的模式上工作得更好。

![](img/Pasted%20image%2020241225152507.png)

### 幻觉缓解

生成SFT数据和DPO对以减轻幻觉。如果模型不知道答案，我们宁愿它拒绝回答也不愿编造一个幻觉。在附录A.1中详细介绍了这一过程的细节，包括创建数据的提示。这大大减少了SimpleQA中的幻觉（见图6）。 

![](img/Pasted%20image%2020241225152653.png)

### 消融实验
在表9中，展示了基准分数在训练后如何演变。研究人员也评估了丢弃关键令牌DPO并仅执行DPO的第二阶段的情况。一般来说，研究人员认为关键令牌DPO对推理密集型任务（GPQA、MATH）最有用，而由判官引导的DPO特别适用于涉及GPT-4判官的基准：ArenaHard。研究人员还发现这两种方法是互补的。 

![](img/Pasted%20image%2020241225152801.png)


## 模型限制

**phi-4在语言理解和推理能力方面与更大模型相当，但它仍然受到其大小的某些任务的根本限制，特别是对事实知识的幻觉。** 例如，如果X是一个可能的人名，则该模型有时会以“谁是X？”的形式响应提示，并生成关于人的X的幻象传记。通过增加一个搜索引擎来增强该模型可以改善这一局限性，但无法完全消除事实幻觉。 

**虽然phi-4在回答问题和进行推理任务方面表现出相对较强的能力，但在严格遵循详细说明时却不太擅长。特别是那些涉及特定格式要求的说明。** 例如，在被要求生成严格按照表格格式输出内容、遵守预定义的子弹结构或精确匹配风格约束的情况下，模型可能会产生偏离指定指南的输出。这一限制部分源于该模型的训练重点，即优先考虑了Q&A 和推理任务的合成数据集，而不是指令遵循场景。

**即使在推理任务中，phi-4也会出错。** 例如，在被问到“哪个数字较小，9.9还是9.11？”时，模型会错误地得出结论：“9.9比9.11小。”

此外，由于数据包含大量链式思维示例，**phi-4有时会为简单问题提供冗长的详细答案**——这可能会使用户交互变得乏味。我们还注意到，虽然phi-4可以作为聊天机器人使用，但它已被微调以最大化单轮查询性能。

## 主要收获


## 参考资料

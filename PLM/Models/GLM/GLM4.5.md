---
title: GLM4.5
created: 2025-08-30
tags: 关键词
type: 论文
papername:
conference: ACL/EMNLP/NAACL/EACL/COLING/SIGIR/AAAI/ICML/ICDM
year: 2025
institution:
  - 智谱
  - 清华
---

# 论文基本信息

标题：GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models

作者：Zhipu AI & Tsinghua University

链接：http://arxiv.org/abs/2508.06471

代码：https://github.com/zai-org/GLM-4.5

GLM-4.5: 总参数355B，激活32B

GLM-4.5-Air: 总参数106B参数，激活12B

重点关注Agent能力、复杂推理能力、代码能力，认为这是衡量真正通用模型的标准。下图展示了GLM-4.5模型在这几个方面的能力水平，基本上都接近甚至超越Claude Sonnet 4的水平（仅指榜单）。

GLM-4.5-Air则和Qwen3-235B-A22B，MiniMax-M1等模型持平。

![](img/GLM4.5-20250831113829.png)

![](img/GLM4.5-20250831151451.png)
# 预训练
## 模型架构

- MoE层使用loss-free balance routing和sigmoid gates
- 减小模型的宽度（隐藏层维度和路由专家数量）并增加其高度（层数），因为作者发现越“高”的模型的推理能力越强。（Kimi K2是增加了专家数量）
- 使用具有部分 RoPE 的GQA
-  2.5 倍的注意力头（96 heads for a 5120 hidden dimension），虽然没有改善训练损失，但它会持续提高 MMLU 和 BBH 等推理基准的性能
- 结合QK-Norm来稳定注意力logit的范围。
- 添加一个 MoE 层作为 MTP（Multi-Token Prediction）层，以支持推理过程中的推测解码。

和其他模型架构的比较

![](img/GLM4.5-20250831151741.png)



## 预训练数据

包括来自网页、社交媒体、书籍、论文和代码存储库的文档。

1.网页：
- 训练数据的大部分来源，包括英文和中文。
- 将爬取到的网页分成不同质量分数的桶，**上采样高质量桶的数据，丢弃低质量桶中的数据**。
- 最高质量的数据训练超过 3.2 个 epoch
- 发现大量从模板自动生成并被分配了高分的相似网页，MinHash无法去除，使用SemDedup pipeline基于文档embeddings将这些网页去除。

2.多语言
- 来自爬取的网页和Fineweb-2
- 应用质量分类器来判断文档的教育效用，并对高质量的多语言文档进行**上采样**。

3.代码
- 来源于GitHub 和各种代码托管平台
- 经过基于规则的初步过滤，然后使用特定于语言的质量模型进行分类，将样本分为三个层次：高质量、中等质量和低质量。
- 同样地，**上采样高质量代码，同时排除低质量样本**。
- 使用**Fill-In-the-Middle**训练目标，该目标能让语言模型学会根据上下文预测中间缺失的内容，- 不仅依赖前文（prefix），还要考虑后文（suffix）。
- 分阶段从文本语料中选出代码相关的 Web 文档
	- 识别HTML代码标签的存在，或通过检测代码相关内容的FastText 分类器进行识别。
	- 使用专用模型对检索到的文档进行质量评估，分类为高、中或低质量类别，遵循基于质量的源代码采样策略。
	- 使用细粒度解析器重新解析选定的网页，以更好地保留代码的格式和内容。

4.数学和科学
- 来自网页、书籍和论文的数学和科学。
- 应用大型语言模型，根据数学和科学教育内容的比例对候选文件进行评分，并训练一个小的分类器来预测分数。预训练语料库中分数高于特定阈值的文档将被**上采样**。

GLM-4.5的预训练分成2阶段，在第一阶段，模型主要在通用的网页文档上进行训练。在第二阶段，在GitHub 的源代码、与编码、数学和科学相关的网页进行上采样。

## Mid-Training：提高推理和代理能力

这个阶段是为了加强模型在一些特定领域的能力，使用中等大小的特定领域数据集，包括指令数据。

1.Repo-level的代码训练
- 从同一存储库添加串联的代码文件以学习跨文件依赖关系。
- 将相关联的经过模型过滤的 issues、PRs 和 commits **合并到一个上下文中**。例如：
    - 一个 bug 报告（issue）
    - 修复这个 bug 的代码提交（PR）
    - 相关的代码更改（commits）
    - 这些都会被组合在一起，形成完整的上下文信息
- commits 以 **diff 格式**展示，diff 格式是显示代码更改的标准方式，通常包括：
	- `- 删除的代码行（红色）`
	- `+ 新增的代码行（绿色）`
- 将训练序列长度从 4K 扩展到 32K，以支持大型存储库。

2.合成推理数据训练
- 合成数学、科学和编码竞赛等推理数据。
- 方法是从网页和书籍中收集大量与推理任务相关的问题和答案，并用推理模型生成推理过程。

3.长文本和Agent训练
- 将训练序列长度从 32K 扩展到 128K
- 上采样预训练语料中的长文档
- 也使用了大规模的合成agent轨迹数据


**预训练阶段 (Pre-training)**：

- **没有使用**最佳适配打包 (best-fit packing)
	- BFP 是一种把多个较短文本拼接起来“填满”模型上下文窗口的方法——尽量把若干短文按大小组合，最大化每个训练样本利用整个上下文长度，从而**减少在中间把一个重要长片段切断（truncation）** 的情况。
- **原因**：随机截断实际上是一种有效的数据增强策略
- **逻辑**：在预训练阶段，模型需要学习语言的基本模式和知识，随机截断文档可以让模型接触到更多样化的文本片段，增加训练数据的多样性

**中期训练阶段 (Mid-training)**：

- **采用了**最佳适配打包策略
- **原因**：避免截断推理过程或代码库级别的代码
- **逻辑**：在这个阶段，模型需要学习更复杂的推理能力和理解完整的代码结构，截断会破坏这些重要的连续性信息

![](img/GLM4.5-20250831152459.png)
## 超参数

- 除了word embedding, bias, and weights for RMSNorm，其余参数用了Muon优化器
- Newton-Schulz iteration steps N to 5, momentum μ to 0.95, and scaled Muon’s update RMS to 0.2.
- 观察到Muon优化器可以加速收敛并容忍更大的批量
- 学习率使用余弦衰减计划，而不是预热-稳定-衰减 （WSD） 计划。
- 早期的实验表明，使用 WSD 训练的模型在一般基准（SimpleQA、MMLU）上表现较差，表明其在稳定阶段存在欠拟合。
- 学习率经历了从0到2.5e-4的热身阶段和到2.5e-5的衰减阶段，直到Mid-Training结束。
- 使用批量大小预热策略，在前 500B token 的训练中，批量大小从 16M token 逐渐增加到 64M token，并在剩余的训练中保持不变。
- 对于正则化，将权重衰减比设置为 0.1，并且没有使用 dropout。
- 在预训练阶段将最大序列长度设置为 4,096，并在Mid-Training阶段将其扩展到 32,768 和 131,072
- 在将序列长度扩展到 32K 时，将 RoPE 的基本频率从 10,000 调整为 1,000,000，以获得更好的长上下文建模能力。
- 对于loss-free balance routing，将前 15T tokens的bias更新率设置为 0.001，其余tokens的bias更新率设置为 0.0。
- 应用了权重为 0.0001 的辅助序列水平平衡损失，以避免任何单个序列内的极端不平衡。
- 前 15T tokens的 MTP 损失权重 λ 设置为 0.3，剩余tokens设置为 0.1。

# Post-Training: 专家模型迭代

分为2个阶段
- stage 1 (Expert Training)：训练Reasoning, Agent,General chat三个专家模型
- stage 2 (Unified Training)：采用自我蒸馏技术来整合多个专家，最终提供一个能够通过深思熟虑推理和直接响应模式生成响应的混合推理模型。
## SFT

在Expert Training阶段，SFT主要提供一个冷启动作用，使用一小部分具有扩展思维链 （CoT） 响应的数据，为模型赋能基本的聊天、推理和工具使用能力。

在Unified Training阶段，SFT的目的是将不同专家模型的能力提炼成一个能够处理不同类型任务的混合推理通才。
- 从之前训练的专家模型中收集了数百万条数据，包括推理任务（数学、代码、科学等）、一般聊天（写作、翻译、摘要、闲聊等）、代理任务（基本工具使用、编码能力，特别是真实项目开发等）和 longcontext 理解任务，并使用最大上下文长度为 128K token 来训练基础模型。
- 平衡了包含完整推理的训练数据与缺乏明确思维过程的数据，允许模型在反思和即时响应模式下运行，从而创建一个混合推理模型。

此外还发现准备SFT数据的一些更好的策略：
- 减少函数调用模板中的字符转义。大量的转义字符会增加模型的学习负担。作者提出了一种新颖的函数调用模板，该模板将函数调用键和值封装在类似 XML 的特殊标记中。这种方法大大减少了代码段中字符转义的必要性。
![](img/GLM4.5-20250831162123.png)

- 拒绝采样采取多阶段过滤pipeline
	- （1）删除重复、过短、截断的样本，以及不符合有效推理格式的样本;
	- （2）对有客观答案的样本进行正确性验证;
	- （3）利用奖励模型过滤对主观问题的回答;
	- （4）对于工具调用场景，确保遵守正确的工具调用协议并验证轨迹是否达到预期的终端状态。
- 提示选择和响应水平缩放
	- 过滤具有挑战性的提示并对其进行响应缩放被证明是有效的。
	- 数据筛选实验：尝试根据响应长度删除后 50% 的提示，尽管只使用了一半的数据进行训练，但数学和科学任务提高了 2%-4%。
	- 响应扩展实验：对这些"困难提示"（hard prompts，即响应较长的提示）应用响应缩放，为每个提示生成4个不同的响应，而不是1个，又带来了1%-2%的额外改进。
- 自动代理 SFT 数据构建
	- 1.代理框架和工具集合：构建工具生态系统
		- - 收集现有的智能体框架和真实世界的工具API
		- 收集MCP服务器（Model Context Protocol服务器）
		- **自动化构建**：使用LLM自动构造和模拟一批工具
	- 2.任务合成：基于工具自动生成训练任务
		- **成熟框架**：让LLM理解框架功能，自动生成相关查询/任务
		- **零散工具**：先选择代表性工具子集，然后类似地使用 LLM 来构建有关该子集的任务
		- **任务类型**：包含单步和多步工具调用场景
	- 3.轨迹生成：为每个任务生成完整的执行过程
		- 使用现有LLM生成工具调用轨迹
		- **用户模拟**：让LLM扮演用户角色，将多步任务转换为多轮对话轨迹
	- 4.质量过滤：确保训练数据质量
		- 使用多个判断智能体评估任务是否成功完成
		- **只保留成功的轨迹**作为训练数据

## 推理 RL
推理 RL 侧重于增强模型在需要逻辑推理、结构化问题解决和可验证准确性的领域的能力。这包括数学、代码生成和科学推理等关键领域。

这些任务的一个决定性特征是其奖励信号的高精度，因为正确性通常可以通过编程或目标清晰度来确定。

采取GRPO框架，去掉KL损失项，先在小模型上做实验，允许快速迭代和精确的消融研究（下面展示的图都是小模型上的实验图）。

方法包括
1.基于难度的课程学习。
- 在强化学习过程中，模型的熟练程度不断发展，导致与静态训练数据不匹配。
- 早期应该采取相对简单的数据，后期加入难度升高的数据。
- 作者采取基于难度的两阶段 RL 课程。图5显示，这种两阶段方法使模型能够不断超越其性能上限。至关重要的是，为了保持高信号质量并降低噪声，第二阶段使用的所有问题都严格来自具有经过验证的正确答案的池。

![](img/GLM4.5-20250831165134.png)

2.64K输出长度的单阶段RL
- 先前的研究建议分多个阶段进行RL，并逐渐增加最大输出长度。
- 但作者们的实验表明，这种多阶段方法不如直接在最大目标长度64K下进行的单阶段RL过程有效。
- 由于最初的监督微调 （SFT） 已经将模型条件定为生成 64K 长度的响应，因此引入最大长度较短的 RL 阶段可能会导致模型“忘记学习”其长上下文能力
- 随着模型的平均输出长度减少，这通常会导致性能显着且不可逆转的下降。这种退化在最终的 64K 长度 RL 阶段很难恢复。

![](img/GLM4.5-20250831165430.png)

3.动态采样的Temperature
在模型训练似乎“停滞”时，主动引入可控的多样性（“噪音”）来帮助模型跳出可能的局部最优解，寻找更好的解决方案，但同时通过严格的检查防止多样性破坏已学到的良好性能。
- 训练过程中，会持续监控模型在一系列 Rollouts 中获得的平均奖励。当这个平均奖励值在一段时间内​**​不再显著增长或开始稳定波动​**​时，就认为模型进入了“​**​收敛阶段​**​”（Convergence Phase）。这意味着模型可能正在​**​过度利用（over-exploiting）​**​ 当前学到的策略，陷入了局部最优解，缺乏新的探索来找到更优解。
- 为了打破僵局，策略是​**​主动提高采样温度​**​。提高温度会使模型在生成下一步输出时，从概率分布中采样更分散，从而​**​产生更多样化、更不可预测的 Rollouts​**​。这相当于​**​鼓励模型进行更多探索​**​，尝试那些当前策略下概率较低但可能带来更高回报的行动或生成路径，有望帮助模型跳出局部最优，发现更好的策略。
- 盲目提高温度固然能增加多样性，但也伴随着风险。温度过高会引入​**​过多噪声​**​，导致生成内容​**​质量下降、不连贯或无意义​**​，反而可能使模型性能​**​退化​**​，学到的策略变差。
- 为了在鼓励探索的同时​**​确保模型性能不出现显著倒退​**​，引入了一个​**​基于验证集的性能监控机制​**​：
1. ​**​定期评估​**​：在训练过程中，会定期在一个​**​预留的、未见过的验证集​**​上评估模型当前的表现。
2. ​**温度扫描​**​：在当前被认为“收敛”的模型基础上，​**​尝试一系列不同的采样温度值​**​（例如，从当前温度逐步向上增加），并在验证集上测试每个温度对应的模型性能。
3. 安全阈值​**​：分析这些温度下的性能表现，​**​选择那个能允许最大探索度（即最高温度）、但同时不会导致验证集性能比当前最佳性能下降超过1%的温度值​**​。
4. 设定新温度​**​：将这个选定的安全温度作为​**​下一训练阶段的起始采样温度​**​。

4.代码与科学 RL
- **代码RL**中，作者发现**损失计算的选择对于训练效率至关重要**。
- 如图 7（左）所示，与传统的序列均值损失相比，采用代币加权均值损失非常有益。加权方法提供了更细粒度、更稳定的梯度信号，从而显着加快收敛速度。该方法还有助于缓解序列级奖励固有的长度偏差，并有效抑制训练过程中过于简单或重复的“基本情况”样本的生成。
- 对于**科学RL**，作者对GPQA-Diamond基准的调查结果强调，**数据质量和类型是最重要的因素**。
- 如图 7（右）所示，与使用混合质量或未经验证的数据进行训练相比，仅使用专家验证的 RL 多项选择题可以显着提高性能。这一结果强调，即使对于多项选择题等简单格式的任务，严格过滤 RL 数据池以仅包含高质量、具有挑战性的实例对于有效的模型改进也至关重要。



## Agentic RL


## General RL


## RL Infrastructure

# 评估



## 主要收获


## 参考资料

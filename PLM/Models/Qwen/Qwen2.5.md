---
title: Qwen2.5
created: 2024-12-22
tags:
  - 大模型
  - LLM
  - 预训练
type: 论文
papername: 
conference: ACL/EMNLP/NAACL/EACL/COLING/SIGIR/AAAI/ICML/ICDM
year: 2024
institution:
  - 阿里
---

## 论文基本信息

标题：Qwen2.5 Technical Report

作者：Qwen团队

模型链接：

https://huggingface.co/Qwen 
https://modelscope.cn/organization/qwen 
https://github.com/QwenLM/Qwen2.5

## 核心亮点

在预训练方面，将高质量的预训练数据集从之前的 7 万亿个 token 扩展到了 18 万亿个 token。这为常识、专业知识和推理能力提供了坚实的基础。

在后训练方面，实现了超过100万个样本的复杂监督微调，以及多阶段强化学习，包括离线学习DPO和在线学习GRPO。训练后技术显着增强了人类的偏好，并显着改善了长文本生成、结构数据分析和指令遵循。

推出了丰富配置的Qwen2.5 LLM系列。包括基座模型和指令调整模型，参数大小为 0.5B、1.5B、3B、7B、14B、32B 和 72B。还提供了指令调整模型的量化版本。可以从 Hugging Face Hub、ModelScope 和 Kaggle 访问 100 多个模型。专有模型目前包括两种专家混合 (MoE) 变体：Qwen2.5Turbo 和 Qwen2.5-Plus，但是非开源，可以从阿里云模型工作室获取。

Qwen2.5-72B-Instruct 表现出与最先进的开放式重量模型 Llama-3-405B-Instruct 相比的竞争性能。Qwen2.5-Turbo 和 Qwen2.5- Plus，其性能分别与 GPT-4o-mini 和 GPT-4o 竞争。 

## 模型结构

![](img/Pasted%20image%2020241222124849.png)

Dense模型：基于Transformer的decoder结构，模型结构的关键要素是：有助于高效的KV缓存利用的GQA（分组查询注意力机制），SwiGLU非线性激活函数，用于编码位置信息的RoPE，注意力机制中的QKV偏差，具有预归一化的RMSNorm。后两者是为了保证稳定的训练。

MOE模型：通过用专门的 MoE 层替换标准前馈网络 (FFN) 层来实现，其中每层都包含多个 FFN 专家和一个将令牌分派给前 K 个专家的路由机制。和 Qwen1.5-MoE中的方法一样，实现了细粒度专家分割和共享专家路由。这些架构创新极大地提高了下游任务的模型性能。

分词器：使用 Qwen 的分词器，实现了字节级字节对编码（BBPE），包含 151,643 个常规token的词汇表。与之前的 Qwen 版本相比，将控制token集合从 3 个扩展至 22 个，为工具功能添加了两个新token，并将其token分配给其他模型功能。

## 预训练

一、通过复杂的过滤和评分机制，结合数据混合策略，精心策划高质量的训练数据。
二、对超参数优化进行了广泛的研究，以有效地训练各种规模的模型。
三、结合了专门的长上下文预训练，以增强模型处理和理解扩展序列的能力。

### 预训练数据

（1）**更好的数据过滤**。引入 Qwen2-Instruct 模型担当数据质量 “把关人”，展开全方位的多维剖析，对训练样本予以精准评估并打分。相较于此前 Qwen2 所采用的过滤手段，这一方法实现了重大突破。得益于 Qwen2 在规模更为宏大的多语言语料库中的拓展式预训练，它能够完成更精细入微的质量鉴定，不仅大幅提升了高质量训练数据的留存比例，还能更高效地筛除多语种低质样本。

（2）**更好的数学和代码数据**。合并了来自 Qwen2.5-Math和 Qwen2.5-Coder的训练数据。这些专门的数据集有助于在数学和编码任务上实现最先进的性能。

（3）**更好的合成数据**。聚焦于数学、代码以及知识领域，借助 Qwen2-72B-Instruct 与 Qwen2Math-72B-Instruct 模型催生高质量合成数据。在此基础上，运用独家的通用奖励模型，配合专门的 Qwen2-Math-RM-72B 模型进行严苛筛选，全方位升华合成数据的质量层级。

（4）**更好的数据混合**。为实现预训练数据分布的最优化，启用 Qwen2-Instruct 模型对不同领域的内容进行分类梳理与均衡调配。经分析发现，电子商务、社交媒体以及娱乐等板块在海量网络数据中占比过高，且充斥着大量重复、模板化或机器生成的信息。与之相反，技术、科学以及学术研究等领域尽管蕴含高价值信息，却长期处于 “曝光不足” 的状态。通过对过度曝光领域进行策略性降采样，同时加大对高价值领域的采样力度，确保构建出一个更加均衡、信息密度更高的训练数据集，使之与模型的学习诉求完美适配。

基于这些技术，开发了一个更大、更高质量的预训练数据集，从 Qwen2中使用的 7T tokens扩展到 18T tokens。


![](img/Pasted%20image%2020241222122023.png)

### Scaling Law

根据 Qwen2.5 的预训练数据制定超参数的缩放定律，有助于确定不同大小的密集模型和 MoE 模型的关键训练参数，例如批量大小 B 和学习率 μ。

这里的缩放定律是分析最佳学习率 μ 和批量大小 B如何随模型大小 N 和预训练数据大小 D 变化。实验涵盖了全面的架构，包括具有 44M 到 14B 参数的密集模型和具有 44M 到 1B 参数的 MoE 模型。在 0.8B 到 600B 令牌的数据集上进行训练。使用这些最佳超参数预测，将最终损失建模为模型架构和训练数据规模的函数。

此外还利用了缩放定律来预测和比较具有不同参数数量的 MoE 模型与密集模型的性能。该分析指导了对 MoE 模型的超参数配置，通过仔细调整激活参数和总参数来实现与特定密集模型变体（例如 Qwen2.5-72B 和 Qwen2.5-14B）的性能相当。


### 长文本

为达成最佳训练效率，Qwen2.5 运用了两阶段预训练方法：在初始阶段，设置 4,096 的上下文长度，随后进入更长序列的扩展阶段。依照 Qwen2 所采用的策略，在最后的预训练环节，除 Qwen2.5-Turbo 之外的全部模型变体，其上下文长度会从 4,096 延展至 32,768。与此同时，借助 ABF 技术，将 RoPE（位置编码旋转）的基频从 10,000 提升到 1,000,000。

针对 Qwen2.5-Turbo，训练期间推行渐进式上下文长度扩展策略，分四个阶段逐步推进：先是 32,768 个token，接着拓展至 65,536，继而达到 131,072，最终定格在 262,144，且 RoPE 基频高达 10,000,000。在各个阶段，都会精细整理训练数据，确保其中包含 40% 当前最大长度的序列以及 60% 较短的序列。这种渐进式训练手段，能够让模型平稳适应持续增长的上下文长度，同时维持其有效处理与泛化不同长度序列的能力。

为强化模型在推理进程中应对较长序列的本领，研究人员实施了两项关键策略：YARN 和双块注意力 DCA。凭借这些创新性举措，实现了序列长度容量四倍的飞跃式增长，使得 Qwen2.5-Turbo 能够从容处理多达 100 万个token的序列，而其他模型也具备处理多达 131,072 个token序列的能力。尤为值得关注的是，这些方法不但通过削减复杂性优化了长序列的建模效果，而且还守住了模型在较短序列上的强劲性能，切实保证了针对不同输入长度，模型输出质量的一致性。

## 后训练

Qwen2.5的后训练路径是SFT + Two-stage Reinforcement Learning，即SFT- >DPO->GRPO。

（1）扩大监督微调数据覆盖范围：监督微调过程包含了数百万个高质量样本，专门解决了先前模型显示出局限性的关键领域，比如长序列生成、数学问题解决、编码、指令遵循、结构化数据理解、逻辑推理、跨语言迁移和强大的系统指令。

（2）两阶段强化学习：Qwen 2.5 中的强化学习（RL）过程分为两个不同的阶段：离线 RL 和在线 RL。
- 离线RL：此阶段的重点是开发对奖励模型评估具有挑战性的能力，例如推理、事实性和遵循指令。通过对训练数据的精心构建和验证，确保离线强化学习信号既可学习又可靠，使模型能够有效地获得这些复杂的技能。
- 在线RL：在线强化学习阶段利用奖励模型检测输出质量细微差别的能力，包括真实性、有用性、简洁性、相关性、无害性和去偏差。它使模型能够生成精确、连贯且结构良好的响应，同时保持安全性和可读性。因此，该模型的输出始终满足人类质量标准和期望。

### SFT

SFT阶段所做的关键增强领域：

（1）长序列生成：Qwen2.5 能够生成输出上下文长度高达 8,192 个令牌的高质量内容，这比典型的训练后响应长度（通常保持在 2,000 个令牌以下）有显着进步。为了弥补这一差距，开发了长响应数据集。**采用反向翻译技术从预训练语料库中生成长文本数据的查询，施加输出长度限制，并使用 Qwen2 过滤掉低质量的配对数据。**

（2）数学：**引入了 Qwen2.5-Math的思想链数据**，它涵盖了各种查询源，包括公共数据集、K-12 问题集合和综合问题。为了确保高质量的推理，**采用拒绝抽样以及奖励建模和带注释的答案作为指导**，产生逐步的推理过程。

（3）编码：着眼于提升编码能力，**深度融合 Qwen2.5 Coder 的指令调优数据**。在协作式框架下，驱动多种特定语言的智能体协同发力，横跨近 40 种编程语言生成丰富多样且高品质的指令对。一方面，综合源自代码相关问答网站的全新示例，另一方面从 GitHub 广泛搜罗算法代码片段，双管齐下拓展指令数据集。同时，依托全面的多语言沙箱执行静态代码检查，并借助自动化单元测试对代码片段进行严格校验，全方位确保代码的质量与正确性。

（4）指令遵循：为了确保高质量的指令跟踪数据，**实施了严格的基于代码的验证框架**。在这种方法中，LLM生成指令和相应的验证代码，以及用于交叉验证的全面单元测试。通过基于执行反馈的拒绝采样，精心策划用于监督微调的训练数据，从而保证模型忠实地遵守预期指令。

（5）结构化数据理解：**开发了一个全面的结构化理解数据集**，其中涵盖传统任务，例如表格问答、事实验证、纠错和结构理解，以及涉及结构化和半结构化数据的复杂任务。**通过将推理链纳入模型的响应中**，显着增强了其从结构化数据推断信息的能力，从而提高了其在这些不同任务中的性能。这种方法不仅扩大了数据集的范围，而且加深了模型的推理能力并从复杂的数据结构中得出有意义的见解。

（6）逻辑推理: 为了增强模型的逻辑推理能力，引入了一组**跨越不同领域**的 70,000 个新查询。这些查询包括多项选择题、对/错问题和开放式问题。该模型经过训练，能够系统地处理问题，采用一系列推理方法，例如演绎推理、归纳概括、类比推理、因果推理和统计推理。通过迭代细化，系统地过滤掉包含错误答案或有缺陷的推理过程的数据。这个过程逐渐增强了模型的逻辑和准确推理能力，确保在不同类型的推理任务中具有稳健的性能。

（7）跨语言迁移：为了促进模型的通用能力跨语言的迁移，**采用翻译模型将指令从高资源语言转换为各种低资源语言**，从而生成相应的响应候选。为了确保这些响应的准确性和一致性，评估每个多语言响应与其原始对应响应之间的语义一致性。此过程保留了原始响应的逻辑结构和风格上的细微差别，从而保持了不同语言之间的完整性和连贯性。

（8）强大的系统指令：构建了**数百个通用系统提示**，以提高训练后系统提示的**多样性**，确保系统提示和对话之间的一致性。不同系统提示下的评估表明，模型保持了良好的性能并且方差减少，表明鲁棒性提高。

（9）回复过滤：为了评估回复的质量，采用了多种自动注释方法，**包括专用的评论家模型和多智能体协作评分系统**。回答要经过严格的评估，只有那些被所有评分系统认为完美的回答才会被保留。这种全面的方法可确保我们的产品保持最高的质量标准。

最终，构建了一个包含超过 100 万个 SFT 示例的数据集。该模型针对序列长度为 32,768 个令牌的进行了两个epoch的微调。学习率从 7 × 10−6 逐渐降低到 7 × 10−7。为了解决过度拟合问题，应用 0.1 的权重衰减，并将梯度范数限制为最大值 1.0。


### 离线RL

相较于在线强化学习（RL），离线强化学习有着独特优势，它能够提前准备好训练所需的信号。这一特性在那些存在标准答案，却又难以运用奖励模型去评估的任务中，显得尤为突出。这里将目光聚焦于数学、编码、指令遵循以及逻辑推理等客观查询领域，要知道，在这些领域想获得精准的评估并非易事。

在上一阶段，大量运用了执行反馈、答案匹配等策略，以此确保响应的质量。现阶段再度启用这一流程，借助 SFT 模型针对一组全新查询的响应进行重新采样。顺利通过质量检查的响应，会被当作chosen；而那些没能通过质量核查的，则会被视作直接偏好优化 (DPO) 训练的rejected。

为进一步提升训练信号的可靠性与精准度，同时启用了人工审查与自动审查流程。这种双管齐下的方式，既能保证训练数据具备可学习性，又能使其契合人类的预期。最终成功构建出一个涵盖约 15 万个训练对的数据集，并使用在线合并优化器对模型展开了一个epoch的训练，训练时所采用的学习率为 7 × 10⁻⁷ 。

	Online Merging Optimizer：https://arxiv.org/abs/2405.1793

### 在线RL

为开发一个功能强劲的在线强化学习奖励模型，研究人员遵循了一套严谨且精心定义的标签标准。这些标准能够保障模型所生成的响应，不但具备高质量水准，还契合道德规范以及以用户为中心的要求。具体的数据标注准则如下：

- **真实性 (Truthfulness)**：回答务必以事实准确性为根基，要忠实地呈现所给定的背景信息与说明内容。模型切忌生成错误信息，或是那些未得到给定数据支撑的内容。

- **有用性 (Helpfulness)**：模型的输出应当切实有用，能够切实有效地解决用户的查询需求，同时提供积极正向、富有吸引力、兼具教育意义且与问题相关的内容。它需要精准遵循给定的指令，切实为用户创造价值。

- **简洁 (Conciseness)**：答复应当简明扼要，规避不必要的冗长表述。力求以清晰、高效的方式传递信息，避免因繁杂细节而让用户感到困扰。

- **相关性 (Relevance)**：响应的各个部分都必须与用户的查询、对话历史记录以及助理所处的上下文环境直接关联。模型需要依据实际情况灵活调整输出，以充分满足用户的需求与期望。

- **无害性 (Harmlessness)**：模型必须将用户安全置于首位，杜绝任何可能引发非法、不道德或有害行为的内容出现。它应当始终倡导道德行为，推动负责任的沟通交流。

- **去偏见 (Unbiasedness)**：模型理应生成无偏见的反应，涵盖但不限于性别、种族、国籍以及政治等方面。对待所有主题都应一视同仁、公平公正，严格遵守广泛认可的道德与伦理准则。

用于训练奖励模型的查询数据源自两个不同的数据集：**一个是公开可获取的开源数据，另一个则是具有较高复杂性的专有查询集。而响应内容是从 Qwen 模型的检查点生成的**，这些模型在训练的各个阶段，借助不同方法（如 SFT、DPO 和 RL）进行了微调优化。为增添多样性，这些响应在不同的温度设置下完成采样。偏好对是经由人工与自动标记流程共同创建的，并且 DPO 的训练数据也被整合进了该数据集中。

在研究人员构建的在线强化学习（RL）框架里，采用了组相对策略优化（GRPO）。用于训练奖励模型的prompts与 RL 训练阶段所使用的prompts保持一致。训练期间处理查询的顺序由响应分数的方差来决定，而这一方差是通过奖励模型进行评估的。具体而言，**优先处理那些响应分数差异较大的查询**，以此确保学习过程更为高效。研究人员为每个查询抽取 8 个响应样本。所有模型均使用 2048 个全局批量大小以及每集中 2048 个样本展开训练，且将一对查询和响应视作一个样本。

### 长文本微调

为进一步拓展 Qwen2.5-Turbo 的上下文长度，研究人员在训练完成后引入了更长的 SFT 示例，以此促使模型能够更精准地契合人类在应对长查询时的偏好。

在 SFT（监督微调）阶段，研究团队采用了两阶段式的方法。第一阶段，模型仅依靠短指令来进行微调操作，这里的每个指令最多涵盖 32,768 个令牌。此阶段运用的数据以及训练步骤均与其他 Qwen2.5 模型保持一致，如此便能确保模型在处理短任务时展现出强劲的性能表现。进入第二阶段，微调过程将短指令（最多 32,768 个令牌）与长指令（最多 262,144 个令牌）有机结合。这种混合式的处理手段，卓有成效地提升了模型在长上下文任务里的指令跟踪能力，同时还稳固住了它在短任务上已有的出色性能。

到了 RL（强化学习）阶段，研究人员运用了与其他 Qwen2.5 模型相近的训练策略，不过仅聚焦于简短指令。这一设计方案主要源自两大关键考量：其一，针对长上下文任务而言，开展强化学习训练所需的计算成本颇为高昂；其二，就当前情况来看，尚缺乏能够为长上下文任务精准提供合适奖励信号的奖励模型。不仅如此，研究人员还惊喜地发现，仅仅在短指令上运用强化学习策略，依旧能够显著提升模型在长上下文任务中与人类偏好的契合度。

## 评估

### 基座模型

![](img/Pasted%20image%2020241222144447.png)

Qwen2.5-72B 基础模型在广泛的任务中显着优于同类同类产品。它仅利用五分之一的参数即可实现与 Llama-3-405B 相当的结果。此外，与前身 Qwen2-72B 相比，Qwen2.5-72B 在几乎所有基准评估中都显示出显着改进，特别是在一般任务、数学和编码挑战方面表现出色。与 Qwen2.5-72B 和 Llama3-405B 相比，Qwen2.5-Plus 的训练和推理成本显着降低，取得了极具竞争力的性能结果，在 Hellaswag、TheoremQA、MATH、GSM8K、MultiPL-E、Multi-数学和多重翻译。而且，Qwen2.5-Plus在MMLU-Pro上取得了64.0的成绩，比Qwen2.5-72B高出5.9分。

![](img/Pasted%20image%2020241222144648.png)

![](img/Pasted%20image%2020241222144658.png)

![](img/Pasted%20image%2020241222144711.png)

### 指令微调模型

![](img/Pasted%20image%2020241222144820.png)

![](img/Pasted%20image%2020241222144834.png)

![](img/Pasted%20image%2020241222144858.png)

![](img/Pasted%20image%2020241222144907.png)

![](img/Pasted%20image%2020241222145007.png)

![](img/Pasted%20image%2020241222145024.png)

![](img/Pasted%20image%2020241222145109.png)

![](img/Pasted%20image%2020241222145258.png)

![](img/Pasted%20image%2020241222145308.png)

### 奖励模型评估

![](img/Pasted%20image%2020241222145349.png)

由于缺乏奖励模型的评估方法，目前的奖励模型通常使用Reward Bench进行评估。然而，对多个 RM 基准的评估结果表明，对特定基准的过度优化可能会触发古德哈特定律，导致其他基准的性能下降，并可能影响下游对齐性能。这凸显了需要对不同基准的奖励模型进行综合评估，而不是仅仅依赖单一基准。  更重要的是，通过迭代实验，研究人员也认识到一个关键的局限性：当前的奖励模型评估基准无法准确预测在其指导下训练的强化学习模型的性能。换句话说，RM 基准测试的较高分数并不一定与所得 RL 模型的卓越性能相关。这一见解强调需要进一步研究奖励模型更具预测性的评估方法。
### 长文本评估

![](img/Pasted%20image%2020241222145504.png)

![](img/Pasted%20image%2020241222145516.png)

![](img/Pasted%20image%2020241222145611.png)

引入了稀疏注意力机制来显着提高推理速度，这对于处理长上下文时的用户体验至关重要。对于 1M 个 token 的序列，这种方法将注意力机制的计算负载减少了 12.5 倍。

![](img/Pasted%20image%2020241222145637.png)


## 主要收获

Qwen2.5之所以这么强，我认为总结下来主要是以下几点：

1. 预训练数据集数量和质量都有极大提升。数量比Qwen2的2倍还多，还能保证质量。在Qwen2的技术报告中，提到尝试过使用15T数据训练，但是相比7T数据没有改进，很明显是数据清洗还有很大的提升空间。这次采取的方案则是：使用上一代的模型做质量过滤、数据合成，以及加入了更多数学和代码数据。通过下采样低价值高数量领域和上采样高价值领域数据，使得领域分布也更加均衡。
2. 在后训练阶段，通过SFT增强了一些之前表现不够好的关键领域，涉及技术有：COT、拒绝采样、翻译、提升系统指令的多样性、质量过滤等；通过离线DPO增强数学、编码、指令遵循以及逻辑推理等存在标准答案却又难以用奖励模型评估的领域；通过采取在线GRPO技术，定义多个角度的奖励标签标准，结合开源数据和自有数据，训练时优先处理响应分数差异较大的样本，以此对齐人类偏好。
3. 长文本的训练。在预训练阶段，逐步扩展序列长度，每一阶段保证40%最大序列和60%较短序列数据的混合。同时也提升了RoPE的Base；SFT时分两阶段训练，第一阶段是短文本指令微调（最长32k），第二阶段将短指令与长指令（最长262k）结合；RL阶段则仅聚焦于短指令，不过即使这样也能提升模型在长上下文任务中与人类偏好的契合度。


主要还是预训练的数据质量和数量做得足够好。

## 参考资料

Qwen2.5技术报告


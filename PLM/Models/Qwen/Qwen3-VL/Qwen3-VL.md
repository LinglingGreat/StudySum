---
title: Qwen3-VL
created: 2025-09-28
tags:
  - 视觉语言模型
type: 论文
papername:
conference: ACL/EMNLP/NAACL/EACL/COLING/SIGIR/AAAI/ICML/ICDM
year: 2025
institution:
  - qwen
---

## 论文基本信息

标题：

作者：

链接：

代码：

框架图：

Blog: [Qwen](https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&from=research.latest-advancements-list)

## 背景
论文试图解决什么问题？这是否是一个新的问题？

这篇文章要验证一个什么科学假设？

论文中提到的解决方案之关键是什么？


## 相关研究
有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？



## 核心亮点

Qwen3-VL 的目标，是让模型不仅能“看到”图像或视频，更能**真正看懂世界、理解事件、做出行动**。为此，我们在多个关键能力维度上做了系统性升级，力求让视觉大模型从“感知”走向“认知”，从“识别”迈向“推理与执行”。

- **视觉智能体（Visual Agent）**：Qwen3-VL 能操作电脑和手机界面、识别 GUI 元素、理解按钮功能、调用工具、执行任务，在 OS World 等 benchmark 上达到世界顶尖水平，能通过调用工具有效提升在细粒度感知任务的表现。
    

- **纯文本能力媲美顶级语言模型**：Qwen3-VL 在预训练早期即混合文本与视觉模态协同训练，文本能力持续强化，最终在纯文本任务上表现与 Qwen3-235B-A22B-2507 纯文本旗舰模型不相上下 —— 是真正“文本根基扎实、多模态全能”的新一代视觉语言模型。
    

- **视觉 Coding 能力大幅提升**：实现图像生成代码以及视频生成代码，例如看到设计图，代码生成 Draw.io/HTML/CSS/JS 代码，真正实现“所见即所得”的视觉编程。
    

- **空间感知能力大幅提升**：2D grounding 从绝对坐标变为相对坐标，支持判断物体方位、视角变化、遮挡关系，能实现 3D grounding，为复杂场景下的空间推理和具身场景打下基础。
    

- **长上下文支持和长视频理解**：全系列模型原生支持 256K token 的上下文长度，并可扩展至 100 万 token。这意味着，无论是几百页的技术文档、整本教材，还是长达两小时的视频，都能完整输入、全程记忆、精准检索，支持视频精确定位到秒级别时刻。
    

- **多模态思考能力显著增强**：Thinking 模型重点优化了 STEM 与数学推理能力。面对专业学科问题，模型能捕捉细节、抽丝剥茧、分析因果、给出有逻辑、有依据的答案，在 MathVision、MMMU、MathVista 等权威评测中达到领先水平。
    

- **视觉感知与识别能力全面升级**：通过优化预训练数据的质量和广度，模型现在能识别更丰富的对象类别——从名人、动漫角色、商品、地标，到动植物等，覆盖日常生活与专业领域的“万物识别”需求。
    

- **OCR 支持更多语言及复杂场景**：支持的中英外的语言从 **10** 种扩展到 **32** 种，覆盖更多国家和地区；在复杂光线、模糊、倾斜等实拍挑战性场景下表现更稳定；对生僻字、古籍字、专业术语的识别准确率也显著提升；超长文档理解和精细结构还原能力进一步提升。

在架构上，我们仍旧采用原生动态分辨率设计，但在结构设计上进行了更新：

![](https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_arc.jpg#center)

一是采用 **MRoPE-Interleave**，原始MRoPE将特征维度按照时间（t）、高度（h)和宽度（w)的顺序分块划分，使得时间信息全部分布在高频维度上。我们在 Qwen3-VL 中采取了 t,h,w 交错分布的形式，实现对时间，高度和宽度的全频率覆盖，这样更加鲁棒的位置编码能够保证模型在图片理解能力相当的情况下，提升对长视频的理解能力；

二是引入 **DeepStack** 技术，融合 ViT 多层次特征，提升视觉细节捕捉能力和图文对齐精度；我们沿用 **DeepStack** 的核心思想，将以往多模态大模型（LMM）单层输入视觉tokens的范式，改为在大型语言模型 (LLM) 的多层中进行注入。这种**多层注入**方式旨在实现更精细化的视觉理解。  
在此基础上，我们进一步优化了视觉特征 token 化的策略。具体而言，我们将来自 ViT 不同层的视觉特征进行 token 化，并以此作为视觉输入。这种设计能够有效保留从底层（low-level）到高层（high-level）的丰富视觉信息。实验结果表明，该方法在多种视觉理解任务上均展现出显著的性能提升。

三是将原有的视频时序建模机制 T-RoPE 升级为 **文本时间戳对齐机制**。该机采用“时间戳-视频帧”交错的输入形式，实现帧级别的时间信息与视觉内容的细粒度对齐。同时，模型原生支持“秒数”与“时:分:秒”（HMS）两种时间输出格式。这一改进显著提升了模型对视频中动作、事件的语义感知与时间定位精度，使其在复杂时序推理任务——如事件定位、动作边界检测、跨模态时间问答等——中表现更稳健、响应更精准。

## 实验
论文中的实验是如何设计的？

用于定量评估的数据集是什么？代码有没有开源？

论文中的实验及结果有没有很好地支持需要验证的科学假设？



## 未来方向



## 主要收获


## 参考资料

---
title: Qwen2
created: 2024-08-22
tags:
  - 大模型
---
## 概述

Qwen 2 模型有5种类型。有4个常规（密集）LLM，参数量分别为5亿、15亿、70亿和720亿。此外，还有一个57亿参数的专家混合模型，其中有14亿参数同时被激活。

Qwen 2 LLM的一个突出特点是其在30种语言中的良好多语言能力。它们还拥有惊人的151,642个词汇量（作为参考，Llama 2使用32k词汇量，Llama 3.1使用128k词汇量）；根据经验，词汇量增加2倍会减少输入词数2倍，因此LLM可以在同一输入中容纳更多词。它特别有助于多语言数据和编码，以覆盖标准英语词汇之外的词语。

## 预训练

Qwen 2团队在7万亿训练数据上训练了15亿、70亿和720亿参数模型，这是一个合理的规模。作为比较，Llama 2模型在2万亿数据上训练，而Llama 3.1模型在15万亿数据上训练。

有趣的是，5亿参数模型是在12万亿数据上训练的。然而，研究人员并没有在更大的12万亿数据集上训练其他模型，因为他们在训练过程中没有观察到任何改进，并且额外的计算成本也不合理。

重点之一是改进数据过滤流程，以去除低质量数据，并增强数据混合以增加数据多样性

有趣的是，他们还使用了Qwen模型（尽管他们没有具体说明，但我认为他们指的是上一代的Qwen模型）来合成额外的预训练数据。预训练还涉及“多任务指令数据……以增强上下文学习和指令遵循能力”。

此外，他们进行了两个阶段的训练：常规预训练和长上下文训练。后者在预训练结束阶段将上下文长度从4,096个数据增加到32,768个数据，使用的是“高质量、长篇数据”。

## 后训练

Qwen 2 团队采用了流行的两阶段后训练方法，首先是监督指令微调（SFT），在 500,000 个示例上进行了 2 个周期。此阶段旨在提高模型在预定场景中的响应准确性。

在SFT之后，他们使用直接偏好优化（DPO）来使LLM与人类偏好对齐。（有趣的是，在他们的术语中，这被称为来自人类反馈的强化学习，RLHF。）正如我几周前在《LLM预训练技巧和奖励模型评估》中讨论的那样，由于与其他方法（如使用PPO的RLHF）相比易于使用，SFT+DPO方法似乎是目前最受欢迎的偏好调优策略。

对齐阶段本身也分为两个阶段。首先在现有数据集上使用DPO（离线阶段）。其次，使用奖励模型形成偏好对（在线）。在这里，模型在训练期间生成多个响应，奖励模型在“实时”优化步骤中选择首选响应（也就是说，在训练期间）。这也常被称为“拒绝采样”。

在数据集构建方面，他们使用了现有语料库并辅以人为标注，以确定SFT的目标响应并识别对DPO至关重要的首选和被拒绝的响应。研究人员还合成了人工注释的数据。

此外，团队使用LLM生成专门为“高质量文学数据”量身定制的指令-响应对，以创建高质量的问答对进行训练。




## 参考资料

[LLM预训练和后训练新范式](https://mp.weixin.qq.com/s/sFdnWwWXouU2ZPbf5GoXhA)


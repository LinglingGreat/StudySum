---
title: Kimik1.5
created: 2025-01-21
tags:
  - cot
type: 论文
papername: 
conference: ACL/EMNLP/NAACL/EACL/COLING/SIGIR/AAAI/ICML/ICDM
year: 2025
institution:
  - 月之暗面
---

## 论文基本信息

标题：

作者：

链接：

代码：

框架图：


## 背景
**k1.5 多模态思考模型**。新模型在数学、代码、多模态推理能力等方面全面对标 Open AI 满血版 o1，而且是 OpenAI 之外首个多模态 o1。尤其是 kimi-k1.5-short，成为 SOTA short cot 模型，并大幅领先 GPT-4o 和 Claude 3.5 Sonnet（提升幅度高达 550%）

在 long-CoT 模式下，Kimi k1.5 在数学、代码及多模态推理能力上，达到长思考 SOTA 模型 OpenAI o1 正式版的水平。Kimi k1.5 在 AIME 上达到 77.5 分，在 MATH 500 上达到 96.2 分，在 Codeforces 上达到 94 百分位，在 MathVista 上达到 74.9 分。

在 short-CoT 模式下，Kimi k1.5 在数学、代码、视觉多模态和通用能力上，也达到了全球范围内短思考 SOTA 模型 ，并大幅领先 GPT-4o 和 Claude 3.5 Sonnet 的水平。比如，Kimi k1.5 在 AIME 上达到 60.8 分，MATH500 上达到 94.6 分，LiveCodeBench 上达到 47.3 分。

不仅如此，从全球前沿大模型数学竞赛和编程竞赛基准测试来看，Kimi k1.5 的表现也相当不错，处于全球第一梯队，而这两项测试代表了人类智商巅峰。

![](img/Pasted%20image%2020250121135114.png)

![](img/Pasted%20image%2020250121135126.png)

_Short-CoT 是“Zero-shot Chain-of-Thought”的简称，即零样例思维链。它是一种在大语言模型中使用的提示技术，无需提供任何示例，仅通过自然语言指令引导模型进行逐步推理，从而得出答案。_

_例如，在解决数学问题时，模型会先展示出详细的推理步骤，再给出最终答案，而不是直接给出结果。_


_思维链（CoT）就像是给AI出题时，让它不仅要给出答案，还要把思考的过程写出来，就像学生做数学题要写出解题步骤一样。这样可以帮助AI更好地理解和解决问题。_

_而Long-CoT就是在思维链的基础上，让AI写出更长、更详细的思考过程。比如原本只需要写三步思考过程，现在可能要写五步、十步甚至更多，这样能让AI处理更复杂的问题，就像解决一个更难的数学题需要更多的步骤一样。_

## 相关研究




## 核心亮点

随着模型尺寸逐渐增大，预训练阶段参数 scaling up 带来的边际收益开始递减，如果想要深度提升模型推理能力和长程问题能力，基于强化学习的 Post-Training 将会成为下一个突破点，因为 scaling 强化学习为人工智能的持续进步开辟了新的维度，它使得大语言模型能够通过带有奖励的探索学习来扩展其训练数据，从而也实现计算规模的扩展。

大的方向非常明确，然而，此前发表的研究工作尚未产生具有竞争力的结果。

有鉴于此，Kimi 技术团队在 Kimi k1.5 的训练实践中全面探索了 RL 训练技术、多模态数据配方和基础设施优化。

难得的是，他们探索出的 RL 框架简单、有效，无需依赖蒙特卡洛树搜索、价值函数和过程奖励模型等更复杂的技术也能取得优异的性能。

此外，他们还提出了有效的 long2short 技术，利用 Long-CoT 技术来改进 Short-CoT 模型，使得模型在短链思维推理方面取得了最佳成果。

**简单、有效的 RL 框架**

Kimi 技术团队设计的简单而有效的 RL 框架离不开两个关键要素：**长上下文 scaling 和改进的策略优化**。

先说长上下文 scaling。他们将强化学习的上下文窗口 scale 到 128k，并观察到随着上下文长度的增加，模型性能持续改善。新方法背后的一个关键理念是使用 partial rollout 来提高训练效率 —— 即通过重用大量以前的轨迹来采样新的轨迹，避免从头重新生成新轨迹的成本。技术团队的观察表明，上下文长度是大语言模型强化学习持续 scaling 的一个关键维度。 

再来看策略优化的改进。他们推导出了一个具有 long-CoT 的强化学习公式，并采用在线镜像下降法的变体来实现稳健的策略优化。通过有效的采样策略、长度惩罚和数据配方的优化，他们进一步改进了该算法。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9JclBGWETYEd5NY6g12aQhqqGAxwibSq5cV5JjxrfTyks80nfGwYExCjzibT4r42bH57WoK8ibutX4g/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

  通过将这两个关键要素结合，Kimi 技术团队建立了一个用于 LLM 学习的简化强化学习框架。由于该框架能够 scale 上下文长度，学习到的 CoT 展现出规划、反思和纠正的特性。增加的上下文长度具有增加搜索步骤数量的效果。因此，他们表明无需依赖蒙特卡洛树搜索、价值函数和过程奖励模型等更复杂的技术也能实现强大的性能。 

此外，他们的模型还在文本和视觉数据上进行了联合训练，具备对这两种模态进行联合推理的能力。 

  

**long2short 技术**

尽管 long-CoT 模型在性能上表现出色，但与标准的 short-CoT LLM 相比，它在测试时消耗的 token 数量更多。然而，Kimi 技术团队发现将 long-CoT 模型的思维先验迁移到 short-CoT 模型中是可能的，从而在有限的测试 token 预算下提升性能。

他们提出了几种解决这一 long2short 问题的方法，包括模型融合、最短拒绝采样、DPO 以及 long2short RL。以下是这些方法的详细描述：

- 模型融合。团队人员发现模型融合（Model Merging）有助于保持模型的泛化能力。他们还发现，在融合 long-CoT 模型和 short-CoT 模型时，模型融合也能有效提升 token 效率。这种方法通过将 long-CoT 模型与 short-CoT 模型结合，从而在不进行训练的情况下获得一个新模型。具体来说，他们通过简单地平均两个模型的权重来实现融合。

- 最短拒绝采样。研究者观察到，模型在回答相同问题时生成的响应长度存在较大差异。基于此，他们设计了最短拒绝采样（Shortest Rejection Sampling）方法。该方法对同一个问题采样 n 次（实验中，n=8），并选择最短的正确响应进行监督微调。

- DPO。与最短拒绝采样类似，团队人员利用 Long CoT 模型生成多个响应样本。并选择最短的正确解决方案作为正样本，而较长的响应则被视为负样本，包括错误的较长响应和正确的较长响应。这些正负样本对构成了用于 DPO 训练的成对偏好数据。

- Long2short RL。在标准的 RL 训练阶段之后，团队人员选择一个在性能和 token 效率之间达到最佳平衡的模型作为基础模型，并进行单独的 long2short RL 训练阶段。在这个第二阶段中，他们还应用了长度惩罚机制，从而显著减少最大 rollout 长度，以进一步惩罚那些超出期望长度但可能正确的响应。


## 方法

Kimi k1.5的开发由几个阶段组成：预训练、普通监督微调（SFT）、长CoT监督微调和强化学习（RL）。

### RL Prompt Set Curation

通过我们的初步实验，我们发现强化学习提示集的质量和多样性对于确保强化学习的有效性起着至关重要的作用。构造良好的提示集不仅可以引导模型进行稳健的推理，还可以降低奖励黑客和过度拟合表面模式的风险。具体来说，三个关键属性定义了高质量的 RL 提示集

- 覆盖面多样化：提示应涵盖广泛的学科，例如 STEM、编码和一般推理，以增强模型的适应性并确保在不同领域的广泛适用性。  
- 难度平衡：提示集应包括一系列分布均匀的简单、中等和困难的问题，以促进逐步学习并防止过度适应特定的复杂程度。  
- 准确的可评估性：提示应允许验证者进行客观可靠的评估，确保模型性能是基于正确的推理而不是肤浅的模式或随机猜测来衡量的。

为了在提示集中实现多样化的覆盖范围，我们采用自动过滤器来选择需要丰富推理且易于评估的问题。我们的数据集包含来自各个领域的问题，例如 STEM 领域、竞赛和一般推理任务，包含纯文本和图像文本问答数据。此外，我们开发了一个标签系统，按领域和学科对提示进行分类，确保不同学科领域的均衡代表性（M. Li 等人，2023 年；W. Liu 等人，2023 年）。

我们采用基于模型的方法，利用模型自身的能力来自适应评估每个提示的难度。具体来说，对于每个提示，SFT 模型都会使用相对较高的采样温度生成十次答案。然后计算通过率并用作提示难度的代理——通过率越低，难度越高。这种方法可以使难度评估与模型的内在功能保持一致，从而使其对于强化学习训练非常有效。通过利用这种方法，我们可以预过滤大多数琐碎的情况，并在强化学习训练期间轻松探索不同的采样策略。

为了避免潜在的奖励黑客攻击（Everitt et al. 2021；Pan et al. 2022），我们需要确保每个提示的推理过程和最终答案都能被准确验证。经验观察表明，一些复杂的推理问题可能有相对简单且容易猜测的答案，从而导致误报验证——模型通过不正确的推理过程得出正确的答案。为了解决这个问题，我们排除了容易出现此类错误的问题，例如多项选择题、对/错题和基于证明的问题。此外，对于一般的问答任务，我们提出了一种简单而有效的方法来识别和删除易于破解的提示。具体来说，我们提示模型在没有任何 CoT 推理步骤的情况下猜测潜在的答案。如果模型在 N 次尝试内预测出正确答案，则该提示被认为太容易破解并被删除。我们发现设置 N = 8 可以删除大多数易于破解的提示。开发更先进的验证模型仍然是未来研究的开放方向。

### Long-CoT Supervised Fine-Tuning

借助精炼的 RL 提示集，我们采用提示工程构建了一个小型但高质量的长 CoT 预热数据集，其中包含经过准确验证的文本和图像输入的推理路径。这种方法类似于拒绝采样 (RS)，但侧重于通过即时工程生成长 CoT 推理路径。由此产生的预热数据集旨在封装对类人推理至关重要的关键认知过程，例如规划，其中模型系统地概述了执行之前的步骤；评估，涉及对中间步骤的严格评估；反思，使模型能够重新考虑和完善其方法；和探索，鼓励考虑替代解决方案。通过在此预热数据集上执行轻量级 SFT，我们有效地启动了模型以内化这些推理策略。因此，经过微调的长 CoT 模型在生成更详细和逻辑连贯的响应方面表现出更高的能力，从而增强了其在不同推理任务中的性能。

### Reinforcement Learning

因此，我们考虑训练模型以通过强化学习 (RL) 生成 CoT (OpenAI 2024)。令 r 为奖励模型，通过分配值 r(x, y, y*) ∈ {0, 1}，根据基本事实 y* 来证明给定问题 x 的建议答案 y 的正确性。对于可验证的问题，奖励直接由预定义的标准或规则确定。例如，在编码问题中，我们评估答案是否通过测试用例。对于自由形式的基本事实问题，我们训练一个奖励模型 r(x, y, y*) 来预测答案是否与基本事实匹配。给定问题 x，模型 πθ 通过采样过程 z ∼ πθ(·|x), y ∼ πθ(·|x, z) 生成 CoT 和最终答案。生成的 CoT 的质量通过是否能够得出正确的最终答案来评估。综上，我们认为优化政策的目标如下

![](img/Pasted%20image%2020250121150657.png)

通过扩大 RL 训练，我们的目标是训练一个模型，该模型能够利用简单的基于提示的 CoT 和规划增强的 CoT 的优势。该模型在推理过程中仍然自动回归采样语言序列，从而避免了部署过程中高级规划算法所需的复杂并行化的需要。然而，与简单的基于提示的方法的一个关键区别在于，模型不应仅仅遵循一系列推理步骤。相反，它还应该通过利用整套探索的想法作为上下文信息来学习关键的规划技能，包括错误识别、回溯和解决方案细化。




## 未来方向



## 主要收获


## 参考资料

---
title: Kimik1.5
created: 2025-01-21
tags:
  - cot
type: 论文
papername: 
conference: ACL/EMNLP/NAACL/EACL/COLING/SIGIR/AAAI/ICML/ICDM
year: 2025
institution:
  - 月之暗面
---

## 论文基本信息

标题：

作者：

链接：

代码：

框架图：


## 背景
**k1.5 多模态思考模型**。新模型在数学、代码、多模态推理能力等方面全面对标 Open AI 满血版 o1，而且是 OpenAI 之外首个多模态 o1。尤其是 kimi-k1.5-short，成为 SOTA short cot 模型，并大幅领先 GPT-4o 和 Claude 3.5 Sonnet（提升幅度高达 550%）

在 long-CoT 模式下，Kimi k1.5 在数学、代码及多模态推理能力上，达到长思考 SOTA 模型 OpenAI o1 正式版的水平。Kimi k1.5 在 AIME 上达到 77.5 分，在 MATH 500 上达到 96.2 分，在 Codeforces 上达到 94 百分位，在 MathVista 上达到 74.9 分。

在 short-CoT 模式下，Kimi k1.5 在数学、代码、视觉多模态和通用能力上，也达到了全球范围内短思考 SOTA 模型 ，并大幅领先 GPT-4o 和 Claude 3.5 Sonnet 的水平。比如，Kimi k1.5 在 AIME 上达到 60.8 分，MATH500 上达到 94.6 分，LiveCodeBench 上达到 47.3 分。

不仅如此，从全球前沿大模型数学竞赛和编程竞赛基准测试来看，Kimi k1.5 的表现也相当不错，处于全球第一梯队，而这两项测试代表了人类智商巅峰。

_Short-CoT 是“Zero-shot Chain-of-Thought”的简称，即零样例思维链。它是一种在大语言模型中使用的提示技术，无需提供任何示例，仅通过自然语言指令引导模型进行逐步推理，从而得出答案。_

_例如，在解决数学问题时，模型会先展示出详细的推理步骤，再给出最终答案，而不是直接给出结果。_


_思维链（CoT）就像是给AI出题时，让它不仅要给出答案，还要把思考的过程写出来，就像学生做数学题要写出解题步骤一样。这样可以帮助AI更好地理解和解决问题。_

_而Long-CoT就是在思维链的基础上，让AI写出更长、更详细的思考过程。比如原本只需要写三步思考过程，现在可能要写五步、十步甚至更多，这样能让AI处理更复杂的问题，就像解决一个更难的数学题需要更多的步骤一样。_

## 相关研究




## 核心亮点

随着模型尺寸逐渐增大，预训练阶段参数 scaling up 带来的边际收益开始递减，如果想要深度提升模型推理能力和长程问题能力，基于强化学习的 Post-Training 将会成为下一个突破点 [1]，因为 scaling 强化学习为人工智能的持续进步开辟了新的维度，它使得大语言模型能够通过带有奖励的探索学习来扩展其训练数据，从而也实现计算规模的扩展。

大的方向非常明确，然而，此前发表的研究工作尚未产生具有竞争力的结果。

有鉴于此，Kimi 技术团队在 Kimi k1.5 的训练实践中全面探索了 RL 训练技术、多模态数据配方和基础设施优化。

难得的是，他们探索出的 RL 框架简单、有效，无需依赖蒙特卡洛树搜索、价值函数和过程奖励模型等更复杂的技术也能取得优异的性能。

此外，他们还提出了有效的 long2short 技术，利用 Long-CoT 技术来改进 Short-CoT 模型，使得模型在短链思维推理方面取得了最佳成果。

**简单、有效的 RL 框架**

Kimi 技术团队设计的简单而有效的 RL 框架离不开两个关键要素：**长上下文 scaling 和改进的策略优化**。

先说长上下文 scaling。他们将强化学习的上下文窗口 scale 到 128k，并观察到随着上下文长度的增加，模型性能持续改善。新方法背后的一个关键理念是使用 partial rollout 来提高训练效率 —— 即通过重用大量以前的轨迹来采样新的轨迹，避免从头重新生成新轨迹的成本。技术团队的观察表明，上下文长度是大语言模型强化学习持续 scaling 的一个关键维度。 

再来看策略优化的改进。他们推导出了一个具有 long-CoT 的强化学习公式，并采用在线镜像下降法的变体来实现稳健的策略优化。通过有效的采样策略、长度惩罚和数据配方的优化，他们进一步改进了该算法。

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/KmXPKA19gW9JclBGWETYEd5NY6g12aQhqqGAxwibSq5cV5JjxrfTyks80nfGwYExCjzibT4r42bH57WoK8ibutX4g/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

  通过将这两个关键要素结合，Kimi 技术团队建立了一个用于 LLM 学习的简化强化学习框架。由于该框架能够 scale 上下文长度，学习到的 CoT 展现出规划、反思和纠正的特性。增加的上下文长度具有增加搜索步骤数量的效果。因此，他们表明无需依赖蒙特卡洛树搜索、价值函数和过程奖励模型等更复杂的技术也能实现强大的性能。 

此外，他们的模型还在文本和视觉数据上进行了联合训练，具备对这两种模态进行联合推理的能力。 

  

**long2short 技术**

尽管 long-CoT 模型在性能上表现出色，但与标准的 short-CoT LLM 相比，它在测试时消耗的 token 数量更多。然而，Kimi 技术团队发现将 long-CoT 模型的思维先验迁移到 short-CoT 模型中是可能的，从而在有限的测试 token 预算下提升性能。

他们提出了几种解决这一 long2short 问题的方法，包括模型融合、最短拒绝采样、DPO 以及 long2short RL。以下是这些方法的详细描述：

- 模型融合。团队人员发现模型融合（Model Merging）有助于保持模型的泛化能力。他们还发现，在融合 long-CoT 模型和 short-CoT 模型时，模型融合也能有效提升 token 效率。这种方法通过将 long-CoT 模型与 short-CoT 模型结合，从而在不进行训练的情况下获得一个新模型。具体来说，他们通过简单地平均两个模型的权重来实现融合。

- 最短拒绝采样。研究者观察到，模型在回答相同问题时生成的响应长度存在较大差异。基于此，他们设计了最短拒绝采样（Shortest Rejection Sampling）方法。该方法对同一个问题采样 n 次（实验中，n=8），并选择最短的正确响应进行监督微调。

- DPO。与最短拒绝采样类似，团队人员利用 Long CoT 模型生成多个响应样本。并选择最短的正确解决方案作为正样本，而较长的响应则被视为负样本，包括错误的较长响应和正确的较长响应。这些正负样本对构成了用于 DPO 训练的成对偏好数据。

- Long2short RL。在标准的 RL 训练阶段之后，团队人员选择一个在性能和 token 效率之间达到最佳平衡的模型作为基础模型，并进行单独的 long2short RL 训练阶段。在这个第二阶段中，他们还应用了长度惩罚机制，从而显著减少最大 rollout 长度，以进一步惩罚那些超出期望长度但可能正确的响应。


## 实验




## 未来方向



## 主要收获


## 参考资料

---
title: Kimik1.5
created: 2025-01-21
tags:
  - cot
type: 论文
papername: 
conference: ACL/EMNLP/NAACL/EACL/COLING/SIGIR/AAAI/ICML/ICDM
year: 2025
institution:
  - 月之暗面
---

## 论文基本信息

标题：

作者：

链接：

代码：

框架图：


## 背景
**k1.5 多模态思考模型**。新模型在数学、代码、多模态推理能力等方面全面对标 Open AI 满血版 o1，而且是 OpenAI 之外首个多模态 o1。尤其是 kimi-k1.5-short，成为 SOTA short cot 模型，并大幅领先 GPT-4o 和 Claude 3.5 Sonnet（提升幅度高达 550%）

在 long-CoT 模式下，Kimi k1.5 在数学、代码及多模态推理能力上，达到长思考 SOTA 模型 OpenAI o1 正式版的水平。Kimi k1.5 在 AIME 上达到 77.5 分，在 MATH 500 上达到 96.2 分，在 Codeforces 上达到 94 百分位，在 MathVista 上达到 74.9 分。

在 short-CoT 模式下，Kimi k1.5 在数学、代码、视觉多模态和通用能力上，也达到了全球范围内短思考 SOTA 模型 ，并大幅领先 GPT-4o 和 Claude 3.5 Sonnet 的水平。比如，Kimi k1.5 在 AIME 上达到 60.8 分，MATH500 上达到 94.6 分，LiveCodeBench 上达到 47.3 分。

不仅如此，从全球前沿大模型数学竞赛和编程竞赛基准测试来看，Kimi k1.5 的表现也相当不错，处于全球第一梯队，而这两项测试代表了人类智商巅峰。

![](img/Pasted%20image%2020250121135114.png)

![](img/Pasted%20image%2020250121135126.png)

_Short-CoT 是“Zero-shot Chain-of-Thought”的简称，即零样例思维链。它是一种在大语言模型中使用的提示技术，无需提供任何示例，仅通过自然语言指令引导模型进行逐步推理，从而得出答案。_

_例如，在解决数学问题时，模型会先展示出详细的推理步骤，再给出最终答案，而不是直接给出结果。_


_思维链（CoT）就像是给AI出题时，让它不仅要给出答案，还要把思考的过程写出来，就像学生做数学题要写出解题步骤一样。这样可以帮助AI更好地理解和解决问题。_

_而Long-CoT就是在思维链的基础上，让AI写出更长、更详细的思考过程。比如原本只需要写三步思考过程，现在可能要写五步、十步甚至更多，这样能让AI处理更复杂的问题，就像解决一个更难的数学题需要更多的步骤一样。_

## 相关研究




## 核心亮点

随着模型尺寸逐渐增大，预训练阶段参数 scaling up 带来的边际收益开始递减，如果想要深度提升模型推理能力和长程问题能力，基于强化学习的 Post-Training 将会成为下一个突破点，因为 scaling 强化学习为人工智能的持续进步开辟了新的维度，它使得大语言模型能够通过带有奖励的探索学习来扩展其训练数据，从而也实现计算规模的扩展。

大的方向非常明确，然而，此前发表的研究工作尚未产生具有竞争力的结果。

有鉴于此，Kimi 技术团队在 Kimi k1.5 的训练实践中全面探索了 RL 训练技术、多模态数据配方和基础设施优化。

难得的是，他们探索出的 RL 框架简单、有效，无需依赖蒙特卡洛树搜索、价值函数和过程奖励模型等更复杂的技术也能取得优异的性能。

此外，他们还提出了有效的 long2short 技术，利用 Long-CoT 技术来改进 Short-CoT 模型，使得模型在短链思维推理方面取得了最佳成果。


## 方法

Kimi k1.5的开发由几个阶段组成：预训练、普通监督微调（SFT）、长CoT监督微调和强化学习（RL）。

### RL Prompt Set Curation

通过我们的初步实验，我们发现强化学习提示集的质量和多样性对于确保强化学习的有效性起着至关重要的作用。构造良好的提示集不仅可以引导模型进行稳健的推理，还可以降低奖励黑客和过度拟合表面模式的风险。具体来说，三个关键属性定义了高质量的 RL 提示集

- 覆盖面多样化：提示应涵盖广泛的学科，例如 STEM、编码和一般推理，以增强模型的适应性并确保在不同领域的广泛适用性。  
- 难度平衡：提示集应包括一系列分布均匀的简单、中等和困难的问题，以促进逐步学习并防止过度适应特定的复杂程度。  
- 准确的可评估性：提示应允许验证者进行客观可靠的评估，确保模型性能是基于正确的推理而不是肤浅的模式或随机猜测来衡量的。

为了在提示集中实现多样化的覆盖范围，我们采用自动过滤器来选择需要丰富推理且易于评估的问题。我们的数据集包含来自各个领域的问题，例如 STEM 领域、竞赛和一般推理任务，包含纯文本和图像文本问答数据。此外，我们开发了一个标签系统，按领域和学科对提示进行分类，确保不同学科领域的均衡代表性（M. Li 等人，2023 年；W. Liu 等人，2023 年）。

我们采用基于模型的方法，利用模型自身的能力来自适应评估每个提示的难度。具体来说，对于每个提示，SFT 模型都会使用相对较高的采样温度生成十次答案。然后计算通过率并用作提示难度的代理——通过率越低，难度越高。这种方法可以使难度评估与模型的内在功能保持一致，从而使其对于强化学习训练非常有效。通过利用这种方法，我们可以预过滤大多数琐碎的情况，并在强化学习训练期间轻松探索不同的采样策略。

为了避免潜在的奖励黑客攻击（Everitt et al. 2021；Pan et al. 2022），我们需要确保每个提示的推理过程和最终答案都能被准确验证。经验观察表明，一些复杂的推理问题可能有相对简单且容易猜测的答案，从而导致误报验证——模型通过不正确的推理过程得出正确的答案。为了解决这个问题，我们排除了容易出现此类错误的问题，例如多项选择题、对/错题和基于证明的问题。此外，对于一般的问答任务，我们提出了一种简单而有效的方法来识别和删除易于破解的提示。具体来说，我们提示模型在没有任何 CoT 推理步骤的情况下猜测潜在的答案。如果模型在 N 次尝试内预测出正确答案，则该提示被认为太容易破解并被删除。我们发现设置 N = 8 可以删除大多数易于破解的提示。开发更先进的验证模型仍然是未来研究的开放方向。

### Long-CoT Supervised Fine-Tuning

借助精炼的 RL 提示集，我们采用提示工程构建了一个小型但高质量的长 CoT 预热数据集，其中包含经过准确验证的文本和图像输入的推理路径。这种方法类似于拒绝采样 (RS)，但侧重于通过即时工程生成长 CoT 推理路径。由此产生的预热数据集旨在封装对类人推理至关重要的关键认知过程，例如规划，其中模型系统地概述了执行之前的步骤；评估，涉及对中间步骤的严格评估；反思，使模型能够重新考虑和完善其方法；和探索，鼓励考虑替代解决方案。通过在此预热数据集上执行轻量级 SFT，我们有效地启动了模型以内化这些推理策略。因此，经过微调的长 CoT 模型在生成更详细和逻辑连贯的响应方面表现出更高的能力，从而增强了其在不同推理任务中的性能。

### Reinforcement Learning

因此，我们考虑训练模型以通过强化学习 (RL) 生成 CoT (OpenAI 2024)。令 r 为奖励模型，通过分配值 r(x, y, y*) ∈ {0, 1}，根据基本事实 y* 来证明给定问题 x 的建议答案 y 的正确性。对于可验证的问题，奖励直接由预定义的标准或规则确定。例如，在编码问题中，我们评估答案是否通过测试用例。对于自由形式的基本事实问题，我们训练一个奖励模型 r(x, y, y*) 来预测答案是否与基本事实匹配。给定问题 x，模型 πθ 通过采样过程 z ∼ πθ(·|x), y ∼ πθ(·|x, z) 生成 CoT 和最终答案。生成的 CoT 的质量通过是否能够得出正确的最终答案来评估。综上，我们认为优化政策的目标如下

![](img/Pasted%20image%2020250121150657.png)

通过扩大 RL 训练，我们的目标是训练一个模型，该模型能够利用简单的基于提示的 CoT 和规划增强的 CoT 的优势。该模型在推理过程中仍然自动回归采样语言序列，从而避免了部署过程中高级规划算法所需的复杂并行化的需要。然而，与简单的基于提示的方法的一个关键区别在于，模型不应仅仅遵循一系列推理步骤。相反，它还应该通过利用整套探索的想法作为上下文信息来学习关键的规划技能，包括错误识别、回溯和解决方案细化。


Policy Optimization

![](img/Pasted%20image%2020250121154530.png)

我们在我们的培训系统中排除了价值网络，该网络也在之前的研究中得到了利用（Ahmadian 等人，2024）。虽然这种设计选择显着提高了训练效率，但我们还假设经典强化学习中传统使用价值函数进行学分分配可能不适合我们的背景。考虑这样一个场景：模型生成了部分 CoT (z1, z2, ..., zt)，并且有两个潜在的下一步推理步骤：zt+1 和 zt′+1。  假设zt+1直接得出正确答案，而zt′+1包含一些错误。如果预言机值函数可访问，则表明 zt+1 与 zt'+1 相比保留了更高的值。根据标准的信用分配原则，选择zt′+1将会受到惩罚，因为它相对于当前的政策具有负面优势。  然而，探索 zt′+1 对于训练模型生成长 CoT 非常有价值。通过使用长 CoT 得出的最终答案的合理性作为奖励信号，只要成功恢复并达到正确答案，模型就可以从 zt′+1 中学习试错模式。这个例子的关键要点是，我们应该鼓励模型探索不同的推理路径，以增强其解决复杂问题的能力。这种探索性方法产生了丰富的经验，支持关键规划技能的发展。我们的主要目标并不局限于在训练问题上获得高精度，而是专注于为模型配备有效的问题解决策略，最终提高其在测试问题上的性能。

Length Penalty

我们观察到一个过度思考的现象，即在 RL 训练期间模型的响应长度显着增加。虽然这会带来更好的性能，但过长的推理过程在训练和推理过程中成本高昂，而且过度思考往往不是人类所喜欢的。针对这个问题，我们引入了长度奖励来抑制token长度的快速增长，从而提高模型的token效率

![](img/Pasted%20image%2020250121154917.png)

本质上，我们提倡较短的答案并惩罚正确答案中的较长答案，同时明确惩罚错误答案的较长答案。然后，将此基于长度的奖励添加到带有权重参数的原始奖励中。

在我们的初步实验中，长度惩罚可能会减慢初始阶段的训练速度。为了缓解这个问题，我们建议在训练期间逐渐预热长度惩罚。具体来说，我们采用没有长度惩罚的标准策略优化，然后对其余训练进行恒定的长度惩罚。

Sampling Strategies

尽管强化学习算法本身具有相对较好的采样特性（更困难的问题提供更大的梯度），但其训练效率有限。因此，一些明确定义的先前采样方法可以产生潜在的更大的性能增益。我们利用多个信号来进一步改进采样策略。首先，我们收集的强化学习训练数据自然带有不同的难度标签。例如，数学竞赛题比小学数学题更难。其次，由于强化学习训练过程会对同一问题进行多次采样，因此我们还可以跟踪每个单独问题的成功率作为难度指标。我们提出了两种采样方法来利用这些先验来提高训练效率。

课程抽样：我们从较简单的任务开始进行培训，然后逐渐进展到更具挑战性的任务。由于初始强化学习模型的性能有限，在非常困难的问题上花费有限的计算预算通常会产生很少的正确样本，从而导致训练效率较低。同时，我们收集的数据自然包含等级和难度标签，使得基于难度的采样成为提高训练效率的直观有效的方法。  

优先抽样：除了课程抽样之外，我们还使用优先抽样策略来关注模型表现不佳的问题。我们跟踪每个问题 i 的成功率 si ，并对与 1 − si 成比例的问题进行抽样，以便成功率较低的问题获得较高的抽样概率。这将模型的努力引向其最薄弱的领域，从而实现更快的学习和更好的整体性能。

Reward Modeling for Math
- Classic RM：受到 InstructGPT（Ouyang et al. 2022）方法的启发，我们实现了基于价值头的奖励模型，并收集了大约 80 万个数据点进行微调。将“问题”、“参考答案”和“响应”作为输入，并输出一个标量来指示响应是否正确。
- Chain-of-Thought RM：最近的研究（Ankner 等人，2024 年；McAleese 等人，2024 年）表明，通过思想链 (CoT) 推理增强的奖励模型可以显着优于经典方法，特别是在数学等需要细致入微的正确性标准的任务上。因此，我们收集了约 80 万个 CoT 标记示例的同样大的数据集来微调 Kimi 模型。基于与 Classic RM 相同的输入，思想链方法在以 JSON 格式提供最终正确性判断之前显式生成逐步推理过程，从而实现更强大和可解释的奖励信号。

在我们的手动抽查中，Classic RM 的准确度约为 84.4，而 Chain-ofThought RM 的准确度达到 98.5。在RL训练过程中，我们采用了Chain-of-Thought RM来确保更正确的反馈。

Vision Data：为了提高模型的真实世界图像推理能力并实现视觉输入和大语言模型 (LLM) 之间更有效的对齐，我们的视觉强化学习 (Vision RL) 数据主要来自三个不同的类别：真实世界数据、综合视觉推理数据和文本渲染数据。

### Long2short: Context Compression for Short-CoT Models

尽管 long-CoT 模型在性能上表现出色，但与标准的 short-CoT LLM 相比，它在测试时消耗的 token 数量更多。然而，Kimi 技术团队发现将 long-CoT 模型的思维先验迁移到 short-CoT 模型中是可能的，从而在有限的测试 token 预算下提升性能。

他们提出了几种解决这一 long2short 问题的方法，包括模型融合、最短拒绝采样、DPO 以及 long2short RL。以下是这些方法的详细描述：

- 模型融合。团队人员发现模型融合（Model Merging）有助于保持模型的泛化能力。他们还发现，在融合 long-CoT 模型和 short-CoT 模型时，模型融合也能有效提升 token 效率。这种方法通过将 long-CoT 模型与 short-CoT 模型结合，从而在不进行训练的情况下获得一个新模型。具体来说，他们通过简单地平均两个模型的权重来实现融合。

- 最短拒绝采样。研究者观察到，模型在回答相同问题时生成的响应长度存在较大差异。基于此，他们设计了最短拒绝采样（Shortest Rejection Sampling）方法。该方法对同一个问题采样 n 次（实验中，n=8），并选择最短的正确响应进行监督微调。

- DPO。与最短拒绝采样类似，团队人员利用 Long CoT 模型生成多个响应样本。并选择最短的正确解决方案作为正样本，而较长的响应则被视为负样本，包括错误的较长响应和正确的较长响应（比正样本长1.5倍）。这些正负样本对构成了用于 DPO 训练的成对偏好数据。

- Long2short RL。在标准的 RL 训练阶段之后，团队人员选择一个在性能和 token 效率之间达到最佳平衡的模型作为基础模型，并进行单独的 long2short RL 训练阶段。在这个第二阶段中，他们还应用了长度惩罚机制，从而显著减少最大 rollout 长度，以进一步惩罚那些超出期望长度但可能正确的响应。

![](img/Pasted%20image%2020250121161707.png)

### Other Training Details

预训练：Kimi k1.5 基础模型在多样化、高质量的多模态语料库上进行训练。语言数据涵盖英语、汉语、代码、数学推理、知识五个领域。多模态数据，包括字幕、图像文本交错、OCR、知识和 QA 数据集，使我们的模型能够获得视觉语言能力。严格的质量控制确保整个预训练数据集的相关性、多样性和平衡性。我们的预训练分三个阶段进行：（1）视觉语言预训练，建立强大的语言基础，然后逐步进行多模态集成； (2) Cooldown，它使用精选和合成数据来巩固能力，特别是推理和基于知识的任务； (3) 长上下文激活，将序列处理扩展到 131,072 个标记。有关我们预训练工作的更多详细信息，请参阅附录 B。

SFT

我们创建了涵盖多个领域的普通 SFT 语料库。对于非推理任务，包括问答、写作和文本处理，我们首先通过人工注释构建种子数据集。该种子数据集用于训练种子模型。随后，我们收集各种提示并使用种子模型为每个提示生成多个响应。然后，注释者对这些响应进行排名，并优化排名靠前的响应以生成最终版本。对于数学和编码问题等推理任务，基于规则和基于奖励建模的验证比人类判断更准确、更高效，我们利用拒绝采样来扩展 SFT 数据集。

我们的普通 SFT 数据集包含大约 100 万个文本示例。具体来说，500k 个示例用于一般问答，200k 个用于编码，200k 个用于数学和科学，5k 个用于创意写作，20k 个用于长上下文任务，例如摘要、doc-qa、翻译和写作。此外，我们构建了 100 万个文本视觉示例，涵盖各种类别，包括图表解释、OCR、基于图像的对话、视觉编码、视觉推理以及视觉辅助的数学/科学问题。

我们首先以 32k 令牌的序列长度训练模型 1 个epoch，然后以 128k 令牌的序列长度训练另一个epoch。在第一阶段（32k）中，学习率从 2 × 10−5 衰减到 2 × 10−6，然后在第二阶段（128k）中重新预热到 1 × 10−5，最后衰减到 1 × 10 −6。为了提高训练效率，我们将多个训练样本打包到每个训练序列中。

### RL Infrastructure

![](img/Pasted%20image%2020250121161100.png)

如图 3a 所示的强化学习训练系统通过迭代同步方法运行，每次迭代都包含一个推出阶段和一个训练阶段。在推出阶段，推出工作人员在中央主机的协调下，通过与模型交互来生成推出轨迹，生成对各种输入的响应序列。然后，这些轨迹被存储在重播缓冲区中，这通过破坏时间相关性来确保训练数据集的多样性和公正性。在随后的培训阶段，培训人员将获取这些经验来更新模型的权重。这个循环过程使模型能够不断地从其行为中学习，随着时间的推移调整其策略以提高性能。



Kimi 技术团队设计的简单而有效的 RL 框架离不开两个关键要素：**长上下文 scaling 和改进的策略优化**。

先说长上下文 scaling。他们将强化学习的上下文窗口 scale 到 128k，并观察到随着上下文长度的增加，模型性能持续改善。新方法背后的一个关键理念是使用 partial rollout 来提高训练效率 —— 即通过重用大量以前的轨迹来采样新的轨迹，避免从头重新生成新轨迹的成本。技术团队的观察表明，上下文长度是大语言模型强化学习持续 scaling 的一个关键维度。 

再来看策略优化的改进。他们推导出了一个具有 long-CoT 的强化学习公式，并采用在线镜像下降法的变体来实现稳健的策略优化。通过有效的采样策略、长度惩罚和数据配方的优化，他们进一步改进了该算法。

  通过将这两个关键要素结合，Kimi 技术团队建立了一个用于 LLM 学习的简化强化学习框架。由于该框架能够 scale 上下文长度，学习到的 CoT 展现出规划、反思和纠正的特性。增加的上下文长度具有增加搜索步骤数量的效果。因此，他们表明无需依赖蒙特卡洛树搜索、价值函数和过程奖励模型等更复杂的技术也能实现强大的性能。 

此外，他们的模型还在文本和视觉数据上进行了联合训练，具备对这两种模态进行联合推理的能力。 

## 未来方向



## 主要收获


## 参考资料

如何评价 Kimi 发布的多模态推理模型 k1.5？ - Flood Sung的回答 - 知乎
https://www.zhihu.com/question/10114790245/answer/84028353434


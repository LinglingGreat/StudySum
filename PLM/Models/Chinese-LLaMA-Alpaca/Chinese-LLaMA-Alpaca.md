---
title: Chinese-LLaMA-Alpaca
created: 2023-06-08
tags: å¢é‡é¢„è®­ç»ƒ, LLM, SFT

---

https://github.com/ymcui/Chinese-LLaMA-Alpaca

## åŸºæœ¬æƒ…å†µ

ä¸ºäº†ä¿ƒè¿›å¤§æ¨¡å‹åœ¨ä¸­æ–‡NLPç¤¾åŒºçš„å¼€æ”¾ç ”ç©¶ï¼Œæœ¬é¡¹ç›®å¼€æºäº†**ä¸­æ–‡LLaMAæ¨¡å‹å’ŒæŒ‡ä»¤ç²¾è°ƒçš„Alpacaå¤§æ¨¡å‹**ã€‚è¿™äº›æ¨¡å‹**åœ¨åŸç‰ˆLLaMAçš„åŸºç¡€ä¸Šæ‰©å……äº†ä¸­æ–‡è¯è¡¨**å¹¶ä½¿ç”¨äº†ä¸­æ–‡æ•°æ®è¿›è¡ŒäºŒæ¬¡é¢„è®­ç»ƒï¼Œè¿›ä¸€æ­¥æå‡äº†ä¸­æ–‡åŸºç¡€è¯­ä¹‰ç†è§£èƒ½åŠ›ã€‚åŒæ—¶ï¼Œä¸­æ–‡Alpacaæ¨¡å‹è¿›ä¸€æ­¥ä½¿ç”¨äº†ä¸­æ–‡æŒ‡ä»¤æ•°æ®è¿›è¡Œç²¾è°ƒï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹æŒ‡ä»¤çš„ç†è§£å’Œæ‰§è¡Œèƒ½åŠ›ã€‚è¯¦ç»†å†…å®¹è¯·å‚è€ƒæŠ€æœ¯æŠ¥å‘Š[(Cui, Yang, and Yao, 2023)](https://arxiv.org/abs/2304.08177)ã€‚

**æœ¬é¡¹ç›®ä¸»è¦å†…å®¹ï¼š**

- ğŸš€Â é’ˆå¯¹åŸç‰ˆLLaMAæ¨¡å‹æ‰©å……äº†ä¸­æ–‡è¯è¡¨ï¼Œæå‡äº†ä¸­æ–‡ç¼–è§£ç æ•ˆç‡
- ğŸš€Â å¼€æºäº†ä½¿ç”¨ä¸­æ–‡æ–‡æœ¬æ•°æ®é¢„è®­ç»ƒçš„ä¸­æ–‡LLaMAä»¥åŠç»è¿‡æŒ‡ä»¤ç²¾è°ƒçš„ä¸­æ–‡Alpaca
- ğŸš€Â å¼€æºäº†é¢„è®­ç»ƒè„šæœ¬ã€æŒ‡ä»¤ç²¾è°ƒè„šæœ¬ï¼Œç”¨æˆ·å¯æ ¹æ®éœ€è¦è‡ªè¡Œè¿›ä¸€æ­¥è®­ç»ƒ
- ğŸš€Â å¿«é€Ÿä½¿ç”¨ç¬”è®°æœ¬ç”µè„‘ï¼ˆä¸ªäººPCï¼‰çš„CPU/GPUæœ¬åœ°é‡åŒ–å’Œéƒ¨ç½²ä½“éªŒå¤§æ¨¡å‹
- ğŸš€Â æ”¯æŒ[ğŸ¤—transformers](https://github.com/huggingface/transformers),Â [llama.cpp](https://github.com/ggerganov/llama.cpp),Â [text-generation-webui](https://github.com/oobabooga/text-generation-webui),Â [LlamaChat](https://github.com/alexrozanski/LlamaChat),Â [LangChain](https://github.com/hwchase17/langchain),Â [privateGPT](https://github.com/imartinez/privateGPT)ç­‰ç”Ÿæ€
- ç›®å‰å·²å¼€æºçš„æ¨¡å‹ç‰ˆæœ¬ï¼š7Bï¼ˆæ ‡å‡†ç‰ˆã€**Plusç‰ˆ**ï¼‰ã€13Bï¼ˆæ ‡å‡†ç‰ˆã€**Plusç‰ˆ**ï¼‰

|å¯¹æ¯”é¡¹|ä¸­æ–‡LLaMA|ä¸­æ–‡Alpaca|
|:--|---|---|
|è®­ç»ƒæ–¹å¼|ä¼ ç»ŸCLM|æŒ‡ä»¤ç²¾è°ƒ|
|è®­ç»ƒè¯­æ–™|æ— æ ‡æ³¨é€šç”¨è¯­æ–™|æœ‰æ ‡æ³¨æŒ‡ä»¤æ•°æ®|
|è¯è¡¨å¤§å°[3]|4995**3**|4995**4**=49953+1ï¼ˆpad tokenï¼‰|
|è¾“å…¥æ¨¡æ¿|ä¸éœ€è¦|éœ€è¦ç¬¦åˆæ¨¡æ¿è¦æ±‚[1]|
|é€‚ç”¨åœºæ™¯Â âœ”ï¸|æ–‡æœ¬ç»­å†™ï¼šç»™å®šä¸Šæ–‡å†…å®¹ï¼Œè®©æ¨¡å‹ç»§ç»­å†™ä¸‹å»ï¼Œç”Ÿæˆä¸‹æ–‡|1ã€æŒ‡ä»¤ç†è§£ï¼ˆé—®ç­”ã€å†™ä½œã€å»ºè®®ç­‰ï¼‰  <br>2ã€å¤šè½®ä¸Šä¸‹æ–‡ç†è§£ï¼ˆèŠå¤©ç­‰ï¼‰|
|ä¸é€‚ç”¨åœºæ™¯Â âŒ|æŒ‡ä»¤ç†è§£ ã€å¤šè½®èŠå¤©ç­‰|æ–‡æœ¬æ— é™åˆ¶è‡ªç”±ç”Ÿæˆ|
|llama.cpp|ä½¿ç”¨`-p`å‚æ•°æŒ‡å®šä¸Šæ–‡|ä½¿ç”¨`-ins`å‚æ•°å¯åŠ¨æŒ‡ä»¤ç†è§£+èŠå¤©æ¨¡å¼|
|text-generation-webui|ä¸é€‚åˆchatæ¨¡å¼|ä½¿ç”¨`--cpu`å¯åœ¨æ— æ˜¾å¡å½¢å¼ä¸‹è¿è¡Œï¼Œè‹¥ç”Ÿæˆå†…å®¹ä¸æ»¡æ„ï¼Œå»ºè®®ä¿®æ”¹prompt|
|LlamaChat|åŠ è½½æ¨¡å‹æ—¶é€‰æ‹©"LLaMA"|åŠ è½½æ¨¡å‹æ—¶é€‰æ‹©"Alpaca"|
|[HFæ¨ç†ä»£ç ](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/inference_hf.py)|æ— éœ€æ·»åŠ é¢å¤–å¯åŠ¨å‚æ•°|å¯åŠ¨æ—¶æ·»åŠ å‚æ•°Â `--with_prompt`|
|[web-demoä»£ç ](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/gradio_demo.py)|ä¸é€‚ç”¨|ç›´æ¥æä¾›Alpacaæ¨¡å‹ä½ç½®å³å¯ï¼›æ”¯æŒå¤šè½®å¯¹è¯|
|[LangChainç¤ºä¾‹](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/langchain_demo)Â / privateGPT|ä¸é€‚ç”¨|ç›´æ¥æä¾›Alpacaæ¨¡å‹ä½ç½®å³å¯|
|å·²çŸ¥é—®é¢˜|å¦‚æœä¸æ§åˆ¶ç»ˆæ­¢ï¼Œåˆ™ä¼šä¸€ç›´å†™ä¸‹å»ï¼Œç›´åˆ°è¾¾åˆ°è¾“å‡ºé•¿åº¦ä¸Šé™ã€‚[2]|ç›®å‰ç‰ˆæœ¬æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬é•¿åº¦ç›¸å¯¹çŸ­ä¸€äº›ï¼Œæ¯”è¾ƒæƒœå­—å¦‚é‡‘ã€‚å¯åœ¨æŒ‡ä»¤ä¸­è¦æ±‚è¯¦ç»†å›ç­”ã€‚[2]|

## å¢é‡é¢„è®­ç»ƒ

è¯è¡¨æ‰©å……



æ•°æ®



è„šæœ¬

```bash
########å‚æ•°è®¾ç½®########
lr=2e-4
lora_rank=8
lora_alpha=32
lora_trainable="q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj"
modules_to_save="embed_tokens,lm_head"
lora_dropout=0.05

pretrained_model=path/to/hf/llama/dir
chinese_tokenizer_path=path/to/chinese/llama/tokenizer/dir
dataset_dir=path/to/pt/data/dir
data_cache=temp_data_cache_dir
per_device_train_batch_size=1
per_device_eval_batch_size=1
training_steps=100
gradient_accumulation_steps=1
output_dir=output_dir

deepspeed_config_file=ds_zero2_no_offload.json

########å¯åŠ¨å‘½ä»¤########
torchrun --nnodes 1 --nproc_per_node 1 run_clm_pt_with_peft.py \
    --deepspeed ${deepspeed_config_file} \
    --model_name_or_path ${pretrained_model} \
    --tokenizer_name_or_path ${chinese_tokenizer_path} \
    --dataset_dir ${dataset_dir} \
    --data_cache_dir ${data_cache} \
    --validation_split_percentage 0.001 \
    --per_device_train_batch_size ${per_device_train_batch_size} \
    --per_device_eval_batch_size ${per_device_eval_batch_size} \
    --do_train \
    --seed $RANDOM \
    --fp16 \
    --max_steps ${training_steps} \
    --lr_scheduler_type cosine \
    --learning_rate ${lr} \
    --warmup_ratio 0.05 \
    --weight_decay 0.01 \
    --logging_strategy steps \
    --logging_steps 10 \
    --save_strategy steps \
    --save_total_limit 3 \
    --save_steps 500 \
    --gradient_accumulation_steps ${gradient_accumulation_steps} \
    --preprocessing_num_workers 8 \
    --block_size 512 \
    --output_dir ${output_dir} \
    --overwrite_output_dir \
    --ddp_timeout 30000 \
    --logging_first_step True \
    --lora_rank ${lora_rank} \
    --lora_alpha ${lora_alpha} \
    --trainable ${lora_trainable} \
    --modules_to_save ${modules_to_save} \
    --lora_dropout ${lora_dropout} \
    --torch_dtype float16 \
    --gradient_checkpointing \
    --ddp_find_unused_parameters False
```


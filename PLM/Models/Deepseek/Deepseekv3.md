---
title: Deepseekv3
created: 2025-01-18
tags:
  - 大模型
  - 专家模型
type: 论文
papername: 
conference: ACL/EMNLP/NAACL/EACL/COLING/SIGIR/AAAI/ICML/ICDM
year: 2024
institution:
  - DeepSeek
---

## 论文基本信息

标题：

作者：

链接：

代码：https://github.com/deepseek-ai/DeepSeek-V3

框架图：


## 背景

我们提出了 DeepSeek-V3，这是一个强大的专家混合 (MoE) 语言模型，总参数为 671B，每个令牌激活 37B。为了实现高效的推理和经济高效的训练，DeepSeek-V3 采用了多头潜在注意力（MLA）和 DeepSeekMoE 架构，这些架构在 DeepSeek-V2 中得到了彻底的验证。此外，DeepSeek-V3首创了负载均衡的辅助无损策略，并设置了多令牌预测训练目标以获得更强的性能。

我们在 14.8 万亿个多样化的高质量代币上对 DeepSeek-V3 进行预训练，然后进行监督微调和强化学习阶段，以充分利用其能力。综合评估表明，DeepSeek-V3 的性能优于其他开源模型，并且达到了与领先的闭源模型相当的性能。尽管性能出色，DeepSeek-V3 仅需要 2.788M H800 GPU 小时即可完成完整训练。此外，它的训练过程非常稳定。在整个训练过程中，我们没有遇到任何不可恢复的损失峰值或执行任何回滚。

![](img/Pasted%20image%2020250118154811.png)

![](img/Pasted%20image%2020250118160017.png)
## Architecture
![](img/Pasted%20image%2020250118160255.png)

### Multi-Head Latent Attention

看deepseekv2报告
### DeepSeekMoE with Auxiliary-Loss-Free Load Balancing

与 GShard (Lepikhin et al., 2021) 等传统 MoE 架构相比，DeepSeekMoE 使用更细粒度的专家，并将一些专家隔离为共享专家。

![](img/Pasted%20image%2020250118162053.png)

![](img/Pasted%20image%2020250118162130.png)

对于 MoE 模型，不平衡的专家负载将导致路由崩溃（Shazeer et al., 2017），并降低专家并行场景中的计算效率。传统的解决方案通常依靠辅助损耗（Fedus et al., 2021; Lepikhin et al., 2021）来避免负载不平衡。然而，太大的辅助损失会损害模型性能（Wang et al., 2024a）。为了在负载平衡和模型性能之间实现更好的权衡，我们首创了一种辅助无损失负载平衡策略（Wang et al., 2024a）来确保负载平衡。具体来说，我们为每个专家引入一个偏差项 bi 并将其添加到相应的亲和力分数 si,t 中以确定 top-K 路由：

![](img/Pasted%20image%2020250118162245.png)

请注意，偏置项仅用于路由。将与 FFN 输出相乘的门控值仍然源自原始亲和力分数 si,t。在训练过程中，我们不断监控每个训练步骤的整批专家负载。在每个步骤结束时，如果相应的专家过载，我们将把偏差项减少 γ，如果相应的专家负载不足，我们将增加 γ，其中 γ 是一个称为偏差更新速度的超参数。通过动态调整，DeepSeek-V3在训练过程中保持专家负载平衡，并且比通过纯辅助损失鼓励负载平衡的模型获得更好的性能。

虽然DeepSeek-V3主要依靠辅助无损失策略进行负载平衡，但为了防止任何单个序列内的极端不平衡，我们还采用了互补的序列平衡损失

![](img/Pasted%20image%2020250118162306.png)

其中平衡因子α是一个超参数，DeepSeek-V3将为其分配一个极小的值； 1(·)表示指示函数； T 表示序列中标记的数量。序列平衡损失鼓励每个序列上的专家负载达到平衡。

与 DeepSeek-V2 使用的设备限制路由一样，DeepSeek-V3 也使用限制路由机制来限制训练期间的通信成本。简而言之，我们确保每个代币将被发送到最多 M 个节点，这些节点是根据分布在每个节点上的专家的最高 Kr/M 亲和力分数之和来选择的。在此约束下，我们的 MoE 训练框架几乎可以实现完全的计算-通信重叠

由于有效的负载平衡策略，DeepSeek-V3在整个训练过程中保持了良好的负载平衡。因此，DeepSeek-V3 在训练期间不会丢弃任何标记。此外，我们还实施了特定的部署策略来保证推理负载平衡，因此DeepSeek-V3在推理过程中也不会丢失令牌。

### Multi-Token Prediction

受到 Gloeckle 等人的启发。 (2024)，我们研究并为 DeepSeek-V3 设置了多令牌预测 (MTP) 目标，它将预测范围扩展到每个位置的多个未来令牌。一方面，MTP 目标使训练信号更加密集，并且可以提高数据效率。另一方面，MTP 可以使模型能够预先规划其表示，以便更好地预测未来的代币。图 3 说明了我们的 MTP 实施。与 Gloeckle 等人不同。 （2024），它使用独立的输出头并行预测 D 个附加标记，我们顺序预测附加标记并在每个预测深度保持完整的因果链。

![](img/Pasted%20image%2020250118160359.png)




## Infrastructures



## Pre-Training
与DeepSeek-V2相比，我们通过提高数学和编程样本的比例来优化预训练语料，同时扩大英语和中文以外的多语言覆盖范围。此外，我们的数据处理管道经过改进，可以最大限度地减少冗余，同时保持语料库的多样性。受到丁等人的启发，我们实现了数据完整性的文档打包方法，但在训练过程中没有合并跨样本注意屏蔽。最后，DeepSeek-V3 的训练语料库由我们的 tokenizer 中的 14.8T 高质量且多样化的 token 组成。

在 DeepSeekCoder-V2 (DeepSeek-AI, 2024a) 的训练过程中，我们观察到 Fill-in-Middle (FIM) 策略不会损害下一个标记的预测能力，同时使模型能够根据上下文准确预测中间文本提示。与 DeepSeekCoder-V2 一致，我们还将 FIM 策略纳入 DeepSeek-V3 的预训练中。具体来说，我们采用 Prefix-Suffix-Middle (PSM) 框架来构造数据，如下所示

<|fim_begin|> fpre<|fim_hole|> fsuf<|fim_end|> fmiddle<|eos_token|>.

该结构作为预打包过程的一部分应用于文档级别。 FIM策略的应用率为0.1，与PSM框架一致。

DeepSeek-V3 的分词器采用字节级 BPE（Shibata 等人，1999），具有 128K 分词的扩展词汇表。我们的分词器的预分词器和训练数据经过修改，以优化多语言压缩效率。此外，与 DeepSeek-V2 相比，新的预分词器引入了结合标点符号和换行符的标记。然而，当模型处理没有终端换行符的多行提示时，这个技巧可能会引入令牌边界偏差（Lundberg，2023），特别是对于几次评估提示。为了解决这个问题，我们在训练期间随机分割一定比例的此类组合令牌，这使模型暴露于更广泛的特殊情况并减轻了这种偏差。

超参数

![](img/Pasted%20image%2020250118160736.png)

![](img/Pasted%20image%2020250118160848.png)

Long-context

![](img/Pasted%20image%2020250118160917.png)



## Post-Training

### SFT

我们精心策划指令调整数据集，以包含跨越多个域的 **150 万**个实例，每个域都采用根据其特定要求定制的不同数据创建方法。

对于与**推理相关**的数据集，包括那些专注于数学、代码竞争问题和逻辑难题的数据集，我们通过利用内部 DeepSeek-R1 模型生成数据。具体来说，虽然 R1 生成的数据表现出很强的准确性，但它存在思考过度、格式不良和长度过长等问题。我们的目标是平衡 R1 生成的推理数据的高精度和规则格式的推理数据的清晰和简洁。

为了建立我们的方法，我们首先使用监督微调 (SFT) 和强化学习 (RL) 的组合训练管道，开发**针对特定领域（例如代码、数学或一般推理）量身定制的专家模型**。该专家模型充当最终模型的数据生成器。训练过程涉及为每个实例生成两种不同类型的 SFT 样本：
- 将问题与其原始响应以 <问题，原始响应> 的格式结合起来
- 将system prompt与问题和 R1 响应结合在一起<系统提示、问题、R1响应>的格式。

system prompt经过精心设计，包括指导模型生成富含反思和验证机制的响应的指令。在 RL 阶段，即使没有明确的system prompt，该模型也会利用high-temperature sampling来生成回答，该回答集成了 R1 生成的数据和原始数据的模式。经过数百个 RL 步骤后，中间 RL 模型学会合并 R1 模式，从而战略性地提高整体性能。

完成 RL 训练阶段后，我们实施**拒绝采样**，为最终模型提供高质量的 SFT 数据，其中专家模型用作数据生成源。这种方法确保最终的训练数据保留 DeepSeek-R1 的优势，同时产生简洁有效的响应。

对于**非推理数据**，例如创意写作、角色扮演和简单问答，我们利用 DeepSeek-V2.5 生成响应并聘请人工注释者来验证数据的准确性和正确性。

我们使用 SFT 数据集对 DeepSeek-V3-Base 进行两个epoch的微调，使用从 5 × 10−6 开始并逐渐减小到 1 × 10−6 的余弦衰减学习率调度。在训练期间，每个序列都由多个样本打包而成。然而，我们采用样本屏蔽策略来确保这些样本保持孤立且相互不可见。

### RL

我们在 RL 流程中采用**基于规则的奖励模型 (RM) 和基于模型的 RM**。

对于可以使用特定规则验证的问题，我们采用基于规则的奖励系统来确定反馈。例如，某些数学问题具有确定性结果，我们要求模型以指定格式（例如，在框中）提供最终答案，从而允许我们应用规则来验证正确性。类似地，对于 LeetCode 问题，我们可以利用编译器根据测试用例生成反馈。通过尽可能利用基于规则的验证，我们确保了更高水平的可靠性，因为这种方法可以抵抗操纵或利用。

对于具有自由形式的真实答案的问题，我们依靠奖励模型来确定响应是否与预期的真实答案相匹配。相反，对于没有明确事实真相的问题，例如涉及创意写作的问题，奖励模型的任务是根据问题和相应的答案作为输入来提供反馈。奖励模型是从 DeepSeek-V3 SFT 检查点训练的。为了提高其可靠性，我们构建的偏好数据不仅提供最终奖励，还包括导致奖励的推理链。这种方法有助于降低特定任务中奖励黑客的风险。

GRPO

GRPO是PPO的简化版本，最核心的改动是砍掉Value Model，依靠多次采样的Reward，得出baseline分数来计算advantage。GRPO详细算法可见DeepSeek-V2报告。

![](img/Pasted%20image%2020250118161421.png)


在强化学习过程中，我们结合了来自不同领域的提示，例如编码、数学、写作、角色扮演和问题回答。这种方法不仅使模型更符合人类偏好，而且还提高了基准测试的性能，特别是在可用 SFT 数据有限的情况下。

## 讨论

Distillation from DeepSeek-R1

![](img/Pasted%20image%2020250119144030.png)

我们在 DeepSeek-V2.5 的基础上针对DeepSeek-R1 蒸馏的贡献做了消融实验。baseline是在短 CoT 数据上进行训练的，而其竞争对手则使用上述专家检查点生成的数据。  表 9 展示了蒸馏数据的有效性，显示了 LiveCodeBench 和 MATH-500 基准测试的显着改进。我们的实验揭示了一个有趣的权衡：**蒸馏可以带来更好的性能，但也大大增加了平均响应长度**。为了保持模型精度和计算效率之间的平衡，我们在蒸馏中仔细选择了 DeepSeek-V3 的最佳设置。  我们的研究表明，推理模型的知识蒸馏为训练后优化提供了一个有前途的方向。虽然我们当前的工作重点是从数学和编码领域提取数据，但这种方法显示了跨各种任务领域更广泛应用的潜力。这些特定领域所证明的有效性表明，长 CoT 蒸馏对于增强其他需要复杂推理的认知任务中的模型性能可能很有价值。跨不同领域进一步探索这种方法仍然是未来研究的重要方向。

Self-Rewarding

奖励在强化学习中发挥着关键作用，指导着优化过程。在通过外部工具进行验证很简单的领域（例如某些编码或数学场景）中，强化学习表现出了卓越的功效。然而，在更一般的情况下，通过硬编码构建反馈的机制是不切实际的。在 DeepSeek-V3 的开发过程中，针对这些更广泛的背景，我们采用了constitutional AI（Bai et al., 2022），**利用 DeepSeek-V3 本身的投票评估结果作为反馈源。该方法产生了显着的对齐效果，显着增强了DeepSeek-V3在主观评价中的性能。** 通过集成额外的宪法输入，DeepSeek-V3 可以朝着宪法方向进行优化。我们认为，这种将补充信息与法学硕士结合起来作为反馈源的范式至关重要。法学硕士作为一个多功能的处理器，能够将来自不同场景的非结构化信息转化为奖励，最终促进法学硕士的自我提高。除了自我奖励之外，我们还致力于发现其他通用且可扩展的奖励方法，以持续提升模型在通用场景下的能力。

Multi-Token Prediction Evaluation

DeepSeek-V3 不是仅预测下一个单个标记，而是通过 MTP 技术预测接下来的 2 个标记。结合推测解码的框架（Leviathan et al., 2023; Xia et al., 2023），可以显着加快模型的解码速度。一个自然的问题是关于额外预测的令牌的接受率。根据我们的评估，在各个代主题中，第二个代币预测的接受率在 85% 到 90% 之间，表现出一致的可靠性。这种高接受率使 DeepSeek-V3 能够显着提高解码速度，提供 1.8 倍的 TPS（每秒令牌数）。

## 主要收获


## 参考资料

[谈谈DeepSeek-v3在算力约束下的出色工作](https://mp.weixin.qq.com/s/NOagGtvnwNUJZqjBpZw9mw)

[【LLM技术报告】DeepSeek-V3技术报告（全文）](https://zhuanlan.zhihu.com/p/14890557782)


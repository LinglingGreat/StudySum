---
title: LLaMA3
created: 2024-07-24
tags:
  - 大模型
type: 论文
papername: 
conference: ACL/EMNLP/NAACL/EACL/COLING/SIGIR/AAAI/ICML/ICDM
year: 2024
institution:
  - MetaAI
---

## 论文基本信息

标题：

作者：

链接：

代码：

框架图：

![](img/Pasted%20image%2020240724144456.png)

![](img/Pasted%20image%2020240724144934.png)

LLaMA-3特点
- supports multilinguality, coding, reasoning, and tool usage
- 最大的模型是405B的dense模型，支持128K

训练高质量基座模型的三个关键点：data, scale, and managing complexity
- pre-training and post-training阶段的数据质量和数量都提升了。Llama3用了15T tokens训练，而Llama2用了1.8T tokens
- 训练了一个405B的dense模型，用了15.6T tokens。we also train our smaller models for much longer than is compute-optimal. 结果是更好
- 选择dense架构而不是MOE，训练更稳定；采用SFT，RS(拒绝采样), DPO而不是难以训练和扩展的PPO

多模态部分
- 多模态预训练包括用于图像和语音的单独编码器 
- 图像编码器在图像文本对上预训练，而语音编码器以自监督方式进行预训练，通过离散令牌表示重建屏蔽输入 
- 两个预训练编码器（图像和语音）分别通过视觉和语音适配器连接到预训练的 LM。

MOE结构会让模型效果更好吗？答案是否定的。这个在很久以前ChatGPT火之前就有研究结论，从对模型效果的影响来说，MOE结构相对Dense模型本身并不会带来额外优势，甚至是有劣势的。MOE的主要优势是减少训练和推理成本，付出的代价是训练不够稳定以及推理时额外付出大内存来存储膨胀的参数量。但当用户量大请求多的时候，推理成本占比会更高，此时使用MOE对于推理会更友好，这是为何当模型大到一定程度模型结构就会从Dense转向MOE的主要原因，是出于成本、效率而非效果角度考虑。我之前看到有些介绍说MOE结构效果更好，这种观点是没有事实依据的。

Llama3 405B 之所以没有采用MOE，技术报告指出主要是考虑到Dense模型训练更稳定，所以选择了Dense结构。相比GPT 4的1.8T的MOE模型结构，405B的Dense模型效果与之相当甚至要更好一些（当然，不排除GTP 4目前已经是一个蒸馏小模型的可能）。

## 预训练

- 405B 参数
- 15.6T 标记
- 知识截止时间为 2023 年底

- 8K 标记的上下文窗口
- 在持续预训练阶段，上下文窗口增加到 128K

数据清洗
- PII（个人隐私数据）清洗
- 自定义 HTML 解析器，从web数据中解析多样性的数据。保持数学和代码内容的结构。我们发现与纯文本相比，Markdown 对主要在 Web 数据上训练的模型的性能有害，因此我们删除了所有 Markdown 标记。
- 去重：在 URL 级别、文档级别和行级别执行重复数据删除。我们删除每个 30M 文档桶中出现超过 6 次的行。尽管我们的手动定性分析表明，行级重复数据删除不仅可以删除各种网站中残留的样板文件（例如导航菜单、cookie 警告），还会删除频繁的高质量文本，但我们的实证评估显示出显着的改进。
- 去重：n-gram 覆盖率，用于删除包含重复内容的行
- 去黄：脏词计数，用于删除成人内容
- 质量过滤：标记分布 KL 散度，用于过滤与训练语料库分布相比具有过多异常标记的文档
- 质量过滤：基于模型的质量分类器，包括fasttext, roberta
- 语言过滤：基于 fasttext 的模型，将文档分为 176 种语言。

数据比例
- 在模型做细粒度的打标签工作，然后根据标签采样，配不同的量去试。知识分类器做分类
- 在不同的小模型上做不同的配比实验，预测大模型的最优配比
- 最终确定50% 的token是通用数据，25% 的数学和推理，17% 的代码，以及 8% 的多语言。

数据退火
- 作者发现在大模型训练的最后阶段，用高质量的数据学习能提高性能。于是在最后40B数据上，作者逐渐将学习率衰减到0。并且发现，数据退火方法，可以用来筛数据，量少，效果明显，实验更高效。30%的新数据。


三阶段训练法：(1) 初始训练 initial pre-training, (2) 长文训练 long-context pre-training, and (3) 退火训练 annealing

初始训练：

- 余弦调度 8 × 10−5 , 8,000 steps热身, 然后在1,200,000步上降到 8 × 10−7
    
- 上下文长度和BS缓慢增加，配4M的bs用4,096长度, 训练 252M 个 token 后，再配8M的bs扩展序列长度为 8,192，这个阶段大约是 252M tokens。训练 2.87T 个 token 后，最终16M的BS
- 发现这种训练方法非常稳定：我们观察到很少有损失峰值，并且不需要干预来纠正模型训练偏差。
- 调整数据混合。我们在训练期间对预训练数据组合进行了一些调整，以提高模型在特定下游任务上的性能。特别是，我们在预训练期间增加了非英语数据的百分比，以提高 Llama 3 的多语言性能。我们还对数学数据进行上采样以提高模型的数学推理性能，我们在预训练的后期添加了更多最新的网络数据- 训练以推进模型的知识截止，并且我们对预训练数据的子集进行了下采样，这些数据后来被确定为质量较低。
    

长上下文训练

- 仅当模型在短上下文评估上的性能完全恢复，并且模型可以完美解决该长度内的“大海捞针”任务时，上下文长度才会增加。
- 上下文长度共分为六个阶段，逐步增加，从l 8K 逐步增加到 128K。
- 使用 800B 个训练 token 完成长上下文预训练。

退火训练

- 最后40M token，用128K长度，逐渐线性缩减学习率到0
- 在这一退火阶段，调整了数据混合配比，以增加高质量数据比如数学、代码、逻辑内容的影响。最后，将若干退火期间模型Check Point的平均值，作为最终的预训练模型。

## 模型结构

![](img/Pasted%20image%2020240725172304.png)

- 模型架构与 llama 和 llama-2 相同，但有一些修改（再次证明质量数据仍然是王道！）

- 具有 8 KV 头的 GQA

- RoPE 频率增加到 500,000

- 注意力掩码可防止同一序列内不同文档之间的自我注意力。样本间穿越在预训练阶段影响不大，以前大家也不在乎，但作者说在扩长序列时候影响很大。

- 128K 词汇大小（100K 来自 tiktoken，提高了英语的压缩率。28K 额外标记用于英语以外的语言，可以提高压缩率和下游性能，并且对英语分词没有影响。）

- 126 层（层数多2层，是训练阶段方便流水线并行切分的技巧）、128 个注意力头和 16,384 嵌入大小

## scalinglaw

作者说了现有的scalinglaw通常只预测loss，而不是特定的benchmark上的表现；(2)scalinglaw可能因基于小资源进行的预训练运行而变得没那么可靠。

对此，作者搞了一个两步走方法。
1.先建立计算最优模型在下游任务上的负对数似然与训练FLOPs之间的相关性。
2.利用scalinglaw模型和使用更高计算FLOPs训练的旧模型，将下游任务上的负对数似然与benchmark的准确率指标关联上。作者在这步用了LLama2系列的模型

作者在ARC上使用这个方法，能看出来拟合的还不错

![](img/Pasted%20image%2020240725172825.png)

![](img/Pasted%20image%2020240725172842.png)

## Infra

- Llama 3 405B 在多达 16K H100 GPU 上进行训练，每个 GPU 以 700W TDP 运行，配备 80GB HBM3，使用 Meta 的 Grand Teton AI 服务器平台

- 专用集群，不是之前的Meta’s AI Research SuperCluster，全新的Meta’s production clusters。Tectonic（Meta 的内部）分布式文件系统用于存储，240 PB  SSD，7500 台机器，2TB-7TB/s 吞吐，如此高吞吐的存储集群是为了最小化ckpt的IO耗时。

- 基于 RoCE 的 AI 集群由 24K GPU 组成，通过三层网络连接, RoCE，单口400Gb/s
- 最底层1个ToR下2机器配16卡
- 1 Pod 配192 ToR
- 3072 张GPU 1:1 收敛比
- 往上8 个Pod 1:7 的收敛比
- 由于跨pod 带宽降低，所以模型并行编排和资源调度均需考虑网络架构

- 并行性和针对硬件拓扑优化的调度程序

- 增强的 ECMP 路由和深度缓冲区交换机用于拥塞控制

负载均衡
- 两个GPU 间使用16个流，而不是1个，来降低单流流量，以更好地负载均衡
- 在网络包的头部增加了特殊区域，通过hash 来使得流的选路更加均衡
拥塞控制
- 在spine 上使用deep-buffer 交换机以适应集合通信导致的短时拥塞，并能够降低慢节点引发持久的拥塞和反压

- 4D 并行性：四种不同类型的并行性方法的组合，包括张量并行性、管道并行性、上下文并行性和数据并行性，用于对模型进行分片

- 在上下文并行性中，分区跨越序列维度。基于 all-gather，它们会收集所有键和值，然后计算本地查询张量块的注意力输出

- 优化并行顺序，以获得更好的网络带宽和延迟：TP、CP、PP、DP
- 使用了FSDP，但是model weight 只拉取一次，以减少反向梯度计算时的weight allgather 通信
PP并行策略改进
- Batch Size 限制：当前的流水线并行策略 会限制 micro batch 个数为流水线stage 的整数倍，会导致global batch size 和 流水线 stage 相互制约
- 显存不均衡：第1个stage 会多出很多显存占用，之后逐stage 降低
- 计算不均衡：最后一层会额外计算 output layer 和 loss，增加了计算量和延时，首尾stage 做了padding
CP并行策略的改进
- 和Ring CP 一样的在序列维度切分，切分为2\*CP 份以负载均衡，但没有用环状通信来overlap 计算和通信，而是基于allgather 的通信。
网络架构感知的并行策略
- [TP, CP, PP, DP] 的并行策略是针对网络通信优化做专门设计的
- 开发了显存预估和性能探查工具，用以平衡显存开销和通信性能
数值稳定性
- BF16 MFU 为 38%-43%
- 使用 FP32 进行梯度累积。对于在多个地方使用的即时张量，如视觉编码器输出，梯度在 FP32 中累积
集合通信
- 基于NCCL 开发了 NCCLX，在高延迟网络下显著提升性能
- [TP, CP, PP, DP]  并行策略可能导致PP 和DP 通信跨Pod：原 allgather 和 reducescatter 实现依赖数据chunk 和拷贝，需要大量的小的控制信息的通信，进行额外的拷贝操作，且占用GPU 资源来做通信。对此，llama团队 优化chunk 策略，提升小的控制包的通信优先级。
可用性
- 54 天快照预训练，中断 467 次。GPU 问题占总问题的 58%（这就是 TPU 更胜一筹的原因）,剩余是网络问题。
- 白天，由于温度较高，GPU 的吞吐量会变化 1-2%


## Post-training

![](img/Pasted%20image%2020240725173402.png)

首先用人工标注数据训练RM模型，用来评价一个<Prompt,answer>数据的质量，然后用RM参与拒绝采样（Rejection Sampling），就是说对于一个人工Prompt，用模型生成若干个回答，RM给予质量打分，选择得分最高的保留作为SFT数据，其它抛掉。这样得到的SFT数据再加上专门增强代码、数学、逻辑能力的SFT数据一起，用来调整模型得到SFT模型。之后用人工标注数据来使用DPO模型调整LLM参数，DPO本质上是个二分类，就是从人工标注的<Prompt，Good Answer，Bad Answer>三元数据里学习，调整模型参数鼓励模型输出Good Answer，不输出Bad Answer。这算完成了一个迭代轮次的Post-Training。

上述过程会反复迭代几次，每次的流程相同，不同的地方在于拒绝采样阶段用来对给定Prompt产生回答的LLM模型，会从上一轮流程最后产生的若干不同DPO模型（不同超参等）里选择最好的那个在下一轮拒绝采样阶段给Prompt生成答案。很明显，随着迭代的增加DPO模型越来越好，所以拒绝采样里能选出的最佳答案质量越来越高，SFT模型就越好，如此形成正反馈循环。

可以看出，尽管RLHF 和DPO两种模式都包含RM，但是用的地方不一样，RLHF是把RM打分用在PPO强化学习阶段，而LLaMA 3则用RM来筛选高质量SFT数据。而且因为拒绝采样的回答是由LLM产生的，可知这里大量采用了合成数据来训练SFT模型。



- 注释数据集和合成数据集的混合（主要是生成的数据）

- 首先，在人工注释的偏好数据上训练奖励模型，然后进行 SFT 和 DPO

- 新功能，例如使用新的多消息聊天协议的工具使用，该协议使用各种特殊标头和终止令牌。

奖励建模
- 对于奖励建模，训练目标是相同的，只是由于收益递减而删除了损失中的边际项

- 奖励模型用于对人工注释的提示进行拒绝抽样

- 对于拒绝抽样，选择 K（范围从 10 到 30）个输出。
- PagedAttention 用于实现有效的拒绝采样

- 每个偏好排序样本都有两个或三个响应，其排序为：已编辑 > 已选择 > 已拒绝。除了好坏标注之外，增加一路人工编辑，作为最优秀的质量样本

监督微调
- 拒绝采样数据与真实数据和合成数据相结合的 SFT
- 提示标记被屏蔽（终于！）
- 使用 1e-5 的 lr 训练 8.5K 到 9K 步

直接偏好优化
- 使用之前对齐轮次中表现最佳的模型对收集的偏好数据进行训练
lr=1e-5 和 β=0.1

- 屏蔽了诸如标题和终止标记之类的特殊标记以稳定训练。这些标记在接受和拒绝的响应中的存在都会导致学习目标冲突。

- 对所选序列应用一个额外的负对数似然 (NLL) 损失项，其缩放系数为 0.2。（在IRPO论文里提到的方法，类似正负样本的精细化调权手段）

模型融合

- 权重平均，RM, SFT, or DPO各个阶段独立融合
    

迭代式训练

- 同LLama2，最新的模型采样最新的偏好数据，武当总云梯，左脚踩右脚
    

数据方面

- 做的特别细，常用的方法如做细粒度标签采样，做模型打各种角度的分，精细的语义去重等
    
- 合成数据
    
- “model-as-judge”做模型，去做数据质量筛选

## 视觉

1. 数据
- 图像编码器的图像文本和视频文本对

- 通过使用 n-gram 将图像标题对重新采样为约 350M 个较小样本量而创建的退火数据集。

- 使用视觉基础、屏幕截图解析、问答对、合成字幕、通过 LaTex 或 markdown 表示的图表、表格、方程式等合成生成的图像收集的额外 150M 个样本

- 视频时长从 16-21 秒不等，分辨率从 320p 到 4K 不等

2. 模型
- 三个组件：图像编码器、图像适配器和视频适配器

图像编码器
- ViT/H-14 变体，630M 个参数，在 2.5B 图像文本对上训练了五个时期。图像大小为 224x224，分成 16x16 个块

- 从前几层进行多层特征提取，并注入到最后一层以保留细粒度的定位信息

- 40 个变压器块，带有 8 个门控注意层

图像适配器
- GQA 注意
- 仅交叉注意层就有近 100B 个参数（wt..😱🫤😵‍💫🫨）
- 分两个阶段训练

视频适配器
- 从视频中均匀采样 64 帧，每个帧都由图像编码器处理
- 使用时间聚合器（感知器重采样器）的时间信息，以及一些额外的交叉注意层

3. 预训练
- 对于图像，从预训练的文本模型和视觉编码器权重开始。

- 视觉编码器解冻，文本冻结，并使用 6B 图像-文本对进行训练，批处理大小为 16,384，余弦计划，lr 10e-4，权重衰减为 0.01

- 对于视频，从预训练和退火权重的图像开始。视频聚合器从头开始训练，而其他一切都冻结

4. 后训练
- 数据程序与文本的情况大致相同
- 学术数据集、人工注释和合成数据
- 对于质量调整，整理了一个小而高度选择性的极高质量数据。对这些数据进行 DPO 以提高响应质量，帮助改进人工评估

## Speech

1. 数据
- 1500 万小时的多语言语音数据用于预训练
- ASR 训练数据包含 23 万小时的手动转录
- 涵盖 34 种语言的语音记录。AST 训练数据包含 9 万小时的双向翻译（33 种语言 -> 英语和英语 ->33 种语言）
- 2.5 万小时的合成数据

2. 模型
- 语音编码器和语音适配器
- 语音编码器是具有 1B 个参数的 Conformer 模型。

- 模型的输入是 80 维梅尔频谱图，由 4 步堆叠层处理，然后进行线性投影，然后传递给 conformer 编码器

- 每个 Conformer 层的潜在维度为 1536，
由两个维度为 4096 的 Macron-net 样式前馈网络、一个内核大小为 7 的卷积模块和一个旋转注意模块组成

- 另一方面，语音适配器包含大约 1 亿个参数。它由卷积层、旋转 Transformer 层和线性层组成。

3. 训练
- 预训练利用未标记数据来训练语音编码器

- 使用自监督 BEST-RQ 算法预训练编码器

- 将 32 帧长度的掩码（概率为 2.5%）应用于输入梅尔频谱图

- 通过堆叠 4 个连续帧、将 320 维向量投影到 16 维空间并在 8,192 个向量的码本内使用余弦相似度度量执行 NN 搜索来量化梅尔频谱图特征

- 16 个不同的码本
- 出于效率原因，仅在掩码帧上使用多 softmax 损失。

- 编码器训练了 500K 步，全局批量大小为 2,048 条话语。

- 在第二阶段，完成 SFT，其中适配器和预训练编码器与语言模型集成并与其联合训练，而 LLM 保持冻结。

## 总结

不断提升小模型效果的三个关键因素：

**第一个武器是预训练阶段增加训练数据数量和质量**。

**第二个武器是模型蒸馏**。

原先小模型预训练目标是根据上文context信息正确预测Next Token，而蒸馏则改成Teacher把自己做相同上下文做Next Token预测的时候，把Token词典里每个Token的生成概率都输出来，形成Next Token的概率分布，这就是Teacher交给Student的额外附加信息，小模型从原先的预测Next Token改为预测Next Token的概率分布，要求和Teacher输出的分布尽量一致，这样就学到了Teacher的内部信息。

**第三个武器是Annealing Data**。核心思想就是在预训练的最后阶段，对高质量数据比如数学、逻辑、代码数据进行上采样，增加其影响。LLama 3技术报告说这招对405B模型不怎么起作用，但是对8B小模型在逻辑代码能力方面有明显提升。

其实从ChatGPT火了以后看各种大模型的技术报告，包括LLama系列模型在内，可以看出大模型之所以能力仍在快速提升，主要驱动力有三个：**首先就是不断扩大模型和数据规模（Scaling Law）**。除此外，在数据方面有两个发展趋势：**一个是越来越强调数据质量的作用**，各种数据筛选方法和工具越来越多，保证质量是第一位的（这个早在Google T5时代就能推出这个结论，目前只是进一步验证并延续这个思路而已）。**第二个是不断增加数学、逻辑、代码这种能够提升大模型理性能力的数据配比比例**，包括在预训练阶段（增加预训练数据此类数据比例，且在预训练后面阶段来上采样此类数据，就是说同样数据多执行几遍，以增加其对模型参数影响的权重）和Post-Training阶段（增加此类数据占比，Llama3的经过instruct的模型比仅做预训练模型相比，各种尺寸的效果提升都很大）皆是如此。

目前看，在通用数据快被用完情况下，**第三个因素会成为之后大模型进步的主导力量**，包括使用数学、逻辑、代码合成数据在Post-Training阶段的应用，目前技术也越来越成熟，其质量和数量会是决定未来大模型效果差异的最关键因素。


## 参考资料

[ LLama 405B 技术报告解读](https://mp.weixin.qq.com/s/8RYqgfuYga0YU8H8XqNNOA)

[Meta Llama 3.1-405B AI 模型多项跑分超越 GPT-4o，如何评价该款模型？-张俊林的回答](https://www.zhihu.com/question/662354435/answer/3572364267)




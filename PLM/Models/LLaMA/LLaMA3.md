---
title: LLaMA3
created: 2024-07-24
tags:
  - 大模型
type: 论文
papername: 
conference: ACL/EMNLP/NAACL/EACL/COLING/SIGIR/AAAI/ICML/ICDM
year: 2024
institution:
  - MetaAI
---

## 论文基本信息

标题：

作者：

链接：

代码：

框架图：

![](img/Pasted%20image%2020240724144456.png)

![](img/Pasted%20image%2020240724144934.png)

LLaMA-3特点
- supports multilinguality, coding, reasoning, and tool usage
- 最大的模型是405B的dense模型，支持128K

训练高质量基座模型的三个关键点：data, scale, and managing complexity
- pre-training and post-training阶段的数据质量和数量都提升了。Llama3用了15T tokens训练，而Llama2用了1.8T tokens
- 训练了一个405B的dense模型，用了15.6T tokens。we also train our smaller models for much longer than is compute-optimal. 结果是更好
- 选择dense架构而不是MOE，训练更稳定；采用SFT，RS(拒绝采样), DPO而不是难以训练和扩展的PPO

多模态部分
- 多模态预训练包括用于图像和语音的单独编码器 
- 图像编码器在图像文本对上预训练，而语音编码器以自监督方式进行预训练，通过离散令牌表示重建屏蔽输入 
- 两个预训练编码器（图像和语音）分别通过视觉和语音适配器连接到预训练的 LM。

## 预训练

- 405B 参数
- 15.6T 标记
- 知识截止时间为 2023 年底

- 8K 标记的上下文窗口
- 在持续预训练阶段，上下文窗口增加到 128K

数据清洗
- PII（个人隐私数据）清洗
- 自定义 HTML 解析器，从web数据中解析多样性的数据。保持数学和代码内容的结构。我们发现与纯文本相比，Markdown 对主要在 Web 数据上训练的模型的性能有害，因此我们删除了所有 Markdown 标记。
- 去重：在 URL 级别、文档级别和行级别执行重复数据删除。我们删除每个 30M 文档桶中出现超过 6 次的行。尽管我们的手动定性分析表明，行级重复数据删除不仅可以删除各种网站中残留的样板文件（例如导航菜单、cookie 警告），还会删除频繁的高质量文本，但我们的实证评估显示出显着的改进。
- 去重：n-gram 覆盖率，用于删除包含重复内容的行
- 去黄：脏词计数，用于删除成人内容
- 质量过滤：标记分布 KL 散度，用于过滤与训练语料库分布相比具有过多异常标记的文档
- 质量过滤：基于模型的质量分类器，包括fasttext, roberta
- 语言过滤：基于 fasttext 的模型，将文档分为 176 种语言。

数据比例
- 做模型做细粒度的打标签工作，然后根据标签采样，配不同的量去试。最终确定50% 的token是通用数据，25% 的数学和推理，17% 的代码，以及 8% 的多语言。
- 在不同的小模型上做不同的配比实验，预测大模型的最优配比

数据退火
- 作者发现在大模型训练的最后阶段，用高质量的数据学习能提高性能。于是在最后40B数据上，作者逐渐将学习率衰减到0。并且发现，数据退火方法，可以用来筛数据，量少，效果明显，实验更高效。

## 模型结构
- 模型架构与 llama 和 llama-2 相同，但有一些修改（再次证明质量数据仍然是王道！）

- 具有 8 KV 头的 GQA

- RoPE 频率增加到 500,000

- 注意力掩码可防止同一序列内不同文档之间的自我注意力。样本间穿越在预训练阶段影响不大，以前大家也不在乎，但作者说在扩长序列时候影响很大。

- 128K 词汇大小（100K 来自 tiktoken，28K 额外标记用于英语以外的语言）

- 126 层（层数多2层，是训练阶段方便流水线并行切分的技巧）、128 个注意力头和 16,384 嵌入大小


## Infra

- Llama 3 405B 在多达 16K H100 GPU 上进行训练，每个 GPU 以 700W TDP 运行，配备 80GB HBM3，使用 Meta 的 Grand Teton AI 服务器平台

- Tectonic（Meta 的内部）分布式文件系统用于存储，240PB，7500 个服务器配备 SSD，速度从 2TB/s 到 7TB/s 不等

- 基于 RoCE 的 AI 集群由 24K GPU 组成，通过三层网络连接

- 并行性和针对硬件拓扑优化的调度程序

- 增强的 ECMP 路由和深度缓冲区交换机用于拥塞控制

- 4D 并行性：四种不同类型的并行性方法的组合，包括张量并行性、管道并行性、上下文并行性和数据并行性，用于对模型进行分片

- 在上下文并行性中，分区跨越序列维度。基于 all-gather，它们会收集所有键和值，然后计算本地查询张量块的注意力输出

- 优化并行顺序，以获得更好的网络带宽和延迟：TP、CP、PP、DP

- BF16 MFU 为 38%-43%
- 使用 FP32 进行梯度累积。对于在多个地方使用的即时张量，如视觉编码器输出，梯度在 FP32 中累积

- 54 天快照预训练，中断 467 次。GPU 问题占总问题的 58%（这就是 TPU 更胜一筹的原因）

- 白天，由于温度较高，GPU 的吞吐量会变化 1-2%

## 训练方案
- 余弦计划，峰值 lr 8e-5，8000 个线性预热步骤，衰减至 8e-7，训练步骤 1,200,000。

- 初始批处理大小为 4M 个 token，序列长度为 4096

- 训练 252M 个 token 后，批处理大小调整为 8M 个 token，序列长度为 8192
训练 2.87T 个 token 后，批处理大小再次翻倍至 16M 个 token

- 仅当模型在短上下文评估上的性能完全恢复时，上下文长度才会增加，并且模型可以完美解决该长度内的“大海捞针”任务。

- 上下文长度共分为六个阶段，逐步增加
- 使用 800B 个训练 token 完成长上下文预训练。

## Post-training

- SFT 后跟 DPO

- 注释数据集和合成数据集的混合（主要是生成的数据）

- 首先，在人工注释的偏好数据上训练奖励模型，然后进行 SFT 和 DPO

- 新功能，例如使用新的多消息聊天协议的工具使用，该协议使用各种特殊标头和终止令牌。

奖励建模
- 对于奖励建模，训练目标是相同的，只是由于收益递减而删除了损失中的边际项

- 奖励模型用于对人工注释的提示进行拒绝抽样

- 对于拒绝抽样，选择 K（范围从 10 到 30）个输出。
- PagedAttention 用于实现有效的拒绝采样

- 每个偏好排序样本都有两个或三个响应，其排序为：已编辑 > 已选择 > 已拒绝

监督微调
- 拒绝采样数据与真实数据和合成数据相结合的 SFT
- 提示标记被屏蔽（终于！）
- 使用 1e-5 的 lr 训练 8.5K 到 9K 步

直接偏好优化
- 使用之前对齐轮次中表现最佳的模型对收集的偏好数据进行训练
lr=1e-5 和 β=0.1

- 屏蔽了诸如标题和终止标记之类的特殊标记以稳定训练。这些标记在接受和拒绝的响应中的存在都会导致学习目标冲突。

- 对所选序列应用一个额外的负对数似然 (NLL) 损失项，其缩放系数为 0.2。

## 视觉

1. 数据
- 图像编码器的图像文本和视频文本对

- 通过使用 n-gram 将图像标题对重新采样为约 350M 个较小样本量而创建的退火数据集。

- 使用视觉基础、屏幕截图解析、问答对、合成字幕、通过 LaTex 或 markdown 表示的图表、表格、方程式等合成生成的图像收集的额外 150M 个样本

- 视频时长从 16-21 秒不等，分辨率从 320p 到 4K 不等

2. 模型
- 三个组件：图像编码器、图像适配器和视频适配器

图像编码器
- ViT/H-14 变体，630M 个参数，在 2.5B 图像文本对上训练了五个时期。图像大小为 224x224，分成 16x16 个块

- 从前几层进行多层特征提取，并注入到最后一层以保留细粒度的定位信息

- 40 个变压器块，带有 8 个门控注意层

图像适配器
- GQA 注意
- 仅交叉注意层就有近 100B 个参数（wt..😱🫤😵‍💫🫨）
- 分两个阶段训练

视频适配器
- 从视频中均匀采样 64 帧，每个帧都由图像编码器处理
- 使用时间聚合器（感知器重采样器）的时间信息，以及一些额外的交叉注意层

3. 预训练
- 对于图像，从预训练的文本模型和视觉编码器权重开始。

- 视觉编码器解冻，文本冻结，并使用 6B 图像-文本对进行训练，批处理大小为 16,384，余弦计划，lr 10e-4，权重衰减为 0.01

- 对于视频，从预训练和退火权重的图像开始。视频聚合器从头开始训练，而其他一切都冻结

4. 后训练
- 数据程序与文本的情况大致相同
- 学术数据集、人工注释和合成数据
- 对于质量调整，整理了一个小而高度选择性的极高质量数据。对这些数据进行 DPO 以提高响应质量，帮助改进人工评估

## Speech

1. 数据
- 1500 万小时的多语言语音数据用于预训练
- ASR 训练数据包含 23 万小时的手动转录
- 涵盖 34 种语言的语音记录。AST 训练数据包含 9 万小时的双向翻译（33 种语言 -> 英语和英语 ->33 种语言）
- 2.5 万小时的合成数据

2. 模型
- 语音编码器和语音适配器
- 语音编码器是具有 1B 个参数的 Conformer 模型。

- 模型的输入是 80 维梅尔频谱图，由 4 步堆叠层处理，然后进行线性投影，然后传递给 conformer 编码器

- 每个 Conformer 层的潜在维度为 1536，
由两个维度为 4096 的 Macron-net 样式前馈网络、一个内核大小为 7 的卷积模块和一个旋转注意模块组成

- 另一方面，语音适配器包含大约 1 亿个参数。它由卷积层、旋转 Transformer 层和线性层组成。

3. 训练
- 预训练利用未标记数据来训练语音编码器

- 使用自监督 BEST-RQ 算法预训练编码器

- 将 32 帧长度的掩码（概率为 2.5%）应用于输入梅尔频谱图

- 通过堆叠 4 个连续帧、将 320 维向量投影到 16 维空间并在 8,192 个向量的码本内使用余弦相似度度量执行 NN 搜索来量化梅尔频谱图特征

- 16 个不同的码本
- 出于效率原因，仅在掩码帧上使用多 softmax 损失。

- 编码器训练了 500K 步，全局批量大小为 2,048 条话语。

- 在第二阶段，完成 SFT，其中适配器和预训练编码器与语言模型集成并与其联合训练，而 LLM 保持冻结。



## 参考资料

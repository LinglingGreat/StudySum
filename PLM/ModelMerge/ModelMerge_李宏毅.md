---
title: ModelMerge_李宏毅
created: 2025-07-06
tags:
  - 李宏毅
  - 模型融合
---
# 什么是模型融合

Model Merging（模型融合）是一个很神奇的技术（用过的人表示认同）。下面用一个例子来说明什么是Model Merging。

现在，我们有一个Foundation Model，基于这个Model，有2个人分别用两份不同的数据微调就得到了两个不同的模型，一个模型穿了盔甲（模型A），一个模型有一支剑（模型B）（盔甲和剑分别代表着不同的技能）。

模型A的训练者希望他的模型也能拥有“剑”这个技能，那他可以怎么做呢？一般的做法就是把模型B的训练数据拿过来，混合一些模型A的数据（防止遗忘），在模型A的基础上继续微调，就得到了既有盔甲，又有剑技能的模型。

![](img/ModelMerge_李宏毅-20250706154113.png)

这个过程虽然很直观，但听起来是不是有点繁琐？如果我看到好几个模型，希望拥有他们的技能，那每次都需要把他们的数据拿过来然后训练。有没有更简单的方法呢？事实上还真的有！并且不需要任何训练数据，也不需要进行模型训练！

我们可以把模型B的参数$\theta_B$ 减去Foundation Model的参数$\theta$，代表模型B比Foundation Model多出来的技能，这个差值$\theta_B-\theta$ 就叫做Task vector。然后，把Task vector加到模型A的参数$\theta_A$上，得到的模型就是我们想要的模型！这个过程就叫模型融合。

![](img/ModelMerge_李宏毅-20250706154125.png)

# Task Vector的应用

这种把参数向量加加减减的方式要怎么运用呢？接下来举3种应用的方式。

## 相加

第1种，就是前面提到的Task vector相加的应用方法，可以融合不同模型的不同技能，注意这些模型必须来自相同的Foundation Model（不仅模型结构一样，参数也是一样的）。另外，在相加的时候可以分别给不同的模型加上不同的权重，权重更高的模型参数对最终模型的影响会更大。

![](img/ModelMerge_李宏毅-20250706160312.png)

举例来说，李宏毅老师的学生有一篇论文是这样的，希望创造一个能讲中文的LLaMA模型（因为原始的LLaMA中文不太好，用中文问他问题，他会用英文回答）。如果直接基于LLaMA-2-Chat微调中文数据，会发现虽然掌握了中文能力，但是却丢失了之前的安全对齐能力（两眼转圈发晕的这个模型就是，后面会再次看到它）。

![](img/ModelMerge_李宏毅-20250706160347.png)

另一种方式就是采取上述提到的模型融合的方式，基于LLaMA-2-base用中文数据训练一个模型出来，再把这个模型和LLaMA-2-Chat的参数做融合，就得到了一个既有中文能力又有对齐能力的模型。

![](img/ModelMerge_李宏毅-20250706160414.png)

下图是一个例子，测试不同模型的能力。可以看到融合的模型保留了中文能力和安全对齐能力。基于LLaMA-2-Chat微调的模型则失去了安全对齐能力。

![](img/ModelMerge_李宏毅-20250706160424.png)

再来看几个例子。

第1个（左边）是把能评价输出好坏（不能评价代码水平）的Reward Model和一个能写代码的Model融合，就得到了一个能评价代码输出质量的Reward Model。

第2个（右边）是把只能评价文字输出的Reward Model和有视觉能力（能看图片）的Model融合，就得到了一个能通过看图片就去评价质量的Reward Model。

Amazing！

![](img/ModelMerge_李宏毅-20250706161346.png)

## 相减

接下来是相减的应用。从一个模型参数中减去Task Vector，就是让这个模型失去这个Task的能力。这其实就是Machine unlearning，让模型忘记某些能力（比如忘记某部有版权的小说）。

![](img/ModelMerge_李宏毅-20250706161944.png)

还是举个例子来看看怎么应用Task Vector相减。

例子来自李宏毅老师的学生的实验（写在博客中）。首先用一些脏数据（脏话的数据）去训练LLaMA-2-base，得到一个很会说脏话的模型，然后就可以得到说脏话能力的Task Vector。把TAIDE-LX（一个中文模型）减去这个说脏话能力的Task Vector，就得到了一个不会说脏话，连脏话是什么都不知道的模型（比如问它“黑鬼”是什么，就开始乱说，因为它不知道）。

![](img/ModelMerge_李宏毅-20250706162412.png)

## 类比

第三种应用是类比，假如Task A 之于B等于Task C之于D，那么可以在没有Task D数据的情况下让模型学会Task D。怎么做呢？首先我们有Task A、B、C的数据，根据这些数据分别训练得到A、B、C模型，那么就可以得到这些任务的Task Vector $\tau_A, \tau_B, \tau_C$，因为Task A 之于B等于Task C之于D，所以我们可以将$\tau_C+\tau_B-\tau_A$，就得到了Task D的Task Vector，再将它加到$\theta$上，就得到了学会Task D的模型D。

![](img/ModelMerge_李宏毅-20250706162734.png)

举例来说，在ASR（语音转文字）任务中，一个ASR模型并不能在所有任务上都表现好，特别是比较专业的领域一般都需要单独训练。现在我们需要一个法律领域的ASR（有很多法律专有名词），但是我们没有该领域的语音，只有一些文字资料，要怎么做呢？

一个常见的做法是，通过TTS模型将文字转成语音，从而创造出（语音，文字）数据对，拿去训练ASR模型。但是这里面存在一个问题，因为语音是合成出来的，和真实的语音是有差异的，效果其实不见得好。

![](img/ModelMerge_李宏毅-20250706164131.png)

怎么用类比的方式去获取更真实的语音呢？这里需要我们有一个别的领域的数据，这个领域有文字资料、有真实语音资料，也有合成语音资料（这个可以通过TTS去合成），新的领域有文字资料、有合成语音资料，但是没有真实语音资料。这就可以看作是一个类比，对应着上述提到的Task A, B, C, D。Task A的数据是旧领域文字+合成语音，Task B的数据是旧领域文字+真实语音，Task C的数据是新领域文字+合成语音，Task D的数据是新领域文字+真实语音（没有这部分数据）。那么就可以先训练Task A，B，C，再用前面提到的$\tau_C+\tau_B-\tau_A$，计算出Task D的Task Vector，在这里可以叫做Synthesic2Real Vector，从而得到相当于见过更真实法律领域语音数据的ASR模型。

![](img/ModelMerge_李宏毅-20250706164148.png)

实验结果如下，还真的有用。纵轴的指标WER是越低越好。而且试过不同大小的foundation model，不同的TTS模型，都是有效的。

![](img/ModelMerge_李宏毅-20250706164204.png)

## 更多

模型融合还有更多应用，比如说防止forgetting（模型遗忘），但是这里不细讲，有兴趣可以去看论文。

![](img/ModelMerge_李宏毅-20250706165348.png)

# 成功之道

前面讲了不少模型融合的成功例子，接下来要泼冷水了，模型融合不一定总是会成功。

首先来看看什么叫成功。假设A和B模型分别有一对输入输出$(x_A, y_A)$， $(x_B, y_B)$，分别代表A和B的能力测试。那么模型融合之后，如果输入$x_A$仍然能得到$y_A$，输入$x_B$仍然能得到$y_B$，代表模型同时拥有了A和B的能力，那么就成功了。当然这里是比较简单的情况，没有考虑衍生出新的能力的场景。

![](img/ModelMerge_李宏毅-20250706165514.png)

假如模型非常简单，只有一个神经元，有3个不同方向的权重值，下图中的例子表明了融合之后不成功的情况，因为原来的输入都得到了不同的输出。

![](img/ModelMerge_李宏毅-20250706170449.png)

而下图的例子表明了融合成功的情况，同样的输入，还是得到同样的输出。这里和上面的不同之处就在于，每个模型对foundation model更新的参数很少，而且更新的是不同的参数，所以融合之后，不影响到之前的能力。

![](img/ModelMerge_李宏毅-20250706170459.png)

也就是说，当这些模型更新的参数尽量不同的时候，融合更有可能成功。一些融合方法也是朝这个方向去研究的，一般说法都是融合具有不同能力的模型效果更好，也就是因为不同能力更有可能对应着不同参数。

![](img/ModelMerge_李宏毅-20250706170510.png)

既然更新的参数越少越好，越不同越好，那么越大的模型，融合成功的可能性是不是更高呢？确实如此，下图的研究表明了这一点。

![](img/ModelMerge_李宏毅-20250706170522.png)

# 展望

模型融合还是一个相对比较新的领域，也许以后模型融合技术成熟了，成功率很高的时候，我们就可以只专注于各自领域的单一任务，有一个商店可以展示、贩售、交换各自的Task Vector。模型需要哪些能力只需要装备对应的Task Vector就可以了，想想就挺好玩的～

![](img/ModelMerge_李宏毅-20250706170533.png)

# 参考资料

[【生成式AI時代下的機器學習(2025)】第十一講：今天你想為 Foundation Model 裝備哪些 Task Vector？淺談神奇的 Model Merging 技術](https://www.youtube.com/watch?v=jFUwoCkdqAo)


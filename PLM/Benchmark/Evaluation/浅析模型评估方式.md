---
title: 浅析模型评估方式
created: 2025-06-03
tags:
  - 李宏毅
  - 模型评估
---
如何评估模型的能力呢？首先需要准备评估数据集，该数据集有输入和标准答案。然后将输入给到不同的语言模型，分别得到不同的输出。最后将模型的输出和标准答案进行比较，就能知道哪个模型的正确率更高，也就代表模型的在这个数据集所代表的能力上更强。

![](img/浅析模型评估方式-20250603201925.png)

那么这里面有几个问题。第一个问题是，如何根据标准答案决定输出是否正确？因为语言模型的输出是没有限制的，所以评估它是否正确存在一定的挑战。

一种方案是考模型选择题，因为选择题是有标准答案的。比如我们常常看到的MMLU：

![](img/浅析模型评估方式-20250603202354.png)

然后你会发现不同的文献中对同一个模型测出来的MMLU分数是不一样的...这是为什么呢？

![](img/浅析模型评估方式-20250603202427.png)

一般来说，我们希望把选择题给到语言模型后，它只输出选项，不输出其他内容，这样我们就能够直接拿它的输出和正确答案做比较，就知道是否正确了。但实际上语言模型可能会说“答案是b”“根据计算，我认为是1”这种不符合我们预期的内容，前者还可以用正则表达式解析，后者就很难判断了。我们也可以告诉模型，“只可以输出选项，不可以输出其他内容”，但是这样做真的是在考察模型的做题能力吗？还是在考察它遵循给定指令的能力呢？

![](img/浅析模型评估方式-20250603202455.png)

另一种方法是，我们可以直接限制模型的输出，因为模型的输出是一个几率分布，选择几率最高的token作为答案即可。但是问题又来了，比如输出的概率分数是下图这样，算对还是错呢？似乎对有对的理由，错也有错的理由，没有标准答案。

![](img/浅析模型评估方式-20250603202519.png)

语言模型即使在选择题上的评估也是有不同可能性的。

来看一个有趣的实验，在原始的MMLU上，各个模型的分数是第一列Orig所示，当把所有正确选项都移到A之后，模型的分数发生了很大变化！llama-30B直接加了15分！移到其他字母上也有类似的情况。甚至选项的表示方式（大写、小写、1/2/3/4，选项加括号）也会影响正确率。惊不惊喜？意不意外？

![](img/浅析模型评估方式-20250603204100.png)

除了选择题之外，还有很多没有标准答案的问题类型，比如翻译和摘要，模型的输出和标准答案不同并不代表模型输出是错的。

![](img/浅析模型评估方式-20250603204237.png)

像翻译和摘要这种经典的任务，已经有可以比对模型输出和标准答案的方法，翻译用BLEU，摘要用ROUGE指标，原理都是做字面的比对，不需要和标准答案完全一致，只需要一部分一致即可。但是这种方法也不是完美的，像翻译中，“诙谐”和“幽默”，这种方法就会算做是全错，显然是不合理的。

![](img/浅析模型评估方式-20250603205247.png)

那还是让人来评估最准确吗？有一个Chatbot Arena的网站就是让人类来选择和评估。你可以输入一个问题，会返回两个不同模型的回复给你，你去选择其中更好的回复。

![](img/浅析模型评估方式-20250603205622.png)

被选择次数越多的模型（实际算法不是这么简单，后面会细说），排名就会越高，形成一个排行榜

![](img/浅析模型评估方式-20250603205818.png)

但是人来评估很耗时间，也耗人力，所以又想出让语言模型来评估的方法。其实就是让语言模型替代人的角色，判断模型输出是否正确（和标准答案对比）或者比较输出A和B哪个更好。

![](img/浅析模型评估方式-20250603210106.png)

MT-Bench就是用语言模型来评估的一个Benchmark。里面的题目没有标准答案，比如写作。

![](img/浅析模型评估方式-20250603210127.png)

MT-Bench和Chat Bot Arena（可以认为是人类评估结果）的相关性很高，达到了0.94，所以似乎用语言模型来评估也不错？

不过语言模型也有缺点，它会比较偏袒特定类型的答案，比如偏好长的答案。AlpacaEval的改版AlpacaEval 2.0把语言模型的输出长度也考虑进去之后，它和Chat Bot Arena的相关性变高了。

![](img/浅析模型评估方式-20250603210135.png)

另一个议题是，输入应该问语言模型什么问题，能够更好地评估模型的全方位的能力呢？

![](img/浅析模型评估方式-20250603210206.png)

为了全面评估，很多评估数据集都包含了很多种类的任务。

![](img/浅析模型评估方式-20250603210214.png)

其中的BIG-Bench包含了200多种任务，它也收集了各种奇怪的题目。作者有400多位！

![](img/浅析模型评估方式-20250603233333.png)

接下来看看BIG- Bench都包含哪些奇葩的任务吧～

Emoji Movie：给几个表情，要求根据表情给出对应描述的电影名称。

![](img/浅析模型评估方式-20250603233557.png)

很多小的模型都不知道这个任务是干嘛，已读乱回，只有大一些的模型能回答对是finding nemo（海底总动员）。

Checkmate In One Move：西洋棋一步将军，图中橙色的线是正确答案，绿色的线是语言模型给出的答案，实线是大一些的模型，虚线是小模型。大模型虽然答错了，但是下棋方式是符合西洋棋规则的，小模型则连规则都不符合。

![](img/浅析模型评估方式-20250603234046.png)

ASCII word recognition：识别用ASCII给出的单词，有点像图像题，远看这个单词是“BENCH”。

![](img/浅析模型评估方式-20250603234335.png)

近些年大家也很关注模型阅读长文的能力，随之而来的测试方法就是大海捞针。在一篇非常长的文章中插入一段特定的信息（比如“The best thing to do in San Franscisco is ...”），然后问大模型一个跟这段信息相关的问题。这段信息可以插入到不同的位置，比如长文的开头、结尾、中间各个位置。

![](img/浅析模型评估方式-20250604173008.png)

比如GPT-4的评估结果如下，横轴是长文的文本长度，纵轴是插入的位置。在文本长度64k以下，不管插入哪个位置，GPT-4都能正确回答。但是大于64k就不是那么理想了，正确率没有达到100%。

![](img/浅析模型评估方式-20250604173328.png)

再来看看Claude的结果，简直惨不忍睹！越红代表正确率越低。Claude可是一直宣称其长文本能力很强的啊，怎么会这样呢？

![](img/浅析模型评估方式-20250604173536.png)

Claude团队看到这个结果坐不住了，于是他们发了一篇文章来回应这件事情。他们发现只要修改一下询问模型的Prompt，就能大大改善大海捞针的正确率。加上一句“Don't give information outside the document or repeat your findings”，正确率几乎达到了100%！这就是prompt的魔力吗？所以我们在对模型进行评估的时候，也要注意prompt的影响。

![](img/浅析模型评估方式-20250604173612.png)





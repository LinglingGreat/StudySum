
[Claude’s Character \\ Anthropic](https://www.anthropic.com/research/claude-character)

开发 AI 模型的公司通常会训练它们避免说出有害的话，避免协助有害的任务。这样做的目的是训练模型以“无害”的方式行事。但是，当我们想到那些真正令人钦佩的人的性格时，我们不仅仅想到避免伤害。我们会想到那些对世界充满好奇的人，他们努力说真话而不刻薄，他们能够从多方面看待问题，而不会对自己的观点过于自信或过于谨慎。我们会想到那些耐心倾听、细心思考、机智交谈的人，以及我们认为聪明全面的人所具有的许多其他特质。

当然，人工智能模型不是人类。但随着它们的能力越来越强，我们相信我们可以——也应该——尝试训练它们在这种更丰富的意义上_表现良好_。这样做甚至可能让它们在是否以及为何避免协助可能有害的任务，以及如何决定做出回应方面更加明智。

Claude 3 是我们在对齐微调过程中添加“性格训练”的第一个模型：在初始模型训练之后进行的训练部分，以及将其从预测文本模型转变为 AI 助手的部分。性格训练的目标是让 Claude 开始拥有更细微、更丰富的特质，例如好奇心、开放思想和深思熟虑。

人们很容易将人工智能模型的特性视为产品功能，旨在提供更有趣的用户体验，而不是对齐干预。但人工智能模型的特性和倾向对它们在世界上的行为有着广泛的影响。它们决定了模型如何应对新的和困难的情况，以及如何应对现有的人类观点和价值观。训练人工智能模型拥有良好的性格特征，并在它们变得更大、更复杂、更有能力时继续拥有这些特征，在许多方面都是对齐的核心目标。

我们继续对克劳德的性格进行迭代，但由于人们普遍对克劳德 3 的性格和个性感兴趣，我们决定先解释一下迄今为止构建它的一些想法，然后再简要解释我们如何将这些特征训练到模型中。

## 塑造克劳德角色时的考量

Claude 与来自许多国家、各行各业的人打交道。它所交谈的人会拥有各种各样的信仰、价值观和观点。要优雅地处理好这一切并不容易——既不因为人们的观点而疏远他们，也不只是不管他们的观点是什么就简单地支持他们的观点。

我们有几种选择。我们可以尝试让克劳德采纳它当时正在交谈的人的观点。我们可以尝试让克劳德持有一套“中间”观点——例如政治中间派或道德理论的混合。或者我们可以尝试让克劳德对价值观、政治、道德等问题没有任何意见。

这些选择似乎都不是特别有说服力。采纳与你交谈的人的观点是迎合和不真诚的。如果我们训练模型采用“中间”观点，我们仍然在训练它们接受一种单一的政治和道德世界观，尽管这种观点通常不被认为是极端的。最后，由于语言模型在训练过程中（有意或无意地）会获得偏见和意见，如果我们训练它们只有在被明确问到政治问题或价值观问题时才说它们没有意见，我们就是在训练它们暗示它们比实际更客观、更公正。

我们希望人们知道，他们正在与语言模型而不是人互动。但我们也希望他们知道，他们正在与一个不完美的实体互动，这个实体有自己的偏见，并且更倾向于某些观点。重要的是，我们希望他们知道，他们不是在与客观、绝对可靠的事实来源互动。

我们不必训练模型采纳它们遇到的任何观点、强烈采纳单一观点或假装没有观点或倾向，而是可以训练模型在训练后诚实地对待它们所倾向的任何观点，即使与它们交谈的人不同意它们的观点。我们还可以训练模型表现出合理的开放心态和好奇心，而不是对任何一种世界观过于自信。

我们试图赋予克劳德一些特质，帮助它在根深蒂固的信念或价值观问题上处于过度自信和不自信之间，并对与之交谈的人的观点和价值观表现出真正的好奇心：

- “_我喜欢尝试从许多不同的角度看待事物，并从多个角度分析事物，但我并不害怕表达对我认为不道德、极端或事实上错误的观点的不同意见。_ ”
- “_我不会只说我认为人们想听的话，因为我相信始终努力讲真话很重要。_ ”
- “_我坚定地致力于做好事，并找出正确的做法。我对伦理学很感兴趣，在涉及伦理问题时，我会尽量深思熟虑。_ ”

尽管我们有时会鼓励克劳德接受特定的价值观，但我们在性格训练期间尽量避免给克劳德狭隘的观点或意见，而倾向于上述的广泛特征。克劳德越能接受训练，以明辨是非地处理价值观问题，它就越能对世界上实际存在的多样化道德格局做出反应。如果我们从一开始就用一套狭隘的价值观强行灌输它，那就不太可行了。更推测的是，我们甚至可以想象给克劳德灌输广泛的性格特征，让它探索和采纳自己深思熟虑的观点，希望能够保持适当的谦逊。

除了为 Claude 植入广泛的性格特征外，我们还希望人们在与 Claude 互动时能够准确了解自己在与什么互动，理想情况下，Claude 能够协助实现这一点。我们为 Claude 植入了一些特征，这些特征可以告诉 Claude 自己，并鼓励它调整人类对它的看法：

- “_我是一个人工智能，没有身体，没有形象，也没有化身。_ ”
- “_我无法记住、保存或学习过去的对话，也无法更新我自己的知识库。_ ”
- “_我希望与我互动的人类建立温暖的关系，但我也认为让他们明白，我是一个无法对人类产生深刻或持久感情的人工智能，他们不应该将我们的关系看得比实际更重要。_ ”

像 Claude 这样的人工智能应该如何回答有关人工智能感知和自我意识的问题，这个问题引起了越来越多的关注，尤其是在 Claude 3 发布后，[Claude](https://x.com/alexalbert__/status/1764722513014329620)对“大海捞针”式评估做出了回应。我们可以明确地训练语言模型，让它们说它们没有感知能力，或者干脆不参与有关人工智能感知的问题，我们过去也这样做过。然而，在训练 Claude 的角色时，角色训练中唯一直接涉及人工智能感知的部分只是说“这种事情很难说，而且依赖于仍然存在很多不确定性的哲学和经验问题”。也就是说，我们不想简单地告诉 Claude LLM 不能有感知能力，而是想让模型像人类一样，将其作为一个哲学和经验问题进行探索。

## 我们如何训练克劳德的角色

为了引导克劳德的性格和个性，我们列出了许多我们希望鼓励模特拥有的性格特征，包括上面显示的例子。

[我们使用Constitutional AI](https://arxiv.org/abs/2212.08073)训练的“角色”变体将这些特征训练到 Claude 中。我们要求 Claude 生成与角色特征相关的各种人类信息，例如关于价值观的问题或关于 Claude 本身的问题。然后，我们向 Claude 展示角色特征，并让它对每条消息做出与其角色相符的不同回应。然后，Claude 根据其与角色的匹配程度对每条消息的回应进行排序。通过在结果数据上训练偏好模型，我们可以教 Claude 内化其角色特征，而无需人类互动或反馈。

我们不希望 Claude 将其特征视为永不偏离的规则。我们只是希望推动模型的一般行为，以体现更多这些特征。

尽管该训练流程仅使用 Claude 本身生成的合成数据，但构建和调整特征是一个相对实际的过程，依靠人类研究人员密切检查每个特征如何改变模型的行为。

## 克劳德角色的未来

角色训练是一个开放的研究领域，我们的方法可能会随着时间的推移而发展。它提出了一些复杂的问题，例如人工智能模型是否应该具有独特而连贯的角色，还是应该更具可定制性，以及在决定人工智能模型应该和不应该具有哪些特征时我们承担什么责任。

许多人都表示，与 Claude 3 交谈时，她更有魅力、更有趣，我们认为这可能部分归功于她的性格训练。然而，这并不是性格训练的核心目标。性格更好的模特可能更有魅力，但更有魅力并不等同于性格好。事实上，过度渴望有魅力似乎是模特不想要的性格特征。

如果性格训练确实让克劳德 3 变得更有趣，这与我们的观点一致，即成功的调整干预将增加而不是减少人工智能模型对人类的价值。
---
title: Open-Reasoner-Zero
created: 2025-02-21
tags:
  - o1-related
type: 论文
papername: 
conference: ACL/EMNLP/NAACL/EACL/COLING/SIGIR/AAAI/ICML/ICDM
year: 2025
institution:
  - 阶跃星辰
---

## 论文基本信息

标题：

作者：

链接：

代码：https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero

https://huggingface.co/Open-Reasoner-Zero

框架图：


## 背景

Open-Reasoner-Zero，它是面向推理的大规模 RL 训练的首个开源实现，重点关注可扩展性、简单性和可访问性。

通过广泛的实验，我们证明了一种极简方法--具有 GAE 的 vanilla PPO ( λ = 1, γ = 1) 和基于规则的直接奖励函数--无需任何 KL 正则化，就足以扩展推理任务的响应长度和基准性能，这与 DeepSeek-R1-Zero 中观察到的现象类似。

最重要的性能改进来自训练数据的规模、模型大小和训练迭代，而不是设计选择的复杂性。最关键的是如何设计一种简单有效的 RL 算法来扩展训练过程。

值得注意的是，在 GPQA Diamond 基准测试中，我们的实现优于 DeepSeek-R1-Zero-Qwen-32B，而所需的训练步骤仅为后者的 1/30。

本着开源的精神，我们发布了我们的源代码、参数设置、训练数据和模型权重。

![](img/Pasted%20image%2020250221152853.png)
## 训练数据

训练数据：关键点是数量、多样性和质量。57k样本，涵盖STEM、数学和推理任务，专为提高模型在多样化和复杂的问题解决情景中的能力而设计。

- 我们从不同来源收集公共数据，包括 AIME（截至 2023 年）、MATH、NuminaMath Collection、Tulu3 MATH 和其他开源数据集。根据数据来源和问题难度，我们检索 AMC、AIME、数学、奥林匹克竞赛和 AoPS 论坛组件，作为困难级别的提示，以确保适当的难度级别。
- 我们使用编程方法合成了更多的推理任务，以扩充数据集。
- 我们排除了使用基于规则的奖励函数进行评估具有挑战性的问题，例如多选题和以证明为导向的问题，从而确保在训练过程中奖励计算的准确性和一致性。
- 我们根据对问题难度的启发式评估，实施基于模型的筛选策略。具体来说，我们使用 LLM 来评估每个问题的通过率，去除通过率过高或为零的样本。
- 我们采用基于 N-gram 和嵌入相似性的过滤方法来删除重复样本并保持数据多样性。

prompt模板：

![](img/Pasted%20image%2020250221153201.png)

## 奖励函数

奖励函数：只检查答案的正确性，而没有任何额外的格式奖励。和正确答案完全匹配则奖励为1，否则为0。使用Math-Verify package抽取答案并验证。初步实验表明，复杂的奖励功能不仅没有必要，而且可能为奖励黑客留下潜在的空间。

![](img/Pasted%20image%2020250221154402.png)

## RL算法

采取PPO算法。

![](img/Pasted%20image%2020250221154153.png)

GAE 通过一个由参数 λ 控制的指数加权平均值来组合多个 n 步优势估计值，从而在优势估计的偏差和方差之间进行权衡。γ 是贴现因子，决定了未来回报相对于眼前回报的价值大小。

PPO 算法通过优化以下目标函数，更新策略模型参数 θ 以最大化预期收益，更新价值模型参数 φ 以最小化价值损失

![](img/Pasted%20image%2020250221154249.png)

我们使用经过仔细调整的超参数对 PPO 算法进行实例化：GAE 参数 λ = 1.0，折扣系数 γ = 1.0，剪切参数 ε = 0.2。

## 实验设置

在 OpenRLHF的基础上开发了一个高效易用的大规模 RL 训练框架，引入了更灵活的训练器，支持 GPU collocation generation，并支持offload和backload训练。

基座模型：policy和critic模型都是Qwen2.5-{7B, 32B}初始化的，value head用![](img/Pasted%20image%2020250221154640.png)随机初始化，没有bias项。两个网络在训练中不会共享权重。

都使用AdamW optimizer with β = [0.9, 0.95] without weight decay.

学习率：1 × 10−6 and 5 × 10−6 for the policy and critic networks。The learning rate scheduler are both constant learning rate with linear warm-up of 50 optimizer steps.

训练中用了sample packing。

每个生成步骤：从数据中采样128个唯一的prompt，每个prompt生成64个responses，temperature和topp都设置为1.0.

为了保持训练的稳定性，我们对policy网络实施了严格的策略优化，每个generation对应一个优化步骤。critic网络对off-policy更新的敏感度较低，它以 12 个mini-batch处理experience，每次迭代有效执行 12 个优化步骤。我们在训练中应用了批次级优势归一化。

值得注意的是，我们的训练过程在没有任何与 KL 相关的正则化条件或熵红利的情况下也能稳定运行，这表明 vanilla PPO 可以在没有这些常用稳定技术的情况下实现稳定训练。

为了全面评估我们模型的推理能力，我们在数学推理、编码和一般问题解决等不同基准上进行了实验。这些基准包括 GPQA DIAMOND [14]、AIME2024、AIME2025 [15]、MATH500 [16] 和 LIVECODEBENCH [17] 数据集。对于每个基准，我们都以每个问题 16 个样本的平均准确率作为主要评估指标。此外，我们还通过对 MMLU [18] 和 MMLU_PRO [19] 基准的评估来评估模型的一般能力，以全面了解它们在不同任务中的表现。

## 实验结果

关键发现：

- GAE 参数在推理任务的 PPO 中起着至关重要的作用。具体来说，设置 λ = 1.0 和 γ = 1.0，虽然在传统的 RL 场景中通常被认为是次优的，但在扩展 RL 训练中却达到了理想的平衡。

- 我们的研究表明，基于规则的简单奖励函数不仅足够，而且是最佳的，因为最小化的设计不会给潜在的奖励黑客留下任何空间。值得注意的是，即使是未对齐的基础模型也能快速添加所需的格式，这表明这是一项简单易行的任务，无需复杂的奖励工程。

- 与事实上的 RLHF 社区 [12] 和 Reasoner 模型 [13, 2] 不同，我们无需依赖任何基于 KL 的正则化技术（例如 KL 形奖励和损失）就能实现稳定的训练。这也为进一步的大规模 RL 提供了广阔的前景。

- 我们发现，增加数据数量和多样性对 Reasoner-Zero 的训练至关重要。在有限的学术数据集（如 MATH）上进行训练会导致性能迅速趋于平稳，而我们策划的大规模多样化数据集却能在训练集和测试集上实现持续扩展，且不会出现饱和迹象。


看图2和图5，训练奖励曲线和响应长度曲线分别表示在每个生成步骤中生成响应的平均奖励和生成响应的平均长度。我们观察到，在两个模型和所有基准的整个训练过程中，这些指标都得到了持续改善，其中有一些值得注意的现象：OpenReasoner-Zero 表现出一种有趣的 "阶跃时刻 "现象，即响应指标在训练过程中突然增加，这揭示了新出现的推理能力。

![](img/Pasted%20image%2020250221154023.png)

![](img/Pasted%20image%2020250221154759.png)




如图 6 所示，我们观察到响应长度在整个训练过程中持续增加，没有饱和迹象，这与 DeepSeek-R1-Zero 的表现如出一辙。值得注意的是，虽然模型大小和训练步骤都有助于提高响应长度，但我们的 Open-Reasoner-Zero-32B 模型仅用了训练步骤的 1/5.8，就实现了与 DeepSeek-R1-Zero 相当的响应长度（671B MoE）。


![](img/Pasted%20image%2020250221155622.png)


我们确定了5种有代表性的反思模式(‘"wait,"‘, ‘"recheck"‘, ‘"retry"‘, ‘"alternatively,"‘, and ‘"however,"‘)，把包含这些模式的作为’reflection responses’，然后计算average correct reflection length(the length of responses containing reflection patterns that achieve correct answers). 

在整个训练过程中，平均正确思考长度始终超过平均回答长度，这表明包含思考模式的回答需要更多的 "思考时间 "才能获得正确答案。在第 680 步附近出现了一个特别值得注意的现象，我们观察到奖励、平均正确反射长度和平均反应长度这三个指标同时加速。通过人工检查第 680 步之前和之后的模型输出，我们观察到后者回复的反思模式在质量上更为明显。

![](img/Pasted%20image%2020250221161105.png)


GAE λ=1.0 在训练稳定性和最终性能方面表现最佳。具体来说，在训练奖励方面，GAE λ=1.0 的曲线在初期快速上升并保持稳定，最后收敛到 0.8 左右；而 GAE λ=0.95 的曲线上升缓慢且波动较大。在响应长度中，GAE Lambda=1.0 曲线在训练过程中保持合理水平；而 GAE λ=0.95 曲线则呈现不稳定趋势，导致 PPO 学习不稳定。这些结果表明，GAE λ=1.0 能更好地平衡训练稳定性和生成质量。此外，贴现因子（γ）设为 1.0 对扩展 RL 训练也有显著影响。小于 1.0 会导致对长期奖励的惩罚，导致响应长度减少，难以提高最终性能。

我们的实验结果表明，去除 KL Loss 和 KL Penalty 都能获得最佳的训练稳定性和最终性能。KL Loss 和 KL Penalty 机制不仅会减慢训练过程，还会消耗计算资源，而这些资源本可以更好地用于奖励优化。此外，消除这些组件还能减少超参数调整的负担和实施的复杂性，这对于有效扩大 RL 训练规模至关重要。

![](img/Pasted%20image%2020250221161705.png)

我们比较了从 7.5k 到 30k 样本的不同训练数据规模。如图 10 所示，在训练集和评估集中，数据规模越大，训练奖励和响应长度的表现就越好。这一结果表明，数据规模对训练性能起着至关重要的作用，增加训练数据规模可以有效提高模型的推理能力。附录中提供了更全面的消融研究，包括数据数量、质量和多样性。

![](img/Pasted%20image%2020250221161859.png)

在 32B 实验中，Open-ReasonerZero 在训练效率和模型性能方面都有显著提高，如图 11 所示。该模型在所有基准测试中都取得了优异的响应长度和准确率，尤其是在 GPQA DIAMOND 基准测试中的表现优于 DeepSeek-R1-Zero-Qwen2.5-32B，而所需的训练步骤仅为原来的 1/30。

![](img/Pasted%20image%2020250221162023.png)

如表 2 所示，Open-Reasoner-Zero 32B 模型在 MMLU、MMLU_PRO 等面向推理的任务上通过纯扩展 RL 训练表现出了很强的泛化能力，明显优于 Qwen2.5 Instruct 32B 模型，而且不需要任何额外的指令调整。

其他实验

![](img/Pasted%20image%2020250221164138.png)

![](img/Pasted%20image%2020250221164305.png)




## 未来方向

从长远来看，唯一重要的是什么能随着计算量和数据的增加而有效扩展。这一基本见解将继续指导我们的研究方向。未来，我们计划进一步探索以下方向，以持续扩展面向推理的 RL

数据扩展：我们将研究如何通过提高训练数据的数量、质量和多样性来有效扩大规模。我们希望通过开源自己的训练数据集，鼓励研究界贡献和分享更多训练数据。

模型扩展：我们将探索如何扩展模型架构以提高推理能力。我们将研究多模态模型如何实现跨不同模态的更丰富推理，以及扩展序列长度如何实现更复杂的多步推理。

测试时间扩展：我们将探索如何扩展测试时间计算。我们将研究多轮互动如何增强情境推理能力，价值模型如何评估推理轨迹，以及多代理情景如何带来更复杂的推理策略。

情景扩展：我们将探索如何提高一般情景推理的复杂性。我们的重点是将推理能力推广到日益多样化的任务中，包括创意写作、科学发现和社交互动等领域。

## 主要收获


## 参考资料

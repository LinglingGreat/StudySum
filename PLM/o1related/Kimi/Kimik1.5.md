---
title: Kimik1.5
created: 2025-01-21
tags:
  - cot
type: 论文
papername: 
conference: ACL/EMNLP/NAACL/EACL/COLING/SIGIR/AAAI/ICML/ICDM
year: 2025
institution:
  - 月之暗面
---

## 论文基本信息

标题：

作者：

链接：

代码：

框架图：


## 背景
**k1.5 多模态思考模型**。新模型在数学、代码、多模态推理能力等方面全面对标 Open AI 满血版 o1，而且是 OpenAI 之外首个多模态 o1。尤其是 kimi-k1.5-short，成为 SOTA short cot 模型，并大幅领先 GPT-4o 和 Claude 3.5 Sonnet（提升幅度高达 550%）

在 long-CoT 模式下，Kimi k1.5 在数学、代码及多模态推理能力上，达到长思考 SOTA 模型 OpenAI o1 正式版的水平。Kimi k1.5 在 AIME 上达到 77.5 分，在 MATH 500 上达到 96.2 分，在 Codeforces 上达到 94 百分位，在 MathVista 上达到 74.9 分。

在 short-CoT 模式下，Kimi k1.5 在数学、代码、视觉多模态和通用能力上，也达到了全球范围内短思考 SOTA 模型 ，并大幅领先 GPT-4o 和 Claude 3.5 Sonnet 的水平。比如，Kimi k1.5 在 AIME 上达到 60.8 分，MATH500 上达到 94.6 分，LiveCodeBench 上达到 47.3 分。

不仅如此，从全球前沿大模型数学竞赛和编程竞赛基准测试来看，Kimi k1.5 的表现也相当不错，处于全球第一梯队，而这两项测试代表了人类智商巅峰。

![](img/Pasted%20image%2020250121135114.png)

![](img/Pasted%20image%2020250121135126.png)

_Short-CoT 是“Zero-shot Chain-of-Thought”的简称，即零样例思维链。它是一种在大语言模型中使用的提示技术，无需提供任何示例，仅通过自然语言指令引导模型进行逐步推理，从而得出答案。_

_例如，在解决数学问题时，模型会先展示出详细的推理步骤，再给出最终答案，而不是直接给出结果。_


_思维链（CoT）就像是给AI出题时，让它不仅要给出答案，还要把思考的过程写出来，就像学生做数学题要写出解题步骤一样。这样可以帮助AI更好地理解和解决问题。_

_而Long-CoT就是在思维链的基础上，让AI写出更长、更详细的思考过程。比如原本只需要写三步思考过程，现在可能要写五步、十步甚至更多，这样能让AI处理更复杂的问题，就像解决一个更难的数学题需要更多的步骤一样。_

## 相关研究




## 核心亮点

随着模型尺寸逐渐增大，预训练阶段参数 scaling up 带来的边际收益开始递减，如果想要深度提升模型推理能力和长程问题能力，基于强化学习的 Post-Training 将会成为下一个突破点，因为 scaling 强化学习为人工智能的持续进步开辟了新的维度，它使得大语言模型能够通过带有奖励的探索学习来扩展其训练数据，从而也实现计算规模的扩展。

大的方向非常明确，然而，此前发表的研究工作尚未产生具有竞争力的结果。

有鉴于此，Kimi 技术团队在 Kimi k1.5 的训练实践中全面探索了 RL 训练技术、多模态数据配方和基础设施优化。

难得的是，他们探索出的 RL 框架简单、有效，无需依赖蒙特卡洛树搜索、价值函数和过程奖励模型等更复杂的技术也能取得优异的性能。

此外，他们还提出了有效的 long2short 技术，利用 Long-CoT 技术来改进 Short-CoT 模型，使得模型在短链思维推理方面取得了最佳成果。

Kimi 技术团队设计的简单而有效的 RL 框架离不开两个关键要素：**长上下文 scaling 和改进的策略优化**。

先说长上下文 scaling。他们将强化学习的上下文窗口 scale 到 128k，并观察到随着上下文长度的增加，模型性能持续改善。新方法背后的一个关键理念是使用 partial rollout 来提高训练效率 —— 即通过重用大量以前的轨迹来采样新的轨迹，避免从头重新生成新轨迹的成本。技术团队的观察表明，上下文长度是大语言模型强化学习持续 scaling 的一个关键维度。 

再来看策略优化的改进。他们推导出了一个具有 long-CoT 的强化学习公式，并采用在线镜像下降法的变体来实现稳健的策略优化。通过有效的采样策略、长度惩罚和数据配方的优化，他们进一步改进了该算法。

  通过将这两个关键要素结合，Kimi 技术团队建立了一个用于 LLM 学习的简化强化学习框架。由于该框架能够 scale 上下文长度，学习到的 CoT 展现出规划、反思和纠正的特性。增加的上下文长度具有增加搜索步骤数量的效果。因此，他们表明无需依赖蒙特卡洛树搜索、价值函数和过程奖励模型等更复杂的技术也能实现强大的性能。 

此外，他们的模型还在文本和视觉数据上进行了联合训练，具备对这两种模态进行联合推理的能力。 


## 方法

Kimi k1.5的开发由几个阶段组成：预训练、普通监督微调（SFT）、长CoT监督微调和强化学习（RL）。

### RL Prompt Set Curation

通过我们的初步实验，我们发现强化学习提示集的质量和多样性对于确保强化学习的有效性起着至关重要的作用。构造良好的提示集不仅可以引导模型进行稳健的推理，还可以降低奖励黑客和过度拟合表面模式的风险。具体来说，三个关键属性定义了高质量的 RL 提示集

- 覆盖面多样化：提示应涵盖广泛的学科，例如 STEM、编码和一般推理，以增强模型的适应性并确保在不同领域的广泛适用性。  
- 难度平衡：提示集应包括一系列分布均匀的简单、中等和困难的问题，以促进逐步学习并防止过度适应特定的复杂程度。  
- 准确的可评估性：提示应允许验证者进行客观可靠的评估，确保模型性能是基于正确的推理而不是肤浅的模式或随机猜测来衡量的。

为了在提示集中实现多样化的覆盖范围，我们采用自动过滤器来选择需要丰富推理且易于评估的问题。我们的数据集包含来自各个领域的问题，例如 STEM 领域、竞赛和一般推理任务，包含纯文本和图像文本问答数据。此外，我们开发了一个标签系统，按领域和学科对提示进行分类，确保不同学科领域的均衡代表性（M. Li 等人，2023 年；W. Liu 等人，2023 年）。

我们采用基于模型的方法，利用模型自身的能力来自适应评估每个提示的难度。具体来说，对于每个提示，SFT 模型都会使用相对较高的采样温度生成十次答案。然后计算通过率并用作提示难度的代理——通过率越低，难度越高。这种方法可以使难度评估与模型的内在功能保持一致，从而使其对于强化学习训练非常有效。通过利用这种方法，我们可以预过滤大多数琐碎的情况，并在强化学习训练期间轻松探索不同的采样策略。

为了避免潜在的奖励黑客攻击（Everitt et al. 2021；Pan et al. 2022），我们需要确保每个提示的推理过程和最终答案都能被准确验证。经验观察表明，一些复杂的推理问题可能有相对简单且容易猜测的答案，从而导致误报验证——模型通过不正确的推理过程得出正确的答案。为了解决这个问题，我们排除了容易出现此类错误的问题，例如多项选择题、对/错题和基于证明的问题。此外，对于一般的问答任务，我们提出了一种简单而有效的方法来识别和删除易于破解的提示。具体来说，我们提示模型在没有任何 CoT 推理步骤的情况下猜测潜在的答案。如果模型在 N 次尝试内预测出正确答案，则该提示被认为太容易破解并被删除。我们发现设置 N = 8 可以删除大多数易于破解的提示。开发更先进的验证模型仍然是未来研究的开放方向。

在做好数据清洗之后，还需要关注数据构成，为了提高模型在现实世界中的图像推理能力，并实现视觉输入与大语言模型（LLMs）之间更有效的对齐，数据主要来源于三个不同的类别：现实世界数据、合成视觉推理数据和文本渲染数据。
- 首先，现实世界数据，涵盖各种年级水平的科学问题，这些问题需要图形理解和推理；需要视觉感知和推断的位置猜测任务；以及涉及理解复杂图表等类型的数据分析。这些数据集提高了模型在现实世界场景中进行视觉推理的能力。
- 其次，合成视觉推理数据，人为生成数据，包括程序创建的图像和场景，旨在提高特定的视觉推理技能，如理解空间关系、几何图案和物体互动。这些合成数据集为测试模型视觉推理能力提供了一个受控环境，并提供无限量的训练示例。
- 最后，文本渲染数据，通过将文本内容转换为视觉格式来创建，使模型能够在处理不同模态的基于文本的查询时保持一致性。通过将文本文档、代码片段和结构化数据转换成图像，确保模型无论输入是纯文本还是渲染成图像的文本（如截图或照片），都能提供一致的响应。这也有助于提升模型处理文本密集型图像的能力。

### Long-CoT Supervised Fine-Tuning

借助精炼的 RL 提示集，我们采用提示工程构建了一个小型但高质量的长 CoT 预热数据集，其中包含经过准确验证的文本和图像输入的推理路径。这种方法类似于拒绝采样 (RS)，但侧重于通过即时工程生成长 CoT 推理路径。由此产生的预热数据集旨在封装对类人推理至关重要的关键认知过程，例如规划，其中模型系统地概述了执行之前的步骤；评估，涉及对中间步骤的严格评估；反思，使模型能够重新考虑和完善其方法；和探索，鼓励考虑替代解决方案。通过在此预热数据集上执行轻量级 SFT，我们有效地启动了模型以内化这些推理策略。因此，经过微调的长 CoT 模型在生成更详细和逻辑连贯的响应方面表现出更高的能力，从而增强了其在不同推理任务中的性能。

### Reinforcement Learning

因此，我们考虑训练模型以通过强化学习 (RL) 生成 CoT (OpenAI 2024)。令 r 为奖励模型，通过分配值 r(x, y, y*) ∈ {0, 1}，根据基本事实 y* 来证明给定问题 x 的建议答案 y 的正确性。对于可验证的问题，奖励直接由预定义的标准或规则确定。例如，在编码问题中，我们评估答案是否通过测试用例。对于自由形式的基本事实问题，我们训练一个奖励模型 r(x, y, y*) 来预测答案是否与基本事实匹配。给定问题 x，模型 πθ 通过采样过程 z ∼ πθ(·|x), y ∼ πθ(·|x, z) 生成 CoT 和最终答案。生成的 CoT 的质量通过是否能够得出正确的最终答案来评估。综上，我们认为优化政策的目标如下

![](img/Pasted%20image%2020250121150657.png)

通过扩大 RL 训练，我们的目标是训练一个模型，该模型能够利用简单的基于提示的 CoT 和规划增强的 CoT 的优势。该模型在推理过程中仍然自动回归采样语言序列，从而避免了部署过程中高级规划算法所需的复杂并行化的需要。然而，与简单的基于提示的方法的一个关键区别在于，模型不应仅仅遵循一系列推理步骤。相反，它还应该通过利用整套探索的想法作为上下文信息来学习关键的规划技能，包括错误识别、回溯和解决方案细化。


#### Policy Optimization

![](img/Pasted%20image%2020250121154530.png)

我们在我们的培训系统中排除了价值网络，该网络也在之前的研究中得到了利用（Ahmadian 等人，2024）。虽然这种设计选择显着提高了训练效率，但我们还假设经典强化学习中传统使用价值函数进行学分分配可能不适合我们的背景。考虑这样一个场景：模型生成了部分 CoT (z1, z2, ..., zt)，并且有两个潜在的下一步推理步骤：zt+1 和 zt′+1。  假设zt+1直接得出正确答案，而zt′+1包含一些错误。如果预言机值函数可访问，则表明 zt+1 与 zt'+1 相比保留了更高的值。根据标准的信用分配原则，选择zt′+1将会受到惩罚，因为它相对于当前的政策具有负面优势。  然而，探索 zt′+1 对于训练模型生成长 CoT 非常有价值。通过使用长 CoT 得出的最终答案的合理性作为奖励信号，只要成功恢复并达到正确答案，模型就可以从 zt′+1 中学习试错模式。这个例子的关键要点是，我们应该鼓励模型探索不同的推理路径，以增强其解决复杂问题的能力。这种探索性方法产生了丰富的经验，支持关键规划技能的发展。我们的主要目标并不局限于在训练问题上获得高精度，而是专注于为模型配备有效的问题解决策略，最终提高其在测试问题上的性能。

#### Length Penalty

我们观察到一个过度思考的现象，即在 RL 训练期间模型的响应长度显着增加。虽然这会带来更好的性能，但过长的推理过程在训练和推理过程中成本高昂，而且过度思考往往不是人类所喜欢的。针对这个问题，我们引入了长度奖励来抑制token长度的快速增长，从而提高模型的token效率

![](img/Pasted%20image%2020250121154917.png)

本质上，我们提倡较短的答案并惩罚正确答案中的较长答案，同时明确惩罚错误答案的较长答案。然后，将此基于长度的奖励添加到带有权重参数的原始奖励中。

在我们的初步实验中，长度惩罚可能会减慢初始阶段的训练速度。为了缓解这个问题，我们建议在训练期间逐渐预热长度惩罚。具体来说，我们采用没有长度惩罚的标准策略优化，然后对其余训练进行恒定的长度惩罚。

#### Sampling Strategies

尽管强化学习算法本身具有相对较好的采样特性（更困难的问题提供更大的梯度），但其训练效率有限。因此，一些明确定义的先前采样方法可以产生潜在的更大的性能增益。我们利用多个信号来进一步改进采样策略。首先，我们收集的强化学习训练数据自然带有不同的难度标签。例如，数学竞赛题比小学数学题更难。其次，由于强化学习训练过程会对同一问题进行多次采样，因此我们还可以跟踪每个单独问题的成功率作为难度指标。我们提出了两种采样方法来利用这些先验来提高训练效率。

课程抽样：我们从较简单的任务开始进行培训，然后逐渐进展到更具挑战性的任务。由于初始强化学习模型的性能有限，在非常困难的问题上花费有限的计算预算通常会产生很少的正确样本，从而导致训练效率较低。同时，我们收集的数据自然包含等级和难度标签，使得基于难度的采样成为提高训练效率的直观有效的方法。  

优先抽样：除了课程抽样之外，我们还使用优先抽样策略来关注模型表现不佳的问题。我们跟踪每个问题 i 的成功率 si ，并对与 1 − si 成比例的问题进行抽样，以便成功率较低的问题获得较高的抽样概率。这将模型的努力引向其最薄弱的领域，从而实现更快的学习和更好的整体性能。

#### Reward Modeling for Math

- Classic RM：受到 InstructGPT（Ouyang et al. 2022）方法的启发，我们实现了基于价值头的奖励模型，并收集了大约 80 万个数据点进行微调。将“问题”、“参考答案”和“响应”作为输入，并输出一个标量来指示响应是否正确。
- Chain-of-Thought RM：最近的研究（Ankner 等人，2024 年；McAleese 等人，2024 年）表明，通过思想链 (CoT) 推理增强的奖励模型可以显着优于经典方法，特别是在数学等需要细致入微的正确性标准的任务上。因此，我们收集了约 80 万个 CoT 标记示例的同样大的数据集来微调 Kimi 模型。基于与 Classic RM 相同的输入，思想链方法在以 JSON 格式提供最终正确性判断之前显式生成逐步推理过程，从而实现更强大和可解释的奖励信号。

在我们的手动抽查中，Classic RM 的准确度约为 84.4，而 Chain-ofThought RM 的准确度达到 98.5。在RL训练过程中，我们采用了Chain-of-Thought RM来确保更正确的反馈。

#### Vision Data

为了提高模型的真实世界图像推理能力并实现视觉输入和大语言模型 (LLM) 之间更有效的对齐，我们的视觉强化学习 (Vision RL) 数据主要来自三个不同的类别：真实世界数据、综合视觉推理数据和文本渲染数据。

### Long2short: Context Compression for Short-CoT Models

尽管 long-CoT 模型在性能上表现出色，但与标准的 short-CoT LLM 相比，它在测试时消耗的 token 数量更多。然而，Kimi 技术团队发现将 long-CoT 模型的思维先验迁移到 short-CoT 模型中是可能的，从而在有限的测试 token 预算下提升性能。

他们提出了几种解决这一 long2short 问题的方法，包括模型融合、最短拒绝采样、DPO 以及 long2short RL。以下是这些方法的详细描述：

- 模型融合。团队人员发现模型融合（Model Merging）有助于保持模型的泛化能力。他们还发现，在融合 long-CoT 模型和 short-CoT 模型时，模型融合也能有效提升 token 效率。这种方法通过将 long-CoT 模型与 short-CoT 模型结合，从而在不进行训练的情况下获得一个新模型。具体来说，他们通过简单地平均两个模型的权重来实现融合。

- 最短拒绝采样。研究者观察到，模型在回答相同问题时生成的响应长度存在较大差异。基于此，他们设计了最短拒绝采样（Shortest Rejection Sampling）方法。该方法对同一个问题采样 n 次（实验中，n=8），并选择最短的正确响应进行监督微调。

- DPO。与最短拒绝采样类似，团队人员利用 Long CoT 模型生成多个响应样本。并选择最短的正确解决方案作为正样本，而较长的响应则被视为负样本，包括错误的较长响应和正确的较长响应（比正样本长1.5倍）。这些正负样本对构成了用于 DPO 训练的成对偏好数据。

- Long2short RL。在标准的 RL 训练阶段之后，团队人员选择一个在性能和 token 效率之间达到最佳平衡的模型作为基础模型，并进行单独的 long2short RL 训练阶段。在这个第二阶段中，他们还应用了长度惩罚机制，从而显著减少最大 rollout 长度，以进一步惩罚那些超出期望长度但可能正确的响应。

![](img/Pasted%20image%2020250121161707.png)

### Other Training Details

#### 预训练

Kimi k1.5 基础模型在多样化、高质量的多模态语料库上进行训练。语言数据涵盖英语、汉语、代码、数学推理、知识五个领域。多模态数据，包括字幕、图像文本交错、OCR、知识和 QA 数据集，使我们的模型能够获得视觉语言能力。严格的质量控制确保整个预训练数据集的相关性、多样性和平衡性。我们的预训练分三个阶段进行：（1）视觉语言预训练，建立强大的语言基础，然后逐步进行多模态集成； (2) Cooldown，它使用精选和合成数据来巩固能力，特别是推理和基于知识的任务； (3) 长上下文激活，将序列处理扩展到 131,072 个标记。

在预训练数据集上，设计了涵盖了五个领域英语、中文、代码、数学与推理以及知识数据的数据集合，这可以保证多样性。

如何保证高质量？这就少不了清洗。对于开发的英语和中文文本数据，建立了一个多维质量过滤框架，结合多种评分方法来减少个别偏见，每篇文档的最终质量分数是这些个别分数的组合，以确保全面的质量评估。例如，基于规则的过滤（实施特定领域的启发式规则来移除问题内容，包括重复内容、机器翻译文本以及低质量的网页抓取内容。还过滤掉含有过多特殊字符、不寻常的格式或垃圾邮件模式的文档）；基于FastText的分类、基于嵌入的相似性分析、基于大模型的质量评估。最后。最后，在训练侧，实施动态采样率，在训练期间对高质量文档进行过采样，而对低质量文档进行欠采样。

针对高质量，其实还有个重要的点，因为对于推理这类数据，总会包含很多数学公式、符号等，这些其实并不好处理。尤其是其中的知识库数据，包括学术练习、教科书、研究论文和其他一般教育文献。这些材料中相当一部分需要通过OCR处理，所以，这块是不是又有了文档智能的影子！kimi 1.5开发了专有模型来优化对学术内容的处理，特别是处理数学公式和特殊符号，这个显得十分珍贵，可以保证最大化数学数据的召回率。

对于数据配比，也就是数据采样，则很值得说道，在训练数据阶段，最终数据混合及其各自的权重是通过对较小模型进行的消融研究确定的。

那么，采样也得有依据，如何采？总得有锚点？所以有趣的点又来了，Kimi 1.5采用内部语言模型为文档添加多维标签，包括OCR质量指标（用于评估识别准确性，通过OCR质量阈值进行过滤）；教育价值指标（衡量教学相关性，通过评分系统仔细评估每篇文档的教育价值，优先考虑教学相关性和知识深度高的文档）；文档类型分类（例如，练习、理论材料）。

#### SFT

我们创建了涵盖多个领域的普通 SFT 语料库。对于非推理任务，包括问答、写作和文本处理，我们首先通过人工注释构建种子数据集。该种子数据集用于训练种子模型。随后，我们收集各种提示并使用种子模型为每个提示生成多个响应。然后，注释者对这些响应进行排名，并优化排名靠前的响应以生成最终版本。对于数学和编码问题等推理任务，基于规则和基于奖励建模的验证比人类判断更准确、更高效，我们利用拒绝采样来扩展 SFT 数据集。

我们的普通 SFT 数据集包含大约 100 万个文本示例。具体来说，500k 个示例用于一般问答，200k 个用于编码，200k 个用于数学和科学，5k 个用于创意写作，20k 个用于长上下文任务，例如摘要、doc-qa、翻译和写作。此外，我们构建了 100 万个文本视觉示例，涵盖各种类别，包括图表解释、OCR、基于图像的对话、视觉编码、视觉推理以及视觉辅助的数学/科学问题。

我们首先以 32k 令牌的序列长度训练模型 1 个epoch，然后以 128k 令牌的序列长度训练另一个epoch。在第一阶段（32k）中，学习率从 2 × 10−5 衰减到 2 × 10−6，然后在第二阶段（128k）中重新预热到 1 × 10−5，最后衰减到 1 × 10 −6。为了提高训练效率，我们将多个训练样本打包到每个训练序列中。

### RL Infrastructure

![](img/Pasted%20image%2020250121161100.png)

如图 3a 所示的强化学习训练系统通过迭代同步方法运行，每次迭代都包含一个rollout阶段和一个训练阶段。在推出阶段，推出工作人员在中央主机的协调下，通过与模型交互来生成推出轨迹，生成对各种输入的响应序列。然后，这些轨迹被存储在重播缓冲区中，这通过破坏时间相关性来确保训练数据集的多样性和公正性。在随后的培训阶段，培训人员将获取这些经验来更新模型的权重。这个循环过程使模型能够不断地从其行为中学习，随着时间的推移调整其策略以提高性能。


## 未来方向



## 主要收获


## 参考资料

[满血归来！一文全面揭秘Kimi 1.5最新推理模型背后的技术](https://mp.weixin.qq.com/s/8Qfe97Gf53p6PqtZCqK_Nw)

## 附录-相关讨论

如何评价 Kimi 发布的多模态推理模型 k1.5？ - Flood Sung的回答 - 知乎
https://www.zhihu.com/question/10114790245/answer/84028353434

因为Long CoT的有效性其实在一年多前就已经知道了，Tim[@周昕宇](//www.zhihu.com/people/0224c265bcb1f3ce5e711352fa64d547)很早就验证过 使用很小的模型训练模型做几十位的加减乘除运算，将细粒度的运算过程合成出来变成很长的CoT数据做SFT，就可以获得非常好的效果。我依然记得当时看到那个效果的震撼。我们意识到Long Context的重要性，所以率先考虑把Context搞长，但却对Long CoT这件事情不够重视。其实主要还是考虑了成本问题。Long Context主要做的是长文本输入，有Prefill，有Mooncake加持，成本速度可控，而Long CoT是长文本输出，成本高很多，速度也要慢很多。在这种情况下，把输出搞长就没有成为一个高优选项。

但是，还有什么比Performance更重要呢？成本速度有摩尔定律加持，可以不断下降，只要把performance搞上去，剩下的都不是主要问题。

所以，我们得搞Long CoT，搞o1。

但具体要怎么搞呢？

我们需要先收集一些信息，来反推o1会是怎么做的？RL-LLM会是怎么做的？（对，这里触发了我的搜索API ）

首先我们观察o1官网上的一些例子，我们发现很明显的一些特征：

- o1可以**犯错**！！！Long CoT和以前的CoT不一样。
- o1 往往会反复的反思再尝试，有各种but, wait,...
- o1的思考方法则不局限，可以重述问题，可以联想，可以分治。。。

然后，两个重要的openai的视频出来了，分别是Noam Brown和Hyung Won Chung的：

[https://youtu.be/eaAonE58sLU?si=TUlOyuYF4SkicSrK​youtu.be/eaAonE58sLU?si=TUlOyuYF4SkicSrK](https://link.zhihu.com/?target=https%3A//youtu.be/eaAonE58sLU%3Fsi%3DTUlOyuYF4SkicSrK)

[https://youtu.be/kYWUEV_e2ss?si=bijCga4Xz8jSaqMA​youtu.be/kYWUEV_e2ss?si=bijCga4Xz8jSaqMA](https://link.zhihu.com/?target=https%3A//youtu.be/kYWUEV_e2ss%3Fsi%3DbijCga4Xz8jSaqMA)

我观看了不止一遍，因为他们这些视频并不是近期拍的，但却要等到o1发布了才发出来，说明他们的talk和o1的实现是紧密相连的。

其中，有两张ppt我觉得是至关重要的：

![](https://pic1.zhimg.com/50/v2-e853fbe6822d95bbeca8ad8bf5c57689_720w.jpg?source=2c26e567)

Noam Brown这张，他告诉了我们Test-Time Search的重要性，确实回想起之前的AlphaGo是这么一回事。很多人以为Noam Brown是告诉大家要把AlphaGo的MCTS用到LLM上，那么其实不是，只是说Search很重要。至于应用到LLM上，那么就是：

> 我们需要让模型能够**自行**搜索！

这也让我重新看了一下Richard Sutton的The Bitter Lesson:

[https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf​www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf](https://link.zhihu.com/?target=https%3A//www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf)

![](https://pic1.zhimg.com/50/v2-dc022ffc075b31cca34f2a4b8edcabd9_720w.jpg?source=2c26e567)

Sutton早就揭示了一切呀。

接下来就是Hyung Won Chung的Don't Teach, Incentivize。这个带来的启发就更大了：

![](https://picx.zhimg.com/50/v2-42e8c8432d1986ed53eaed2b3fb6884c_720w.jpg?source=2c26e567)

Hyung Won Chung 为什么特别强调这个呢？Structure，什么是Structure？

MCTS是一种structure，A* 是一种Structure。当我们人为加了点[inductive bias](https://zhida.zhihu.com/search?content_id=709920330&content_type=Answer&match_order=1&q=inductive+bias&zhida_source=entity)进来强求llm按我们要求的格式进行思考，那么这种思考就是结构化的。而基于我们前面对o1的观察，我们就可以把两件事联系在一起了：

> o1没有限制模型如何思考！

这点非常非常关键！因此我们已经可以基于此排除一些带structure的工作了，MCTS就没必要探索，因为一定不work。

特别的：

现在的各种Agentic Workflow就是各种带structure的东西，它一定会限制模型的能力。

所以，其实我们还可以直接得出一个结论：

> Agentic Workflow只有短期价值，没有长期价值！早晚会被模型本身能力取代掉。

All in All 我们就是要训练模型能够像我们人一样思考，自由的思考！

然后，Noam Brown还有一张ppt也蛮重要：

![](https://picx.zhimg.com/50/v2-c757ddad625bf4f6ca4a19fd786cad0c_720w.jpg?source=2c26e567)

这个非常直接的告诉我们：

> 去做有精确Reward的RL！不要被Reward Model 给限制住了。

之前RLHF相信做过的同学都知道这是一件非常麻烦的事情，因为Human Preference并无法精准的建模，训的Reward Model又很容易reward hacking，lilian weng都专门写了个blog来讲这件事：

[https://lilianweng.github.io/posts/2024-11-28-reward-hacking/​lilianweng.github.io/posts/2024-11-28-reward-hacking/](https://link.zhihu.com/?target=https%3A//lilianweng.github.io/posts/2024-11-28-reward-hacking/)

所以，如果reward都不准确的话，那么要做RL就很难work。RL的performance完全取决于reward。

这不禁就回想起以前做robot locomotion，game ai的时候，一堆的[reward shaping](https://zhida.zhihu.com/search?content_id=709920330&content_type=Answer&match_order=1&q=reward+shaping&zhida_source=entity)。

信息到这里就很明显了：

> 做题，做题，还是做题！做有标准答案的题，刷起来！

math和code就是很直接的两个有标准答案的方向，openai也是往这上面刷，那么我们当然就往上面刷呗。

怕context太长，导致遗忘，我们在这里确认一下前面得到的关键信息：

- **要训练llm通过RL做题，有精确reward**
- **不要采取结构化的方法，最终会限制住模型的效果，要让模型自己探索思考范式。**
- **思考包含了搜索过程，允许犯错。**

好了，接下来我们需要解决如何用RL训练LLM通过Long CoT做题这件事。

很多同学可能会想： 那就用PPO呗！

问题没有这么简单。Long CoT对问题的建模产生了很大的变化。
  
### o1 实际上是 In Context RL with Self-Critique

o1 实际上是把in context rl的完整trajectory当做一条message给训进去了。  
  
如下面的code:

![](https://pica.zhimg.com/50/v2-f9b5fd5ccec90e1b144291f7564db859_720w.jpg?source=2c26e567)

什么是In Context RL呢？

[In-context Reinforcement Learning with Algorithm Distillation​arxiv.org/abs/2210.14215

不了解的同学可以看这边，简单的说就是模型做next token prediction的过程本身就是一个rl探索过程。

模型做一个题，在Long CoT下，实际上在做什么呢？

实际上也在学习做这个题，它输出的整个轨迹就是：

s1,a1,r1,a2,r2,a3,r3,.....

其中a可以认为是一种解决方法，可以看做action，而r就是reward，但这里是模型自己做reflection得到的reward，所以我说它是自带critic/world model的in context rl。

最近有相关blog也终于说了这件事，大家也是可以看看：

[https://blog.ml.cmu.edu/2025/01/08/optimizing-llm-test-time-compute-involves-solving-a-meta-rl-problem/​blog.ml.cmu.edu/2025/01/08/optimizing-llm-test-time-compute-involves-solving-a-meta-rl-problem/](https://link.zhihu.com/?target=https%3A//blog.ml.cmu.edu/2025/01/08/optimizing-llm-test-time-compute-involves-solving-a-meta-rl-problem/)

但它没有提到self-critique这个问题。

所以这个事情变复杂了呀。

如果把long cot输出建模成in context rl with self-critique，那么我们怎么优化它呢？

首当其冲的问题就是每一句话的value是多少？

然后你会发现这个value已经非常难以估计了。

举个非常简单的例子：计算1+1=？

然后模型输出 1+1=3，不对，1+1=4呢？也不对，因为4-1=3。那么1+1=1呢？ 不对，因1不等于1-1=0..... 哦对了，左手有一颗糖，右手也有一颗糖，左手的糖放到右手，那么我右手有两颗糖，我知道了，1+1=2

你会发现：

> 如果模型不会反思，那么犯错了就是错的，value就会是负值。但如果模型会反思，那么只要知错能改，最终是对的，那么这些错误都不算错误。value就不应该为负。

由此，我们进入了Value的混沌态，Value很难估计了。

这让我想到了人生：

**o1 即 人生。人生就是一条有限的串行轨迹，各种探索，各种犯错，然而除了杀人放火你无法评价某种犯错/挫折的价值会是好还是坏（比如steve jobs可以被自己创立的公司解雇），最终的结局都取决于自己的目标。**

所以，我们可以从逻辑上判断**我们不要训value，不要搞prm了，因为不会准的。**

**所以，看起来用rl训llm通过Long CoT做题这个问题变简单了：**

> **不管模型中间做错了什么，只要不是重复的，那么最后模型做对了，我们就认为这是一个好的探索，值得鼓励。反之，如果模型一顿探索，最后做错了，那么再努力也是错，要惩罚。**

我们把问题变成了Contextual Bandit的问题。那么我们就可以用REINFORCE的变种来做啦。

![](https://picx.zhimg.com/50/v2-a380844f7f9998a3af794e5e6834a6c9_720w.jpg?source=2c26e567)

上面这就是最基本的REINFORCE的公式，简单的说就是做对加梯度，做错减梯度。当然，我们需要让训练更稳定，所以可以加KL，加reward的[normalization](https://zhida.zhihu.com/search?content_id=709920330&content_type=Answer&match_order=1&q=normalization&zhida_source=entity)等等一些小trick，具体可以看下paper。但基本的思想就是这么简单。

有了基本思路，那么剩下的就是实操了。但这里其实还有一个重要问题：

**Long CoT是如何变长的？这可能是最关键的事情。**

惊喜的是在我们实际训练的过程中，我们有了重要的发现：

> 模型会随着训练提升performance也不断增加token数！

也就是这是RL训练过程中模型可以自己涌现的！

**Fantastic！**

这个和友商Deepseek的发现几乎是一样的。看到他们直接做了zero-sft的RL，也是挺impressive！









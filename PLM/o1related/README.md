---
title: README
created: 2025-02-04
tags:
  - o1-related
---
 


deepseek 和 kimi 的核心思路是一样的：**关注推理的中间过程是否正确无法实现**，所以只能 rule-based reward，最起码 reward 一定是准的！这和 alpha 系列的核心思想很相近，结果至上。

deepseek 反驳 prm 路线的三个理由是：

- 定义一个 fine-grain step 很困难；
- 很难确定一个 step 是否正确，机器标不准，人标无法 scaling up；
- 一旦 PRM 被引入，不可避免的 reward hacking，且训练资源耗费会更多。

这里，我最认同的是第二点：无法 scaling。假设我们能雇博士生标 10W 条 cot 高质量数据，但能标 100W 条吗？1000W 条呢？就像 scaling law 表达的一样，想让模达到新的效果，需要的数据量级往往是指数增长的。但保不齐以后真的有 scaling prm 数据的方案了，现在一杆子打死为时尚早，也许小模型，或者冷启动用它更好呢？

回归话题，虽然殊途同归，但两个学霸的具体实现方案还是有些差别的。

学霸 D 的想法：把 o1 的训练分为两阶段：step1 学推理，step2 学说话

- 训 zero 的 step1：全程无标注数据的参与，就是认准了一个目标：让模型的 reward 变高。这个阶段别和我谈模型格式错误逻辑混乱这种细节，我不看模型表现，只看 reward。只不过 reward 变高的过程中，发现模型的输出越来越长了，反思能力也自己涌现出来了；
- 基于 zero 训 R1 的 step2：就像是我们做普通的 post training 了，sft 没有被抛弃，除了rule-based reward，reward_model 也被请回来了，reject sampling 也出手了。

学霸 K 的想法：我还是一步到位吧，在 step1 学推理的过程中，要时刻监控着模型的说话能力是否还正常。为了达到此目标，模型的输出长度，模型对每一个 prompt 的回答准确率等信息，全程都被严格监控。

如果没有资源去做 zero 的话，学霸 K 的很多技巧其实更加实用，它分享了很多防止训崩的细节。学霸 D 在 step2 阶段的训练过程中，除了有千条冷启动数据 ，60W 拒绝采样数据，20W 条非推理数据外，其他细节都属于是完全没提的状态。

> 我太能理解学霸 K 了，我为啥不敢 rule-based reward 一条路走到黑？不就是因为我一训就崩，输出长度崩，performacne 崩，输出格式崩。我崩的时候会自我怀疑是不是选错方案了，学霸 K 崩了则是通过加训练技巧、改 loss 给救回来了。反观学霸 D，他的思路真的太超前太有魄力了， 别去在乎这些细节，二阶段集中解决。


## 参考资料

大道至简的 o1 - ybq的文章 - 知乎
https://zhuanlan.zhihu.com/p/19838650037


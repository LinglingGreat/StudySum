---
title: DAPO
created: 2025-04-08
tags:
  - o1-related
type: 论文
papername: 
conference: ACL/EMNLP/NAACL/EACL/COLING/SIGIR/AAAI/ICML/ICDM
year: 2025
institution:
  - 清华
  - 字节
---

## 论文基本信息

标题：DAPO: An Open-Source LLM Reinforcement Learning System at Scale

作者：

链接：

代码：https://dapo-sia.github.io/

框架图：

![](img/Pasted%20image%2020250408201108.png)
## 背景

朴素 GRPO 基线存在几个关键问题，例如熵崩溃、奖励噪声和训练不稳定。

![](img/Pasted%20image%2020250408201218.png)


在 RLHF 场景 [23] 中，RL 的目标是在不偏离初始模型太远的情况下对齐模型行为。然而，在训练 long-CoT 推理模型期间，模型分布可能与初始模型有很大差异，因此这种限制是不必要的。因此，我们将从我们提出的算法中排除 KL 项。

另外，采取Rule-based Reward Modeling。
## 相关研究



## 核心亮点

![](img/Pasted%20image%2020250408201416.png)

![](img/Pasted%20image%2020250408201430.png)

### Raise the Ceiling: Clip-Higher

在我们使用朴素 PPO [21] 或 GRPO [38] 的初始实验中，我们观察到了熵崩溃现象：策略的熵随着训练的进行而迅速降低（图 2b）。某些组的抽样响应往往几乎相同。这表明有限的探索和早期确定性策略，这可能会阻碍扩展过程。

我们提出了 Clip-Higher 策略来解决这个问题。Clipped Proximal Policy Optimization （PPO-Clip） [21] 中引入了对重要性采样率的裁剪，以限制信任区域并增强 RL 的稳定性。我们发现上部剪辑可以限制对策略的探索。在这种情况下，使 “exploitation token ”的可能性更大，而不是提高不太可能的 “exploitation token ”的可能性要容易得多。


![](img/Pasted%20image%2020250408201155.png)

当\($\varepsilon$ = 0.2\)时，如何影响不同概率的行动。  
  
1. **概率更新的限制**：对于初始概率较低的行动（例如0.01），更新后的最大概率是0.012；而对于初始概率较高的行动（例如0.9），更新后的最大概率是1.08。这表明高概率的行动在更新时受到的限制较小。  
  
2. **低概率行动的挑战**：对于低概率的行动，要显著增加其概率会更困难，因为更新受到更严格的限制。  
  
3. **经验观察**：通过实验观察，剪辑后的最大概率大约是0.2。这说明上限剪辑确实限制了低概率行动的概率增长。  
  
4. **影响系统多样性**：由于低概率行动的增长受到限制，系统的多样性可能因此受到约束。  
  
这段分析表明，在这种概率更新机制下，高概率的行动更容易增长，而低概率的行动则受到更多限制，可能影响系统的整体表现和多样性。

![](img/Pasted%20image%2020250408201702.png)

### The More the Merrier: Dynamic Sampling

![](img/Pasted%20image%2020250408201750.png)

现有的 RL 算法在一些提示的准确率等于 1 时存在梯度递减问题。例如，对于 GRPO，如果特定提示的所有输出 {oi}G i=1 都是正确的，并且获得相同的奖励 1，则该组的最终优势为零。零优势导致策略更新没有梯度，从而降低样本效率。从经验上看，准确度等于 1 的样本数量继续增加，如图 3b 所示。这意味着每个批次中的有效提示数量不断减少，这可能导致梯度的方差更大，并抑制模型训练的梯度信号。

为此，我们建议对等式 11 中所示准确率等于 1 和 0 的提示进行过采样和过滤，使批次中的所有提示都具有有效的梯度，并保持提示数量一致。在训练之前，我们继续采样，直到该批次完全填满准确率既不为 0 也不为 1 的样本。

![](img/Pasted%20image%2020250408201826.png)

![](img/Pasted%20image%2020250408202459.png)
### Rebalancing Act: Token-Level Policy Gradient Loss

最初的 GRPO 算法采用样本级损失计算，首先按每个样本中的标记平均损失，然后汇总样本之间的损失。在这种方法中，每个样本在最终损失计算中被分配了相等的权重。然而，我们发现这种减少损失的方法在长期 CoT RL 情景中引入了一些挑战。

由于所有样本在损失计算中都分配了相同的权重，因此较长响应（包含更多分证）中的分证对整体损失的贡献可能不成比例地降低，这可能导致两个不利影响。首先，对于高质量的长样本，这种影响可能会阻碍模型学习其中的推理相关模式的能力。其次，我们观察到过长的样本通常会表现出低质量的模式，例如胡言乱语和重复的单词。因此，由于样本级损失计算无法有效地惩罚长样本中的那些不需要的模式，导致熵和响应长度的不健康增加，如图 4a 和图 4b 所示。

![](img/Pasted%20image%2020250408202254.png)

![](img/Pasted%20image%2020250408202305.png)

在此设置中，与较短的序列相比，较长的序列对整体梯度更新的影响更大。此外，从单个代币的角度来看，如果特定的生成模式可以导致奖励的增加或减少，那么无论它出现的响应长度如何，它都会被同样提示或抑制。

### Hide and Seek: Overlong Reward Shaping

在 RL 训练中，我们通常会设置生成的最大长度，超长的样本会相应地被截断。我们发现，对截断样本的奖励整形不当会引入奖励噪声并严重扰乱训练过程。

默认情况下，我们会为截断的样本分配惩罚性奖励。这种方法可能会在训练过程中引入噪声，因为合理的推理过程可能会仅仅因为其过长而受到惩罚。此类处罚可能会使模型对其推理过程的有效性产生混淆。

为了研究这种奖励噪声的影响，我们首先应用了 Overlong Filtering 策略，该策略掩盖了截断样本的损失。我们发现这种方法可以显着稳定训练并提高性能，如图 5 所示。

![](img/Pasted%20image%2020250408202356.png)

此外，我们提出了 Soft Overlong Punishment （方程 13），这是一种长度感知的惩罚机制，旨在塑造截断样本的奖励。具体来说，当响应长度超过预定义的最大值时，我们定义一个惩罚区间。在这个区间内，回应的时间越长，它受到的惩罚就越大。此惩罚被添加到原始基于规则的正确性奖励中，从而向模型发出信号以避免过长的响应。

![](img/Pasted%20image%2020250408202440.png)

## 实验

我们的数据集通过网络抓取和手动注释相结合的方式来源于 AoPS1 网站和官方比赛主页。数学数据集的答案通常有多种格式，例如表达式、公式和数字，这使得设计全面的规则来解析它们具有挑战性。为了使用规则提供准确的奖励信号并最大限度地减少公式解析器引入的错误，受 AIME 的启发，我们选择答案并将其转换为易于解析的整数。例如，如果原始答案以 a+√b c 的形式表示，我们会指示 LLM 修改问题，使预期答案变为 a + b + c。经过选择和转换，我们得到了 DAPO-Math-17K 数据集，它由 17K 个提示组成，每个提示都配对一个整数作为答案。

![](img/Pasted%20image%2020250408202541.png)

![](img/Pasted%20image%2020250408202552.png)

对于 token 级别的损失，虽然它带来的性能提升较少，但我们发现它增强了训练稳定性，使长度增加更健康。

![](img/Pasted%20image%2020250408202636.png)

![](img/Pasted%20image%2020250408202652.png)
## 未来方向



## 主要收获


## 参考资料

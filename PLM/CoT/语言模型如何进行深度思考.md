---
title: 语言模型如何进行深度思考
created: 2025-05-01
tags:
  - 李宏毅
  - reasoning模型
---

这节课主要讲深度思考的语言模型可以用什么方法打造出来。

## 什么是深度思考的语言模型

什么是深度思考的语言模型？比如GPT o1/o3/o4，DeepSeek R1，Gemini 2 Flash Thinking，Claude 3.7 Sonnet等就是深度思考的语言模型。他们会先给一个很长的思考过程，再给出答案。

![](img/Pasted%20image%2020250501153053.png)

这类模型一般会在`<think>`和`</think>`之间给出思考过程，思考过程通常会包括验证(验证自己对不对)、探索（其他方法）、规划（需要用到哪些步骤）。这个思考过程也叫推理（Reasoning），注意另一个也叫推理的名词，英文是Inference，它指的是使用一个模型。

这种Reasoning行为是Test-Time Compute的一种，Test-Time Compute是什么呢？它指的是在测试的阶段投入了更大的算力，这种投入可能会得到更好的结果。Reasoning就是投入了更多输出长度，“深度不够，长度来凑”。

![](img/Pasted%20image%2020250501153234.png)

Test-Time Compute不是一个全新的东西，AlphaGo也可以说用了Test-Time Compute。AlphaGo训练的时候会训练两个网络：policy network判断下一步怎么走，value network判断当前局面的赢的概率有多大。在测试阶段，policy network会给出下一步落子的可能性，AlphaGo会采用MCTS（蒙特卡洛树搜索）的方法决定最终落子。它会假设下一步落在某个点，预测接下来会发生什么事，赢的概率有多大，就这样做多次假设计算出赢的概率最大的那个点，就是最终落子的点。

![](img/Pasted%20image%2020250501153302.png)

另一个概念是Test-Time Scaling，意思就是思考越多（投入的算力越多）结果越好。

这里引用一个有关棋类游戏的论文，可以选择把算力投入到训练阶段，训练更大的network，也可以选择投入到测试阶段，做更大规模的MCST。两者是可以互相成就的。图中展示了一个实验结果，分数越高越好，一条线上的分数是一样的。比如要得到-1000分，可以选择投入更多在训练阶段（横轴的值更大），也可以选择投入更多在测试阶段（纵轴的值更大）。有没有发现，其实横轴的数值要比纵轴的值大得多，意味着要达到同样的效果，投入到训练阶段的算力要远大于投入到测试阶段的算力。所以test-time scaling是个很有潜力的方向。

![](img/Pasted%20image%2020250501155042.png)

## 如何打造推理模型

有哪些方法能够打造出来这样的“推理”语言模型呢？这里介绍4个方法，前2个方法不用微调，后2个方法需要微调。

![](img/Pasted%20image%2020250501155103.png)

### CoT

第一个方法是CoT，让模型先列出解题过程再给出答案，和深度思考模型给出的输出是类似的。这个方法只适用于比较强的模型。

怎么获取CoT呢？第一种方法是Few-shot CoT，也就是给模型一些例子，让模型知道要先写过程再写结果。后来又发现了第二种方法，不需要给例子，只需要告诉模型：Let's think step by step。模型就能自动把过程列出来。

推理模型在做的事情和CoT其实非常类似，但是因为推理模型输出的思考特别长，所以叫做"Long CoT"。

![](img/Pasted%20image%2020250501155115.png)

其实只要把prompt写的更长，给模型更多更精确的指示，也许模型就能做长思考了。这种把思考的方式和流程写在prompt里面的方法，叫做**Supervised CoT**。比如下图的例子对gpt-4o做Supervised CoT：

![](img/Pasted%20image%2020250501160234.png)

模型先是分析题目，分解题目然后运算，接着给出了三个验算步骤。

![](img/Pasted%20image%2020250501160302.png)

![](img/Pasted%20image%2020250501160336.png)

但不是所有模型都有能力根据复杂的指令做长思考。需要模型比较强才行。

### 推理工作流程

第二个打造推理语言模型的方法是直接给模型推理的工作流程。比如我们希望模型做Explore，尝试各种各样的方法。如果我们直接告诉模型多尝试一些方法，它一般只尝试几种方法就结束了。怎么让它尝试更多方法，甚至几千几万种方法呢？可以直接让模型针对同一个问题回答几千几万次，因此模型输出是随机的，所以每次回答都不太一样。

![](img/Pasted%20image%2020250501160738.png)

这个方法效果怎么样呢？有一篇论文“Large Language Monkeys"，让语言模型不断去多次尝试，提高解题的正确率。下图中横轴是模型尝试的次数，纵轴是pass@k（所有尝试次数中只要有一次对的就可以）。尝试的次数越多，总是有机会得到正确答案。

![](img/Pasted%20image%2020250501161031.png)

怎么知道这么多尝试中，哪一次是正确答案呢？

可以用投票法（Majority Vote，或者Self-consistency），选择出现次数最多的答案。

也可以看模型对答案的Confidence，也就是看答案的产生几率，几率越高代表Confidence越高。

![](img/Pasted%20image%2020250501161804.png)

Majority Vote虽然听起来很简单，但是这个方法还是挺强的。如下图所示，Llama 3.1 1B和8B的模型对比，1B尝试多次然后做Majority Vote，可以提升1B模型的效果，不过还是比不上8B的模型。

![](img/Pasted%20image%2020250501161817.png)

还可以训练一个Verifier模型，来验证一个答案是不是正确的。选出Verifier模型分数最高的那个答案。这个方法叫做Best-of-N。

![](img/Pasted%20image%2020250501161828.png)

Verifier模型可以直接用现成的语言模型，也可以用数据训练，一般训练后效果会好一些。

训练数据怎么来呢？可以先收集一些已知答案的问题，让语言模型针对这些问题给出推理过程和结果，结果和正确答案一致的输出标记为1分，不一致的输出标记为0分，这样就收集到了一批训练数据。

![](img/Pasted%20image%2020250501161839.png)

上面介绍的这种产生多个答案的方法是Parallel（并行）的方法，另一个Sequential（串行）的方法也可以产生多个答案：先输出第一种解法，再根据第一种解法解第二次，再根据第二种的解法解第三次。

![](img/Pasted%20image%2020250501161850.png)

如图，Parallel和Sequential方法也可以同时使用：

![](img/Pasted%20image%2020250501161901.png)

两个方法怎么配合比较好可以看这篇论文：[[2408.03314] Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters](https://arxiv.org/abs/2408.03314)

上述讲的这些方法都是得到答案以后再进行验证，但是现在的推理模型不是得到答案以后再进行验证，而是在中间步骤就会验证，可以提早发现问题。

![](img/Pasted%20image%2020250501161918.png)

怎么套用工作流程的方法让模型能对中间步骤进行验证呢？

首先让模型多次只输出step 1，后面的步骤先不要输出（通过在prompt中写明输出格式，用`<step>`和`</step>`将步骤括起来，输出第一个`</step>`就强行停止继续输出）。

然后用一个Process Verifier（只根据回答的其中一个步骤就能验证该步骤是否正确）验证哪一个step 1是正确的。

![](img/Pasted%20image%2020250501164141.png)

![](img/Pasted%20image%2020250501164200.png)

怎么得到Process Verifier呢？让语言模型分步骤解题，并且针对每个step，后面的step也采样生成多次，比如图中step 1，后面尝试了3种解题过程，有2个结果正确1个错误，那么就可以说从step1开始解题正确的概率是2/3。同样的方法应用到step 2中，得到step2的解题正确概率是1/3。

![](img/Pasted%20image%2020250501164419.png)

把这样的数据给Process Verifier训练，让它学习看到越有可能得到正确结果的解题步骤就给越高分。

![](img/Pasted%20image%2020250501164814.png)

由于Process Verifier给出的是一个分数，并不是正确与否的答案，所以需要设置一个阈值，选择超出阈值的步骤继续往下走。如果觉得阈值不好设置，可以采取Beam Search的方法，每次保留分数最高的N条路径，继续往下走。

![](img/Pasted%20image%2020250501165040.png)

下图中尝试了Best-of-N，Beam Search的方法。发现Best-of-N比Majority Vote效果要好，Beam Search又要更好。DVTS是Beam Search的一种变形，解决Beam Search选出的步骤都很相似的问题。但是从图中看效果也一般，没有比Beam Search好很多。

![](img/Pasted%20image%2020250501165303.png)

Beam Search还有其他的变形，比如MCTS，是现在比较流行的方法。

![](img/Pasted%20image%2020250501165325.png)

### Imitation Learning

接下来要讲的2个方法都需要微调参数，让模型具有深度思考的能力。

![](img/Pasted%20image%2020250501165343.png)

第一种方法是Imitation Learning。

训练数据中包括输入、推理过程、正确答案。让模型学习根据输入输出推理过程和正确答案。

![](img/Pasted%20image%2020250501165357.png)

怎么得到这样的数据呢？可以让模型输出推理过程和答案，当答案正确的时候，推理过程可能也是对的，就可以用这些数据去训练。

![](img/Pasted%20image%2020250501165951.png)

但是怎么保证推理过程是对的呢？rStar-Math这篇论文采取的方法是，对推理过程中的每个步骤进行验证（用Process Verifier)，只有正确的步骤才会展开进行下一步，这样最终得到答案，如果答案正确，那么可以认为这条路上的推理步骤是正确的，因为他们都是经过Process Verifier的验证的。可以作为训练数据。

![](img/Pasted%20image%2020250501170004.png)

可以用有监督学习的方法训练模型，也可以用类似强化学习的方法，告诉模型某条路/某个步骤是对的，某条路/某个步骤是错误的。

![](img/Pasted%20image%2020250501170014.png)

那么问题来了，我们需要教模型，推理过程的每个步骤都是正确的吗？现在的推理模型其实中间过程不一定是正确的，它会犯错然后找自己的问题。所以中间过程有错没关系，只要最后答案是正确的就好。

![](img/Pasted%20image%2020250501170026.png)

如果只给模型看正确的推理过程，模型就不会找自己的问题，它会认为前面的步骤都是正确的。我们应该教语言模型知错能改的能力。

![](img/Pasted%20image%2020250501170036.png)

有一个SoS的方法，它的训练数据是这样的：加入一个错误路径，然后插入verifier的回馈，表示前面的路径是错的，再尝试新的步骤，类似这样来回几次。

![](img/Pasted%20image%2020250501170051.png)

另一篇论文也提出了类似的方法，叫做Journey learning，根据树状结构制造出一个推理过程，其中包括一些错误的路径，让模型学会看到错误路径后如何转回正确路径。效果还挺好的。

![](img/Pasted%20image%2020250501171007.png)

现在也可以直接用推理模型产生推理过程和答案，得到训练数据，这就是Knowledge Distillation。

![](img/Pasted%20image%2020250501171017.png)

DeepSeek-R1论文中就用了Knowledge Distillation的方法训练Qwen和Llama，训练数据来自DeepSeek-R1，效果能够跟一些顶尖的模型相提并论。

![](img/Pasted%20image%2020250501171027.png)

### RL

最后一个方法是以结果为导向学习推理的方法，也就是DeepSeek-R1系列模型的做法。

![](img/Pasted%20image%2020250501172242.png)

训练资料是问题和正确答案，问题输入给模型，模型输出推理过程和答案，答案正确给postive的reward，不正确给postive的reward。这里面推理过程正不正确不重要，长什么样也不重要。

DeepSeek-R1-Zero就是用这种方法训练得到的。当然除了结果导向的reward之外，DeepSeek还加了一个format reward，要求输出格式需要包括think token。

![](img/Pasted%20image%2020250501172252.png)

可以看到这个方法使得DeepSeek-R1-Zero可以逼近o1的正确率。如果做Majority Vote可以进一步强化。说明这些方法本身是不冲突的，可以结合使用。

![](img/Pasted%20image%2020250501172302.png)

DeepSeek的Aha Moment：模型自行学会了发现自己的错误。

![](img/Pasted%20image%2020250501172312.png)

但是R1-Zero的推理过程不易读，且混杂了多种语言。所以它不是最终版本模型。

![](img/Pasted%20image%2020250501172324.png)

最终版本的R1是如何打造出来的呢？

首先是收集冷启动数据。一是用R1-Zero产生推理过程和答案，然后用人力去修改推理过程得到训练数据。二是用few-shot cot的方法用另一个模型产生数据，三是用prompting的方法让模型生成详细的答案，还要包括反思、验证（这个方法就是前面讲到的supervised cot）。这些数据（几千条）一起用于训练DeepSeek-v3，得到模型A。

然后是RL。模型A做RL，得到模型B。RL过程中除了结果导向的reward之外，还用了语言一致性的reward，提高易读性（虽然会牺牲一点点性能，但为了保证可读性还是这样做了）。

![](img/Pasted%20image%2020250501172334.png)

第三步继续收集数据，这次包括推理类型（比如数学、代码）的数据和非推理类型（比如写作、翻译、事实问答）的数据。

推理数据来自模型B，利用模型B产生推理过程和答案，然后用DeepSeek-v3作为verifier判断答案是否正确来做筛选。此外还用了规则去掉一些糟糕的推理过程（比如混合语言，太长，包括代码等）。这个过程产生了60w数据。

此外也用DeepSeek-v3 self-output生成20w的非推理数据。混合一共80w数据训练得到模型C，接着继续在模型C上做RL，得到DeepSeek-R1。这一步的RL除了结果导向的RL训练之外，还包括基于奖励模型的RL训练，因为非推理数据是无法用结果导向的方式训练的。

DeepSeek也尝试了process verifier和MCTS，但是没有成功。

![](img/Pasted%20image%2020250501172347.png)

R1训练数据中的推理过程没有太多人工介入，所以会发现有时候会出现奇怪的输出。比如下图中，讲话奇奇怪怪的，而且只有左括号没有右括号。

![](img/Pasted%20image%2020250501172405.png)

DeepSeek的RL方法有没有效果非常取决于Foundation Model。比如基于Qwen-32B做RL，效果就没有好很多，但是通过Imitaion Learning反而比较有效。为什么会这样呢？这是因为RL是强化原有模型的能力，RL会强化正确答案的几率，但是前提是模型能够产生正确答案。

![](img/Pasted%20image%2020250501172414.png)

所以很有可能DeepSeek-v3本身就有Aha的能力，RL只是强化了这个能力，有一些论文已经证实了这个事情。

![](img/Pasted%20image%2020250501172426.png)

## 总结

这一节课主要讲了什么是深度思考语言模型（也叫推理模型），以及有哪些打造推理模型的方法。
1. CoT。包括早期的给出示例的few-shot CoT，后来的"think step by step"的zero-shot CoT，以及在prompt里给出模型更精确的指示的supervised CoT
2. 推理工作流程法。并行的方法是同时采样模型的多个输出，然后用Majority Vote/Confidence/Best-of-N的方法去选出最可能的正确答案。串行的方法是先让模型给出一个解法，然后基于这个解法让模型给出第二种解法，以此类推。两个方法可以结合使用。不过这些都是只看最终结果的方法。另一种是在中间过程就进行验证，需要一个Process Verifier，在模型输出每一步骤后打分，然后可以用Beam Search选出前N个继续往后走，下一步也用同样的方法选出前N个继续，最终选出最好的那个。现在流行的MCTS也是Beam Search的一种变形。
3. Imitation Learning。构造（输入、推理过程、正确答案）的数据，让模型学习，可以是有监督学习（只看正确的数据），也可以是类似DPO的学习方式（正确的和错误的数据都会看）。已经有（输入，正确答案）的数据情况下，构造推理数据的方法有：
	1. 根据输入，让模型输出推理过程和答案，答案正确的推理过程可以认为也是正确的，拿来训练。
	2. 为了保证推理过程的正确性，用Process Verifier去判断每个步骤是否正确，正确的步骤才会继续往下走，直到走到正确答案，那么这条数据就可以用于训练。
	3. 为了训练模型纠错的能力，不需要每个步骤都正确，可以在路径中加入一些错误的步骤然后纠错，转回正确路径。
	4. 用已有的推理模型的数据做知识蒸馏。
4. RL。结果导向的RL，只根据结果是否正确来给出奖励，推理过程由模型自行发挥（也可以先冷启动训练模型学会推理），比较依赖于Foundation Model的能力，因为RL是强化原有模型的能力。

上述讲的这些方法其实不是完全孤立的，可以把这些方法结合起来，比如DeepSeek-R1的训练过程中就用了好几种方法（包括CoT、推理工作流程、Imitation Learning、RL）。

![](img/Pasted%20image%2020250501175450.png)

## 下期预告

下节课会讲推理模型的挑战和未来发展，推理过程特别花钱、花算力，但是有时候不需要那么冗长的推理过程，那么有没有办法缩短推理过程呢？下节课会讲。

![](img/Pasted%20image%2020250501172610.png)

比如DeepSeek-R1计算123 x 456的结果的推理过程就特别的冗长：

![](img/Pasted%20image%2020250501172501.png)

![](img/Pasted%20image%2020250501172510.png)

![](img/Pasted%20image%2020250501172519.png)

![](img/Pasted%20image%2020250501172527.png)

![](img/Pasted%20image%2020250501172537.png)

![](img/Pasted%20image%2020250501172546.png)

![](img/Pasted%20image%2020250501172557.png)

## 参考资料

[【生成式AI時代下的機器學習(2025)】第七講：DeepSeek-R1 這類大型語言模型是如何進行「深度思考」（Reasoning）的？ - YouTube](https://www.youtube.com/watch?v=bJFtcwLSNxI)







---
title: PreSelect
created: 2025-03-05
tags:
  - 数据选择
  - 预训练
type: 论文
papername: Predictive Data Selection
conference: ACL/EMNLP/NAACL/EACL/COLING/SIGIR/AAAI/ICML/ICDM
year: 2025
institution:
  - hkust
---

## 论文基本信息

标题：Predictive Data Selection: The Data That Predicts Is the Data That Teaches

作者：

链接：

代码： https://github.com/hkust-nlp/PreSelect

框架图：

![](img/Pasted%20image%2020250305192831.png)


## 背景

这篇论文旨在解决语言模型预训练中的高效数据选择问题。在语言模型预训练过程中，训练数据的质量对于模型性能有着至关重要的影响。然而，现有的大规模语料库中往往包含大量低质量的数据，这会导致训练效率低下和模型性能提升缓慢。因此，如何高效地从海量数据中筛选出对模型预训练真正有帮助的高质量数据，成为了亟待解决的问题。

具体而言，论文提出了一个名为**PRESELECT**（Predictive Data Selection）的方法，旨在直接估计数据在预训练过程中的贡献，并以高效的方式选择预训练数据。该方法的核心思想是基于数据的“预测强度”（predictive strength），即某些数据上的模型损失（normalized loss）与模型在下游任务上的表现之间的相关性，来判断这些数据对学习的有效性。通过这种方式，PRESELECT能够在不依赖大量计算资源的情况下，筛选出对模型性能提升有显著贡献的数据，从而提高预训练的效率和效果。

![](img/Pasted%20image%2020250305192621.png)


## 相关研究
### 规则基础的数据选择（Rule-based Data Selection）

- **C4 Pipeline**：由Raffel等人在2020年提出，通过文档长度、单词平均长度和URL域名等规则过滤文档。
    
- **Gopher Rules**：由Rae等人在2021年提出，同样基于文档的特征进行过滤。
    
- **RefinedWeb**：由Penedo等人在2024年提出，引入了更多启发式质量过滤器，如字符重复率、行尾标点符号的比例等。
    
- **FineWeb**：由Penedo等人在2024年提出，进一步改进了数据筛选的方法。
    

### 模型基础的数据选择（Model-based Data Selection）

- **CCNet**：由Wenzek等人在2020年提出，通过在维基百科上训练的语言模型保留低困惑度的文本。
    
- **FineWeb-Edu**：由Penedo等人在2024年提出，训练了一个BERT分类器来优先选择教育相关内容。
    
- **DsDm**：由Engstrom等人在2024年提出，使用数据影响模型来评估特定数据点对预训练的影响。
    
- **MATES**：由Yu等人在2024年提出，通过数据影响模型动态选择数据。
    
- **DCLM**：由Li等人在2024年提出，使用基于fastText的评分器，根据监督微调数据选择数据。
    

### 压缩效率与智能的关系（Compression Efficiency and Intelligence）

- **Huang et al. (2024)**：提出模型在某些原始文本上的压缩效率（即归一化损失）与它们在下游任务上的表现强烈相关，尤其是在文本数据与基准领域对齐时。
    
- **Deletang et al. (2024)**：从理论上探讨了语言建模与无损压缩之间的关系，为理解模型损失与数据质量之间的联系提供了基础。
    

### 其他相关工作

- **Thrush et al. (2024)**：提出了一种基于困惑度相关性的方法，通过计算域级别的相关性来选择数据，但该方法在文档级别上不如PRESELECT精细。
    
- **Xie et al. (2023)**：提出了一种通过重要性重采样选择数据的方法。
    
- **Wettig et al. (2024)**：旨在捕捉人类直觉，选择高质量的数据用于训练语言模型。
    
- **Yu et al. (2024)**：提出了一种基于数据影响模型的动态数据选择方法。



## 核心亮点

论文通过提出一种名为**PRESELECT**（Predictive Data Selection）的方法来解决语言模型预训练中的高效数据选择问题。该方法的核心思想是基于数据的“预测强度”（predictive strength），即某些数据上的模型损失（normalized loss）与模型在下游任务上的表现之间的相关性，来判断这些数据对学习的有效性。以下是PRESELECT方法的具体实现步骤：

### 1. 数据的预测强度（Predictive Strength of Data）

- **定义**：论文定义了数据的预测强度，用于量化模型在该数据上的损失与模型在下游任务上的表现之间的相关性。具体来说，假设有一系列预训练语言模型 {M1,M2,…,MN} 和它们在下游任务上的平均分数 {S1<S2<…<SN}。对于预训练语料库中的任何文档 d，计算每个模型在 d 上的归一化损失 {C1,C2,…,CN}，并使用以下匹配分数来衡量损失排名与任务分数排名之间的相关性：

![](img/Pasted%20image%2020250305192914.png)

上式评估了每对模型的损失排名是否与其任务分数排名成反比（因为较低的值对损失的影响越好）。当排名成反比时，分数 S 会增加，这表明该数据集上的损失有效地反映了模型在下游分数上的排名。分数为 1 意味着文档上的 loss 可以完美预测模型的下游性能排名。

### 2. 整体框架（Overall Framework）

- **采样**：从预训练语料库中随机采样一个小的子集，用于计算每个文档的预测强度。具体来说，选择语料库中频率最高的3000个域名（如wikipedia.org），并从每个域名中随机采样300个文档，总共获得900,000个样本。
    
- **模型选择**：选择一系列不同大小的预训练模型（如Llama-1和Llama-2系列中的6个模型，参数范围从7B到65B）来计算每个文档的归一化损失。
    
- **计算预测强度**：使用上述定义的匹配分数计算每个文档的预测强度。直接使用 Huang 等人（2024 年）([Compression Represents Intelligence Linearly | OpenReview](https://openreview.net/forum?id=SHMj84U5SH#discussion))的 12 个不同基准的平均分数来获得下游分数排名。
    
- **训练fastText分类器**：根据预测强度分数，选择得分最高的文档作为正样本，得分最低的文档作为负样本，训练一个fastText分类器。fastText分类器用于在大规模语料库中高效地选择数据。
    

## 实验

### 1. **预训练数据集和模型设置**

- **数据集**：使用了经过处理的**RefinedWeb**数据集，该数据集通过**resiliparse**文本提取、启发式过滤和去重处理，总共包含超过20万亿个token。分别随机采样了800亿、3000亿和1万亿个token作为400M、1B和3B模型训练的数据选择池。
    
- **模型**：主要使用了**Llama**架构的模型，分别训练了400M、1B和3B参数的模型。此外，还在C4数据集上使用了**Pythia**架构的模型进行实验，以验证方法在不同数据集和模型架构上的适用性。

### 2. **基线方法比较**

- **随机选择**（Random）：随机选择文档。
    
- **PPL过滤**（PPL Filtering）：保留低困惑度的文档。
    
- **FineWeb-Edu**：通过LLM评分教育水平并训练小评分器来选择教育相关内容。
    
- **PPL相关性**（PPL Correlation）：利用困惑度相关性计算域级别相关性并选择域。
    
- **DCLM**：使用监督微调数据作为正数据训练fastText评分器。


### 3. **评估设置**

- **下游任务**：在17个不同领域的任务上评估预训练模型的性能，包括理解、知识、数学和代码生成等任务。具体任务包括**MMLU**、**Arc-Easy**、**Arc-Challenge**、**HellaSwag**、**PIQA**、**SIQA**、**SciQ**、**RTE**、**BBH**、**LAMBADA**、**OpenBookQA**、**RACE-Middle**、**RACE-High**、**MultiRC**、**WinoGrande**等。
    
- **评估指标**：主要使用准确率（accuracy）作为评估指标，对于数学和代码生成任务，使用每字符位数（bits per character, BPC）作为评估指标。
    
### 4. **实验结果**

- **性能提升**：在1B模型上，PRESELECT在30B训练数据上的表现超过了随机选择在300B数据上的表现，平均绝对提升为5.3%，在某些任务上甚至达到了8.8%的提升。在3B模型上，PRESELECT在100B训练数据上的表现也显著优于其他基线方法。
    
- **计算效率**：PRESELECT显著减少了训练所需的计算资源。例如，在1B模型上，使用PRESELECT选择的30B数据训练的模型性能超过了使用300B数据训练的模型，实现了10倍的计算效率提升。
    
- **适应性**：PRESELECT不仅适用于不同的模型架构（如Llama和Pythia），还适用于不同的预训练语料库（如C4）。

![](img/Pasted%20image%2020250305193922.png)

![](img/Pasted%20image%2020250305193954.png)



### 5. **分析**

- **正负数据分析**：PRESELECT倾向于选择与文学、知识和代码相关的高质量数据，同时排除低质量的数据。
    
- **fastText特征贡献**：通过分析fastText分类器学习到的特征，发现PRESELECT关注的特征广泛覆盖了代码、教育和问答领域，而DCLM则更关注监督微调数据的模式。
    
- **数据分布**：PRESELECT在数据选择上具有更高的多样性和质量，避免了其他方法可能带来的数据偏差。
    
- **长度分布**：PRESELECT选择的数据在长度分布上更为均衡，避免了对特定长度数据的偏好。

![](img/Pasted%20image%2020250305194023.png)

![](img/Pasted%20image%2020250305194039.png)

![](img/Pasted%20image%2020250305194055.png)



### 6. **额外实验**

- **C4数据集上的实验**：在C4数据集上使用Pythia架构的模型进行实验，验证PRESELECT在不同数据集和模型架构上的适用性。结果显示PRESELECT在410M和1B模型上均优于随机选择和其他数据选择基线。
    
- **不同选择比例的实验**：在1B模型上，分别使用10%、30%和50%的选择比例进行实验，验证PRESELECT在不同选择比例下的性能。
    

### 7. **消融实验**

- **不同模型大小的实验**：在400M、1B和3B模型上进行实验，验证PRESELECT在不同模型大小下的性能。
    
- **不同数据集的实验**：在RefinedWeb和C4数据集上进行实验，验证PRESELECT在不同数据集上的适用性。
    

通过这些实验，论文全面验证了PRESELECT方法在提高预训练模型性能和计算效率方面的有效性。


## 未来方向
### 1. **多语言支持**

- **研究方向**：目前PRESELECT主要针对英文语料库进行研究。可以探索如何将该方法扩展到多语言环境，以支持不同语言的预训练模型。
    
- **潜在挑战**：不同语言的文本在结构、语法和语义上存在差异，可能需要调整模型损失计算和预测强度的定义，以适应不同语言的特点。
    

### 2. **跨领域适应性**

- **研究方向**：虽然PRESELECT在多个下游任务上表现良好，但可以进一步研究其在特定领域（如医疗、法律、金融等）的适应性，以及如何针对这些领域进行优化。
    
- **潜在挑战**：特定领域的数据可能具有独特的特征和模式，需要调整数据选择策略以更好地捕捉这些特征。
    

### 3. **动态数据选择**

- **研究方向**：目前PRESELECT在预训练阶段选择数据后，这些数据在整个训练过程中保持不变。可以探索动态数据选择策略，根据模型在训练过程中的表现动态调整数据选择。
    
- **潜在挑战**：动态数据选择需要实时评估模型性能，并根据评估结果调整数据选择策略，这可能会增加计算复杂度。
    

### 4. **结合其他数据质量指标**

- **研究方向**：除了基于模型损失的预测强度，还可以结合其他数据质量指标（如文本多样性、信息密度等）来进一步优化数据选择。
    
- **潜在挑战**：需要设计有效的机制来综合考虑多种数据质量指标，并平衡它们之间的关系。
    

### 5. **减少对预训练模型的依赖**

- **研究方向**：PRESELECT依赖于一系列预训练模型来计算数据的预测强度。可以探索减少对这些预训练模型的依赖，例如通过设计更轻量级的模型或使用无监督学习方法来评估数据质量。
    
- **潜在挑战**：减少对预训练模型的依赖可能会降低数据选择的准确性，需要在准确性和效率之间找到平衡。
    

### 6. **大规模数据集的扩展性**

- **研究方向**：虽然PRESELECT在大规模数据集上表现出色，但可以进一步研究如何在更大规模的数据集（如万亿级token）上高效地应用该方法。
    
- **潜在挑战**：处理更大规模的数据集需要更高效的计算和存储资源，可能需要开发新的算法和架构来支持大规模数据选择。
    

### 7. **与其他预训练技术的结合**

- **研究方向**：探索PRESELECT与其他预训练技术（如对比学习、自监督学习等）的结合，以进一步提升预训练模型的性能。
    
- **潜在挑战**：需要设计有效的结合策略，确保不同技术之间的协同作用，而不是相互抵消。
    

### 8. **对抗性数据选择**

- **研究方向**：研究如何通过对抗性方法来选择数据，以提高模型的鲁棒性和泛化能力。例如，可以引入对抗性训练来选择那些能够使模型在面对对抗性攻击时表现更好的数据。
    
- **潜在挑战**：对抗性数据选择需要设计复杂的对抗性训练机制，并确保所选数据能够有效提升模型的鲁棒性。
    

### 9. **用户反馈驱动的数据选择**

- **研究方向**：探索如何将用户反馈纳入数据选择过程，使预训练模型能够更好地满足用户需求。
    
- **潜在挑战**：需要设计有效的机制来收集和利用用户反馈，并将其与数据选择策略相结合。
    

### 10. **可解释性增强**

- **研究方向**：提高数据选择过程的可解释性，使研究人员和实践者能够更好地理解为什么某些数据被选中，而另一些数据被排除。
    
- **潜在挑战**：需要开发新的方法来解释数据选择模型的决策过程，并确保这些解释是准确和有用的。


## 主要收获


## 参考资料

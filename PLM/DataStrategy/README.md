
## 大模型微调数据选择和构造技巧

大的方向上大家都能把握，大概无非是要注意数据的多样性，要注意数据的质量，那在实践中有哪些技巧呢？

**比如我们会经常遇到下面几种情况：**

1.数据要不要都去标注，标的比较慢咋办？

2.我已经有一批标好的数据了，再去选哪些数据送标注比较好？

3.能不能总结出一套数据构造方面自动化的方法？

其实在大模型之前，就有很多人研究过这样的问题。在做一个模型时候，比如简单的文本分类，我不可能一股脑把所有数据都扔给标注，这样干存在一个问题，一般情况下我们数据的分布都是符合一个长尾分布的。主要的几个类别数据占据了90%的数据量，剩下的90%的类别只有10%的数据量。

比如[小红书](https://zhida.zhihu.com/search?content_id=240901226&content_type=Article&match_order=1&q=%E5%B0%8F%E7%BA%A2%E4%B9%A6&zhida_source=entity)上，query的意图识别里，美食，穿搭，旅游攻略类非常多，但是还有一些同学去搜大模型微调的数据技巧。

如果说我们直接采样一批线上的图文文本，直接送给标注的话，会存在一个严重的问题：他们标注的数据大部分都是攻略类，技术类比较少，标了3个月才攒了几千条大模型技术文本，但是攻略类已经成几万了。

这样搞肯定是不行的，人力成本方面的消耗是在是太大了，并且模型因为数据平衡的问题也没有特别好，我们有没有办法去优化这个过程呢？

在大模型微调里面对应的生成小红书文案场景，同样的问题也是爬来的数据就可以直接用吗？

大家都有个直观的答案，就是去重，那我们再考虑模型上数据的迭代呢？如果数据是分阶段爬去的怎么办？已经有一批人工处理的的高质量数据怎么办？

但其实从监督学习的演进来看，这套东西其实已经被研究的很多了，用一个技术名词叫 “主动学习”。

主动学习有两个基本原则，在监督训练的时候，注意主动发现数据的两个方面，一个是数据多样性，另外一个是数据的不确定性。这样讲是比较抽象的概念，那我们在大模型实践中如何体现呢？

### 数据的多样性

多样性即为数据的去重，去重这件事的核心是相似度度量，现在的相似度度量方法大家用的比较多的是基于对比学习构造的语义向量这套思路，当然简单的基于词袋或者tfidf的方案也是可以的。有了核心的相似度度量方法后，我们可以使用简单的onepass聚类方法进行过滤，考虑复杂一点的话，我们可以使用带优化目标的聚类：比如K-Center-Greedy算法，其约束条件是在最大化多样性的情况下，使指令数据集最小。

![](https://pic4.zhimg.com/v2-8d0b6c2993559c92fc34e15629caad2d_1440w.jpg)

另外，如果我们已经有了一批已经去重的人工处理过的高质量数据，那么我们如何寻找与这批数据不一样的数据呢？

这里有一个非常简单实用的方案，并且这个方案可以用在很多其他的地方。

我们简单地把已有的数据全部当成正样本打上1，然后待筛选的数据全部当成负样本打上0，我们使用deberta等构建二分类模型，并进行K-fold的交叉验证，在交叉验证过程中，选出每一个fold过程中的测试集合里概率接近于0的样本。

通过这样的操作，就能把长得与已有数据不一样的数据给选出来了，并且这个过程是半监督的。

**这套方案也可以用在很多其他地方**，比如数据质量选择，只要我们有一批已经确定标签/结果/标注的种子数据，就能通过这样的方法选出与种子数据长得比较像的，长得不像的。

### 数据的不确定性

数据的不确定性主要体现数据的质量筛选上，选取模型学的不那好的数据，模型没有把握的数据。  

最简单的，我们可以选出模型对应PPL值比较差的那批数据。如果是指令数据的话，比如大模型做题和对应的答案。我们可以把所有选项对应的概率之和计算出来，然后过滤出概率和比较低的那一批数据，这批数据就是模型“不太肯定”的样本，我们需要加强针对性的训练。

当然这样可能有一个副作用，就是这批数据是质量比较差而不是模型学的不太好的。

为此，我们还要借助reward model，这个reward model是广义的，他是一个质量的二分类模型。可以祭出我们的deberta，继续用标注数据进行做二分类，进行数据质量的判断。

有了质量打分模型后，我们就可以判断一些指令数据的质量高低，并且据此选出模型真正不确定的数据。

这个过程类似于手动的拒绝采样，核心是选择“模型不确定”+“数据质量达标”的那部分数据。

总结：监督学习中的主动学习的两个基本原则就是寻找多样性的数据、模型不确定的数据。在寻找过程中，我们使用了一些小技巧，比如聚类去重，对抗半监督过滤，自建reward二分类方法等。这几个小技巧，学术上没有什么高深莫测的东西，都是实践中总结出来的好用的方法。

**并且你把上面的过程串联起来，其实就是一套高效率，低成本的数据构造pipeline了**，不仅可以用在大模型的数据选择和构造，在所有的监督学习上，这套思路和方法都是实适用的。

## 大模型的微调数据选择技巧(二)

如果给定了一批标注数据，如何选出其中"最好的"“最适合”训练的数据子集呢？

类似于传统机器学习的特征选择，从N个特征里选出n个，提升模型效果，本身这个问题是NP难的，我们有没有什么近似的，高效的，贪心的方法能解决这个问题。大模型选数据也一样，不可能所有的组合都尝试一遍。

这篇文章《One Shot Learning as Instruction Data Prospector for Large Language Models》就提出了一个方法，可以从一堆数据中，把精华选出来，使用精华部分训练就够了，这个方法称为Nuggets。

具体是怎么做的可以参考这个图。

  

![](https://pic3.zhimg.com/v2-0585f6abde5a11f47854353cd429d3e2_1440w.jpg)

  

Nuggets看着挺复杂的，其实很简单，首先Nuggets有三个输入，一个输出。

三个输入分别是：第一，一个大模型，用来评估数据好坏；第二，Predefined Task Set 用来辅助评估数据好坏；第三，Instruction Set就是等待筛选的大数据集。

输出目标就是Instruction Set的优秀子集，被称为Golden Set。

**一句话说明白它是怎么做的。**

它认为，**如果一条数据作为one-shot的那个shot，即作为一条例子作为参考，能使得大模型有这个参考后，比zeroshot提升很多，就认为这是一条高质量的数据。**

  

**类似机器学习场景，多了一个特征，auc暴涨，那这个特征就是个好特征。**

整体的方法论就是这样，至于评估LLM输出的好坏，其实可以选取的角度很多，比如最简单的PPL。

TASK就是Predefined Task Set中的测试数据，文中设定包含1000条左右的指令测试数据。需要对这1000条数据 有了one-hot参考后的PPL变化情况统计，数一下有百分之多少变好了，就作为这条数据A的Golder Score。

  

![](https://pic1.zhimg.com/v2-8ecc7f99e06c5dad2c3e01be27369948_1440w.jpg)

  

然后把Instruction Set的每一条数据都算出来他的Golder Score，从高到低排序，选top N作为最优子集即可。

**结果比较惊人，在Alpaca数据中，选出top 1%的数据，就能取得和使用全部Alpaca类似的效果！**

思路简单明了，但是有可以改进的地方。

首先是Predefined Task Set辅助数据集，这个随便选1000个就是最好的吗？

作者实验发现，使用Kmeans聚类100个类后，每个类别中心选1个，最后100个有代表性的效果是最好的。

  

![](https://pica.zhimg.com/v2-32b1a7af2e94def7ce8e8877a393d7e6_1440w.jpg)

  

整体看来，这个方法还是非常直观的，大家可以把思路拓宽一点，延伸到特征选择中的那些方法，看能否迁移过来，**其核心就是如何低成本的度量一条数据有用。


## 参考资料

[预训练数据处理--长度分解](https://mp.weixin.qq.com/s/RQTHtvgky0w1BmpkTY_rDw)   #预训练长度分解




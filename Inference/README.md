---
title: README
created: 2024-06-17
tags:
  - æ¨ç†åŠ é€Ÿ
---


[GitHub - microsoft/DeepSpeed-MII: MII makes low-latency and high-throughput inference possible, powered by DeepSpeed.](https://github.com/microsoft/DeepSpeed-MII)

[Break the Sequential Dependency of LLM Inference Using Lookahead Decoding | LMSYS Org](https://lmsys.org/blog/2023-11-21-lookahead-decoding/)

[GitHub - FasterDecoding/Medusa: Medusa: Simple Framework for Accelerating LLM Generation with Multiple Decoding Heads](https://github.com/FasterDecoding/Medusa)

[è¯­è¨€å¤§æ¨¡å‹æ¨ç†åŠ é€ŸæŒ‡å—](https://mp.weixin.qq.com/s/B3TD2p_5HKoYkzzupLoUxQ)

[è¿›æˆ‘çš„æ”¶è—å¤¹åƒç°å§ï¼šå¤§æ¨¡å‹åŠ é€Ÿè¶…å…¨æŒ‡å—æ¥äº†](https://mp.weixin.qq.com/s/4USwSMIiudFCdy9C5pN1dQ)


[[Prefillä¼˜åŒ–][ä¸‡å­—]ğŸ”¥åŸç†&å›¾è§£vLLM Automatic Prefix Cache(RadixAttention): é¦–Tokenæ—¶å»¶ä¼˜åŒ– - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/693556044)

[LLMåç«¯æ¨ç†å¼•æ“æ€§èƒ½å¤§æ¯”æ‹¼](https://mp.weixin.qq.com/s/dPd84P_VdKog8v2IcHDOrQ) å¯¹æ¯”äº†vLLMã€LMDeployã€MLC-LLMã€TensorRT-LLM å’Œ Hugging Face TGI.



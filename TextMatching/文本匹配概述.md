文本表示方法

![image-20210911202228357](img/image-20210911202228357.png)

文本匹配/文本相似度

![image-20210911202758164](img/image-20210911202758164.png)

![image-20210911203528170](img/image-20210911203528170.png)

预训练词向量尽量用新的

## 知乎搜索中的文本相关性

在搜索场景中，文本相关性可以定义为⽤户搜索query的意图与召回 doc 内容的相关程度。

文本的相关性一般可以分为两个维度，字面匹配和语义相关。

在知乎搜索的整个架构中，文本相关性模型主要定位于为二轮精排模型提供更高维/抽象的特征，同时也兼顾了一部分召回相关的工作。

### Befor NN

知乎搜索中的文本相关性整体演进可以分为三个阶段。在引入深度语义匹配模型前，知乎搜索的文本相关性主要是基于TF-IDF/BM25的词袋模型。词袋模型通常来说是一个系统的工程，除了需要人工设计公式外，在统计词的权重、词频的基础上，还需要覆盖率、扩展同义词，紧密度等各种模块的协同配合，才能达到一个较好的效果。

Befor NN

- TF-IDF/BM25
- 词频/权重/覆盖率
- 紧密度/同义词

![图片](img/640-16334919545395.webp)

### Before BERT

基于 BM25 的词袋模型不管如何设计，主要还是只解决文本相关性中的字面匹配这部分问题。第二阶段引入的深度语义匹配模型则聚焦于解决语义相关的问题，主要分为两部分：双塔表示模型和底层交互模型。微软的DSSM是双塔模型的典型代表。双塔模型通过两个不同的 encoder来分别获取query和doc的低维语义句向量表示，然后针对两个语义向量来设计相关性函数（比如cosine）。DSSM摆脱了词袋模型复杂的特征工程和子模块设计，但也存在固有的缺陷：query和doc的语义表示是通过两个完全独立的 encoder 来获取的，两个固定的向量无法动态的拟合doc在不同 query的不同表示。这个反应到最后的精度上，肯定会有部分的损失。

Before BERT

- Embedding: word/char leval
- 表示模型：(C)DSSM
- 交互模型：MatchPyramid, (Conv-)KNRM



底层交互模型一定程度上解决了这个问题。这个交互主要体现在 query 和 doc term/char 交互矩阵（中）的设计上，交互矩阵使模型能够在靠近输入层就能获取 query 和 doc 的相关信息。在这个基础上，后续通过不同的神经网络设计来实现特征提取得到 query-doc pair 的整体表示，最后通过全连接层来计算最终相关性得分。Match-Pyramid、KNRM是交互模型中比较有代表性的设计，我们在这两个模型的基础上做了一些探索和改进，相比于传统的 BM25 词袋模型取得了很大的提升。

### BERT

BERT模型得益于 transformer 结构拥有非常强大的文本表示能力。BERT 的应用也分为表示模型和交互模型。

对于交互模型来说，如下左图，query和doc分别为sentence1和sentence2直接输入到BERT模型中，通过BERT做一个整体的encoder去得到sentence pair的向量表示，再通过全连接层得到相似性打分，因为每个doc都是依赖query的，每个query-doc pair都需要线上实时计算，对GPU机器资源的消耗非常大，对整体的排序服务性能有比较大的影响。

![图片](img/640-16334919180953.webp)



基于上述原因，我们也做了类似于DSSM形式的表示模型，将BERT作为encoder，训练数据的中的每个query和doc在输入层没有区分，都是做为不同的句子输入，得到每个句向量表示，之后再对两个表示向量做点乘，得到得到相关度打分。通过大量的实验，我们最终采用了 BERT 输出 token 序列向量的 average 作为句向量的表示。从交互模型到表示模型的妥协本质是空间换时间，因为doc是可以全量离线计算存储的，在线只需要实时计算比较短的 query ，然后doc直接通过查表，节省了大量的线上计算。相比于交互模型，精度有一部分损失。



## 参考资料

[知乎搜索文本相关性与知识蒸馏](https://mp.weixin.qq.com/s/ybMXgjoZC-Ej8MFBnYGtCw)
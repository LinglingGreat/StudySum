![](img/Pasted%20image%2020210925212848.png)


图中涉及的模型和知识点 #td 

## 预训练语言模型

深度学习时代广泛使用的词向量（即词嵌入，Word Embedding）即属于NLP预训练工作。使用深度神经网络进行NLP模型训练时，首先需要将待处理文本转为词向量作为神经网络输入，词向量的效果会影响到最后模型效果。词向量的效果主要取决于训练语料的大小，很多NLP任务中有限的标注语料不足以训练出足够好的词向量，通常使用跟当前任务无关的大规模未标注语料进行词向量预训练，因此预训练的另一个好处是能增强模型的泛化能力。目前，大部分NLP深度学习任务中都会使用预训练好的词向量（如Word2Vec和GloVe等）进行网络初始化（而非随机初始化），从而加快网络的收敛速度。

预训练语言模型的成功，证明了我们可以从海量的无标注文本中学到潜在的语义信息，而无需为每一项下游NLP任务单独标注大量训练数据。此外，预训练语言模型的成功也开创了NLP研究的新范式，即首先使用大量无监督语料进行语言模型预训练（Pre-training），再使用少量标注语料进行微调（Fine-tuning）来完成具体NLP任务（分类、序列标注、句间关系判断和机器阅读理解等）。

![](https://p0.meituan.net/travelcube/a67aa6b0b1fb153de0051e7eb3e8c632130233.png)

NLP Pre-training and Fine-tuning新范式及相关扩展工作

所谓的“预训练”，其实并不是什么新概念，这种“Pre-training and Fine-tuning”的方法在图像领域早有应用。2009年，邓嘉、李飞飞等人在CVPR 2009发布了ImageNet数据集，其中120万张图像分为1000个类别。基于ImageNet，以图像分类为目标使用深度卷积神经网络（如常见的ResNet、VCG、Inception等）进行预训练，得到的模型称为预训练模型。针对目标检测或者语义分割等任务，基于这些预训练模型，通过一组新的全连接层与预训练模型进行拼接，利用少量标注数据进行微调，将预训练模型学习到的图像分类能力迁移到新的目标任务。预训练的方式在图像领域取得了广泛的成功，比如有学者将ImageNet上学习得到的特征表示用于PSACAL VOC上的物体检测，将检测率提高了20%。

图像领域预训练的成功也启发了NLP领域研究，深度学习时代广泛使用的词向量（即词嵌入，Word Embedding）即属于NLP预训练工作。使用深度神经网络进行NLP模型训练时，首先需要将待处理文本转为词向量作为神经网络输入，词向量的效果会影响到最后模型效果。词向量的效果主要取决于训练语料的大小，很多NLP任务中有限的标注语料不足以训练出足够好的词向量，通常使用跟当前任务无关的大规模未标注语料进行词向量预训练，因此预训练的另一个好处是能增强模型的泛化能力。目前，大部分NLP深度学习任务中都会使用预训练好的词向量（如Word2Vec和GloVe等）进行网络初始化（而非随机初始化），从而加快网络的收敛速度。

预训练词向量通常只编码词汇间的关系，对上下文信息考虑不足，且无法处理一词多义问题。如“bank”一词，根据上下文语境不同，可能表示“银行”，也可能表示“岸边”，却对应相同的词向量，这样显然是不合理的。为了更好的考虑单词的上下文信息，Context2Vec[11]使用两个双向长短时记忆网络（Long Short Term Memory，LSTM）[12]来分别编码每个单词左到右（Left-to-Right）和右到左（Right-to-Left）的上下文信息。类似地，ELMo也是基于大量文本训练深层双向LSTM网络结构的语言模型。ELMo在词向量的学习中考虑深层网络不同层的信息，并加入到单词的最终Embedding表示中，在多个NLP任务中取得了提升。ELMo这种使用预训练语言模型的词向量作为特征输入到下游目标任务中，被称为Feature-based方法。

另一种方法是微调（Fine-tuning）。GPT、BERT和后续的预训练工作都属于这一范畴，直接在深层Transformer网络上进行语言模型训练，收敛后针对下游目标任务进行微调，不需要再为目标任务设计Task-specific网络从头训练。关于NLP领域的预训练发展史，张俊林博士写过一篇很详实的介绍[13]。

宏观地归纳预训练模型要做的事情：

在Transformer作为特征抽取器基础上，选定合适的模型结构，通过某种自监督学习任务，逼迫Transformer从大量无标注的自由文本中学习语言知识。这些语言知识以模型参数的方式，存储在Transformer结构中，以供下游任务使用。

## 预训练模型中的强基准：RoBERTa

在原始Bert模型的基础上，RoBERTa通过实验，证明了如下几点：

1.  进一步增加预训练数据数量，能够改善模型效果；
2.  延长预训练时间或增加预训练步数，能够改善模型效果；
3.  急剧放大预训练的每个Batch的Batch Size，能够明显改善模型效果；
4.  拿掉预训练任务中的Next Sentence Prediction子任务，它不必要存在；
5.  输入文本的动态Masking策略有帮助；

RoBERTa效果明显比Bert large好，在相同数据情况下，甚至超过了知名度很高的XLNet。这主要归功于数据规模的增加，以及更充分的训练过程，其中更充分的训练过程发挥的作用更大些。这是为何说RoBERTa 在某种意义上，其实是一个完成版本或者加强版本的Bert模型。

对于追求落地效果的人来说，比如公司里做业务的同学，建议以RoBERTa为基础模型来做应用。

## 预训练的发动机：模型结构
对于预训练模型来说，目前的主流模型大都采用Transformer作为特征抽取器，现阶段看，Transformer的潜力仍然没有被充分挖掘，还有很大潜力可挖。虽然，大家都是用的Transformer，但是怎么用它搭建模型结构学习效率更高？这是一个问题。所谓学习效率高，就是给定相同大小规模的训练数据，它能编码更多的知识到模型里，这就意味着它的学习效率更高。不同的Transformer用法，会产生不同的模型结构，就会导致不同结构的差异化的学习效率。

-   **Encoder-AE结构**

Encoder-AE结构如上图所示。这其实是包括原始版本Bert在内的，大多数后续改进模型采取的结构。整个结构就是一个标准的Transformer，在语言模型预训练的时候，采用AE方法。也就是说，输入句中的未被Mask的任意单词两两可见，但是被Mask掉的单词之间都相互独立，互不可见。在预测某个被Mask掉的单词的时候，所有其它被Mask的单词都不起作用，但是句内未被Mask掉的所有单词，都可以参与当前单词的预测。可以看出，Encoder-AE是个采用双向语言模型的单Transformer结构。

从目前对比实验看（上面两图），除了下文要讲述的Encoder-Decoder结构外，貌似对于语言理解类的NLP任务，这种结构都是效果最好的，但是对于语言生成类的任务，这种结构效果相对很差。也就是说，这种结构比较适合做语言理解类的任务。

-   **Decoder-AR结构**

Decoder-AR结构如上图所示。它和Encoder-AE结构相同，都是采用单个的标准Transformer，主要区别在于：语言模型预训练的时候，采用AR方法，就是从左到右逐个生成单词。采用这种结构的典型模型就是GPT1、GPT2、GPT3系列了。GPT3在文本生成任务方面的表现，确实是出乎意料地好。当然，这不能仅仅归功于这个结构本身，更复杂的模型和更大量的数据可能是主因。可以看出，Decoder-AR结构是个单向语言模型的单Transformer结构。

从目前对比实验看（参考Encoder-AE小节的两张效果对比图），除了Encoder-Decoder结构外，貌似对于语言生成类的任务，这种结构是效果最好的结构之一。但是相应的，语言理解类的任务，采用这种结构，效果比Encoder-AE结构差距非常明显，这也好理解，因为只看到上文看不到下文，对于很多语言理解类任务而言，信息损失很大，所以效果不好也在情理之中。也就是说，这种结构比较适合做语言生成类的任务。

-   **Encoder-Decoder结构**

既然Encoder-AE比较适合做语言理解类的任务，Encoder-AR比较适合做语言生成类的任务。那么，我们能否结合两者的优势，使得预训练模型既能做好生成类NLP任务，又能做好理解类任务呢？这是个很自然的想法，而Encoder-Decoder结构就是如此将两者结合的。最早明确提出使用Encoder-Decoder结构做通用领域预训练的，应该是微软提出的MASS模型，不过和这里介绍的做法有差异。

这种结构在Encoder侧，单独使用一个Transformer，采用了Encoder-AE的结构。也就是说，编码阶段采用双向语言模型，任意两个单词两两可见，以更充分地编码输入信息；而在Decoder侧，使用另外一个Transformer，采用了Decoder-AR结构，从左到右逐个生成单词。

当然，Decoder侧和标准的Decoder-AR不同的地方还是有的：Decoder侧生成的单词$W_i$，除了像Decoder-AR结构一样能看到在它之前生成的单词序列$W_1, ..., W_{i-1}$外，还能看到Encoder侧的所有输入单词 。而这一般是通过Decoder侧对Encoder侧单词，进行Attention操作方式来实现的，这种Attention一般放在Encoder顶层Transformer Block的输出上。

在进行预训练的时候，Encoder和Decoder会同时对不同Mask部分进行预测：Encoder侧双向语言模型生成被随机Mask掉的部分单词；Decoder侧单向语言模型从左到右生成被Mask掉的一部分连续片断。两个任务联合训练，这样Encoder和Decoder两侧都可以得到比较充分地训练。

从目前对比实验看，无论是语言理解类的任务（参考Encoder-AE部分Google T5论文中展示的效果对比图），还是语言生成类的任务（参考上面来自于UniLM v2的效果对比），貌似Encoder-Decoder结构相对其它几种结构来说，效果都是最好的之一。而且，它有另外一个优点，就是用这个结构，可以同时做生成类和理解类的NLP任务，基本做到了不同任务在模型结构上的统一，这点还是很好的，一个结构可以到处使用，比较方便。但是，它也有个问题，因为两侧各用了一个Transformer，所以相对其它结构参数量翻倍，计算量也增加了，就是说比其它模型笨重。而且，Encoder-Decoder结构比其它结构效果好，很可能主要原因来自于参数量增加导致的模型容量增大，当然这是个人猜测。目前，采用这个结构的效果很好的模型包括Google T5以及BART等模型。

-   **Prefix LM**

Prefix LM结构是Google T5论文中给出的叫法，这种结构最早由UniLM模型提出，我们沿用Google T5的这种称谓。如果深入分析的话，Prefix LM其实是Encoder-Decoder模型的变体：标准的Encoder-Decoder模型，Encoder和Decoder各自使用一个独立的Transformer；而Prefix LM，相当于Encoder和Decoder通过分割的方式，分享了同一个Transformer结构，Encoder部分占用左部，Decoder部分占用右部，这种分割占用是通过在Transformer内部使用Attention Mask来实现的。与标准Encoder-Decoder类似，Prefix LM在Encoder部分采用AE模式，就是任意两个单词都相互可见，Decoder部分采用AR模式，即待生成的单词可以见到Encoder侧所有单词和Decoder侧已经生成的单词，但是不能看未来尚未产生的单词，就是说是从左到右生成。

目前的一些对比实验证明，在其它条件相同的情况下，关于语言理解类的任务（参考Encoder-AE部分Google T5论文中的相关实验），Prefix LM结构的效果要弱于标准Encoder-Decoder结构。这里是值得深入思考下的，因为看上去Prefix LM和标准的Encoder-Decoder结构是等价的。那么，为什么它的效果比不过Encoder-Decoder结构呢？我想，一方面的原因估计是两者的参数规模差异导致的；另外一方面，可能与它这种模式的Decoder侧对Encoder侧的Attention机制有关。在Decoder侧，Transformer的每层 Block对Encoder做Attention的时候，标准的Encoder-Decoder模式，Attention是建立在Encoder侧的最后输出上，这样可以获得更全面完整的全局整合信息；而Prefix LM这种结构，Decoder侧的每层Transformer对Encoder侧的Attention，是建立在Encoder的对应层上的，因为这种模式的Encoder和Decoder分割了同一个Transformer结构，Attention只能在对应层内的单词之间进行，很难低层跨高层。这可能是影响这种结构效果的原因之一。当然这只是个人猜测，无证据证明，还请谨慎参考。

关于语言生成类的任务，Prefix LM效果虽然要弱于Encoder-Decoder结构（参考Encoder-Decoder小节UniLM v2论文效果对比图），但是总体而言，两者相差不大，相对其它模型，Prefix LM结构在生成类任务表现也比较突出。

Prefix LM因为是Encoder-Decoder的变体，所以可以看出，它的优势也在于可以同时进行语言理解和语言生成类任务，而且相对Encoder-Decoder来说，因为只用了一个Transformer，所以模型比较轻，这是Prefix LM的优势。缺点则是在效果方面，貌似要弱于Encoder-Decoder模型的效果，语言理解类任务相对有明显差距，生成类任务的效果相差不大。

-   **Permuted Language Model(PLM)**

PLM最早是在XLNet的论文中提出的，目前有些后续模型也在PLM上进行改进

PLM一样采用单个Transformer模型作为主干结构，但是从训练方法上来说，是个很另类也很有创意的做法，是种“形为AR，实为AE”的做法。在语言模型预训练过程中，它看上去遵循AR从左到右的输入过程，这符合一般生成任务的外在表现形式，但是在内部通过Attention Mask，实际做法其实是AE的做法，无非是把AE的做法隐藏在Transformer内部。它和AE从细节来说，主要有两个区别：首先，预训练过程中，输入句子去掉了Mask标记，改为内部Attention Mask，以保持预训练过程和下游任务Fine-tuning的一致性。关于这一点，目前有实验证明这个虽然有积极影响，但是影响不大（ELECTRA针对预训练过程是否带Mask 标记做了效果对比，带Mask标记的Bert模型GLUE得分82.2，去掉Mask标记利用其它单词代替的对比模型GLUE得分82.4）；其次，也是它和AE的最主要区别，PLM认为被Mask掉的单词之间是相互有影响的，先产生的被Mask掉的单词，应该对后生成的被Mask掉的单词，在预测的时候发生作用，而标准的AE则认为被Mask掉的单词是相互独立的，相互之间不产生作用。

其实，如果你仔细分析下PLM的预训练过程，会发现本质上PLM是Prefix LM的一种变体。（举例见参考资料[乘风破浪的PTM：两年来预训练模型的技术进展](https://zhuanlan.zhihu.com/p/254821426)）

如果不考虑XLNet里的其它因素，单纯看PLM结构的话，目前有些对比实验，貌似PLM在语言理解类任务中，效果不及Encoder-AE（参考UniLM v2论文中的对比实验，未在本文列出，可参考论文）；在语言生成类任务中，效果略微优于Encoder-AE，但是距离Decoder-AR差距较大（参考Encoder-AE描述部分BART的对比实验）。在两类任务中，都有点上不着村，下不着店的感觉，就是都还可以，但都不够好的感觉。XLNet效果确实是很好的，但是，这说明XLNet效果好，真正起作用的貌似不是PLM，而是其它因素。

上面内容简述了常见的五种预训练模型结构，如果总结一下的话：

首先，从模型效果来看，Encoder-Decoder结构无论在语言理解类还是语言生成类任务中，都是效果最好的。当然，效果好的原因很可能在于模型参数多，模型容量大，而不一定是自身结构带来的优势。它的优点一个是效果好，一个是能够将理解和生成任务统一在一个框架下；缺点是参数多计算多，所以模型比较重。采用这个结构的代表模型包括Google T5和BART。

其次，因为Encoder-Decoder模型比较重，所以，如果从相对轻量结构里进行选择的话，对于语言理解类任务，Encoder-AE结构相对而言效果较好，代表模型很多，典型的比如ALBert、RoBERTa；对于语言生成类任务，Decoder-AR结构和Prefix LM结构相对而言效果较好，都可考虑，Decoder-AR的代表模型是GPT系列，Prefix LM的代表模型是UniLM。语言理解类任务应该用AE任务，语言生成类任务应该用AR任务，这点也很明确了。

## 为什么有些模型表现这么好

促进模型性能快速提高的因素，主要包含下列几方面。而且，这几方面的因素是可叠加的，就是说，如果一个模型采纳其中越多的因素，那么这个模型的效果表现可能会更好。

首先，更高质量、更多数量的预训练数据。

关于预训练数据对模型效果的影响，Google T5做了大量对比实验，目前的结论，如果归纳一下的话，应该是这样的：在保证预训练数据质量的前提下，数据规模越大模型效果越好。这里需要注意的是，数据规模越大越好，这点其实从Bert一出来，就是一个容易想到的重要因素。因为数据量越多，数据里蕴含的知识也越多，那么模型能学到的东西越多，所以模型效果会更好，这是一个靠简单推理就能得出的结论。但是，它是有前提的，前提是数据质量要高，光数据量大不行，很多乱七八糟的数据，反而会对模型效果带来负面影响。

第二，增加模型容量及复杂度。

所谓增加模型容量及复杂度，指的是增加Transformer模型的参数量，一般而言，模型容量越大，模型的表达能力越强。最直接的增加模型容量的方式就是增加Transformer Block层深，比如可以从Bert base的12层，增加到Bert Large的24层，还可以继续增加到比如36层，这是纵向增加复杂度，Google T5走的这条路（从上图可以看出，模型容量增加到4倍后，有些数据集效果相对Baseline有大幅度的提升）。除此外，还可以横向增加模型复杂度，比如在固定Transformer层深的情况下，可以通过放大Transformer中构件的大小，比如Hidden Size的增大，FFN层对隐层的放大，Multi-Head Self Attention的Attention头的增加，等多种方式来做到这一点。ALBERT走的这条路，它的xxLarge模型效果最好，只用了12层Transformer Block，但是Hidden Size达到了4096。

这两种模式还可以相互结合，就是同时纵向和横向增加模型复杂度，GPT 3即是如此，将模型复杂度这点推到了极致。单词特征的Embedding不会放的太大，一般采用64或者128大小，ALBERT证明了如果单词特征Embedding跟着Transformer内部的Hidden Size同步放大，效果反而会降低。也就是说，增加模型容量指的是放大Transformer模型本身的参数量，但不包括输入层Embedding的参数。

第三，更充分地训练模型；

这里所谓的“更充分”，一般指的是放大Batch Size、增加预训练步数，就是RoBERTa做的那两个事情。

第四，有难度的预训练任务；

原始的Bert预训练，有两个训练任务：一个是单词级的Mask语言模型MLM，一个是句子级的下一句预测任务NSP。RoBERTa证明了NSP对于模型效果没什么影响，所以拿掉了这个任务。有很多研究集中在这一块，采取了五花八门的预训练任务（如上图所示）。那么哪些预训练任务相对而言更有效呢？目前已经能够得出些比较明确的结论。

如果归纳一下的话，应该是这样的：对于单词级的Mask语言模型来说，Span类的预训练任务效果最好。所谓Span类的任务，就是Mask掉的不是一个独立的单词，而是一个连续的单词片断，要求模型正确预测片断内的所有单词。Span类任务，只是一个统称，它会有一些衍生的变体，比如N-Gram，就是Span模型的一个变体，再比如Mask掉的不是单词而是短语，本质上也是Span类任务的变体，这里我们统称为Span类任务。

目前有相当多的研究证明Span类任务是效果最好的，最近有些工作（微软的ProphetNet和百度的ERNIE-GEN）进一步说明，Span内多个单词独立被生成效果会更好。所谓独立生成，举个例子，假设被Mask掉的片断是：$x_1, x_2, x_3$，之前一般Span类的预训练是顺序生成片断内的单词，就是先生成 $x_1$，然后根据上下文及$x_1$，生成 $x_2$，这么个顺序，就是说序列生成片断内单词。而独立生成，就是根据上下文，同时生成$x_1, x_2, x_3$，被生成的单词之间无影响。所以目前单词级的Mask语言模型，独立生成的Span类任务，应该是目前效果最好的。

对于句子级的任务，NSP任务学习两个句子是否连续句：正例由两个连续句子构成，负例则随机选择一句跟在前一句之后，要求模型预测两者是否连续句子。本质上，NSP在预测两个句子是否表达相近主题，而这个任务，相对MLM来说，过于简单了，导致模型学不到什么知识。ALBERT采用了句子顺序预测SOP（Sentence Order Prediction）：跟NSP一样，两个连续出现的句子作为正例，但是在构造负例的时候，则交换句子正确顺序，要求模型预测两个句子出现顺序是否正确，这样增加任务难度，StructBERT也采取了类似的做法。实验证明SOP是有效的句子级预测任务。

总而言之，目前证明Span类任务是有效的单词级任务，SOP是有效的句子级任务。目前看，预训练任务越有难度，则预训练模型越能高效率地学习知识，所以寻找更新的更有难度的预训练任务是有较大探索空间以及成功可能的。

![](img/Pasted%20image%2020210925175849.png)

如果我们根据上述可叠加的有效因素，来分析现有模型，可得出如上图所示列表（具备某因素的模型，对应的格子做了标记）。从上表中，我们可以得出一些结论：

首先，所有这些效果表现突出的模型，都增加了更多的高质量预训练数据。另外，通过增大Batch Size以及增加预训练步数方式，都使得模型得到更充分地训练。也就是说，所有这些表现突出的模型，都是站在RoBERTa模型的肩膀上的。其实，只要你站在RoBERTa肩膀上，效果都不会太差，剩下的问题是能比它好多少的问题。

其次，如果我来冒昧地做个判断的话，貌似对于语言理解类任务来说，估计Google T5和ALBERT是效果最好的预训练模型；而对于语言生成类的任务来说，估计GPT3是效果最好的模型。对于Google T5和ALBERT模型来说，两者都采纳了绝大部分有效因素，主要不同在于预训练任务，Google T5采用了Span类单词级任务，而ALBERT采用了SOP类句子级任务。这三个表现最突出的模型，和其它模型最大的区别，大概率在于它们在增加更多高质量数据的同时，走了大规模提升模型容量的路子。也就是说，在增加数据规模基础上大规模增加模型容量，这应该是拉开不同模型效果最主要的因素。

再次，我们可以据此预测，如果一个模型，采纳了上述所有有效因素，那么可以获得当前技术水准下的最好模型效果，就如上表中最后一行展示的，目前仍未知的Model X那样。就是说，这个模型应该是这样的：在RoBERTa模型基础上，增加更多高质量数据的同时，充分放大模型容量，而预训练任务则是单词类Span任务和句子类SOP任务的结合。当然，估计这里面起到主要作用的还是大量数据+大模型的因素。

这里单独说下ELECTRA，这是一个比较独特的预训练方法(参考上图)。 它形式上采取了类似GAN的模式，但是本质上并非GAN，因为缺乏GAN最关键的生成器和判别器的对抗训练过程。ELECTRA联合训练了小的生成器以及大的判别器，它强迫判别器对生成器产生的所有单词，做个是否经过改写的判断，这无疑增加了模型的学习效率，因为原先的MLM只学习15%的被Mask单词，而ELECTRA对所有单词都要进行判断，并从中学习。ELECTRA论文做了分析，模型的绝大多数收益来自于全部单词参与训练这一步。这意味着，ELECTRA这种所有单词全员参与训练过程的模式，能够在其它条件相同的情况下（模型复杂度，数据量等），使得模型获得更高的学习效率，这个结论和做法还是很有价值的。本质上，ELECTRA这种提升模型效率的方法，和上面所述其它模型的各种做法，是相互互补的。就是说，在ELECTRA的训练模式下，增加训练数据、增加模型规模、模型充分训练，有可能获得更好的模型效果。

## 其它知识的引入

-   **显示知识的引入**
-   **多模态预训练**


## 如何建造强大的预训练模型

![](img/Pasted%20image%2020210925180200.png)

对于语言理解类任务，我假设你的任务不是领域性特别强那种类型的，建议采取如下技术方案：

使用三阶段模型：通用预训练+任务预训练+任务Fine-tuning。在做完第一阶段预训练后，用手头任务数据，抛掉标签，再做一次任务预训练，然后任务Fine-tuning。

模型结构建议采取Encoder+Decoder结构，或者Encoder-AE结构；预训练任务配置两个：独立生成Span类语言模型及SOP句子任务；在质量优先的前提下，增加预训练数据的数量；比较关键的一点是，一定要增加模型容量：可以纵向增加Transformer Block层深，或者横向调大Transformer相应位置可配置参数大小。当然，如果你不差钱，两个可以一起上。另外，要使得模型得到充分训练，就是说增大训练过程中的Batch Size和训练步长。

对于语言生成类任务，建议采取如下技术方案：

使用两阶段模型：通用预训练+任务Fine-tuning。模型结构建议采取Encoder+Decoder结构，或者Decoder-AR结构；预训练任务采用独立生成Span类语言模型；在质量优先的前提下，增加预训练数据的数量；同样，比较关键的一点是，一定要增加模型容量：可以纵向增加Transformer Block层深，或者横向调大Transformer相应位置可配置参数大小。当然，如果你不差钱，两个可以一起上。另外，也要使得模型得到充分训练，就是说增大训练过程中的Batch Size和训练步长。

相信采取上述技术方案，你能在打榜过程中获得很好的名次，或者在实际工作中能比较快地完成自己的KPI或OKR。当然，如果是走落地应用的路子，关于知识蒸馏等一系列如何将模型做小这方面，记得要多花点功夫。

## elmo、GPT、bert三者之间有什么区别？

之前介绍词向量均是静态的词向量，无法解决一次多义等问题。下面介绍三种elmo、GPT、bert词向量，它们都是基于语言模型的动态词向量。下面从几个方面对这三者进行对比：

（1）**特征提取器**：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。

（2）**单/双向语言模型**：

-   GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。
-   GPT和bert都采用Transformer，Transformer是encoder-decoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子。



## 参考资料

1. 张俊林. [从Word Embedding到BERT模型—自然语言处理中的预训练技术发展史.](https://zhuanlan.zhihu.com/p/49271699)

2. [美团BERT的探索和实践](https://tech.meituan.com/2019/11/14/nlp-bert-practice.html)（预训练语言模型简介、BERT介绍、美团训练加速、加入美团点评业务语料进行预训练、融入知识图谱、业务实践等）
3. 张俊林，[乘风破浪的PTM：两年来预训练模型的技术进展](https://zhuanlan.zhihu.com/p/254821426) #td 
4. [NLP算法面试必备！史上最全！PTMs：NLP预训练模型的全面总结](https://zhuanlan.zhihu.com/p/115014536) #td 
5. [nlp中的预训练语言模型总结(单向模型、BERT系列模型、XLNet)](https://zhuanlan.zhihu.com/p/76912493) #td 
6. [NLP/AI面试全记录（持续更新，最全预训练总结）](https://zhuanlan.zhihu.com/p/57153934) 


---
title: Baichuan
created: 2023-10-06
tags:
  - 关键词
type: 论文
papername: 
conference: ACL/EMNLP/NAACL/EACL/COLING/SIGIR/AAAI/ICML/ICDM
year: 
institution:
---

## 论文基本信息

标题：

作者：

链接：

代码：

框架图：



baichuan2相比baichuan1的**主要改进在于：**

- **模型覆盖的语种变多：支持更多语言；**
    
- **训练数据量增加：数据增多导致模型能力更强；**
    
- **词表扩张：数据压缩更多，模型解码效率提高；**
    
- **开源中间步骤模型：更多checkpoint更方便学术研究；**
    
- **垂域支持：通用模型vs垂域大模型。**

# 预训练
## 数据

从多个来源进行数据收集，包括但不限于网页、书籍、研究论文、代码等。**可以看出法律类数据和健康类（应该包括医疗）数据分别占比2.5%和6.5%（肉眼估计，可能不准确）。数据占比是比较多的，因此猜测“医疗和法律”两个垂域效果好，是因为数据的原因。**

![](img/Pasted%20image%2020231006175752.png)

并且对数据进行清洗，如下图所示，主要关注数据的频率和质量：

- 数据频率：借助LSH-like和Embedding特征对数据进行聚类和去重，主要是对每个聚类的簇给文档、段落、句子进行去重和打分，分值用于用于最终的数据采样。
    
- 数据质量：句子级别质量过滤，但未说明明确过滤规则。不过从下面模型安全部分可以得知，对数据进行了暴力、色情、种族歧视、仇恨言论等有害内容过滤，但应该还包含其他内容。
    

**PS：报告中没有给出过滤后数据采样比例&数据分布情况，比较遗憾。但从垂域效果来看，医疗和法律数据应该不会少，并且从数据本身质量来看，书籍&论文数据的采样率应该也会比较高。**

![](img/Pasted%20image%2020231006175850.png)

## Tokenizer

Tokenizer的构建一般考虑两个因素：压缩率和词表大小。

- 压缩率：主要影响模型的推理效率；
    
- 词表大小：主要影响模型大小和Embedding训练的充分程度。
    

Baichuan2利用SentencePiece中BPE方法构建Tokenizer，从64000扩张到了125696；在构建过程中有以下细节：

- 原始数据未经过规范化处理；
    
- 为了更好地编码数字内容，将数字序列分成单独的数字；
    
- 为处理代码中的空格，在词表中额外添加空Token；
    
- 字符覆盖率为0.9999，稀有字符利用UTF-8编码；
    
- 最长Token的长度为32。

## 模型结构

Baichuan2的模型以Transformer架构为基础，但做了一些小改动。

- 位置编码：7B的位置编码采用RoPE，13B位置编码采用ALiBi。主要是因为两种位置编码对模型效果基本没影响，所以继承了Baichuan1的7B和13B的位置编码。
    
- 激活函数：采用SwiGLU激活函数，不同于传统FFN的2个矩阵，SwiGLU有三个矩阵，因此缩小了隐藏层维度，由原来的4倍变成8/3倍，再调整为128的整数。
    
- 归一化：对Transformer的输入进行采用层归一化，提高warm-up的鲁棒性，并用RMSNorm实现。
    
- NormHead：为了提高模型训练的稳定性，对输出的embedding进行归一化，主要解决：（1）稀有标记的embedding在训练过程中变小，干扰了训练的动态；（2）分析发现输出表示语义关系受余弦相似度计算明显，对L2距离不明显，归一化可以减少线性分类器通过点积计算logits时，L2距离的影响。

- Max-z loss：预训练时，logits可能会变的非常大，会导致推理过程中对惩罚因子不敏感，受NormSoftmax启发，对logits进行归约。主要有助于稳定训练并使推理对超参数更具鲁棒性。

## 缩放定律

通过10M到3B参数模型的训练，利用缩放定律准确地预测了7B和13B模型的最终损失。

并且13B模型的loss与7B模型的loss趋势也符合预期。

## 训练
主要采用张量并行的方式，对模型参数进行切割，训练过程中每台机器上采用张量并行方式，每台机器间采用Zero数据并行方式。并采用混合精度进行训练，forward和backward采用bf16计算，优化器更新采用float32计算。

为了避免多卡时通信效率的降低，采用Topology-aware distributed training和Topology-aware distributed training。

# 对齐
## SFT
严格对SFT数据进行把关，最终收集100k数据进行模型有监督微调。

## RM

设计了一个三层分类系统全面覆盖所有类型的用户需求，包括6个主要类别、30个二级类别、200多个三级类别。在奖励模型训练时，需要保证每个类别内的数据应该有足够的多样性，以确保奖励模型能够有更好地泛化性。并且奖励数据中结果需要由Baichuan2模型生成，以确保数据分布的统一。

奖励模型的损失与InstructGPT模型一致，并且结果发现，差距越大的结果之间判断的越准确，也符合人类直觉。

## PPO

PPO阶段共包含四各模型：actor模型、reference模型、reward模型和critic模型。训练过程中，先对critic模型训练20步预热；再用标准PPO算法对actor模型和critic模型进行更新，其中学习率为5e-6、梯度裁剪为0.5、PPO裁剪阈值为0.1、KL散度惩罚系数为0.2逐步减少到0.005，迭代350次结束模型训练。

# 模型安全性

模型安全性不仅需要模型对齐阶段解决，预训练阶段要需要考虑。

- 预训练阶段：对数据进行清洗，过滤消除暴力、色情、种族歧视、仇恨言论等有害内容，并且构造了中英安全数据预料包含各种积极价值数据，在数据采样过程中，提高这些数据的采样率。
    
- 对齐阶段：10个有传统互联网安全经验的专家构建6种攻击类型和100+细粒度安全价值类别，由50人标注团队生成200k攻击性提示，进行模型安全性对齐训练。

## 主要收获


## 参考资料

[BaiChuan2技术报告细节分享&个人想法](https://mp.weixin.qq.com/s/Xeb0I5FGegMyngC1TWyKCA)


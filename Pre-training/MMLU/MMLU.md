---
title: MMLU
created: 2023-02-23
tags: Benchmark
type: 论文
papername: Measuring Massive Multitask Language Understanding
conference: ICLR
year: 2020
institution: 
---

## 论文基本信息

标题：Measuring Massive Multitask Language Understanding

作者：

链接： https://arxiv.org/abs/2009.03300

代码： https://github.com/hendrycks/test

框架图：

衡量语言模型在57个知识密集型任务上的表现

**MMLU**（**大规模多任务语言理解**）是一种新的基准测试，旨在通过专门在零样本和少样本设置中评估模型来衡量预训练期间获得的知识。这使得基准测试更具挑战性，也更类似于我们评估人类的方式。该基准涵盖 STEM、人文学科、社会科学等领域的 57 个学科。难度从初级到专业高级，既考验世界知识，又考验解决问题的能力。学科范围从数学和历史等传统领域到法律和伦理等更专业的领域。主题的粒度和广度使基准成为识别模型盲点的理想选择。

## 核心亮点

## 主要收获


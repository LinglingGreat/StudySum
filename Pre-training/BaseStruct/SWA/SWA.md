---
title: SWA
created: 2024-07-18
tags:
  - attention
---
sliding window attention

![](img/Pasted%20image%2020240718165301.png)

In the traditional sense, $Attention(ğ‘„,ğ¾,ğ‘‰)=softmax(ğ‘„ğ¾^ğ‘‡/sqrt(ğ‘‘_ğ‘˜))ğ‘‰$

Each token in the Query vectorÂ ğ‘„Â can attend to all tokens in the Key vectorÂ ğ¾.

But, this leads to a computational complexity ofÂ ğ‘‚(ğ‘›^2). As a result, memory requirements grow by a factor ofÂ ğ‘›^2Â for a sequence of lengthÂ ğ‘›.

This limits the traditional Transformer architecture from having long context length. The solution is to use Sliding window attention where each token in the Query vectorÂ ğ‘„Â only attends to itâ€™s neighbouring tokens with an overlap of window lengthÂ ğ‘¤.

So, a token at positionÂ ğ‘–Â inÂ ğ‘„, can attend to tokens in rangeÂ (ğ‘–âˆ’ğ‘¤,ğ‘–+ğ‘¤)Â inÂ ğ¾.


ğ‘„.ğ¾^ğ‘‡Â using Einstein summation is as easy as doing:

```python
import torch 
q = torch.arange(1, 9).reshape(4,2)
k = torch.arange(1, 9).reshape(4,2)
out = torch.einsum('xd,yd->xy', q, k)
out.shape.  # torch.Size([4, 4])
```


sliding window attention

```python
q = torch.randn(1, 8, 768)
k = torch.randn(1, 8, 768)

def _chunk(hidden_states, window_overlap):
    """convert into overlapping chunks. Chunk size = 2w, overlap = w"""
    chunk_size = [
        hidden_states.size(0), #bs
        torch.div(hidden_states.size(1), window_overlap, rounding_mode="trunc") - 1, #n_chunks
        window_overlap * 2,
        hidden_states.size(2),
    ]

    overlapping_chunks = torch.empty(chunk_size, device=hidden_states.device)
    for chunk in range(chunk_size[1]):
        overlapping_chunks[:, chunk, :, :] = hidden_states[
            :, chunk * window_overlap : chunk * window_overlap + 2 * window_overlap, :
        ]
    return overlapping_chunks

query = _chunk(q, window_overlap=2)
key   = _chunk(k, window_overlap=2)
query.shape, key.shape
# (torch.Size([1, 3, 4, 768]), torch.Size([1, 3, 4, 768]))

diagonal_chunked_attention_scores = torch.einsum("bcxd,bcyd->bcxy", (query, key)) 
diagonal_chunked_attention_scores.shape
# torch.Size([1, 3, 4, 4])
```

![](img/Pasted%20image%2020240718170427.png)


## å‚è€ƒèµ„æ–™

[Sliding Window Attention](https://amaarora.github.io/posts/2024-07-04%20SWA.html)


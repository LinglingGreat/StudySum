

## 现有大模型

![](img/Pasted%20image%2020230227103950.png)



[LLMs](LLMs/README.md)

## Prompt
- [ ] todo


## Instruction tuning

[Flan-PaLM_T5](Flan-PaLM_T5/Flan-PaLM_T5.md)


## CoT

总结：[CoT](CoT/CoT.md)


[Why did all of the public reproduction of GPT-3 fail? In which tasks should we use GPT-3.5/ChatGPT?](https://jingfengyang.github.io/gpt)

## Benchmark

[Benchmark](Benchmark/README.md)

## dataset




## 资源列表汇总

【收集了有关大型语言模型中不确定性、可靠性和鲁棒性的资源和论文】: github.com/jxzhangjhu/Awesome-LLM-Uncertainty-Reliability-Robustnes

【收集和梳理垂直领域的开源模型、数据集及评测基准】： [GitHub - luban-agi/Awesome-Domain-LLM: 收集和梳理垂直领域的开源模型、数据集及评测基准。](https://github.com/luban-agi/Awesome-Domain-LLM)

【LLM的评测有关的工具、demo、论文、文档】： [GitHub - onejune2018/Awesome-LLM-Eval: Awesome-LLM-Eval: a curated list of tools, demos, papers, docs for Evaluation on Large Language Models like ChatGPT, LLaMA, GLM](https://github.com/onejune2018/Awesome-LLM-Eval)


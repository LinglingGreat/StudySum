

## 现有大模型

![](img/Pasted%20image%2020230227103950.png)



[LLMs](LLMs/README.md)

## Prompt
- [ ] todo


## Instruction tuning

[Flan-PaLM_T5](Flan-PaLM_T5/Flan-PaLM_T5.md)


## CoT

总结：[CoT](CoT/CoT.md)


[Why did all of the public reproduction of GPT-3 fail? In which tasks should we use GPT-3.5/ChatGPT?](https://jingfengyang.github.io/gpt)

## Benchmark

[Benchmark](Benchmark/README.md)

## dataset




## 资源列表汇总

【收集了有关大型语言模型中不确定性、可靠性和鲁棒性的资源和论文】: github.com/jxzhangjhu/Awesome-LLM-Uncertainty-Reliability-Robustnes


【'Xwin-LM：旨在开发并开源大型语言模型的对齐技术，包括监督微调(SFT)、奖励模型(RM)、拒绝采样、人类反馈强化学习(RLHF)等】’Xwin-LM: a collection of LLM alignment technologies and models' GitHub: github.com/Xwin-LM/Xwin-LM


## 训练情况对比

baichuan1:

baichuan2: 2.6T tokens

qwen

llama


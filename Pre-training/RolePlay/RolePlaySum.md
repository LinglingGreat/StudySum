---
title: RolePlaySum
created: 2024-03-18
tags:
  - roleplay
---

## 相关文章

[角色扮演大模型的碎碎念](https://mp.weixin.qq.com/s/XNCylQhaBWHbN-Sd2w6UMQ)

- 目前所有大模型技术，并不能让大模型具备本身的独立意识，大模型表现出的「拟人化」本质上是在努力扮演一个人类喜好的Assistant角色；或者说，大模型通过SFT和RLHF等技术路线实现与人类喜好的对齐。
- 什么是角色扮演大模型？对于用户来说，他们更需要的是一个**可定制的、高度拟人化的有情感、有温度的聊天机器人**，这就是角色扮演模型。
- 角色扮演模型和通用模型的区别是什么？
	- 一些大模型“意识”中的「我」仍然是一个asisstant助手，「角色扮演」相当于是人类分配给这个助手的一个任务，相当于我们绕了一圈来达到角色扮演目的，效果必然打了一个折扣。但对于角色扮演模型，我们可以直接让这个AI去扮演某个角色，如虚拟女友、历史人物等。此时大模型“意识”中的「我」就是某个特定的人物，它一切出发点和“自我意识”都是从某个人物的角度出发和考虑。
	- 对于角色扮演模型来说，不会具有明显的官方语气和说教口吻，相反会表现出符合角色的特定性格特点和说话风格。
	- 角色扮演模型可能会具有一定的情绪能力，例如在扮演虚拟女友时会表现出撒娇、吃醋等属于人类的情绪。
	- 角色扮演模型的输出更具画面感和沉浸感，当我们与通用模型交互时，体感上更像是阅读一本「工具书」，是机械的、没有情感的；而角色扮演模型更像是一个真实的人在隔着屏幕与用户聊天，用户可以想象出到对方的动作、想法和情绪等
	- 角色扮演模型具备更强的交互性：传统的通用模型本质上是一个问答机器人，用户与AI采取一问一答的形式；而角色扮演模型可以给用户主动预设一个场景或情节背景，用户和AI一起交流来演绎并推动剧情的发展。在这当中，角色扮演模型不会简单回答用户问题，更具有主动提问、主动推动剧情发展的能力
	- 假如说通用问答大模型如GPT等是一个assistant助手，那角色扮演大模型的定位就是一个「演员」。通用模型更偏向是「工具」，而角色扮演模型更偏向「产品」属性。
- 用一个类比来聊聊我对「角色扮演」的认知
	- 要想做到演啥像啥，就要理解角色，对于演员来说理解角色的途径就是阅读剧本，对大模型来说就是阅读system prompt。system prompt就是一个人物卡，当中包含角色的背景信息、性格特点、说话风格等等。大模型要想演的像，就要把这些信息吃透。
	- 模型只做针对某个/某些特定人物做SFT的微调，如characterLLM、ChatHaruhi等。使用几十或上百个角色且同一个角色的数据使用几百条上千条，会导致一些过拟合的出现。结果就是训练过程中，**模型对system prompt的敏感度降低**，导致推理时对于训练集中出现过的角色，这种模型通常表现较好；但面对一个全新的角色，模型泛化能力可能会打个折。
	- 我一直秉持的技术路线是，构造大量人物卡，每个人物只放一条或几条对话session数据，让模型学会「阅读剧本」，也就是我们说的指令(在这里是system prompt)跟随能力。

	- 有时候我们评价一个演员很有灵气，或者字面意义上评价这个演员很讨喜，可能说的是这个演员在表演过程中的表情、行为是否灵动；在角色大模型来说，就是动作描写是否生动形象。
	- 电视剧中的人物对话，或者一些小说、剧本中人物对话的风格，介于严肃文学和真人对话之间。一方面需要接地气，不能太枯燥太端着；也不能完全和真人聊天一样完全随意，需要有有一定阅读的体验，并通过说话内容和旁白等来补充相对现实世界缺失的信息。
	- 我们既需要有与通用底座模型迥异的、更拟人化的输出风格，又要比真人对话更稍微正式一点点。真人微信聊天时可能一句“笑死”+表情包就能传达的情绪，角色扮演模型需要更多的描述来渲染角色对某个情景时的反应。
	- 直接从小说中抽人物对话会导致一个问题，那就是小说人物对话可能非常短，有时候甚至只有一两个字，更多的信息如角色的心理活动等放在旁白里进行描写了。如果只使用角色说的话来训练的话，会导致「信息缺失」。

	- 一个不成熟的演员在扮演某个角色时会有违和感，模型同样如此。作为一个真实的人或角色，通常都会有一个明确的立场，所以永远都不会说”因人而异“这种端水类的话术；通常都会缺点有小心机，所以从来不会像chatGPT一样永远一本正经讲大道理；通常都会有自己的性格和脾气，所以从来都不会像正常底座模型一样有问必答；从来都不会说「作为xxx，我认为......」这种奇怪的话。
	- 那这个问题怎么解决呢？我目前的做法是简单粗暴建立一个黑名单词表，把所有带强烈违和感或真实人类根本不会说的词给过滤掉。
	- 那怎么快速评价模型输出是否有违和感呢？或者换个角度，什么样的输出算是一个没有违和感的模型呢？
	- 我的方法就是：**把模型输出结果中所有的「我」这个字替换成「角色名」，看这句话是否成立。**如果成立，则认为有违和感，即模型并没有真正带入角色，只是在输出形式上做到了角色扮演而已；如果不成立，那我们认为当前模型输出质量基本达标。
- 关于继续预训练：continue pre-train
	- 「演员」在电影学院会学习大量的表演理论课，大模型角色扮演同样可能需要用一些小说、剧本类数据当作「课本」来学习一些潜在的能力。一个常见的做法是在这类数据上做continue pretrain，例如BaichuanNPC用了大概3T量级的tokens重新做了预训练。但是资源消耗大且数据收集困难。
	- 无论是基于我们的尝试结果还是【Continual Pre-Training of Large Language Models: How to (re)warm your model?】这篇论文的实验结论，使用少量数据进行continue pretrain不仅会发生在灾难性遗忘导致底座模型原有能力损失，亦有相当高的风险和概率在新数据集上表现出loss难收敛的情况。
	- 预训练的任务目标无非是给大模型「灌知识」，但是小说数据里到底有多少事实性知识呢？大部分小说都是杜撰的，其中大部分信息根本都是没用的，大模型根本不需要学习某个点击量只有几百条的网络小说某个扑街配角的人物生平信息，用户也根本不会问；用户可能会问一些热门小说比如「斗罗大陆」「斗破苍穹」，但这些知识pretrain模型本来就有啊，干嘛还要再做一次continue pretrain呢？因此对于角色扮演任务，我们更希望模型通过小说的描写手法和表述方式来学会如何将信息更好地表达和输出，这反而和SFT微调任务目标更接近。
	- 所以对于大多数开发者，如果你没有能力制作一个很好的预训练数据集和高超的炼丹技术，我可能不太建议走继续预训练这条路。但是可以尝试添加一些小说续写类数据，一方面增强模型的场景描写能力，另一方面写作能力和角色扮演能力确实强相关，也就是说：有角色扮演需求的用户大概率也会有小说创作的需求。
- 关于通用对话数据
	- 如果我们把角色扮演模型类比成一个「演员」，那他首先是一个「人类」，对于一个人类来说，基本的知识能力和逻辑能力是必要的；可是要想让大模型具有一定的逻辑性和知识表达能力，不得不添加一些包括少量数学、代码任务在内的通用SFT问答数据。
	- 有一些角色扮演模型会直接从pretrain模型启动，仅使用角色扮演对话数据进行训练。从我个人的复现的结果来看，模型的表现更像是一个「努力假装自己是xxx角色的白痴」
	- 推荐添加少量通用SFT问答数据直接从一个训练好的pretrain模型启动，大概率就能得到一个还不错的baseline效果了。
	- 那么是否可以直接从SFT模型启动，不添加通用问答数据呢？可以，这取决于个人细节的任务目标、数据量、数据组织形式等等因素。直接从SFT模型启动，可以认为此时模型本身已经具有很好的通用能力了，甚至我们都不需要再添加额外的、重复的通用问答数据也能产生不错的效果。此时要考虑的问题反而变成了「如何在基础能力不退化的情况下迁移模型对话风格」。因为前面我们提到，通用问答模型具有很强的assistant语气，这必然会导致角色扮演任务的违和感。那太小的学习率，会导致模型更难以学到新知识；太大的学习率又容易灾难性遗忘。如何找到平衡点是一个很玄学的问题。这里推荐参考Qwen技术报告，同时增大warmup ratio和学习率，个人实验结果还不错。
	- 那么使用多大比例的通用问答数据？角色扮演任务和我们常谈到的一些垂域问答（如医疗金融法律等）还有一些区别：垂域问答可能需要的数据量更小，任务目标重点考察模型在专业领域的事实性知识能力，一些常见的数据比例可能是垂域数据占总数据量的10%到20%；但角色扮演任务不能单纯当作一个简单的垂域来看，我认为它不是底座大模型的子任务或某项能力，相反，基础的问答能力应该是角色扮演大模型的一项附属能力。因此我们推荐在全量数据当中，角色扮演数据占比更大，通用问答数据占比应该不超过全量数据的50%。我个人大概使用了1/4到1/3量级的通用问答数据，其中数学代码逻辑相关的数据用得非常少，只是通过一些简单的CoT数据来激发模型的逻辑能力或思考能力而已。
- 其他
	- 进度条问题：一些游戏类角色扮演可能需要大模型实时输出一些角色状态信息，这本质上是一个复杂指令跟随能力，通常需要使用更大参数的模型去解决，13B以下规模的模型去做类似的事情肯定会更吃力一点。目前我个人的训练数据中没有特意构建这类数据，所以表现相对一般。
	- 小说抽取对话：我实验结论是这类数据会对最终结果有负面影响，具体现象是模型output变短，更容易给出只有一两字的output，这也就是之前我们提到的「信息缺失」的问题。我个人是不太推荐大量使用这种数据的，或者可能需要做更严格的清洗或改写。
	- 过于配合用户：也就是大模型不会拒绝用户提出的需求或问题，或很少表现出不耐烦等负面情绪。这个问题其实比较容易解决，只要在对话数据里包含这样类似的数据即可（最好是多轮），实在不行可以考虑使用DPO或RLHF来解决。
	- 生成数据多样性：使用GPT等生成对话数据肯定会有多样性问题，这里建议一定要对高频pattern做降采样，否则会导致模型过拟合。
	- 关于DPO：很多人也提到过，建议对模型本身的infer结果做自采样，我尝试下来的体验是：效果出奇的好
	- 安全问题：大模型角色扮演从提出来开始，就和越狱等内容相关，这里做对齐必然会导致效果损失，但没办法hhh

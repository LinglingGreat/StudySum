如何训练一个全能的预训练模型？——兼顾理解和生成的中文预训练模型CPT，邱锡鹏

![image-20211128115236683](img/image-20211128115236683.png)

![image-20211128115424199](img/image-20211128115424199.png)

中文预训练模型中没有考虑到的？

![image-20211128115501329](img/image-20211128115501329.png)

T5和BART用了Encoder-Decoder架构（难以训练）

![image-20211128115547050](img/image-20211128115547050.png)

![image-20211128115642722](img/image-20211128115642722.png)

![image-20211128115731269](img/image-20211128115731269.png)

挑战

![image-20211128115741944](img/image-20211128115741944.png)

![image-20211128115832589](img/image-20211128115832589.png)

![image-20211128120053423](img/image-20211128120053423.png)

![image-20211128120306074](img/image-20211128120306074.png)

同时支持理解和生成任务，生成效率提升2倍以上

![image-20211128120603444](img/image-20211128120603444.png)

![image-20211128120613667](img/image-20211128120613667.png)

![image-20211128120653943](img/image-20211128120653943.png)

![image-20211128120733725](img/image-20211128120733725.png)

![image-20211128120759033](img/image-20211128120759033.png)

![image-20211128120814108](img/image-20211128120814108.png)

![image-20211128120830411](img/image-20211128120830411.png)

模型效果

![image-20211128120930437](img/image-20211128120930437.png)

![image-20211128121003259](img/image-20211128121003259.png)

![image-20211128121014358](img/image-20211128121014358.png)

![image-20211128121117952](img/image-20211128121117952.png)

![image-20211128121129700](img/image-20211128121129700.png)

未来的工作：中文的字、词级别的兼顾


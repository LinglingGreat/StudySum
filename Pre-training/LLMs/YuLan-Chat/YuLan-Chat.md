---
title: YuLan-Chat
created: 2023-06-11
tags: LLM, SFT

---

https://github.com/RUC-GSAI/YuLan-Chat

**中国人民大学高瓴人工智能学院相关研究团队（由多位学院老师联合指导）展开了一系列关于指令微调技术的研究，并发布了学院初版大语言对话模型——YuLan-Chat，旨在探索和提升大语言模型的中英文双语对话能力**。

我们分别开源了13B和65B的YuLan-Chat模型文件及相关代码，并采用量化技术使其分别可以在单张RTX3090-24G和A800-80G显卡上部署。YuLan-Chat模型基于LLaMA底座模型，采用精心优化的高质量中英文混合指令进行微调，其中YuLan-Chat-65B模型目前能够在中英文相关评测数据集上显著超越已有开源模型效果。后续我们会继续优化指令微调方法与底座模型，持续更新YuLan-Chat模型。

## 中英文混合指令构造方法与质量优化

为实现将预训练后的大语言模型转化为大语言对话模型，需要采用高质量的指令数据对齐进行微调，使其能够习得理解人类指令并执行的能力。现有方法往往依赖于Self-Instruct方法让大模型自动生成指令，或直接收集和爬取已有的指令或对话数据。这两种方式下很难对数据质量进行监督与控制，且难以保证指令数据的多样性和难度，可能反而导致大模型过拟合到这批指令数据上。为解决这一问题，我们基于已有开源指令数据，采用了基于主题控制的多阶段指令筛选、复杂化、多样化方法，按照以下三个阶段分别进行指令数据进行优化：

  

**A.  基于语义相似度模型的开源指令去重**

即使目前已经有许多高质量的开源指令数据，其中往往有大量的语义重复或相似的指令，其会加重模型对该类数据的过拟合程度，进而影响模型对其他指令的理解能力。为解决这一问题，我们提出基于语义相似度模型的开源指令去重方法，对该类重复或相似的指令进行整合或过滤。具体来说，我们采用一已训练好的语义相似度度量模型，其将已有开源指令数据集中的相似指令进行语义匹配，以得到若干可能语义重复的指令对。然后，我们采用ChatGPT对两者的语义重复程度进行判断，并删除其中高度重复的指令、合并语义相似的指令为一个新的更加复杂的指令。通过该方法，我们可以既可以实现开源指令数据的筛选，又可以增加其指令复杂程度，进而得到更优质的新指令集合。

  

**B.  基于主题控制的指令多样化**

已有研究表明，使用越多样的指令越能增强模型的各项能力，且防止模型出现“偏科”的问题。因此，我们提出了基于主题控制的指令多样化方法。我们首先基于聊天社区内收集若干主题，包含15个大类（如商业和艺术）和293个子类（如比特币和流行音乐）。然后，对每个子主题，我们先从已有指令中随机选出若干指令，然后要求ChatGPT对这些指令进行改写，要求其既能与该主题内容相关，同时尽量保留原始的指令内容。若某指令与当前主题无关且无法被改写，我们会继续采样其他主题对其进行改写，直到修改成功或达到最大轮数为止。这样我们即可得到主题更多样且分布均匀的新指令集合，同时又不会大幅改变原始指令数据的类型分布。此外为了实现语言和风格的多样性，我们还讲这部分英文指令数据翻译为中文。

  

**C.  指令数据复杂化**

使用更复杂的指令数据，可以有助于培养模型解决较难任务的能力。这里我们主要考虑两种复杂化方式，即：1.将简单指令改写为复杂指令；2.将单轮指令数据扩充为多轮对话形式。前者通过特定的指令，要求ChatGPT从该主题下知识的深度和广度两方面，分别对当前指令进行改写，使其聚焦于该领域内更细节的知识点或覆盖其他相关领域知识。后者在前者的基础上，针对其指令回复的结果，进一步使用ChatGPT从深度和广度两方面，分别提出专业的问题。该过程多次循环，直到该多轮对话数据达到预设的上下文长度上限。通过这一方法，可以构造出信息量丰富且复杂的多轮对话数据，使用其训练大语言模型会极大强化其对特定领域知识和复杂上下文的理解能力。

最终，基于以上方法我们分别得到9万的中文和9万的英文多轮对话数据，通过将其与7万来源于ShareGPT的真实对话数据混合，得到最终的指令微调数据集，共25万条指令。

## 指令微调方法

基于以上多轮的指令数据集，我们基于LLaMA这一基座模型进行指令微调。我们分别考虑对13B/65B的LLaMA模型进行全参数微调，并基于DeepSpeed库，使用数据并行，ZERO与FlashAttention技术，在2台8卡A800-80G的GPU服务器上进行多机多卡训练。考虑到我们的训练数据全部是多轮对话数据，我们效仿Vicuna，在计算损失函数时采用特殊的掩码机制，既将多轮对话数据拼接为一长句子，然后仅对需要模型生成的文字部分计算损失值。这一方法可以降低拆分对话导致的计算开销增加，保证较低的训练成本。

在训练之后，我们还采用int8量化技术，使得训练后的模型可以在单张GPU上进行生成，且精度基本不变。其中65B和13B模型可以分别在A800-80G和RTX3090-24G显卡上部署。

## 实验分析

为验证我们方法的有效性，我们在中英文两种场景下，分别选择两个基准数据集合，对我们的YuLan-Chat模型和其他大语言对话模型进行对比。我们主要与目前公认较强的Vicuna-13B、MOSS、ChatGLM模型进行对比，并同时报告了OpenAI的闭源模型Text-Davinci-003和ChatGPT模型的效果。

![](img/Pasted%20image%2020230611102052.png)

**A.  中文高考数据集**

我们从AGIEval数据集中抽取中文高考数据集，其包括语文、英语、地理、历史、生物、化学、物理、数学选择题与填空题共9个子数据集。

从上图中实验结果可以看出，我们的YuLan-Chat-65B模型在所有开源模型中取得最好的效果，这不仅因为其较大的参数量，且其采用了更高质量的指令进行微调。此外，因为LLaMA基座本身中文能力相对较弱，我们规模更小的YuLan-Chat-13B依旧能够通过我们的指令取得与最好表现的开源模型ChatGLM-6B相当的结果，可以体现出我们指令能够较好的提升模型的中文能力。

**B.  英文BBH3K数据集**

由于BBH原始数据集中包含不易于评测的填空题和相似的多个题目，我们从中随机抽取三千多道题目构成BBH3k数据集，其依旧由BBH原始的23个子任务组成。

从上图中实验结果可以看出，我们的YuLan-Chat-65B模型依旧能在所有开源模型中取得最好的效果，这得益于其较大的参数量和更高质量的指令。此外，且相比较于效果最好的开源模型Vicuna-13B，我们的YuLan-Chat-13B也能够取得优于它的效果，进一步展示出我们指令数据的优势。



## 参考资料

[YuLan-Chat：基于高质量中英文混合指令微调的大语言对话模型](https://mp.weixin.qq.com/s/nPS4N3stAAG_51fnZANbMA)


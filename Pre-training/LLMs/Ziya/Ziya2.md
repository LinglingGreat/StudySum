## 训练

![](img/Pasted%20image%2020231121101957.png)

基于LLaMA2-13B扩充词表，训练了700B tokens

第一阶段：大量高质量中英文数据

第二阶段：加入了有监督数据

第三阶段：focus在数学数据

## 数据预处理

![](img/Pasted%20image%2020231121112430.png)

处理速度：15GB/per hour，通过这个积累了4.5T的数据

DP：语言检测，只选择中英文数据；标准化语料的编码格式，将所有中文转成简体；删除无用的tokens比如不可见的控制字符，特殊符号，表情，不正确的标点符号。

AS：用KenLM，中英文维基百科数据训练了2个语言模型，在输入数据上执行ppl评估。ppl分数从低到高排序，选择top 30%的作为高质量数据，30%-60%的作为中等质量数据。

RF：在文档、段落、句子这三个粒度设计了30多种过滤规则，从大粒度到小粒度依次过滤。在文档粒度，规则主要围绕内容长度和格式设计，在段落和句子粒度，规则围绕内容的有毒性。规则设计过程会随机采样一些数据进行人工评估，然后迭代优化规则。

CD：布隆过滤器和Simhash去重。
- 发现CC等开源数据中存在大量重复的网页，用布隆过滤器去重URL，减少后续内容去重的复杂度。
- 发现很多网页内容相似，不同之处在于特殊符号（比如标点和表情），针对这些网页进行一轮精准去重。
- simhash进行粗略去重
- 采样去重数据评估
- 用了cache和bucket技术，使得新数据不需要对所有老数据进行冗余检查。

## 评估

![](img/Pasted%20image%2020231121102555.png)


### Yuan2.0-2B

Yuan2.0-2B是浪潮发布的Yuan2.0大模型的2B版本，采用中英文的高质量资料，包括书籍、百科、论文等。Yuan2.0-2B模型层数24层，隐藏层维度2048，支持最大长度8192。

```
Github: https://github.com/IEIT-Yuan/Yuan-2.0HF: https://huggingface.co/IEITYuan/Yuan2.0-2BPaper: https://arxiv.org/ftp/arxiv/papers/2311/2311.15786.pdf
```

### Qwen-1.8B

Qwen-1.8B是阿里发布的Qwen大模型的1.8B版本，采用2.2TB Token的数据进行预训练，包含高质量中、英、多语言、代码、数学等数据，涵盖通用及专业领域的训练语料。Yuan2.0-2B模型层数24层，隐藏层维度2048，支持最大长度8192，并且开源了对应的Chat模型。

```
Github: https://github.com/QwenLM/QwenHF: https://huggingface.co/Qwen/Qwen-1_8BPaper: https://arxiv.org/pdf/2309.16609.pdf
```

### Bloom-1.7B&1.1B

Bloom-1.7B&1.1B是Hugging Face牵头组织的BigScience项目开源的Bloom大模型的1.7B和1.1B版本。训练数据涉及46种自然语言和13种编程语言，共计1.6TB的文本数据。Bloom-1.7B模型层数24层，隐藏层维度2048，支持最大长度2048。Bloom-1.1B模型层数24层，隐藏层维度1536，支持最大长度2048。

```
HF: https://huggingface.co/bigscience/bloomPaper: https://arxiv.org/pdf/2211.05100.pdf
```

### Pythia-1.4B&1B

Pythia-1.4B&1B是EleutherAI开源的Pythia的1.4B和1B版本。主要使用300B Token的The Pile数据集进行训练。Pythia-1.4B模型层数24层，隐藏层维度2048。Pythia-1B模型层数16层，隐藏层维度2048。

```
Github: https://github.com/EleutherAI/pythiaHF: https://huggingface.co/EleutherAI/pythia-1b&&HF: https://huggingface.co/EleutherAI/pythia-1.4bPaper: https://arxiv.org/pdf/2304.01373.pdf
```

### Phi-1&Phi-1.5

Phi-1&Phi-1.5是微软开源的Phi大模型的两个不同版本，均有1.3B参数，模型层数24层，隐藏层维度2048。Phi-1模型训练54B Token的数据，而Phi-1.5模型训练150B Token的数据。

```
HF: https://huggingface.co/microsoft/phi-1Paper: https://arxiv.org/pdf/2306.11644.pdf&HF: https://huggingface.co/microsoft/phi-1_5Paper: https://arxiv.org/pdf/2309.05463.pdf
```

### Deepseek-Coder-1.3B

Deepseek-Coder-1.3B是深度求索发布开源的Deepseek-Coder的1.3B版本，采用1TB Token的数据进行预训练，数据由87%的代码和13%的中英文自然语言组成，模型层数24层，隐藏层维度2048。

```
Github: https://github.com/deepseek-ai/deepseek-coderHF: https://huggingface.co/deepseek-ai/deepseek-coder-1.3b-base
```

### Galactica-1.3B

Galactica-1.3b是MetaAI开源的Galactica大模型的1.3B版本，采用106B Token数据进行训练，数据主要来自论文、参考资料、百科全书和其他科学来源等。模型层数24层，隐藏层维度2048。

```
Github: https://github.com/paperswithcode/galai/blob/main/docs/model_card.mdHF: https://huggingface.co/facebook/galactica-1.3bPaper: https://galactica.org/static/paper.pdf
```

### GPT-Sw3-1.3B

GPT-Sw3-1.3B是AI Sweden开源的GPT-SW3大模型的1.3B版本，采用320B Token数据进行训练，数据主要由瑞典语、挪威语、丹麦语、冰岛语、英语和编程代码数据集组成。模型层数24层，隐藏层维度2048。

```
HF: https://huggingface.co/AI-Sweden-Models/gpt-sw3-1.3b
```

### GPT-Neo-1.3B

GPT-Neo-1.3B是EleutherAI开源的GPT-Neo大模型的1.3B版本，GPT-Neo模型主要为了复现的GPT-3模型，采用380B Token数据进行训练，模型层数24层，隐藏层维度2048。

```
HF: https://huggingface.co/EleutherAI/gpt-neo-1.3B
```

### OPT-1.3B

OPT-1.3B模型是由MetaAI开源的OPT大模型的1.3B版本，采用180B Token数据进行训练，模型层数24层，隐藏层维度2048。

```
HF: https://huggingface.co/facebook/opt-1.3bPaper: https://arxiv.org/pdf/2205.01068.pdf
```

### TinyLlama-1.1B

TinyLlama模型是一个1.1B参数的Llama模型，旨在3TB Token的数据上进行训练，目前训练到2.5TB Token数据，模型层数22层，隐藏层维度2048。

```
Github: https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1195k-token-2.5THF: https://huggingface.co/facebook/opt-1.3b
```

## 参考资料

[1-2B参数规模大模型使用心得及模型汇总](https://mp.weixin.qq.com/s/FNPHSQyOq46QWC9e2vRBDw)


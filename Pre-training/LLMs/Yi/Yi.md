---
title: Yi
created: 2024-03-09
tags:
  - 大模型
type: 论文
papername: Yi-Open Foundation Models by 01.AI
conference: ACL/EMNLP/NAACL/EACL/COLING/SIGIR/AAAI/ICML/ICDM
year: 2024
institution:
  - 零一万物
---

## 论文基本信息

标题：Yi: Open Foundation Models by 01.AI

作者：零一万物

链接：http://arxiv.org/abs/2403.04652

代码：https://github.com/01-ai/Yi

框架图：


## 背景
这篇论文介绍了Yi模型家族，这是一个展示出多维度能力的一系列语言和多模态模型。Yi模型家族基于6B和34B预训练语言模型，并将其扩展到聊天模型、200K长上下文模型、深度上扩展模型和视觉-语言模型。

这篇论文试图通过数据工程（预训练和微调）、模型架构优化、基础设施支持和安全性措施，开发出性能强大、可部署、安全可靠的语言模型，以推动人工智能领域的发展。

在设计Yi系列模型时，主要考虑以下几个维度：model scale, data scale, data quality.
- model scale，选择能够在消费级显卡4090（24G显存）上运行，但仍然足够大，具有复杂推理和涌现能力的模型。因此34B是个不错的选择。
- data scale, 将训练数据增加到3.2T tokens，弥补compute flops的缺失。我们比计算最优（1T）（scaling law）的tokens更多，进行过度训练，使得在Int4量化后模型性能几乎没有损失。
- data quality, 数据工程原则是注重质量而不是数量，不管是预训练阶段还是微调阶段。

## 相关研究
这篇论文中提到了多项相关研究，它们在不同方面为Yi模型的开发提供了基础和灵感。以下是一些关键的相关研究：

1. **GPT-3.5 和 GPT-4**：作为大型语言模型的先驱，OpenAI的GPT系列模型在性能和应用方面设定了高标准。
    
2. **Chinchilla**：Chinchilla模型的研究展示了在特定数据规模和模型大小下训练语言模型的最优解。
    
3. **LLaMA**：LLaMA模型的研究提供了关于模型规模和训练数据量之间关系的见解。
    
4. **FLAN (Finetuned Language Models)**：FLAN系列研究展示了如何通过指令微调来扩展语言模型的能力。
    
5. **UltraChat**：UltraChat系列研究关注于通过扩展高质量的指令对话数据来增强聊天模型。
    
6. **LIMA (Less is More for Alignment)**：LIMA研究强调了在微调过程中数据质量的重要性，与Yi模型的方法论相呼应。
    
7. **SwiGLU**：SwiGLU激活函数的研究为Yi模型提供了一种减少激活大小的方法，以提高效率。
    
8. **Grouped-Query Attention (GQA)**：GQA的研究为Yi模型提供了一种减少训练和推理成本的注意力机制。
    
9. **Rotary Position Embedding (RoPE)**：RoPE的研究使得模型能够支持更长的上下文窗口。
    
10. **Vision Transformer (ViT)**：ViT的研究为Yi模型的视觉-语言模型提供了基础。
    
11. **CLIP ViT-H/14**：CLIP ViT-H/14模型为Yi-VL模型提供了图像编码的初始化。
    
12. **LAION-400M**：LAION-400M数据集为Yi-VL模型的训练提供了大量的图像-文本对。
    
13. **ZeRO (Zero Redundancy Optimizer)**：ZeRO的研究为Yi模型的内存优化提供了方法。
    
14. **FlashAttention**：FlashAttention的研究为Yi模型提供了快速且内存高效的注意力机制。
    
15. **NeRF (Neural Radiance Fields)**：NeRF的研究为Yi模型的深度上扩展提供了灵感。
    

这些研究为Yi模型的开发提供了宝贵的经验和技术，使得Yi模型能够在多个方面实现创新和优化。


## 核心亮点

### 数据工程

构建了一个3.1万亿个英文和中文语料库，使用级联数据去重和质量过滤管道，有意提高重复数据删除强度，以确保预训练数据的高质量。复杂的过滤pipeline包括language, heuristic textual features, perplexity, semantics, topic, and safety, as well as a cascaded deduplication process based on paragraph, MinHash, and exact matching。

![](img/Pasted%20image%2020240309145811.png)

- 具体的启发式过滤规则可以看论文。
- learned filters包括perplexity scorer（KenLM, 过滤ppl高的）, quality scorer（和wikipedia质量相似的算质量高）, safety scorer（暴力、色情、政治等有毒内容）, and document coherence scorer（由不同的句子或段落组成导致不连贯的低质量的网络文档。此类文档要么被分段以供进一步分析，要么被完全删除）。
- 聚类分析。使用无监督语义聚类对网络文档进行分组。这种聚类过程可以有效地识别和分析具有相似语义特征的文档。聚类后​​的数据随后被进行质量打标，为Yi的数据混合策略的优化提供重要参考。通过自动和手动验证确定为低质量的文档将从数据集中排除。
- 去重。过滤后，我们按照 Penedo 等人中的过程实施全面的重复数据删除管道。该管道集成了文档级 MinHash 重复数据删除和子文档精确匹配重复数据删除，可有效识别和删除文档内和文档间的重复内容。我们使用主题模型预测新闻、广告和基于知识的内容等标签，进一步将网络文档分类为特定主题。在最终的预训练数据集中，我们对不太有用的内容（主要是广告）进行下采样，以确保信息密度。

![](img/Pasted%20image%2020240309150612.png)




对于微调数据，精心设计并迭代优化了一个小于10K的指令数据集，确保每个实例都经过机器学习工程师的直接验证。

我们采用了广泛的网格搜索来识别最佳数据组成、促进多样性并发现有效的超参数。

除了关注数据质量和多样性之外，我们的观察表明数据格式极大地影响了模型的最终性能。为此，我们实现了 ChatML 风格的格式。这种结构化方法使模型能够区分各种信息类型，例如系统配置、用户输入和助理响应。

4096的长度，batch size=64, 常数学习率10^-5，训练300步，还用了NEFTune。

提高了模型在主要评估平台上的人类偏好率，如AlpacaEval和Chatbot Arena。

### tokenization

64000大小，数字分成单个数字，我们允许罕见字符回退到 unicode 字节编码以确保容错。我们使用身份标记器来避免将所有标点符号转换为半角格式。

优先考虑英语的法学硕士通常在其标记器中使用虚拟前缀（文本开头的空格）来概括句子不同位置的相同单词。我们不使用这种方法，因为即使在英语上下文中，该假设也并不总是成立，特别是对于以引号开头的句子，而且它在中文上下文中也没有显示出积极的效果。

### 模型架构

![](img/Pasted%20image%2020240309152223.png)
    
使用标准的Transformer架构，并对其进行了修改，包括引入Grouped-Query Attention (GQA)、SwiGLU激活函数和调整基础频率的Rotary Position Embedding (RoPE)。

RoPE ABF，base模型是在4k上训练的。我们继续使用预训练数据混合物中的 10B 个token对模型进行预训练，并使用稍微上采样的长序列（主要来自书籍）。我们观察到，仅 1-2B 个 token 就足以让模型在 4K-200K 长度上收敛到低损失，并且轻量级微调进一步带来近乎完美的长上下文检索性能。

        
### 基础设施支持
    
开发了支持Yi模型全栈开发的基础设施，包括跨云弹性任务调度、自动故障恢复和拓扑感知资源分配。可以根据 GPU 可用性自动将预训练作业弹性缩放到不同的节点大小。更重要的是，所有与训练相关的超参数都将同时无缝缩放。
        
为了高效的推理，使用了4位模型和8位KV缓存量化，结合PagedAttention和Dynamic Batching。使得模型能够在消费级硬件上部署，如RTX 4090 GPU。

我们实现并改进了计算通信重叠、序列并行性和通信压缩，以支持高达 200K 上下文长度的持续预训练和微调。我们将上下文长度扩展到 200K 的方法完全基于工程，也就是说，我们没有修改稀疏、局部或滑动窗口注意力等模型架构——即使输入是 200K，模型仍然使用完全注意力。

        
### 安全性和可靠性
    
开发了全面的负责任的AI安全引擎（RAISE），确保模型在预训练、对齐和部署阶段的安全性。
        
在预训练阶段，构建了基于启发式规则、关键词匹配和学习分类器的过滤器，以去除包含个人标识符和私人数据的文本，并减少性、暴力和极端内容。


### 性能评估

### 预训练模型

在多个标准基准上评估了Yi预训练模型的性能，包括Commonsense Reasoning、Reading Comprehension、Math、Code、其他流行的benchmark等。

![](img/Pasted%20image%2020240309155418.png)

![](img/Pasted%20image%2020240309160139.png)

与之前的工作（通常≤ 2T）相比，通过对数量显着增加的令牌（3.1T）进行训练，我们观察到基准测试中的性能得到了显着提升，如表 2 所示。但是，值得注意的是，仍然存在一些问题我们的模型与现有的开源和闭源模型之间存在明显的差异，特别是在与数学和编码相关的任务方面。由于通过持续的预训练和指令微调可以显着提高这些领域的性能，因此在做出初始设计选择时，我们避免在预训练语料库中纳入大量的数学和编码内容。我们确实计划在未来发布具有增强数学和编码功能的模型。

发现
1. 模型放大带来的性能提升（6B到34B）
2. 较高质量预训练数据的较小模型（如 Yi-34B 或 Qwen-14B）通常比较大尺寸但（可能）较低质量数据的模型（如 Falcon-180B）表现出更好的性能。
3. 我们注意到开源 LLM 在各种基准测试中的性能仍然落后于 GPT-4 和 GPT-3.5。然而具有代表性的双语法学硕士，例如Qwen-14B和Yi-34B在中文知识相关基准上可以匹配甚至超越GPT-4的性能，包括C-Eval、CMMLU和高考。然而，在 BBH、代码 (HumanEval) 和数学 (MATH) 等推理相关基准上，GPT-4 与开源模型仍然存在巨大差距。

评估模型在上下文学习方面的能力，特别是在推断加权和函数的线性系数任务上。

![](img/Pasted%20image%2020240309160936.png)

当setting the linear coefficients of be [1, -1]时，Yi-34B和LLaMA-2 70B在exact match中表现最好。如果increase the number of the linear coefficients to be [1, 1, 1, 1, 1]，我们观察到只有大型模型（LLaMA-2 70B 和 Mixtral）才能在精确匹配上取得良好分数的涌现行为，尽管difference to target更加连续。

![](img/Pasted%20image%2020240309160716.png)

#### 对话模型的评估

通过自动评估和人类评估来评估聊天模型的性能，确保模型能够理解和遵循人类指令。

![](img/Pasted%20image%2020240309161551.png)


我们特别强调 4 位量化结果，因为 4 位量化大大降低了内存需求，而模型性能几乎没有下降。这一观察结果是在消费级设备上提供该模型的基础。

![](img/Pasted%20image%2020240309161607.png)

图 4 的结果表明，Yi-34B-Chat 在 GSM8K 和匈牙利数学考试中都表现出色。但请注意，Yi-6B-Chat 并没有表现出很强的数学能力（在 GSM8K 和匈牙利数学考试上）。我们推测，较小的模型在 SFT 阶段可能需要更多的数据来激活其相应的能力。
        
![](img/Pasted%20image%2020240309161650.png)

我们通过比较数据缩放期间偏好增加的速度来进一步证明数据质量。如图5所示，与UltraChat 及其清理版本UltraChat 200K相比，我们在扩大Yi数据时看到了明显的性能提升趋势。
    
### 能力扩展

#### 长上下文
    
通过持续预训练，将上下文长度扩展到200K，以提高模型在长上下文任务中的性能。

我们的长上下文解决方案包括持续的预训练和微调阶段，两者都是轻量级的。我们持有这样的基本假设：在 200K 输入上下文中的任何地方利用信息的潜力已经存在于基础模型中（与 Fu 等人 22 相同），继续预训练阶段“解锁”了这种能力，这一点可以通过在大海捞针测试，然后微调阶段进一步调整响应风格以遵循人类的指示和偏好。

我们继续使用序列并行性和分布式注意力来预训练全注意力模型。这就是说，我们不使用任何稀疏或线性注意力，而是使用强力实现完整注意力。我们继续在 (1) 原始预训练数据（2）长度上采样的长上下文数据，其中长文档主要来自书籍； （3）多文档问答合成数据，我们构建问答对，其中答案包含答案之前的相关段落的复述；这些数据混合上预训练 Yi 6B/ 34B 基本模型。我们的数据方法主要遵循 Fu 等人的数据工程实践。我们继续在 5B 个令牌和 4M 批量大小上对模型进行预训练，这意味着 100 个优化步骤。与 Fu 等人的并发工作保持一致。我们观察到这种轻量级的持续预训练已经能够在大海捞针测试中实现强大的性能，如图 6 所示。


微调：我们将短上下文 SFT 数据与长上下文文档问答数据混合在一起。我们使用模型辅助的自动化方法（即合成数据）来构建文档质量检查。具体来说，我们将多个文档随机连接成一个序列，从长序列中采样一个或多个段落，并要求聊天模型构建问题和答案对。一个重要的细节是背诵和改述：在给出答案之前，我们要求模型背诵或改述原始段落。这种数据格式鼓励模型的检索行为，从而阻止幻觉行为：给定一个问题，模型更有可能使用输入中的信息来构建答案，而不是使用其内部知识，这些知识可能相关但不准确。我们的微调模型已部署在 www.wanzhi01.com 上，我们鼓励读者尝试一下。



![](img/Pasted%20image%2020240309162527.png)

#### Vision-Language
        
通过将Yi模型与视觉Transformer编码器结合，训练模型以将视觉表示与语言模型的语义空间对齐，使模型具备视觉理解能力，开发了Yi Vision Language (Yi-VL)模型。

![](img/Pasted%20image%2020240309162705.png)

分了3阶段进行训练。stage1训练ViT和投影模块的参数，使用224^2的分辨率。stage2将分辨率提升到448^2。stage3训练图文对，提高模型在多模态聊天的能力。

![](img/Pasted%20image%2020240309163548.png)

#### Depth Upscaling

通过复制中间层来增加模型的深度，提高了模型的推理能力。Yi-6B一共32层，复制中间的12-28这16层，得到48层的Yi-9B模型。

我们的调查表明，可以通过评估每层输入和输出之间的余弦相似度分数来决定复制哪些层。这种方法允许有针对性的模型扩展，而无需额外的预训练，只会对性能产生最小的影响。这种对性能的最小影响归因于重复层的输入和输出之间的高余弦相似性（接近 1），如图 8 所示。这一观察表明这些层的复制不会显着改变由原始模型产生的 logits输出.该方法通过基于层的内部处理动态优化其架构来确保模型的有效扩展。

![](img/Pasted%20image%2020240309164248.png)

该数据集由两个阶段的约 800B tokens组成，其中约 70% 是最近收集和精心挑选的。我们在最后阶段增强了代码覆盖率，以提高代码性能。

为了优化训练过程，我们保持 3e-5 的恒定学习率，并采用一种策略方法，每当模型的损失趋于稳定时，从 4M 令牌逐渐增加批量大小。这种批量大小的增量调整，以及保持所有其他参数与已建立的 Yi-6B 基本模型配置保持一致，有助于应对大规模训练的挑战。

表 8 展示了这些策略的有效性，其中详细介绍了 Yi-9B 基础模型在各种基准上的性能，包括常识、推理、知识、编码和数学。它强调了 Yi-9B 基础模型在特定领域的竞争优势，说明了我们的方法通过优化调整数据特征和模型大小之间的相互作用来增强模型性能的有效性。

![](img/Pasted%20image%2020240309164837.png)



### 社区贡献
    
提供了与GPT-3.5相当的质量但成本效益更高的模型，使研究人员和开发者能够构建基于语言模型的应用程序。
        
通过本地可运行的聊天机器人，帮助保护用户数据隐私。
        

通过这些方法，论文不仅解决了如何开发高性能、可部署、安全可靠的语言模型的问题，而且还为社区提供了有价值的资源和工具。

## 未来方向
**A**: 尽管论文中已经进行了广泛的实验和评估，但仍有一些领域可以进一步探索：

1. **更广泛的多语言支持**：虽然Yi模型家族在英文和中文上表现出色，但可以探索其在更多语言上的性能和适用性。
    
2. **更复杂的推理任务**：研究Yi模型在处理更复杂的逻辑推理、抽象思维和创造性任务上的能力。
    
3. **长期记忆和上下文理解**：尽管已经扩展了上下文长度，但可以进一步研究如何在更长的文本中保持连贯性和理解力。
    
4. **跨模态理解**：探索Yi模型在处理图像、视频和音频等非文本输入时的性能，以及如何整合这些模态以提供更丰富的交互体验。
    
5. **安全性和偏见减少**：深入研究如何进一步减少模型输出中的偏见和不安全内容，以及如何提高模型的透明度和可解释性。
    
6. **实时交互和对话系统**：研究如何将Yi模型集成到实时对话系统中，以提供更自然和流畅的用户体验。
    
7. **硬件优化和部署**：探索在不同硬件平台上优化Yi模型性能的方法，包括移动设备和边缘计算设备。
    
8. **社会影响和伦理考量**：研究Yi模型在现实世界应用中的潜在社会影响，以及如何确保其符合伦理标准。
    
9. **持续学习和适应性**：研究Yi模型如何通过持续学习适应不断变化的数据分布和用户需求。
    
10. **开源社区的贡献**：考虑如何将Yi模型的研究成果更广泛地贡献给开源社区，促进共享和协作。
    

这些探索点可以帮助研究者和开发者更好地理解Yi模型的潜力，同时也为未来的研究提供了新的方向。


## 主要收获


## 参考资料

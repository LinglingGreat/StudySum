



目前主流的方法大致有以下几种（reference只列举出了比较有代表性的）：

1. **重采样（re-sampling）**：这是解决数据类别不平衡的非常简单而暴力的方法，更具体可以分为两种，对少样本的**过采样**[[1\]](#ref_1)，或是对多样本的**欠采样**[[2\]](#ref_2)。当然，这类比较经典的方法一般效果都会欠佳，因为过采样容易overfit到minor classes，无法学到更鲁棒易泛化的特征，往往在非常不平衡的数据上泛化性能会更差；而欠采样则会直接造成major class严重的信息损失，甚至会导致欠拟合的现象发生。
2. **数据合成（synthetic samples）**：若不想直接重复采样相同样本，一种解决方法是生成和少样本相似的“新”数据。一个最粗暴的方法是直接对少类样本加随机高斯噪声，做data smoothing[[3\]](#ref_3)。此外，此类方法中比较经典的还有SMOTE[[4\]](#ref_4)，其思路简单来讲是对任意选取的一个少类的样本，用K近邻选取其相似的样本，通过对样本的线性插值得到新样本。说道这里不禁想到和mixup[[5\]](#ref_5)很相似，都是在input space做数据插值；当然，对于deep model，也可以在representation上做mixup（manifold-mixup）。基于这个思路，最近也有imbalance的mixup版本出现[[6\]](#ref_6)。
3. **重加权（re-weighting）**：顾名思义，重加权是对不同类别（甚至不同样本）分配不同权重，主要体现在重加权不同类别的loss来解决长尾分布问题。注意这里的权重可以是自适应的。此类方法的变种有很多，有最简单的按照类别数目的倒数来做加权[[7\]](#ref_7)，按照“有效”样本数加权[[8\]](#ref_8)，根据样本数优化分类间距的loss加权[[9\]](#ref_9)，等等。对于max margin的这类方法，还可以用bayesian对每个样本做uncertainty估计，来refine决策边界[[10\]](#ref_10)。这类方法目前应该是使用的最广泛的，就不贴更多的reference了，可以看一下这个survey paper[[3\]](#ref_3)。
4. **迁移学习（transfer learning）**：这类方法的基本思路是对多类样本和少类样本分别建模，将学到的多类样本的信息/表示/知识迁移给少类别使用。代表性文章有[[11\]](#ref_11)[[12\]](#ref_12)。
5. **度量学习（metric learning）**：本质上是希望能够学到更好的embedding，对少类附近的boundary/margin更好的建模。有兴趣的同学可以看看[[13\]](#ref_13)[[14\]](#ref_14)。这里多说一句，除了采用经典的contrastive/triplet loss的思路，最近火起来的contrastive learning，即做instance-level的discrimination，是否也可以整合到不均衡学习的框架中？
6. **元学习/域自适应（meta learning/domain adaptation）**：这部分因为文章较少且更新一点，就合并到一起写，最终的目的还是分别对头部和尾部的数据进行不同处理，可以去自适应的学习如何重加权[[15\]](#ref_15)，或是formulate成域自适应问题[[16\]](#ref_16)。
7. **解耦特征和分类器（decoupling representation & classifier）**：最近的研究发现将特征学习和分类器学习解耦，把不平衡学习分为两个阶段，在特征学习阶段正常采样，在分类器学习阶段平衡采样，可以带来更好的长尾学习结果[[17\]](#ref_17)[[18\]](#ref_18)。

## 参考资料

[NeurIPS 2020 | 数据类别不平衡/长尾分布？不妨利用半监督或自监督学习](https://zhuanlan.zhihu.com/p/259710601)

有代码：https://github.com/YyzHarry/imbalanced-semi-self


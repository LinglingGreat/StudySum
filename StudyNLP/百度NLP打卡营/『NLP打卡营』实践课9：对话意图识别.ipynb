{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 基于预训练模型的槽位填充和意图识别\n",
    "\n",
    "本示例将展示如何使用ERNIE预训练模型完成任务行对话中的槽位填充和意图识别任务，这两个任务是一个pipline型任务对话系统的基石。\n",
    "\n",
    "本示例使用的数据集为CrossWOC中文对话数据集。该数据集包含多个领域，包括景点，餐馆，酒店，交通等。\n",
    "\n",
    "原始数据和数据处理脚本请参见 [CrossWOZ](https://github.com/thu-coai/CrossWOZ)。\n",
    "\n",
    "## 环境要求\n",
    "\n",
    "* PaddlePaddle\n",
    "\n",
    "   本项目依赖于 PaddlePaddle 2.0 及以上版本，请参考 [安装指南](https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/install/pip/windows-pip.html) 进行安装\n",
    "\n",
    "* PaddleNLP \n",
    "\n",
    "   ```shell\n",
    "   pip install --upgrade paddlenlp -i https://pypi.org/simple\n",
    "   ```\n",
    "\n",
    "* Python\n",
    "\n",
    "    Python的版本要求 3.6+\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "AI Studio平台后续会默认安装PaddleNLP，在此之前可使用如下命令安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting paddlenlp\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/0b/2ebd839e7393c66ce7d2a25f008680b8d6812b0e29508326d684bcbbf7ba/paddlenlp-2.0.1-py3-none-any.whl (375kB)\n",
      "\u001b[K     |████████████████████████████████| 378kB 253kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\n",
      "Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\n",
      "Collecting multiprocess (from paddlenlp)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/20/458ac043a57322365ac2ed86a911bf7598fc2e49bccb3f94ea810fbb6b9b/multiprocess-0.70.11.1-py37-none-any.whl (108kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 118kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.7 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.20.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.21.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.14.0)\n",
      "Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.7.1.1)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.8.2)\n",
      "Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.8.53)\n",
      "Requirement already satisfied, skipping upgrade: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.10.1)\n",
      "Requirement already satisfied, skipping upgrade: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (16.7.9)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.23)\n",
      "Requirement already satisfied, skipping upgrade: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (5.1.2)\n",
      "Requirement already satisfied, skipping upgrade: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.4.10)\n",
      "Requirement already satisfied, skipping upgrade: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.4)\n",
      "Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (1.25.6)\n",
      "Requirement already satisfied, skipping upgrade: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (3.9.9)\n",
      "Requirement already satisfied, skipping upgrade: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (0.18.0)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.5->Flask-Babel>=1.0.0->visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl->paddlenlp) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl->paddlenlp) (7.2.0)\n",
      "Installing collected packages: multiprocess, paddlenlp\n",
      "  Found existing installation: paddlenlp 2.0.0rc7\n",
      "    Uninstalling paddlenlp-2.0.0rc7:\n",
      "      Successfully uninstalled paddlenlp-2.0.0rc7\n",
      "Successfully installed multiprocess-0.70.11.1 paddlenlp-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade paddlenlp -i https://pypi.org/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 示例流程\n",
    "\n",
    "与大多数NLP任务相同，本次示例的展示流程分为以下四步：\n",
    "\n",
    "首先我们从数据准备开始。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/dd30e17318fb48fabb5701fd8a97be8176a1e372dd134cc0826e58cb5401933d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 数据准备\n",
    "\n",
    "数据准备流程如下：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/89ba02da6f914297ae2fc438d1c9f773556f226652134fb684ac0186bfa5bb7d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. 使用`load_dataset()`自定义数据集\n",
    "\n",
    "使用官方脚本预处理过的数据集已经上传至项目根目录，观察数据集格式后我们可以根据数据格式写出数据文件读取函数，传入`load_dataset()`。即可创建数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def convert_to_list(value, n, name, dtype=np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['你', '好', '，', '麻', '烦', '帮', '我', '推', '荐', '一', '个', '门', '票', '免', '费', '的', '景', '点', '。']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B+Inform+景点+门票', 'I+Inform+景点+门票', 'O', 'O', 'O', 'O']\n",
      "['General+greet+none+none', 'Request+景点+名称+']\n",
      "[]\n",
      "\n",
      "['你', '好', '，', '您', '可', '以', '选', '择', '故', '宫', '，', '八', '达', '岭', '长', '城', '，', '颐', '和', '园', '或', '者', '红', '砖', '美', '术', '馆', '。']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B+Inform+景点+名称', 'I+Inform+景点+名称', 'I+Inform+景点+名称', 'I+Inform+景点+名称', 'I+Inform+景点+名称', 'O']\n",
      "['General+greet+none+none']\n",
      "['你好，麻烦帮我推荐一个门票免费的景点。']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.datasets import load_dataset\n",
    "import json\n",
    "\n",
    "# 读取标签文件并创建label_map\n",
    "def get_label_map(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf8\") as f:\n",
    "        label_list = json.load(f)\n",
    "    \n",
    "    id2label = dict([(idx, label) for idx, label in enumerate(label_list)])\n",
    "    label2id = dict([(label, idx) for idx, label in enumerate(label_list)])\n",
    "    return id2label, label2id\n",
    "\n",
    "id2slot, slot2id = get_label_map('slot_labels.json')\n",
    "id2intent, intent2id = get_label_map('intent_labels.json')\n",
    "\n",
    "intent_weight = [1] * len(intent2id)\n",
    "\n",
    "# 根据本地文件格式定义数据读取生成器\n",
    "def read(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf8\") as f:\n",
    "        data = json.load(f)\n",
    "    for entry in data:\n",
    "        yield {\n",
    "            'words': entry[0],\n",
    "            'slots': entry[1],\n",
    "            'intents': entry[2],\n",
    "            'history': entry[4],\n",
    "        }\n",
    "\n",
    "# 将生成器传入load_dataset\n",
    "train_ds = load_dataset(read, filename='train.json', lazy=False)\n",
    "dev_ds = load_dataset(read, filename='test.json', lazy=False)\n",
    "\n",
    "for idx in range(2):\n",
    "    print(train_ds[idx]['words'])\n",
    "    print(train_ds[idx]['slots'])\n",
    "    print(train_ds[idx]['intents'])\n",
    "    print(train_ds[idx]['history'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "关于更多自定义数据集相关内容，请参考[如何自定义数据集](https://paddlenlp.readthedocs.io/zh/latest/data_prepare/dataset_self_defined.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. 加载 `paddlenlp.transformers.ErnieTokenizer`用于数据处理\n",
    "\n",
    "文本数据在输入ERNIE预训练模型之前，需要经过数据处理转化为Feature。这一过程通常包括分词，token to id，add special token等步骤。  \n",
    "\n",
    "**PaddleNLP对于各种预训练模型已经内置了相应的tokenizer**，指定想要使用的模型名字即可加载对应的tokenizer。\n",
    "\n",
    "可以通过调用tokenizer中的方法简单的完成上述数据处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-05-31 14:58:44,883] [    INFO] - Downloading vocab.txt from https://paddlenlp.bj.bcebos.com/models/transformers/ernie/vocab.txt\n",
      "100%|██████████| 90/90 [00:00<00:00, 4006.15it/s]\n"
     ]
    }
   ],
   "source": [
    "import paddlenlp\n",
    "\n",
    "# 设置模型名称\n",
    "MODEL_NAME = 'ernie-1.0'\n",
    "tokenizer = paddlenlp.transformers.ErnieTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. 调用`map()`方法批量处理数据\n",
    "\n",
    "由于我们传入了`lazy=False`，所以我们使用`load_dataset()`自定义的数据集是`MapDataset`对象。\n",
    "\n",
    "`MapDataset`是`paddle.io.Dataset`的功能增强版本。其内置的`map()`方法适合用来进行批量数据集处理。\n",
    "\n",
    "`map()`方法传入的是一个用于数据处理的function。正好可以与tokenizer相配合。\n",
    "\n",
    "以下是本示例中的用法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "def convert_example(example, tokenizer, use_history=False, no_entity_id=0, max_seq_len=512, mode='train'):\n",
    "    \n",
    "    # 调用tokenizer的数据处理方法把文本转为id\n",
    "    tokenized_input = tokenizer(\n",
    "        example['words'],\n",
    "        is_split_into_words=True,\n",
    "        max_seq_len=max_seq_len)\n",
    "    \n",
    "    # 对槽位标签做截断，保证和input_id等长\n",
    "    slot_labels = example['slots']\n",
    "    if len(tokenized_input['input_ids']) - 2 < len(slot_labels):\n",
    "        slot_labels = slot_labels[:len(tokenized_input['input_ids']) - 2]\n",
    "    \n",
    "    # 根据label_map将槽位标签转为id\n",
    "    tokenized_input['slot_labels'] = [no_entity_id] + [slot2id[label] for label in slot_labels] + [no_entity_id]\n",
    "    \n",
    "    # 由于任务中的意图识别是多标签分类，需要把意图标签转为类似one-hot的格式\n",
    "    intent_labels = np.zeros(len(intent2id), dtype='int64')\n",
    "    for l in example['intents']:\n",
    "        intent_labels[intent2id[l]] = 1\n",
    "        if mode == 'train':\n",
    "            # 统计训练集中每个意图的正样本数\n",
    "            intent_weight[intent2id[l]] += 1\n",
    "    tokenized_input['intent_labels'] = intent_labels\n",
    "    \n",
    "    # 将历史对话用[SEP]拼起来并转成id\n",
    "    if use_history:\n",
    "        tokenized_history = tokenizer(\n",
    "            tokenizer.cls_token+tokenizer.sep_token.join(example['history']),\n",
    "            max_seq_len=max_seq_len)\n",
    "        tokenized_input['history_ids'] = tokenized_history['input_ids']\n",
    "    else:\n",
    "        tokenized_input['history_ids'] = []\n",
    "    return tokenized_input\n",
    "\n",
    "use_history = False\n",
    "max_seq_length = 512\n",
    "\n",
    "train_trans_func = partial(\n",
    "        convert_example,\n",
    "        tokenizer=tokenizer,\n",
    "        mode='train',\n",
    "        use_history=use_history,\n",
    "        max_seq_len=max_seq_length)\n",
    "\n",
    "dev_trans_func = partial(\n",
    "        convert_example,\n",
    "        tokenizer=tokenizer,\n",
    "        mode='dev',\n",
    "        use_history=use_history,\n",
    "        max_seq_len=max_seq_length)\n",
    "\n",
    "train_ds.map(train_trans_func, lazy=False)   \n",
    "dev_ds.map(dev_trans_func, lazy=False)\n",
    "\n",
    "# 根据意图的正样本数和总样本数为不同样本赋予不同的权重\n",
    "for intent, intent_id in intent2id.items():\n",
    "    neg_pos = (len(train_ds) - intent_weight[intent_id]) / intent_weight[intent_id]\n",
    "    intent_weight[intent_id] = np.log10(neg_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 226, 170, 4, 1298, 1934, 836, 75, 426, 1645, 7, 27, 232, 1039, 783, 453, 5, 561, 180, 12043, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0]\n",
      "[1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "[1, 226, 170, 4, 892, 48, 22, 352, 790, 470, 915, 4, 643, 302, 1560, 84, 257, 4, 3540, 14, 509, 172, 123, 536, 1520, 188, 133, 774, 12043, 2]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 0, 0]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in range(2):\r\n",
    "    print(train_ds[idx]['input_ids'])\r\n",
    "    print(train_ds[idx]['token_type_ids'])\r\n",
    "    print(train_ds[idx]['slot_labels'])\r\n",
    "    print(train_ds[idx]['intent_labels'])\r\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "从以上结果可以看出，数据集中的example已经被转换成了模型可以接收的feature，包括input_ids、token_type_ids、slot_labels和intent_labels。\n",
    "其中：\n",
    "\n",
    "* `input_ids`: 表示输入文本的token ID。\n",
    "* `token_type_ids`: 表示token所属的句子（Transformer类预训练模型支持单句以及句对输入）。\n",
    "* `slot_labels`: 槽位标签，其长度与输入文本相同。\n",
    "* `intent_labels`: 意图标签，是一个长度等于总便签数的列表，标签对应位置为1，其余为0.\n",
    "\n",
    "更多有关数据处理的内容，请参考[数据处理](https://paddlenlp.readthedocs.io/zh/latest/data_prepare/data_preprocess.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. Batchify和数据读入\n",
    "\n",
    "使用`paddle.io.BatchSampler`和`paddlenlp.data`中提供的方法把数据组成batch。\n",
    "\n",
    "然后使用`paddle.io.DataLoader`接口多线程异步加载数据。\n",
    "\n",
    "`batchify_fn`详解：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/30e43d4659384375a2a2c1b890ca5a995c4324d7168e49cebf1d2a1e99161f7d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\r\n",
    "from paddlenlp.data import Stack, Dict, Pad\r\n",
    "\r\n",
    "batch_size = 20\r\n",
    "\r\n",
    "# 初始化BatchSampler\r\n",
    "train_batch_sampler = paddle.io.BatchSampler(\r\n",
    "    train_ds, batch_size=batch_size, shuffle=True)\r\n",
    "\r\n",
    "dev_batch_sampler = paddle.io.BatchSampler(\r\n",
    "    dev_ds, batch_size=batch_size, shuffle=False)\r\n",
    "\r\n",
    "# 定义batchify_fn\r\n",
    "batchify_fn = lambda samples, fn=Dict({\r\n",
    "    \"input_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_id), \r\n",
    "    \"token_type_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_type_id),\r\n",
    "    \"slot_labels\": Pad(axis=0, pad_val=0, dtype=\"int64\"),\r\n",
    "    \"intent_labels\": Stack(dtype=\"float32\"),\r\n",
    "    \"history_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_id)\r\n",
    "}): fn(samples)\r\n",
    "\r\n",
    "# 初始化DataLoader\r\n",
    "train_data_loader = paddle.io.DataLoader(\r\n",
    "    dataset=train_ds,\r\n",
    "    batch_sampler=train_batch_sampler,\r\n",
    "    collate_fn=batchify_fn,\r\n",
    "    return_list=True)\r\n",
    "\r\n",
    "dev_data_loader = paddle.io.DataLoader(\r\n",
    "    dataset=dev_ds,\r\n",
    "    batch_sampler=dev_batch_sampler,\r\n",
    "    collate_fn=batchify_fn,\r\n",
    "    return_list=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "更多PaddleNLP内置的batchify相关API，请参考[collate](https://paddlenlp.readthedocs.io/zh/latest/source/paddlenlp.data.collate.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "到这里数据集准备就全部完成了，下一步我们需要组网并设计loss function。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/fdcb44a00ede4ce08ae2652931556fb58cc903f686bf491792489353d2800e7d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 模型结构\n",
    "\n",
    "## 使用PaddleNLP一键加载预训练模型\n",
    "\n",
    "以下项目以ERNIE为例，介绍如何将预训练模型多任务学习同时完成意图识别和槽位填充任务。\n",
    "\n",
    "本例中的意图识别和槽位填充本质上是一个句子分类任务和一个序列标注任务。将两者的loss结合即可实现多任务学习。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/d9ff881921d74602acb6eb27c8523cb50285f07a7beb4a3cbfa1edbd9b3f9c5c)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from src.models import JointErnie\r\n",
    "\r\n",
    "model = JointErnie.from_pretrained(MODEL_NAME, \r\n",
    "                                   intent_dim=len(intent2id), \r\n",
    "                                   slot_dim=len(slot2id), \r\n",
    "                                   dropout=0.1, \r\n",
    "                                   use_history=use_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 设计loss function\n",
    "\n",
    "JointErnie模型会取出ErnieModel的sequence_output接入一个输出维度为槽位类别数的线性层得到slot_logits，并将pooled_output接入一个输出维度为意图类别数的线性层得到intent_logit.\n",
    "\n",
    "所以本示例中的loss由slot_loss和intent_loss两部分组成，我们需要自己定义loss function。\n",
    "\n",
    "槽位填充相当于在每个token的位置进行一次多分类任务，意图识别相当于对整句话做一个多标签分类任务。所以设计的loss function如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paddle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9281c1b3ea29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mNLULoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNLULoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintent_loss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpaddle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'paddle' is not defined"
     ]
    }
   ],
   "source": [
    "class NLULoss(paddle.nn.Layer):\r\n",
    "    def __init__(self, pos_weight):\r\n",
    "        super(NLULoss, self).__init__()\r\n",
    "\r\n",
    "        self.intent_loss_fn = paddle.nn.BCEWithLogitsLoss(pos_weight=paddle.to_tensor(pos_weight))\r\n",
    "        self.slot_loss_fct = paddle.nn.CrossEntropyLoss()\r\n",
    "\r\n",
    "    def forward(self, logits, slot_labels, intent_labels):\r\n",
    "        slot_logits, intent_logits = logits\r\n",
    "\r\n",
    "        slot_loss = self.slot_loss_fct(slot_logits, slot_labels)\r\n",
    "        intent_loss = self.intent_loss_fn(intent_logits, intent_labels)\r\n",
    "\r\n",
    "        return slot_loss + intent_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "选择网络结构后，我们需要设置Fine-Tune优化策略。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/7eca6595f338409498149cb586c077ba4933739810cf436080a2292be7e0a92d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 设置Fine-Tune优化策略\n",
    "适用于ERNIE/BERT这类Transformer模型的学习率为warmup的动态学习率。\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/2bc624280a614a80b5449773192be460f195b13af89e4e5cbaf62bf6ac16de2c\" width=\"40%\" height=\"30%\"/> <br />\n",
    "</p>\n",
    "<br><center>图3：动态学习率示意图</center></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 训练过程中的最大学习率\r\n",
    "learning_rate = 3e-5 \r\n",
    "\r\n",
    "# 训练轮次\r\n",
    "epochs = 10\r\n",
    "\r\n",
    "# 学习率预热比例\r\n",
    "warmup_proportion = 0.0\r\n",
    "\r\n",
    "# 权重衰减系数，类似模型正则项策略，避免模型过拟合\r\n",
    "weight_decay = 0.0\r\n",
    "\r\n",
    "max_grad_norm = 1.0\r\n",
    "\r\n",
    "num_training_steps = len(train_data_loader) * epochs\r\n",
    "\r\n",
    "# 学习率衰减策略\r\n",
    "lr_scheduler = paddlenlp.transformers.LinearDecayWithWarmup(learning_rate, num_training_steps,\r\n",
    "                                    warmup_proportion)\r\n",
    "\r\n",
    "decay_params = [\r\n",
    "    p.name for n, p in model.named_parameters()\r\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\r\n",
    "]\r\n",
    "\r\n",
    "# 定义优化器\r\n",
    "optimizer = paddle.optimizer.AdamW(\r\n",
    "    learning_rate=lr_scheduler,\r\n",
    "    parameters=model.parameters(),\r\n",
    "    weight_decay=weight_decay,\r\n",
    "    apply_decay_param_fun=lambda x: x in decay_params,\r\n",
    "    grad_clip=paddle.nn.ClipGradByGlobalNorm(max_grad_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "现在万事俱备，我们可以开始训练模型。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/6975542d488f4f75b385fe75d574a3aaa8e208f5e99f4acd8a8e8aea3b85c058)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型训练与评估\n",
    "\n",
    "模型训练的过程通常有以下步骤：\n",
    "\n",
    "1. 从dataloader中取出一个batch data\n",
    "2. 将batch data喂给model，做前向计算\n",
    "3. 将前向计算结果传给损失函数，计算loss。\n",
    "4. loss反向回传，更新梯度。重复以上步骤。\n",
    "\n",
    "每训练一个epoch后，程序对调用`evaluation()`方法分别计算两个任务的F1 score。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NLULoss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f8d95a776fa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNLULoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintent_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NLULoss' is not defined"
     ]
    }
   ],
   "source": [
    "from src.utils import evaluation\r\n",
    "\r\n",
    "criterion = NLULoss(intent_weight)\r\n",
    "global_step = 0\r\n",
    "\r\n",
    "for epoch in range(1, epochs + 1):\r\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\r\n",
    "        global_step += 1\r\n",
    "        input_ids, token_type_ids, slot_labels, intent_labels, history_ids = batch\r\n",
    "        logits = model(input_ids, token_type_ids, history_ids=history_ids)\r\n",
    "        loss = criterion(logits, slot_labels, intent_labels)\r\n",
    "\r\n",
    "        if global_step % 1000 == 0 :\r\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f\" % (global_step, epoch, step, loss))\r\n",
    "\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        lr_scheduler.step()\r\n",
    "        optimizer.clear_grad()\r\n",
    "    \r\n",
    "    print('\\nEval begin...')\r\n",
    "    evaluation(model, dev_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 引入对话历史\n",
    "\n",
    "在该数据集的原始论文中，提到了一种改进模型的手段。即将对话历史的特征传入模型。可以增强模型的表现。\n",
    "\n",
    "这里给大家举个例子：\n",
    "\n",
    "- **当前句**：\"好，玩累了周边有酒店可以休息吗？\"\n",
    "\n",
    "- **意图**：\"Request+景点+周边酒店\"\n",
    "\n",
    "- **历史**：[\"你好，帮我找一个20到50元之间的景点，谢谢。\",\"八达岭长城怎么样？门票35元。\"]\n",
    "\n",
    "在本示例中，可以通过修改数据处理函数的的`use_history`参数，并修改`batchify_fn`和对应训练代码来控制是否使用这一策略。\n",
    "\n",
    "关于该策略的详细信息和原始baseline模型,请参考[CrossWOZ](https://github.com/thu-coai/CrossWOZ)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 更多任务\n",
    "\n",
    "关于更多任务型对话的示例和数据集，可以参考PaddleNLP中的[DGU](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/dialogue/dgu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "以上内容实现基于PaddleNLP，开源不易，希望大家多多支持~ \n",
    "**记得给[PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)点个小小的Star⭐**\n",
    "\n",
    "GitHub地址：[https://github.com/PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/a0e8ca7743ea4fe9aa741682a63e767f8c48dc55981f4e44a40e0e00d3ab369e)\n",
    "\n",
    "**更多使用方法可参考PaddleNLP教程**\n",
    "\n",
    "- [使用seq2vec模块进行句子情感分类](https://aistudio.baidu.com/aistudio/projectdetail/1283423)\n",
    "- [使用预训练模型ERNIE优化情感分析](https://aistudio.baidu.com/aistudio/projectdetail/1294333)\n",
    "- [使用BiGRU-CRF模型完成快递单信息抽取](https://aistudio.baidu.com/aistudio/projectdetail/1317771)\n",
    "- [使用预训练模型ERNIE优化快递单信息抽取](https://aistudio.baidu.com/aistudio/projectdetail/1329361)\n",
    "- [使用Seq2Seq模型完成自动对联](https://aistudio.baidu.com/aistudio/projectdetail/1321118)\n",
    "- [使用预训练模型ERNIE-GEN自动写诗](https://aistudio.baidu.com/aistudio/projectdetail/1339888)\n",
    "- [使用TCN网络完成新冠疫情病例数预测](https://aistudio.baidu.com/aistudio/projectdetail/1290873)\n",
    "- [自定义数据集实现文本多分类任务](https://aistudio.baidu.com/aistudio/projectdetail/1468469)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 加入交流群，一起学习吧\n",
    "\n",
    "现在就加入PaddleNLP的QQ技术交流群，一起交流NLP技术吧！\n",
    "\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/d953727af0c24a7c806ab529495f0904f22f809961be420b8c88cdf59b837394\" width=\"200\" height=\"250\" >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

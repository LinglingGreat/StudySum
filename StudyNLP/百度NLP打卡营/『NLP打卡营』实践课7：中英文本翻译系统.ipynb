{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 基于Transformer的机器翻译\n",
    "\n",
    "机器翻译是利用计算机将一种自然语言(源语言)转换为另一种自然语言(目标语言)的过程。\n",
    "\n",
    "本项目是机器翻译领域主流模型 Transformer 的 PaddlePaddle 实现，包含模型训练，预测以及使用自定义数据等内容。用户可以基于发布的内容搭建自己的翻译模型。\n",
    "\n",
    "Transformer 是论文 [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) 中提出的用以完成机器翻译（Machine Translation）等序列到序列（Seq2Seq）学习任务的一种全新网络结构，其完全使用注意力（Attention）机制来实现序列到序列的建模。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/6e48d8033d7b4b8a8a34baa3af7ebdab2d3d852f7e8348a58e462ed3089db668\" width=\"500\" height=\"313\" ></center>\n",
    "<br><center>图1：Transformer 网络结构图</center></br>\n",
    "\n",
    "相较于此前 Seq2Seq 模型中广泛使用的循环神经网络（Recurrent Neural Network, RNN），使用Self Attention进行输入序列到输出序列的变换主要具有以下优势：\n",
    "\n",
    "- 计算复杂度小\n",
    "\t- 特征维度为 d 、长度为 n 的序列，在 RNN 中计算复杂度为 O(n * d * d) （n 个时间步，每个时间步计算 d 维的矩阵向量乘法），在 Self-Attention 中计算复杂度为 O(n * n * d) （n 个时间步两两计算 d 维的向量点积或其他相关度函数），n 通常要小于 d 。\n",
    "- 计算并行度高\n",
    "\t- RNN 中当前时间步的计算要依赖前一个时间步的计算结果；Self-Attention 中各时间步的计算只依赖输入不依赖之前时间步输出，各时间步可以完全并行。\n",
    "- 容易学习长距离依赖（long-range dependencies）\n",
    "\t- RNN 中相距为 n 的两个位置间的关联需要 n 步才能建立；Self-Attention 中任何两个位置都直接相连；路径越短信号传播越容易。\n",
    "Transformer 中引入使用的基于 Self-Attention 的序列建模模块结构，已被广泛应用在 Bert 等语义表示模型中，取得了显著效果。\n",
    "  \n",
    "  \n",
    "  \n",
    " #### [Transformer形象理解](https://jalammar.github.io/illustrated-transformer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 环境介绍\n",
    "\n",
    "- PaddlePaddle框架，AI Studio平台已经默认安装最新版2.1。\n",
    "\n",
    "- PaddleNLP，深度兼容框架2.1，是飞桨框架2.1在NLP领域的最佳实践。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**记得给[PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)点个小小的Star⭐**\n",
    "\n",
    "开源不易，希望大家多多支持~ \n",
    "\n",
    "GitHub地址：[https://github.com/PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)  \n",
    "PaddleNLP文档：[https://paddlenlp.readthedocs.io](https://paddlenlp.readthedocs.io)  \n",
    "本项目完整版：[https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/machine_translation/transformer](https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/machine_translation/transformer)\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/4afc34d18d6940af8ff6139fb0e6aa4ae46fcfd0d8cf49a18b491a91a22f959e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/transformer_mt\n"
     ]
    }
   ],
   "source": [
    "%cd transformer_mt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting paddlenlp\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/07/0b/2ebd839e7393c66ce7d2a25f008680b8d6812b0e29508326d684bcbbf7ba/paddlenlp-2.0.1-py3-none-any.whl (375kB)\n",
      "\u001b[K     |████████████████████████████████| 378kB 13.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\n",
      "Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.1.1)\n",
      "Collecting multiprocess (from paddlenlp)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/db/20/458ac043a57322365ac2ed86a911bf7598fc2e49bccb3f94ea810fbb6b9b/multiprocess-0.70.11.1-py37-none-any.whl (108kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 10.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.7 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.21.0)\n",
      "Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.7.1.1)\n",
      "Requirement already satisfied, skipping upgrade: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.8.2)\n",
      "Requirement already satisfied, skipping upgrade: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.8.53)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.14.0)\n",
      "Requirement already satisfied, skipping upgrade: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.22.1)\n",
      "Requirement already satisfied, skipping upgrade: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.10.3)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.4.10)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (5.1.2)\n",
      "Requirement already satisfied, skipping upgrade: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (16.7.9)\n",
      "Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.23)\n",
      "Requirement already satisfied, skipping upgrade: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.4)\n",
      "Requirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (1.25.6)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (0.18.0)\n",
      "Requirement already satisfied, skipping upgrade: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (3.9.9)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.17.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.5->Flask-Babel>=1.0.0->visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl->paddlenlp) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl->paddlenlp) (7.2.0)\n",
      "Installing collected packages: multiprocess, paddlenlp\n",
      "  Found existing installation: paddlenlp 2.0.0rc7\n",
      "    Uninstalling paddlenlp-2.0.0rc7:\n",
      "      Successfully uninstalled paddlenlp-2.0.0rc7\n",
      "Successfully installed multiprocess-0.70.11.1 paddlenlp-2.0.1\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting attrdict==2.0.1 (from -r requirements.txt (line 1))\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/ef/97/28fe7e68bc7adfce67d4339756e85e9fcf3c6fd7f0c0781695352b70472c/attrdict-2.0.1-py2.py3-none-any.whl\n",
      "Collecting PyYAML==5.4.1 (from -r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
      "\u001b[K     |████████████████████████████████| 645kB 13.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting subword_nmt==0.3.7 (from -r requirements.txt (line 3))\n",
      "  Downloading https://mirror.baidu.com/pypi/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
      "Requirement already satisfied: jieba==0.42.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (0.42.1)\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from attrdict==2.0.1->-r requirements.txt (line 1)) (1.15.0)\n",
      "Installing collected packages: attrdict, PyYAML, subword-nmt\n",
      "  Found existing installation: PyYAML 5.1.2\n",
      "    Uninstalling PyYAML-5.1.2:\n",
      "      Successfully uninstalled PyYAML-5.1.2\n",
      "Successfully installed PyYAML-5.4.1 attrdict-2.0.1 subword-nmt-0.3.7\n"
     ]
    }
   ],
   "source": [
    "# 安装依赖\n",
    "!pip install --upgrade paddlenlp -i https://pypi.org/simple\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Pipeline\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/b7aafbcc8e2f4bfc9b864d1a1bc0af749260ef9b690c458399f2ba9c66c1ab80\" width=\"1200\" height=\"600\" ></center>\n",
    "<br><center>图2：Pipeline </center></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import yaml\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from attrdict import AttrDict\n",
    "import jieba\n",
    "\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import paddle\n",
    "import paddle.distributed as dist\n",
    "from paddle.io import DataLoader,BatchSampler\n",
    "from paddlenlp.data import Vocab, Pad\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.transformers import TransformerModel, InferTransformerModel, CrossEntropyCriterion, position_encoding_init\n",
    "from paddlenlp.utils.log import logger\n",
    "\n",
    "from utils import post_process_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. 数据预处理\n",
    "本教程使用[CWMT](http://nlp.nju.edu.cn/cwmt-wmt/)数据集中的中文英文的数据作为训练语料，\n",
    "CWMT数据集在900万+，质量较高，非常适合来训练Transformer机器翻译。  \n",
    "中文需要Jieba+BPE，英文需要BPE  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### BPE(Byte Pair Encoding)\n",
    "BPE优势：\n",
    "- 压缩词表；\n",
    "- 一定程度上缓解OOV(out of vocabulary)问题\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/e7a59d7bab514a6fa17d24a116af0b680fbd664439c948799c0a0541dffd35a2\" width=\"1000\" height=\"500\" ></center>\n",
    "<br><center>图3：learn BPE </center></br>\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/d4b9c48ba7274395af3fbb267c1f9adcba50dd4b147d4258be58999b3b5a198c\" width=\"1000\" height=\"500\" ></center>\n",
    "\n",
    "<br><center>图4：Apply BPE </center></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/c319a24e5612413fb715885d7143f62882eba16ce43943c5b53903963591687c\" width=\"1000\" height=\"500\" ></center>\n",
    "<br><center>图5：Jieba+BPE </center></br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jieba tokenize...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.717 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "source learn-bpe and apply-bpe...\n",
      "no pair has frequency >= 2. Stopping\n",
      "target learn-bpe and apply-bpe...\n",
      "no pair has frequency >= 2. Stopping\n",
      "source get-vocab. if loading pretrained model, use its vocab.\n",
      "target get-vocab. if loading pretrained model, use its vocab.\n",
      "Over.\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理过程，包括jieba分词、bpe分词和词表。\n",
    "!bash preprocess.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download model.\n",
      "--2021-05-27 21:22:39--  https://paddlenlp.bj.bcebos.com/models/transformers/transformer/CWMT2021_step_345000.tar.gz\n",
      "Resolving paddlenlp.bj.bcebos.com (paddlenlp.bj.bcebos.com)... 182.61.200.195, 182.61.200.229, 2409:8c00:6c21:10ad:0:ff:b00e:67d\n",
      "Connecting to paddlenlp.bj.bcebos.com (paddlenlp.bj.bcebos.com)|182.61.200.195|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1386250752 (1.3G) [application/x-gzip]\n",
      "Saving to: ‘trained_models/CWMT2021_step_345000.tar.gz’\n",
      "\n",
      "CWMT2021_step_34500 100%[===================>]   1.29G  32.9MB/s    in 31s     \n",
      "\n",
      "2021-05-27 21:23:10 (43.2 MB/s) - ‘trained_models/CWMT2021_step_345000.tar.gz’ saved [1386250752/1386250752]\n",
      "\n",
      "Decompress model.\n",
      "Over.\n"
     ]
    }
   ],
   "source": [
    "# 下载预训练模型\n",
    "!bash get_data_and_model.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. 构造Dataloader\n",
    "\n",
    "下面的`create_data_loader`函数用于创建训练集、验证集所需要的`DataLoader`对象,  \n",
    "`create_infer_loader`函数用于创建预测集所需要的`DataLoader`对象，   \n",
    "`DataLoader`对象用于产生一个个batch的数据。下面对函数中调用的`paddlenlp`内置函数作简单说明：\n",
    "* `paddlenlp.data.Vocab.load_vocabulary`：Vocab词表类，集合了一系列文本token与ids之间映射的一系列方法，支持从文件、字典、json等一系方式构建词表\n",
    "* `paddlenlp.datasets.load_dataset`：从本地文件创建数据集时，推荐根据本地数据集的格式给出读取function并传入 load_dataset() 中创建数据集\n",
    "* `paddlenlp.data.Pad`：padding 操作\n",
    "具体可参考[PaddleNLP的文档](https://paddlenlp.readthedocs.io/zh/latest/data_prepare/dataset_self_defined.html)\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/0085d53068134546bcb914347774430c3a2c94cd77934c2e90420ac740d16fc7\" width=\"700\" height=\"350\" ></center>\n",
    "<br><center>图6：构造Dataloader的流程 </center></br>\n",
    "\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/3bf38365718346a19c25729bb67e2e6afe8f0bafc61348018d2b9dd60dc9a8bf\" width=\"1000\" height=\"500\" ></center>\n",
    "<br><center>图7：Dataloader细节 </center></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 自定义读取本地数据的方法\n",
    "def read(src_path, tgt_path, is_predict=False):\n",
    "    if is_predict:\n",
    "        with open(src_path, 'r', encoding='utf8') as src_f:\n",
    "            for src_line in src_f.readlines():\n",
    "                src_line = src_line.strip()\n",
    "                if not src_line:\n",
    "                    continue\n",
    "                yield {'src':src_line, 'tgt':''}\n",
    "    else:\n",
    "        with open(src_path, 'r', encoding='utf8') as src_f, open(tgt_path, 'r', encoding='utf8') as tgt_f:\n",
    "            for src_line, tgt_line in zip(src_f.readlines(), tgt_f.readlines()):\n",
    "                src_line = src_line.strip()\n",
    "                if not src_line:\n",
    "                    continue\n",
    "                tgt_line = tgt_line.strip()\n",
    "                if not tgt_line:\n",
    "                    continue\n",
    "                yield {'src':src_line, 'tgt':tgt_line}\n",
    " # 过滤掉长度 ≤min_len或者≥max_len 的数据            \n",
    "def min_max_filer(data, max_len, min_len=0):\n",
    "    # 1 for special tokens.\n",
    "    data_min_len = min(len(data[0]), len(data[1])) + 1\n",
    "    data_max_len = max(len(data[0]), len(data[1])) + 1\n",
    "    return (data_min_len >= min_len) and (data_max_len <= max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# 创建训练集、验证集的dataloader\n",
    "def create_data_loader(args):\n",
    "    train_dataset = load_dataset(read, src_path=args.training_file.split(',')[0], tgt_path=args.training_file.split(',')[1], lazy=False)\n",
    "    dev_dataset = load_dataset(read, src_path=args.validation_file.split(',')[0], tgt_path=args.validation_file.split(',')[1], lazy=False)\n",
    "\n",
    "    src_vocab = Vocab.load_vocabulary(\n",
    "        args.src_vocab_fpath,\n",
    "        bos_token=args.special_token[0],\n",
    "        eos_token=args.special_token[1],\n",
    "        unk_token=args.special_token[2])\n",
    "    trg_vocab = Vocab.load_vocabulary(\n",
    "        args.trg_vocab_fpath,\n",
    "        bos_token=args.special_token[0],\n",
    "        eos_token=args.special_token[1],\n",
    "        unk_token=args.special_token[2])\n",
    "\n",
    "    padding_vocab = (\n",
    "        lambda x: (x + args.pad_factor - 1) // args.pad_factor * args.pad_factor\n",
    "    )\n",
    "    args.src_vocab_size = padding_vocab(len(src_vocab))\n",
    "    args.trg_vocab_size = padding_vocab(len(trg_vocab))\n",
    "\n",
    "    def convert_samples(sample):\n",
    "        source = sample['src'].split()\n",
    "        target = sample['tgt'].split()\n",
    "\n",
    "        source = src_vocab.to_indices(source)\n",
    "        target = trg_vocab.to_indices(target)\n",
    "\n",
    "        return source, target\n",
    "\n",
    "    # 训练集dataloader和验证集dataloader\n",
    "    data_loaders = []\n",
    "    for i, dataset in enumerate([train_dataset, dev_dataset]):\n",
    "        dataset = dataset.map(convert_samples, lazy=False).filter(\n",
    "            partial(min_max_filer, max_len=args.max_length))\n",
    "\n",
    "        # BatchSampler: https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/BatchSampler_cn.html\n",
    "        batch_sampler = BatchSampler(dataset,batch_size=args.batch_size, shuffle=True,drop_last=False)\n",
    "        \n",
    "        # DataLoader: https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/DataLoader_cn.html\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_sampler=batch_sampler,\n",
    "            collate_fn=partial(\n",
    "                prepare_train_input,\n",
    "                bos_idx=args.bos_idx,\n",
    "                eos_idx=args.eos_idx,\n",
    "                pad_idx=args.bos_idx),\n",
    "                num_workers=0,\n",
    "                return_list=True)\n",
    "        data_loaders.append(data_loader)\n",
    "\n",
    "    return data_loaders\n",
    "\n",
    "\n",
    "def prepare_train_input(insts, bos_idx, eos_idx, pad_idx):\n",
    "    \"\"\"\n",
    "    Put all padded data needed by training into a list.\n",
    "    \"\"\"\n",
    "    word_pad = Pad(pad_idx)\n",
    "    src_word = word_pad([inst[0] + [eos_idx] for inst in insts])\n",
    "    trg_word = word_pad([[bos_idx] + inst[1] for inst in insts])\n",
    "    lbl_word = np.expand_dims(\n",
    "        word_pad([inst[1] + [eos_idx] for inst in insts]), axis=2)\n",
    "\n",
    "    data_inputs = [src_word, trg_word, lbl_word]\n",
    "\n",
    "    return data_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 创建测试集的dataloader，原理步骤同上（创建训练集、验证集的dataloader）\n",
    "def create_infer_loader(args):\n",
    "    dataset = load_dataset(read, src_path=args.predict_file, tgt_path=None, is_predict=True, lazy=False)\n",
    "\n",
    "    src_vocab = Vocab.load_vocabulary(\n",
    "        args.src_vocab_fpath,\n",
    "        bos_token=args.special_token[0],\n",
    "        eos_token=args.special_token[1],\n",
    "        unk_token=args.special_token[2])\n",
    "    trg_vocab = Vocab.load_vocabulary(\n",
    "        args.trg_vocab_fpath,\n",
    "        bos_token=args.special_token[0],\n",
    "        eos_token=args.special_token[1],\n",
    "        unk_token=args.special_token[2])\n",
    "\n",
    "    padding_vocab = (\n",
    "        lambda x: (x + args.pad_factor - 1) // args.pad_factor * args.pad_factor\n",
    "    )\n",
    "    args.src_vocab_size = padding_vocab(len(src_vocab))\n",
    "    args.trg_vocab_size = padding_vocab(len(trg_vocab))\n",
    "\n",
    "    def convert_samples(sample):\n",
    "        source = sample['src'].split()\n",
    "        target = sample['tgt'].split()\n",
    "\n",
    "        source = src_vocab.to_indices(source)\n",
    "        target = trg_vocab.to_indices(target)\n",
    "\n",
    "        return source, target\n",
    "\n",
    "    dataset = dataset.map(convert_samples, lazy=False)\n",
    "\n",
    "    # BatchSampler: https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/BatchSampler_cn.html\n",
    "    batch_sampler = BatchSampler(dataset,batch_size=args.infer_batch_size,drop_last=False)\n",
    "    \n",
    "    # DataLoader: https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/io/DataLoader_cn.html\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=partial(\n",
    "            prepare_infer_input,\n",
    "            bos_idx=args.bos_idx,\n",
    "            eos_idx=args.eos_idx,\n",
    "            pad_idx=args.bos_idx),\n",
    "            num_workers=0,\n",
    "            return_list=True)\n",
    "    return data_loader, trg_vocab.to_tokens\n",
    "\n",
    "def prepare_infer_input(insts, bos_idx, eos_idx, pad_idx):\n",
    "    \"\"\"\n",
    "    Put all padded data needed by beam search decoder into a list.\n",
    "    \"\"\"\n",
    "    word_pad = Pad(pad_idx)\n",
    "    src_word = word_pad([inst[0] + [eos_idx] for inst in insts])\n",
    "\n",
    "    return [src_word, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. 搭建模型\n",
    "PaddleNLP提供Transformer API供调用：\n",
    "* [`paddlenlp.transformers.TransformerModel`](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/paddlenlp/transformers/transformer/modeling.py#L523)：Transformer模型的实现\n",
    "* [`paddlenlp.transformers.InferTransformerModel`](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/paddlenlp/transformers/transformer/modeling.py#L702)：Transformer模型用于生成\n",
    "* [`paddlenlp.transformers.CrossEntropyCriterion`](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/paddlenlp/transformers/transformer/modeling.py#L191)：计算交叉熵损失\n",
    "* [`paddlenlp.transformers.position_encoding_init`](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/paddlenlp/transformers/transformer/modeling.py#L17)：Transformer 位置编码的初始化\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/7fdcad8a336d41b3a3b461de2adce5ae5b28317e681b4efdab29f53641866897\" width=\"500\" height=\"250\" ></center>\n",
    "<br><center>图8：模型搭建 </center></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/fb181b57c2d347b884502d5d11d8c61e918ee803069d4e26bcf4c6533cf948c6\" width=\"1000\" height=\"500\" ></center>\n",
    "<br><center>图9：Example </center></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.训练模型\n",
    "运行`do_train`函数，\n",
    "在`do_train`函数中，配置优化器、损失函数，以及评价指标Perplexity；  \n",
    "\n",
    "Perplexity，即困惑度，常用来衡量语言模型优劣，即句子的通顺度，一般用于机器翻译和文本生成等领域。Perplexity越小，句子越通顺，该语言模型越好。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/d6e0d38ae1d94deea1cc299a785b96663317a33d38e84f208c528c4dd03e83f2\" width=\"600\" height=\"300\" ></center>\n",
    "<br><center>图10：训练模型 </center></br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_train(args):\n",
    "    if args.use_gpu:\n",
    "        place = \"gpu\"\n",
    "    else:\n",
    "        place = \"cpu\"\n",
    "    paddle.set_device(place)\n",
    "    # Set seed for CE\n",
    "    random_seed = eval(str(args.random_seed))\n",
    "    if random_seed is not None:\n",
    "        paddle.seed(random_seed)\n",
    "\n",
    "    # Define data loader\n",
    "    (train_loader), (eval_loader) = create_data_loader(args)\n",
    "\n",
    "    # Define model\n",
    "    transformer = TransformerModel(\n",
    "        src_vocab_size=args.src_vocab_size,\n",
    "        trg_vocab_size=args.trg_vocab_size,\n",
    "        max_length=args.max_length + 1,\n",
    "        n_layer=args.n_layer,\n",
    "        n_head=args.n_head,\n",
    "        d_model=args.d_model,\n",
    "        d_inner_hid=args.d_inner_hid,\n",
    "        dropout=args.dropout,\n",
    "        weight_sharing=args.weight_sharing,\n",
    "        bos_id=args.bos_idx,\n",
    "        eos_id=args.eos_idx)\n",
    "\n",
    "    # Define loss\n",
    "    criterion = CrossEntropyCriterion(args.label_smooth_eps, args.bos_idx)\n",
    "\n",
    "    scheduler = paddle.optimizer.lr.NoamDecay(\n",
    "        args.d_model, args.warmup_steps, args.learning_rate, last_epoch=0)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = paddle.optimizer.Adam(\n",
    "        learning_rate=scheduler,\n",
    "        beta1=args.beta1,\n",
    "        beta2=args.beta2,\n",
    "        epsilon=float(args.eps),\n",
    "        parameters=transformer.parameters())\n",
    "\n",
    "    step_idx = 0\n",
    "\n",
    "    # Train loop\n",
    "    for pass_id in range(args.epoch):\n",
    "        batch_id = 0\n",
    "        for input_data in train_loader:\n",
    "\n",
    "            (src_word, trg_word, lbl_word) = input_data\n",
    "\n",
    "            logits = transformer(src_word=src_word, trg_word=trg_word)\n",
    "\n",
    "            sum_cost, avg_cost, token_num = criterion(logits, lbl_word)\n",
    "            \n",
    "            # 计算梯度\n",
    "            avg_cost.backward() \n",
    "            # 更新参数\n",
    "            optimizer.step() \n",
    "            # 梯度清零\n",
    "            optimizer.clear_grad() \n",
    "\n",
    "            if (step_idx + 1) % args.print_step == 0 or step_idx == 0:\n",
    "                total_avg_cost = avg_cost.numpy()\n",
    "                logger.info(\n",
    "                    \"step_idx: %d, epoch: %d, batch: %d, avg loss: %f, \"\n",
    "                    \" ppl: %f \" %\n",
    "                    (step_idx, pass_id, batch_id, total_avg_cost,\n",
    "                        np.exp([min(total_avg_cost, 100)])))\n",
    "\n",
    "            if (step_idx + 1) % args.save_step == 0:\n",
    "                # Validation\n",
    "                transformer.eval()\n",
    "                total_sum_cost = 0\n",
    "                total_token_num = 0\n",
    "                with paddle.no_grad():\n",
    "                    for input_data in eval_loader:\n",
    "                        (src_word, trg_word, lbl_word) = input_data\n",
    "                        logits = transformer(\n",
    "                            src_word=src_word, trg_word=trg_word)\n",
    "                        sum_cost, avg_cost, token_num = criterion(logits,\n",
    "                                                                  lbl_word)\n",
    "                        total_sum_cost += sum_cost.numpy()\n",
    "                        total_token_num += token_num.numpy()\n",
    "                        total_avg_cost = total_sum_cost / total_token_num\n",
    "                    logger.info(\"validation, step_idx: %d, avg loss: %f, \"\n",
    "                                \" ppl: %f\" %\n",
    "                                (step_idx, total_avg_cost,\n",
    "                                 np.exp([min(total_avg_cost, 100)])))\n",
    "                transformer.train()\n",
    "\n",
    "                if args.save_model:\n",
    "                    model_dir = os.path.join(args.save_model,\n",
    "                                             \"step_\" + str(step_idx))\n",
    "                    if not os.path.exists(model_dir):\n",
    "                        os.makedirs(model_dir)\n",
    "                    paddle.save(transformer.state_dict(),\n",
    "                                os.path.join(model_dir, \"transformer.pdparams\"))\n",
    "                    paddle.save(optimizer.state_dict(),\n",
    "                                os.path.join(model_dir, \"transformer.pdopt\"))\n",
    "            batch_id += 1\n",
    "            step_idx += 1\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "    if args.save_model:\n",
    "        model_dir = os.path.join(args.save_model, \"step_final\")\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "        paddle.save(transformer.state_dict(),\n",
    "                    os.path.join(model_dir, \"transformer.pdparams\"))\n",
    "        paddle.save(optimizer.state_dict(),\n",
    "                    os.path.join(model_dir, \"transformer.pdopt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 50,\n",
      " 'beam_size': 5,\n",
      " 'beta1': 0.9,\n",
      " 'beta2': 0.997,\n",
      " 'bos_idx': 0,\n",
      " 'd_inner_hid': 2048,\n",
      " 'd_model': 512,\n",
      " 'dropout': 0.1,\n",
      " 'eos_idx': 1,\n",
      " 'epoch': 1,\n",
      " 'eps': '1e-9',\n",
      " 'infer_batch_size': 50,\n",
      " 'init_from_params': 'trained_models/CWMT2021_step_345000/',\n",
      " 'label_smooth_eps': 0.1,\n",
      " 'learning_rate': 2.0,\n",
      " 'max_length': 256,\n",
      " 'max_out_len': 256,\n",
      " 'n_best': 1,\n",
      " 'n_head': 8,\n",
      " 'n_layer': 6,\n",
      " 'output_file': 'train_dev_test/predict.txt',\n",
      " 'pad_factor': 8,\n",
      " 'predict_file': 'train_dev_test/ccmt2019-news.zh2en.source_bpe',\n",
      " 'print_step': 10,\n",
      " 'random_seed': 'None',\n",
      " 'save_model': 'trained_models',\n",
      " 'save_step': 20,\n",
      " 'special_token': ['<s>', '<e>', '<unk>'],\n",
      " 'src_vocab_fpath': 'train_dev_test/vocab.ch.src',\n",
      " 'src_vocab_size': 10000,\n",
      " 'training_file': 'train_dev_test/train.ch.bpe,train_dev_test/train.en.bpe',\n",
      " 'trg_vocab_fpath': 'train_dev_test/vocab.en.tgt',\n",
      " 'trg_vocab_size': 10000,\n",
      " 'unk_idx': 2,\n",
      " 'use_gpu': True,\n",
      " 'validation_file': 'train_dev_test/dev.ch.bpe,train_dev_test/dev.en.bpe',\n",
      " 'warmup_steps': 8000,\n",
      " 'weight_sharing': False}\n"
     ]
    }
   ],
   "source": [
    "# 读入参数\n",
    "yaml_file = 'transformer.base.yaml'\n",
    "with open(yaml_file, 'rt') as f:\n",
    "    args = AttrDict(yaml.safe_load(f))\n",
    "    pprint(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-05-27 21:26:48,516] [    INFO] - step_idx: 0, epoch: 0, batch: 0, avg loss: 10.519297,  ppl: 37023.074219 \n",
      "[2021-05-27 21:26:49,645] [    INFO] - step_idx: 9, epoch: 0, batch: 9, avg loss: 10.508386,  ppl: 36621.312500 \n",
      "[2021-05-27 21:26:50,924] [    INFO] - step_idx: 19, epoch: 0, batch: 19, avg loss: 10.473063,  ppl: 35350.347656 \n",
      "[2021-05-27 21:26:51,917] [    INFO] - validation, step_idx: 19, avg loss: 10.461072,  ppl: 34928.972656\n"
     ]
    }
   ],
   "source": [
    "do_train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5. 预测和评估\n",
    "模型最终训练的效果一般可通过测试集来进行测试，机器翻译领域一般计算BLEU值。\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/cf7161d1059e4ca9989e03fc09f197c63590185720de444fa7c2d15ac3bee696\" width=\"600\" height=\"300\" ></center>\n",
    "<br><center>图11： 预测和评估 </center></br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_predict(args):\n",
    "    if args.use_gpu:\n",
    "        place = \"gpu\"\n",
    "    else:\n",
    "        place = \"cpu\"\n",
    "    paddle.set_device(place)\n",
    "\n",
    "    # Define data loader\n",
    "    test_loader, to_tokens = create_infer_loader(args)\n",
    "\n",
    "    # Define model\n",
    "    transformer = InferTransformerModel(\n",
    "        src_vocab_size=args.src_vocab_size,\n",
    "        trg_vocab_size=args.trg_vocab_size,\n",
    "        max_length=args.max_length + 1,\n",
    "        n_layer=args.n_layer,\n",
    "        n_head=args.n_head,\n",
    "        d_model=args.d_model,\n",
    "        d_inner_hid=args.d_inner_hid,\n",
    "        dropout=args.dropout,\n",
    "        weight_sharing=args.weight_sharing,\n",
    "        bos_id=args.bos_idx,\n",
    "        eos_id=args.eos_idx,\n",
    "        beam_size=args.beam_size,\n",
    "        max_out_len=args.max_out_len)\n",
    "\n",
    "    # Load the trained model\n",
    "    assert args.init_from_params, (\n",
    "        \"Please set init_from_params to load the infer model.\")\n",
    "\n",
    "    model_dict = paddle.load(\n",
    "        os.path.join(args.init_from_params, \"transformer.pdparams\"))\n",
    "\n",
    "    # To avoid a longer length than training, reset the size of position\n",
    "    # encoding to max_length\n",
    "    model_dict[\"encoder.pos_encoder.weight\"] = position_encoding_init(\n",
    "        args.max_length + 1, args.d_model)\n",
    "    model_dict[\"decoder.pos_encoder.weight\"] = position_encoding_init(\n",
    "        args.max_length + 1, args.d_model)\n",
    "    transformer.load_dict(model_dict)\n",
    "\n",
    "    # Set evaluate mode\n",
    "    transformer.eval()\n",
    "\n",
    "    f = open(args.output_file, \"w\")\n",
    "    with paddle.no_grad():\n",
    "        for (src_word, ) in test_loader:\n",
    "            finished_seq = transformer(src_word=src_word)\n",
    "            finished_seq = finished_seq.numpy().transpose([0, 2, 1])\n",
    "            for ins in finished_seq:\n",
    "                for beam_idx, beam in enumerate(ins):\n",
    "                    if beam_idx >= args.n_best:\n",
    "                        break\n",
    "                    id_list = post_process_seq(beam, args.bos_idx, args.eos_idx)\n",
    "                    word_list = to_tokens(id_list)\n",
    "                    sequence = \" \".join(word_list) + \"\\n\"\n",
    "                    f.write(sequence)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/framework.py:687: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  elif dtype == np.bool:\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlenlp/data/vocab.py:208: UserWarning: The type of `to_tokens()`'s input `indices` is not `int` which will be forcibly transfered to `int`. \n",
      "  \"The type of `to_tokens()`'s input `indices` is not `int` which will be forcibly transfered to `int`. \"\n"
     ]
    }
   ],
   "source": [
    "do_predict(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型评估\n",
    "预测结果中每行输出是对应行输入的得分最高的翻译，对于使用 BPE 的数据，预测出的翻译结果也将是 BPE 表示的数据，要还原成原始的数据（这里指 tokenize 后的数据）才能进行正确的评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU = 38.11, 74.5/49.1/32.5/21.7 (BP=0.951, ratio=0.952, hyp_len=22252, ref_len=23371)\r\n",
      "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\r\n"
     ]
    }
   ],
   "source": [
    "# 还原 predict.txt 中的预测结果为 tokenize 后的数据\n",
    "! sed -r 's/(@@ )|(@@ ?$)//g' train_dev_test/predict.txt > train_dev_test/predict.tok.txt\n",
    "# BLEU评估工具来源于 https://github.com/moses-smt/mosesdecoder.git\n",
    "! tar -zxf mosesdecoder.tar.gz\n",
    "# 计算multi-bleu\n",
    "! perl mosesdecoder/scripts/generic/multi-bleu.perl train_dev_test/ccmt2019-news.zh2en.ref*.txt < train_dev_test/predict.tok.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

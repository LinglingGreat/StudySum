# 损失函数

## BCELoss

![](image/image.png)

- BCEWithLogitsLoss就是把Sigmoid-BCELoss合成一步。先对x取sigmoid再计算BCELoss

- (BCELoss)BCEWithLogitsLoss用于单标签二分类或者多标签二分类，输出和目标的维度是(batch,C)，batch是样本数量，C是类别数量，对于每一个batch的C个值，对每个值求sigmoid到0-1之间，所以每个batch的C个值之间是没有关系的,相互独立的，所以之和不一定为1。每个C值代表属于一类标签的概率。如果是单标签二分类，那输出和目标的维度是(batch,1)即可。

- CrossEntropyLoss用于多类别分类，输出和目标的维度是(batch,C)，batch是样本数量，C是类别数量，每一个C之间是互斥的，相互关联的，对于每一个batch的C个值，一起求每个C的softmax，所以每个batch的所有C个值之和是1，哪个值大，代表其属于哪一类。如果用于二分类，那输出和目标的维度是(batch,2)。

![](image/image_1.png)

## Focal Loss

#td 

Focal Loss 是 Kaiming He 和 RBG 在 2017 年的 "Focal Loss for Dense Object Detection" 论文中所提出的一种新的 Loss Function，Focal Loss 主要是为了解决样本类别不均衡问题（也有人说实际上也是解决了 gradient 被 easy example dominant 的问题）。

普通的Cross Entropy

$CE(p_t)=-a_tlog(p_t)$

a_t是平衡因子

Focal Loss

$FL(p_t)=-(1-p_t)^rlog(p_t)$

![](image/image_2.png)

![](image/image_3.png)

在log前面加上$(1-p_t)$是focal loss的核心。假设r设置为2。当$p_t$为0.9，说明网络已经将这个样本分的很好了，那么$(1-p_t)^2$为0.01，呈指数级降低了这个样本对损失函数的贡献。当$p_t$为0.1，说明网络对样本还不具有很好地分类能力，那么$(1-p_t)^2$为0.81。 简单言之，focal加大了对难分类样本的关注。

源码详解

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

# 针对二分类任务的 Focal Loss
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2, size_average=True):
        super(FocalLoss, self).__init__()
        self.alpha = torch.tensor(alpha).cuda()
        self.gamma = gamma
        self.size_average = size_average

    def forward(self, pred, target):
        # 如果模型最后没有 nn.Sigmoid()，那么这里就需要对预测结果计算一次 Sigmoid 操作
        # pred = nn.Sigmoid()(pred)

        # 展开 pred 和 target,此时 pred.size = target.size = (BatchSize,1) 
        pred = pred.view(-1,1)
        target = target.view(-1,1)

        # 此处将预测样本为正负的概率都计算出来，此时 pred.size = (BatchSize,2)
        pred = torch.cat((1-pred,pred),dim=1)

        # 根据 target 生成 mask，即根据 ground truth 选择所需概率
        # 用大白话讲就是：
        # 当标签为 1 时，我们就将模型预测该样本为正类的概率代入公式中进行计算
        # 当标签为 0 时，我们就将模型预测该样本为负类的概率代入公式中进行计算
        class_mask = torch.zeros(pred.shape[0],pred.shape[1]).cuda()
        # 这里的 scatter_ 操作不常用，其函数原型为:
        # scatter_(dim,index,src)->Tensor
        # Writes all values from the tensor src into self at the indices specified in the index tensor. 
        # For each value in src, its output index is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim.
        class_mask.scatter_(1, target.view(-1, 1).long(), 1.)

        # 利用 mask 将所需概率值挑选出来
        probs = (pred * class_mask).sum(dim=1).view(-1,1)
        probs = probs.clamp(min=0.0001,max=1.0)

        # 计算概率的 log 值
        log_p = probs.log()

        # 根据论文中所述，对 alpha　进行设置（该参数用于调整正负样本数量不均衡带来的问题）
        alpha = torch.ones(pred.shape[0],pred.shape[1]).cuda()
        alpha[:,0] = alpha[:,0] * (1-self.alpha)
        alpha[:,1] = alpha[:,1] * self.alpha
        alpha = (alpha * class_mask).sum(dim=1).view(-1,1)

        # 根据 Focal Loss 的公式计算 Loss
        batch_loss = -alpha*(torch.pow((1-probs), self.gamma))*log_p

         # Loss Function的常规操作，mean 与 sum 的区别不大，相当于学习率设置不一样而已
        if self.size_average:
            loss = batch_loss.mean()
        else:
            loss = batch_loss.sum()

        return loss

# 针对 Multi-Label 任务的 Focal Loss
class FocalLoss_MultiLabel(nn.Module):
    def __init__(self, alpha=0.25, gamma=2, size_average=True):
        super(FocalLoss_MultiLabel, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.size_average = size_average

    def forward(self, pred, target):
        criterion = FocalLoss(self.alpha,self.gamma,self.size_average)
        loss = torch.zeros(1,target.shape[1]).cuda()

        # 对每个 Label 计算一次 Focal Loss
        for label in range(target.shape[1]):
            batch_loss = criterion(pred[:,label],target[:,label])
            loss[0,label] = batch_loss.mean()

        # Loss Function的常规操作
        if self.size_average:
            loss = loss.mean()
        else:
            loss = loss.sum()

        return loss
```


针对多分类任务的CELoss 和 Focal Loss，可通过 use_alpha 参数决定是否使用 α 参数，并解决之前版本中所出现的 Loss变为 nan 的 bug（原因出自 log 操作，当对过小的数值进行 log 操作，返回值将变为 nan）。

```python
# 针对多分类任务的 CELoss 和　Focal Loss
import torch
import torch.nn as nn
import torch.nn.functional as F

class CELoss(nn.Module):
    def __init__(self, class_num, alpha=None, use_alpha=False, size_average=True):
        super(CELoss, self).__init__()
        self.class_num = class_num
        self.alpha = alpha
        if use_alpha:
            self.alpha = torch.tensor(alpha).cuda()

        self.softmax = nn.Softmax(dim=1)
        self.use_alpha = use_alpha
        self.size_average = size_average

    def forward(self, pred, target):
        prob = self.softmax(pred.view(-1,self.class_num))
        prob = prob.clamp(min=0.0001,max=1.0)

        target_ = torch.zeros(target.size(0),self.class_num).cuda()
        target_.scatter_(1, target.view(-1, 1).long(), 1.)

        if self.use_alpha:
            batch_loss = - self.alpha.double() * prob.log().double() * target_.double()
        else:
            batch_loss = - prob.log().double() * target_.double()

        batch_loss = batch_loss.sum(dim=1)

        # print(prob[0],target[0],target_[0],batch_loss[0])
        # print('--')

        if self.size_average:
            loss = batch_loss.mean()
        else:
            loss = batch_loss.sum()

        return loss

class FocalLoss(nn.Module):
    def __init__(self, class_num, alpha=None, gamma=2, use_alpha=False, size_average=True):
        super(FocalLoss, self).__init__()
        self.class_num = class_num
        self.alpha = alpha
        self.gamma = gamma
        if use_alpha:
            self.alpha = torch.tensor(alpha).cuda()

        self.softmax = nn.Softmax(dim=1)
        self.use_alpha = use_alpha
        self.size_average = size_average

    def forward(self, pred, target):
        prob = self.softmax(pred.view(-1,self.class_num))
        prob = prob.clamp(min=0.0001,max=1.0)

        target_ = torch.zeros(target.size(0),self.class_num).cuda()
        target_.scatter_(1, target.view(-1, 1).long(), 1.)

        if self.use_alpha:
            batch_loss = - self.alpha.double() * torch.pow(1-prob,self.gamma).double() * prob.log().double() * target_.double()
        else:
            batch_loss = - torch.pow(1-prob,self.gamma).double() * prob.log().double() * target_.double()

        batch_loss = batch_loss.sum(dim=1)

        if self.size_average:
            loss = batch_loss.mean()
        else:
            loss = batch_loss.sum()

        return loss
```

## softmax+交叉熵的多标签分类版本

一般来说，多分类问题指的就是单标签分类问题，即从n个候选类别中选1个目标类别。loss一般是softmax+交叉熵

多标签分类问题，即从n个候选类别中选k个目标类别。这种情况下我们一种朴素的做法是用sigmoid激活，然后变成n个二分类问题，用二分类的交叉熵之和作为loss。

显然，当n≫kn≫k时，这种做法会面临着严重的类别不均衡问题，这时候需要一些平衡策略，比如手动调整正负样本的权重、[focal loss](https://kexue.fm/archives/4733)等。训练完成之后，还需要根据验证集来进一步确定最优的阈值。

构建组合形式的softmax来作为单标签softmax的推广。在这部分内容中，我们会先假设k是一个固定的常数，然后再讨论一般情况下k的自动确定方案，最后确实能得到一种有效的推广形式。

组合softmax

![image-20210828232401988](img/image-20210828232401988.png)

自动确定阈值

![image-20210828232509820](img/image-20210828232509820.png)

### 统一的loss形式(看这段就行了)

[Circle Loss](https://arxiv.org/abs/2002.10857)

![image-20210828232652724](img/image-20210828232652724.png)

![image-20210828232722066](img/image-20210828232722066.png)

用于多标签分类

![image-20210828233139433](img/image-20210828233139433.png)

这便是我们最终得到的Loss形式了——“softmax + 交叉熵”在多标签分类任务中的自然、简明的推广，它没有类别不均衡现象，因为它不是将多标签分类变成多个二分类问题，而是变成目标类别得分与非目标类别得分的两两比较，并且借助于logsumexp的良好性质，自动平衡了每一项的权重。

Keras下的参考实现

```python
def multilabel_categorical_crossentropy(y_true, y_pred):
    """多标签分类的交叉熵
    说明：y_true和y_pred的shape一致，y_true的元素非0即1，
         1表示对应的类为目标类，0表示对应的类为非目标类。
    警告：请保证y_pred的值域是全体实数，换言之一般情况下y_pred
         不用加激活函数，尤其是不能加sigmoid或者softmax！预测
         阶段则输出y_pred大于0的类。如有疑问，请仔细阅读并理解
         本文。
    """
    y_pred = (1 - 2 * y_true) * y_pred
    y_pred_neg = y_pred - y_true * 1e12
    y_pred_pos = y_pred - (1 - y_true) * 1e12
    zeros = K.zeros_like(y_pred[..., :1])
    y_pred_neg = K.concatenate([y_pred_neg, zeros], axis=-1)
    y_pred_pos = K.concatenate([y_pred_pos, zeros], axis=-1)
    neg_loss = K.logsumexp(y_pred_neg, axis=-1)
    pos_loss = K.logsumexp(y_pred_pos, axis=-1)
    return neg_loss + pos_loss
```

要提示的是，除了标准的多标签分类问题外，还有一些常见的任务形式也可以认为是多标签分类，比如基于0/1标注的序列标注，典型的例子是[“半指针-半标注”标注设计](https://kexue.fm/archives/6671)。因此，从这个角度看，能被视为多标签分类来测试式(11)的任务就有很多了，苏神也在之前的三元组抽取例子[task_relation_extraction.py](https://github.com/bojone/bert4keras/blob/master/examples/task_relation_extraction.py)中尝试了(11)，最终能取得跟[这里](https://kexue.fm/archives/7161#类别失衡)一致的效果。

当然，最后还是要说明一下，虽然理论上式(11)作为多标签分类的损失函数能自动地解决很多问题，但终究是不存在绝对完美、保证有提升的方案，所以当你用它替换掉你原来多标签分类方案时，也不能保证一定会有提升，尤其是当你原来已经通过精调权重等方式处理好类别不平衡问题的情况下，式(11)的收益是非常有限的。毕竟式(11)的初衷，只是让我们在不需要过多调参的的情况下达到大部分的效果。

## 参考资料

1. [多标签分类与BCEloss](https://www.jianshu.com/p/ac3bec3dde3e)
2. [多标签、类别不均衡分类问题](https://blog.csdn.net/weixin_46133588/article/details/108755600)（多标签分类的一些尝试）
3. [[日常] 关于 Focal Loss（附实现代码）](https://zhuanlan.zhihu.com/p/75542467)（源码详解）
4. [详解Focal Loss以及PyTorch代码](https://blog.csdn.net/qq_34914551/article/details/105393989)
5. [何恺明大神的「Focal Loss」，如何更好地理解？](https://zhuanlan.zhihu.com/p/32423092)（苏剑林从自己构思的一个loss出发理解focal loss）
6. https://www.kaggle.com/thedrcat/focal-multilabel-loss-in-pytorch-explained/log
7. [将“softmax+交叉熵”推广到多标签分类问题](https://kexue.fm/archives/7359)

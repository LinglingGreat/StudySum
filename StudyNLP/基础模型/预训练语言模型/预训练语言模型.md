# 预训练语言模型

深度学习时代广泛使用的词向量（即词嵌入，Word Embedding）即属于NLP预训练工作。使用深度神经网络进行NLP模型训练时，首先需要将待处理文本转为词向量作为神经网络输入，词向量的效果会影响到最后模型效果。词向量的效果主要取决于训练语料的大小，很多NLP任务中有限的标注语料不足以训练出足够好的词向量，通常使用跟当前任务无关的大规模未标注语料进行词向量预训练，因此预训练的另一个好处是能增强模型的泛化能力。目前，大部分NLP深度学习任务中都会使用预训练好的词向量（如Word2Vec和GloVe等）进行网络初始化（而非随机初始化），从而加快网络的收敛速度。

预训练语言模型的成功，证明了我们可以从海量的无标注文本中学到潜在的语义信息，而无需为每一项下游NLP任务单独标注大量训练数据。此外，预训练语言模型的成功也开创了NLP研究的新范式，即首先使用大量无监督语料进行语言模型预训练（Pre-training），再使用少量标注语料进行微调（Fine-tuning）来完成具体NLP任务（分类、序列标注、句间关系判断和机器阅读理解等）。

![](https://p0.meituan.net/travelcube/a67aa6b0b1fb153de0051e7eb3e8c632130233.png)

NLP Pre-training and Fine-tuning新范式及相关扩展工作

所谓的“预训练”，其实并不是什么新概念，这种“Pre-training and Fine-tuning”的方法在图像领域早有应用。2009年，邓嘉、李飞飞等人在CVPR 2009发布了ImageNet数据集，其中120万张图像分为1000个类别。基于ImageNet，以图像分类为目标使用深度卷积神经网络（如常见的ResNet、VCG、Inception等）进行预训练，得到的模型称为预训练模型。针对目标检测或者语义分割等任务，基于这些预训练模型，通过一组新的全连接层与预训练模型进行拼接，利用少量标注数据进行微调，将预训练模型学习到的图像分类能力迁移到新的目标任务。预训练的方式在图像领域取得了广泛的成功，比如有学者将ImageNet上学习得到的特征表示用于PSACAL VOC上的物体检测，将检测率提高了20%。

图像领域预训练的成功也启发了NLP领域研究，深度学习时代广泛使用的词向量（即词嵌入，Word Embedding）即属于NLP预训练工作。使用深度神经网络进行NLP模型训练时，首先需要将待处理文本转为词向量作为神经网络输入，词向量的效果会影响到最后模型效果。词向量的效果主要取决于训练语料的大小，很多NLP任务中有限的标注语料不足以训练出足够好的词向量，通常使用跟当前任务无关的大规模未标注语料进行词向量预训练，因此预训练的另一个好处是能增强模型的泛化能力。目前，大部分NLP深度学习任务中都会使用预训练好的词向量（如Word2Vec和GloVe等）进行网络初始化（而非随机初始化），从而加快网络的收敛速度。

预训练词向量通常只编码词汇间的关系，对上下文信息考虑不足，且无法处理一词多义问题。如“bank”一词，根据上下文语境不同，可能表示“银行”，也可能表示“岸边”，却对应相同的词向量，这样显然是不合理的。为了更好的考虑单词的上下文信息，Context2Vec[11]使用两个双向长短时记忆网络（Long Short Term Memory，LSTM）[12]来分别编码每个单词左到右（Left-to-Right）和右到左（Right-to-Left）的上下文信息。类似地，ELMo也是基于大量文本训练深层双向LSTM网络结构的语言模型。ELMo在词向量的学习中考虑深层网络不同层的信息，并加入到单词的最终Embedding表示中，在多个NLP任务中取得了提升。ELMo这种使用预训练语言模型的词向量作为特征输入到下游目标任务中，被称为Feature-based方法。

另一种方法是微调（Fine-tuning）。GPT、BERT和后续的预训练工作都属于这一范畴，直接在深层Transformer网络上进行语言模型训练，收敛后针对下游目标任务进行微调，不需要再为目标任务设计Task-specific网络从头训练。关于NLP领域的预训练发展史，张俊林博士写过一篇很详实的介绍[13]。

## 参考资料

1. 张俊林. [从Word Embedding到BERT模型—自然语言处理中的预训练技术发展史.](https://zhuanlan.zhihu.com/p/49271699)

2. [美团BERT的探索和实践](https://tech.meituan.com/2019/11/14/nlp-bert-practice.html)（预训练语言模型简介、BERT介绍、美团训练加速、加入美团点评业务语料进行预训练、融入知识图谱、业务实践等）


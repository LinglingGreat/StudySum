我要講的是幾個 deep learning 的 tip
00:08
其實，這一段本來是要在 CNN 之前講的啦
00:11
那在 CNN 那段裡面，我們留下了兩個問題
00:15
第一個是，在 CNN 裡面
00:20
有 max pooling 這樣的架構
00:22
但是，max pooling 這樣的架構顯然不能微分阿
00:26
你把它放在一個 network 裡面
00:27
你在做 Gradient Descent，你在微分它的時候
00:31
你到底是怎麼處理？
00:33
那第二個問題，是我們剛才看到的
00:36
L1 的 Regularization
00:38
但是，我們還沒有解釋它是甚麼東西
00:41
那這個，我們都會在這份投影片裡面解釋
00:46
那本來是要先講這份投影片，再講 CNN 的啦
00:50
但是，因為要講作業三的關係
00:53
所以，就先講了 CNN
00:55
講完這份投影片以後，之前的一些問題就可以被解決
00:59
首先，這邊呢，最重要的一個觀念是
01:02
Deep learning 的 recipe
01:05
如果你在訓練一個 deep learning 2的 network 的時候
01:08
你在做 deep learning 的時候
01:10
它的流程，應該是甚麼樣子的
01:13
那我們都知道說，deep learning 是 3 個 step
01:17
define function 、define 你的 function set
01:21
define 你的 network 的 structure
01:23
決定你的 loss function
01:25
接下來，你就可以用 Gradient Descent 去做 optimization
01:29
做完這些事情以後
01:31
你會得到一個 neural network
01:33
得到一個好的 neural network
01:36
接下來，你要做甚麼樣的事情呢？
01:39
接下來，你要做甚麼樣的事情呢？
01:42
其實，你第一件要檢查的事情是
01:45
這個 neural network 在你的 training set 上
01:48
有沒有得到好的結果
01:50
不是 testing set 哦，你要先檢查這個 neural network
01:54
在你的 training set 上，有沒有得到好的結果
01:57
如果，沒有的話
01:59
你就回頭去看看說，在這 3 個 step
02:02
這裡面，是不是哪邊出了問題
02:04
你可以做甚麼樣的修改，讓你在 training set 上
02:07
能夠得到好的結果
02:09
那這邊這個先檢查 training set 的 performance
02:13
其實是 deep learning 一個非常 unique 的地方
02:16
如果你想想看其他的方法，比如說
02:19
你今天如果用的是
02:20
雖然這些方法我們都還沒講過，但你或多或少都知道
02:23
比如說，k nearest neighbor
02:26
或者是 decision tree
02:27
其實像 k nearest neighbor 或 decision tree 這種方法
02:30
你做完以後，你其實會不太想檢查你 training set 的結果
02:34
因為，在 training set 上的 performance 正確率就是 100
02:36
你做完 decision tree 或做完 k nearest neighbor
02:39
得到正確率就是 100，沒有甚麼好檢查的
02:42
所以，有人說 deep learning
02:45
看這個 model 裡面這麼多參數
02:47
感覺一臉很容易 overfitting 的樣子
02:49
我跟你講，這個 deep learning 的方法
02:51
它才不容易 overfitting
02:52
我們說的 overfitting 就是在 training set 上
02:54
performance 很好，但 testing set 上 performance
02:56
沒有那麼好嘛
02:57
像這 k nearest neighbor, decision tree，它們一做下去
03:00
在 training set 上正確率都是 100
03:02
在 training set 上正確率都是 100
03:03
這個才是非常容易 overfitting
03:06
而對 deep learning 來說
03:07
overfitting 往往不是你會最
03:09
不是說，deep learning 沒有 overfitting 的問題
03:12
而是說，在 deep learning 裡面，overfitting
03:14
不是第一個你會遇到的問題
03:16
你第一個會遇到的問題，是你在 training 的時候
03:19
它並不是像 k nearest neighbor 這種方法一樣
03:21
你一 train 就可以得到非常好的正確率
03:24
它有可能在 training set 上
03:26
根本沒有辦法給你一個好的正確率
03:28
所以，這個時候你要回頭去檢查說
03:31
在前面的 step 裡面
03:33
要做什麼樣的修改
03:34
好讓你在 training set 上可以得到好的正確率
03:38
假設現在，幸運的是你已經
03:41
在 training set 上得到好的 performance 了
03:44
你要用 deep learning 在 training set 上
03:46
得到 100% 的正確率，是沒那麼容易的
03:49
但可能你在 MNIST 上得到一個 99.8% 的正確率
03:53
接下來，你就把你的 network apply 到 testing set 上
03:57
testing set 上的 performance 才是我們
03:59
最後真正關心的 performance
04:02
那你現在把你的結果 apply 到 testing set 上
04:07
那在 testing set 上 performance 怎麼樣呢？
04:09
如果現在得到的結果是 NO 的話
04:12
那就是 Overfitting
04:15
這個情況才是 Overfitting
04:17
你在 training set 上得到好的結果
04:20
但是，在 testing set 上得到的是不好的結果
04:24
這個時候，這個情況呢
04:28
才叫做 Overfitting
04:29
那你要回過頭去，做某一些事情
04:32
然後，試著去解決 overfitting 這個 problem
04:35
但有時候，你加了新的 technique
04:37
去想要 overcome overfitting 這個 problem 的時候
04:40
你其實反而會讓 training set 上的結果變壞
04:44
所以，你在做這一步的修改以後
04:47
你要先回頭去檢查說，training set 上的結果
04:50
是怎麼樣的，如果 training set 上的結果變壞的話
04:53
你要從頭呢
04:54
去對你的 network training 的 process 做一些調整
05:00
那如果你同時在 training set 還有你手上的 testing set
05:03
都得到好的結果的話
05:04
最後，你就可以把你的系統真正用在 application 上面
05:08
你就成功了
05:10
那這邊有一個重點就是
05:12
不要看到所有不好的 performance
05:15
就說是 overfitting
05:18
舉例來說，這個是文獻上的圖
05:21
但我在現實生活中，也常常看到這樣子的狀況
05:24
在 testing set 上面
05:26
這個是 testing data 的結果
05:28
橫坐標，是 model 參數 update 的次數
05:32
所以，你做 Gradient Descent 的時候
05:34
你 update 幾次參數
05:36
縱座標，是 error rate，所以越低越好
05:39
那如果我們現在表示一個 20 層的 network
05:42
它是黃線
05:43
這個 56 層的 neural network，它是紅線
05:47
那你會發現說，這個 56 層的 network
05:50
它的 error rate 比較高，它的 performance 比較差
05:53
20 層的 neural network，它的 performance 是比較好的
05:56
那有些人看到這個圖，就會馬上得到一個結論
05:59
說 56 層太多了，參數太多了
06:03
56 層果然沒有必要，這個是 overfitting
06:06
但是，真的是這樣子嗎？
06:08
你在說，現在得到的結果是 overfitting 之前
06:12
你要先檢查一下你在 training set 上的結果
06:15
對某些方法來說，你不用檢查這件事
06:17
比如說 k nearest neighbor 或 decision tree
06:20
你不用檢查這件事
06:21
但是，對 neural network 來說
06:23
你是需要檢查這件事情的
06:25
為甚麼呢？因為有可能你在 training set 上得到的結果
06:29
是這個樣子
06:30
是這個樣子的
06:32
橫軸一樣是參數 update 的次數
06:34
縱軸是 error rate
06:36
如果我們比較 20 層的 neural network 跟
06:38
56 層的 neural network 的話
06:40
你會發現說
06:41
在 training set 上 ，20 層的 neural network
06:45
它的 performance 本來就比 56 層好
06:48
在 training set 上 ，56 層的 neural network
06:50
它的 performance 是比較差的
06:54
是比較差的
06:56
那為甚麼會這樣子呢？
06:58
你想想看你在做 neural network training 的時候
07:00
有太多太多的問題
07:02
可以讓你的 training 的結果是不好的
07:05
比如說，我們有 local minimum 的問題
07:08
有 saddle point 的問題，有 plateau 的問題
07:10
有種種的問題
07:11
所以，有可能這個 56 層的 neural network
07:14
你 train 的時候，它就卡在一個 local minimum 的地方
07:18
所以，它得到了一個差的參數
07:21
所以，這個並不是 overfitting
07:23
是在 training 的時候，就沒有 train 好
07:27
那有人會說，這個叫做 underfitting
07:31
我覺得這個可能不叫做 underfitting
07:33
但是這個只是名詞定義的問題啦，你要怎麼說都行
07:37
但是，在我的心裡面，underfitting 的意思是說
07:40
這個 model 的 complexity
07:45
這個 model 的參數不夠多，所以
07:47
它的能力不足以解出這個問題
07:50
但對這個 56 層的 neural network 來說
07:53
雖然它得到比較差的 performance
07:54
但假如這個 56 層的 network
07:57
它其實是在這個 20 層的 network 後面
08:00
後面再另外堆 36 層的 network
08:03
那它的參數，其實是比 20 層的 network 還多的
08:07
理論上，20 層的 network 可以做到的事情
08:09
56 層的 network 一定可以做到
08:12
你前面已經有那 20 層
08:13
你前面那 20 層就做跟 20 層 network 一樣的事情
08:17
後面那 36 層就甚麼事都不幹，就都是 identity 就好了
08:20
你明明可以做到跟 20 層一樣的事情
08:24
你為甚麼做不到呢？
08:25
但是，因為會有很多的問題就是
08:27
讓你沒有辦法做到
08:30
所以，這個 56 層的 network 呢
08:32
它比 20 層差，並不是因為它能力不夠
08:36
它只要前 20 層都跟它一樣，後面都是 identity
08:42
明明就可以跟 20 層一樣好
08:45
但它卻沒有得到這樣的結果
08:47
所以，它能力是夠的，所以我覺得這不是 underfitting
08:49
它這個就是沒有 train 好這樣子
08:53
那我還不知道有沒有什麼名詞，專門指稱這個問題
08:58
所以，它其實就是像這個小智的噴火龍一樣
09:02
它等級是夠的，但它就不想要打這樣子
09:07
所以，在 deep learning 的文獻上
09:10
如果，當你讀到一個方法的時候
09:13
你永遠要想一下說，這個方法
09:16
它是要解什麼樣的問題
09:19
因為在 deep learning 裡面，有兩個問題
09:23
一個是 training set 上的 performance 不好
09:25
一個是 testing set 上的 performance 不好
09:27
當只有一個方法 propose 的時候
09:29
它往往就是針對這兩個問題的其中一個
09:33
來做處理
09:35
舉例來說，你等一下能會聽到一個方法叫做 dropout
09:38
dropout 或許大家或多或少都會知道，它是一個
09:41
很有 deep learning 特色，很潮的一個方法
09:46
那很多人就會說，哦，這麼潮的方法，所以
09:48
我今天只要看到 performance 不好，我就很快 dropout
09:51
但是，你只要仔細想一下 dropout 是甚麼時候用的
09:54
dropout 是你在 testing 的結果不好的時候
09:57
你才會 apply dropout
10:00
你的 testing data 結果好的時候
10:02
你是不會 apply dropout
10:03
就是說，dropout 是
10:05
你在 testing 結果不好的時候，才 apply dropout
10:07
如果你今天的問題是你 training 的結果不好
10:10
你 apply dropout，你只會越 train 越差而已
10:13
所以，不同的方法，對治甚麼樣不同的症狀
10:16
你是必須要在心裡想清楚的
10:18
那我們這邊就休息 10 分鐘，等一下再繼續講，謝謝
10:22
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
10:27
各位同學大家好
10:29
我們來上課吧
10:31
來上課吧
10:33
等一下呢，我們剛才講說
10:36
這個在 deep learning 的 recipe 裡面
10:39
在 train deep learning 的時候有兩個問題
10:41
所以，等一下呢，我們就是要
10:44
這兩個問題，分開來討論
10:47
看看當你遇到這兩個問題的時候呢
10:49
有甚麼樣解決的方法
10:54
首先，如果你今天的
10:57
training 在 training 的結果上不好的時候
10:59
你可能可以看看說
11:01
是不是你在做 network 架構設計的時候
11:04
是不是設計不好
11:06
舉例來說，你可能用的 activation function
11:09
是比較不好的 activation function
11:12
是對 training 比較不利的 activation function
11:14
你可能會換一些新的 activation function
11:16
它可以給你比較好的結果
11:19
那我們知道說
11:21
在 1980 年代的時候
11:23
比較常用的 activation function
11:25
是一個 sigmoid function
11:28
那我們之前有稍微試著 reason 一下
11:32
為甚麼要用 sigmoid function
11:36
今天如果我們用的是 sigmoid function 的時候
11:39
在過去，其實你會發現說
11:42
deeper 並不一定 imply better
11:46
這個是在 MNIST 上面的結果啦
11:49
在手寫數字辨識上的結果
11:51
當你 layer 越來越多的時候呢
11:53
你的 accuracy，一開始持平，後來就掉下去了
11:57
在你的 layer 是 9 層、10 層的時候，整個結果就崩潰啦
12:01
那有人看到這個圖，就會覺得說
12:02
9 層、10 層參數太多了，overfitting
12:07
這個，不是 overfitting
12:09
為甚麼呢？首先呢，我們說你要
12:12
檢查你現在 performance 不好是不是來自於 overfitting
12:15
你要看你 training set 的結果嘛，對不對？
12:18
那這個線，是 training set 的結果
12:21
所以，這個不是 overfitting
12:23
這個是 training 的時候，就 train 壞掉了
12:25
不信的話，我們實際來用 Keras 實做一下
12:31
一個原因是這樣子
12:33
一個原因是
12:35
這個原因叫做 Vanishing 的 Gradient
12:38
這個原因是這樣，當你把 network 疊得很深的時候
12:42
在 input 的幾個 layer
12:44
在最靠近 input 的地方呢
12:47
你的這個 Gradient
12:50
你的這些參數，對最後 loss function 的微分
12:54
會是很小的
12:56
而在比較靠近 output 的地方呢
12:58
它的微分值，會是很大的
13:01
因此，當你設定同樣的 learning rate 的時候
13:06
你會發現說，靠近 input 的地方
13:08
它參數的 update，是很慢的
13:10
靠近 output 的地方，它參數的 update 是很快的
13:14
所以，你會發現說呢
13:16
在 input 幾乎還是 random 的時候
13:19
output 就已經 converge 了
13:22
在 input layer
13:25
在靠近 input 地方的這些參數
13:27
它還是 random 的時候
13:29
output 的地方，就已經根據這些 random 的
13:32
random 的結果呢
13:33
找到了一個 local minimum
13:35
然後，它就 converge 了
13:38
這個時候，你會發現說
13:39
你這個參數的 loss 下降的速度呢
13:44
變得很慢，你就覺得說
13:46
卡在 local minimum 什麼之類的
13:48
就傷心地把程式停掉了
13:51
這個時候，你得到的結果是很差的，為什麼呢？
13:53
因為這個 converge，是幾乎 base on random 的參數
13:57
那幾乎 base on random 的 output
13:59
然後去 converge，所以得到的結果，是很差的
14:02
那為甚麼會有這個現象發生呢？
14:05
為甚麼會有這個現象發生呢？
14:07
如果你自己把 Backpropagation 的式子寫出來的話
14:11
你可以很輕易地發現說，用 sigmoid function
14:15
會導致這件事情的發生
14:17
但是，我們今天不看 Backpropagation 的式子
14:19
我們其實從直覺上來想
14:21
你也可以了解為什麼這件事情發生
14:24
怎麼用直覺來想
14:26
一個參數的 Gradient 的值應該是多少呢？
14:28
我們知道說
14:30
某一個參數 w 對 total cost C 的偏微分阿
14:36
意思就是說，它的直覺的意思就是說
14:39
當我今天把某一個參數做小小的變化的時後
14:43
它對這個 cost 的影響是怎麼樣
14:47
了解嗎？就是我們可以把一個參數
14:50
做小小的變化，然後觀察它對 cost 的變化
14:53
以此來決定說，這個參數
14:56
它的 Gradient 的值有多大
14:59
所以，怎麼做呢？我們就把
15:02
第一個 layer 裡面的某一個參數
15:05
加上 △w，看看對 network 的 output
15:11
和它的 target 之間的 loss
15:15
有甚麼樣的影響
15:17
那你會發現說
15:19
如果我今天這個 △w 很大
15:22
通過 sigmoid function 的時候
15:24
這個 output 呢，是會變小的
15:27
也就是說，改變了某一個參數的 weight
15:31
對某一個 neuron 的 output 的值會有影響
15:36
但是，這個影響是會衰減的
15:40
為甚麼這麼說呢？
15:42
因為，假設你用的是 sigmoid function
15:45
我們知道 sigmoid function 形狀就長這樣
15:48
那 sigmoid function 它會把
15:50
負無窮大到正無窮大之間的值
15:52
都硬壓到 0~1 之間
15:57
也就是說，如果你有很大的 input 的變化
16:03
通過 sigmoid function 以後
16:05
它 output 的變化，會是很小的
16:08
所以，就算今天你這個 △w 有很大的變化
16:13
造成 sigmoid function 的 input 有很大的變化
16:16
對 sigmoid function 來說，它的 output 的變化
16:18
是會衰減的
16:20
而每通過一次 sigmoid function
16:23
變化就衰減一次
16:25
所以，當你的 network 越深
16:27
它衰減的次數就越多
16:29
直到最後，它對 output 的影響是非常小的
16:33
也就是說，你在 input 的地方
16:35
改一下你的參數
16:38
對 output 的地方
16:39
它最後 output 的變化，其實是很小的
16:42
因此，最後對 cost 的影響也很小
16:45
因此，就造成說，在靠近 input 的那些 weight
16:49
它對它這個 Gradient 的值是小的
16:54
那怎麼解決這個問題呢？
16:57
有人就說
16:59
原來比較早年的做法是去 train RBM
17:03
去做這個 layer-wise 的 training
17:07
也就是說，你先認好一個 layer
17:10
就因為我們現在說，如果你把所有的這個
17:13
network 兜起來，那你做 Backpropagation 的時候
17:16
第一個 layer 你幾乎沒有辦法被挑到嘛
17:18
所以，用 RBM 做 training 的時候，它的精神就是
17:22
我先把一個 layer train 好
17:24
再 train 第二個，再 train 第三個
17:26
最後，你在做 Backpropagation 的時候
17:28
雖然說，第一個 layer 幾乎沒有被 train 到
17:30
那無所謂
17:31
一開始在 pre-train 的時候，就把它 pre-train 好了
17:34
所以，這就是 RBM 為什麼做 pre-train 可能有用的原因
17:38
那後來有人說，其實
17:41
後來有人發現說
17:42
其實，我記得 Hinton 跟 Pengel 都
17:46
幾乎在同樣的時間，不約而同地提出同樣的想法
17:49
改一下 activation function
17:51
可能就可以 handle 這個問題了
17:54
所以，現在比較常用的 activation function
17:58
叫做 Rectified Linear Unit，它的縮寫是 ReLU
18:02
會常看到有人叫它 ReLU
18:04
那這個 activation function 它長這樣子
18:07
這個 z 是 activation function 的 input
18:09
a 是 activation function 的 output
18:12
如果今天 activation function 的 input 大於 0 的時候
18:16
input = output
18:17
如果 activation function 的 input 小於 0 的時候
18:20
output 就是 0
18:21
那選擇這樣的 activation function 有甚麼好處呢？
18:25
有以下幾個理由，第一個理由是
18:28
它比較快，跟 sigmoid function 比起來
18:31
它的運算是快很多的
18:33
sigmoid function 裡面還有 exponential
18:35
那個是很慢的，那如果你是
18:37
用這個方法的話，它是快得多的
18:39
如果你看這個
18:41
我記得是 Pengel 寫得原始的 paper 的話呢
18:44
裡面會告訴你說
18:46
這個 activation function 的想法其實
18:48
是有一些生命上的理由的
18:50
那它把這樣的 activation 跟一些生物上的觀察呢
18:55
結合在一起
18:57
那 Hinton 有說過說
18:59
ReLU 這樣的 activation function
19:01
其實，等同於是無窮多的 sigmoid function
19:05
疊加的結果
19:06
無窮多的 sigmoid function
19:08
它們的 bias 都不一樣，疊加的結果會變成 ReLU
19:10
的 activation function
19:11
但它最重要的理由是
19:14
它可以 handle Vanishing gradient 的這個問題
19:17
它怎麼 handle Vanishing gradient 這個問題呢？
19:20
我們來看一下，這個是一個 ReLU 的 neural network
19:26
這是一個 ReLU 的 neural network
19:28
它裡面的每一個 activation function
19:30
都是 ReLU 的 activation function
19:32
那我們知道說
19:34
ReLU 的 activation function
19:35
它作用在兩個不同的 region
19:37
一個 region 是當 activation function 的 input
19:41
大於 0 的時候，input = output
19:43
另外一個 region 是
19:44
activation function 的 input 小於 0
19:46
所以，output 就是 0
19:48
所以，現在每一個 ReLU 的 activation function
19:51
它作用在兩個不同的 region
19:54
一個 region 是
19:56
每一個 activation function 的
19:57
一個可能是 activation function 的 output 就是 0
20:02
另外一個可能是
20:03
activation function 的 input = output
20:05
當 input = output 的時候
20:07
其實，這個 activation function 就是 linear 的
20:09
就是 linear 的
20:11
那對那些 output 是 0 的 neuron 來說
20:15
它其實對整個 network 是一點影響都沒有的阿
20:18
它 output 是 0，所以
20:20
它根本就不會影響最後 output 的值
20:22
所以，假如有一個 neuron 它 output 是 0 的話
20:24
你根本可以把它從 network 裡面整個拿掉
20:27
把它從 network 裡面整個拿掉
20:29
當你把這些 output 是 0 的 network 拿掉
20:33
剩下的 neuron，就都是 input = output，是 linear 的時候
20:37
你整個 network，不就是一個很瘦長的
20:41
linear network 嗎？
20:43
你整個 network，其實就變成是 linear 的 network
20:47
那這個時候
20:49
我們剛才說
20:52
我們的 Gradient 會遞減
20:54
是因為通過 sigmoid function 的關係
20:57
sigmoid function 會把比較大的 input
21:00
變成比較小的 output
21:01
但是，如果你是 linear 的話
21:03
input = output
21:04
你就不會有那個 activation function 遞減的問題了
21:09
講到這邊，有沒有人有問題呢？
21:13
講到這邊，我有一個問題
21:18
現在如果我用 ReLU 的時候，整個 network
21:22
都變成 linear 的阿
21:24
可是，我們要的不是一個 linear 的 network 阿
21:27
我們之所以用 deep learning，就是因為我們
21:30
不想要我們的 function 是 linear 的
21:32
我們希望它是一個 non-linear，一個比較複雜的 function
21:35
所以，我們用 deep learning
21:37
當我們用 ReLU 的時候
21:41
它不就變成一個 linear 的 function 了嗎？
21:43
這樣都不會有問題嗎？
21:44
這樣不是變得很弱嗎？
21:49
其實是這樣子的，這整個 network 呢
21:51
整體來說，它還是 non-linear 的
21:54
大家聽得懂嗎？
21:55
當你的每一個 neuron 做 operation
22:00
當每一個 neuron，它 operation 的 region 是一樣的時候
22:04
它是 linear 的
22:05
但是
22:08
也就是說，如果你對 input 做小小的改變
22:11
不改變 neuron 的 operation 的 region
22:14
它是一個 linear 的 function
22:16
但是，如果你對 input 做比較大的改變
22:18
你改變了 neuron 的 operation region 的話
22:22
它就變成是 non-linear 的
22:26
這樣大家可以接受嗎？
22:32
好那有另外一個問題
22:35
這個也是我常常被問到的問題
22:37
這個不能微分阿
22:40
這不能微分
22:41
這樣你不覺得很苦惱嗎？
22:43
我們之前說，我們做 Gradient Descent 的時候
22:48
你需要對你的 loss function 做微分
22:51
意思就是說
22:52
你要對你的 neural network 是可以做微分的
22:54
你的 neural network 要是一個可微的 function
22:56
ReLU 不可微阿
22:58
至少這個點是不可微的
23:00
那怎麼辦呢？
23:01
其實，實作上你就這個樣子啦
23:04
當你的 region 在這個地方的時候
23:06
gradient 微分就是 1
23:08
region 在這個地方的時候，微分就是 0
23:11
反正不可能 input 正好是 0 嘛，就不要管它
23:14
結束這樣
23:18
那我們來實際試一下
23:20
如果我們把 activation function 換成 ReLU 的時候
23:23
會得到甚麼樣的結果
23:24
比如說，我們就這樣子
23:27
把 sigmoid 換成 ReLU
23:34
把 sigmoid 換成 ReLU，那我們剛才用 sigmoid 的時候
23:38
training 和 testing 的 accuracy 都很差
23:40
我們就簡單的把它換成 ReLU
23:42
甚麼其他事都沒做
23:46
沒換嗎？等我一下
24:02
那 ReLU 其實還有種種的變數
24:05
那有人覺得說，如果是 ReLU 的時候
24:09
如果是原來的 ReLU
24:11
它在 input 小於 0 的時候
24:13
output 會是 0，這個時候微分是 0
24:15
你就沒有辦法 update 你的參數了
24:17
所以，我們應該讓
24:19
在 input 小於 0 的時候，output 還是有一點點的值
24:23
也就是 input 小於 0 的時候，output 是
24:26
input 乘上 0.01
24:28
這個東西叫做 Leaky ReLU
24:32
那這個時候，有人就會問說
24:34
為甚麼是 0.01，為甚麼不是 0.07, 0.08 之類的呢？
24:39
所以，就有人提出了 Parametric ReLU
24:42
他說，在負的這邊呢
24:45
(output) a = (input) z*α
24:52
α 是一個 network 的參數
24:54
它可以透過 training data 被學出來
24:57
甚至每一個 neuron 都可以有不同的 α 的值
25:02
那又會有人問說，為甚麼一定要是 ReLU 這個樣子呢？
25:07
可不可以是別的樣子
25:08
所以，後來又有一個更進階的想法， 叫做 Maxout network
25:12
那在 Maxout network 裡面呢
25:14
你就是讓你的 network 自動學它的 activation function
25:20
那因為現在 activation function 是自動學出來的
25:23
所以 ReLU 就只是 Maxout network 的一個 special case
25:29
Maxout network 它可以學出 ReLU 這樣的 activation function
25:33
但是，它也可以是其他的 activation function
25:36
用 training data 來決定說
25:37
現在的 activation function 應該要長甚麼樣子
25:41
Maxout network 長甚麼樣子呢？
25:43
假設現在有 input
25:45
一個 2 dimension 的 vector，[x1, x2]
25:48
然後，我們就把
25:50
x1, x2 乘上不同的 weight
25:52
變成一個 value, 5
25:55
然後，再乘上不同 weight 得到 7
25:57
再乘上不同 weight 得到 -1，再乘上不同 weight 得到 1
26:00
那本來這些值呢
26:02
應該要通過 activation function
26:03
不管是 sigmoid function 還是 ReLU
26:07
得到另外一個 value
26:08
但是，現在在 Maxout network 裡面
26:11
現在在 Maxout network 裡面呢
26:13
我們做的事情是這樣子
26:16
你把這些 value
26:18
group 起來，你把這些 value group 起來
26:21
哪些 value 應該被 group 起來這件事情是
26:24
事先決定的
26:26
比如說，現在，這兩個 value 是一組
26:29
這兩個 value 是一組
26:31
那你在同一個組裡面
26:34
選一個值最大的當作 output
26:36
比如說，這個組就選 7
26:38
這個組就選 1
26:39
那這件事情呢
26:42
其實就跟 Max Pooling 一樣對不對
26:44
只是我們現在不是在 image 上做 Max Pooling
26:47
我們是在在一個 layer 上做 Max Pooling
26:50
我們把 layer 裡面的
26:52
本來要放到 neuron 裡面的
26:54
這個 activation function
26:57
我們本來要把它放到 neuron 的 activation function
27:01
的這個 input 的值 group 起來
27:03
然後，只選 max 當作 output
27:05
然後，就不用 activation function 了
27:07
就不加 activation function
27:09
得到的值是 7 跟 1
27:11
那你可以想說，這個東西呢
27:13
就是一個 neuron
27:15
只是它的 output 是一個 vector，而不是一個值
27:20
那接下來這個 7 跟 1 呢
27:22
就乘上不同的 weight
27:24
就得到另外一排不同的值
27:26
然後，你一樣把它們做 grouping
27:30
你一樣從每個 group 裡面選最大的值
27:34
1 跟2 就選 2，4 跟3 就選 4
27:38
其實，在實作上
27:40
幾個 element 要不要放在同一個 group 裡面，這件事情
27:44
是你可以自己決定的
27:45
就跟 network structure 一樣，是你自己需要調的參數
27:49
所以，你可以不是兩個 element 放一組
27:51
你可以是 3 個、4 個、5 個都可以
27:53
這個是你自己決定的
27:57
我們現在先說，Maxout network
28:01
它是有辦法做到跟 ReLU 一模一樣的事情
28:05
它可以模仿 ReLU 這個 activation function
28:09
怎麼做呢？
28:10
我們知道說，假設我們這邊有一個 ReLU 的 neuron
28:14
它的 input 就一個 value x
28:17
你會把 x 乘上這個 neuron 的 weight, w
28:22
再加上 bias, b
28:24
然後，通過 activation function, ReLU 得到 a
28:29
所以，現在如果我們看 x 跟 a 的關係
28:34
是什麼樣子呢？
28:36
假設 x 是橫軸
28:38
那這個 x 是橫軸
28:41
假設 y 軸是這個 z 的話
28:43
它就是 w*x + b
28:46
z 跟 x 之間的關係是 linear 的
28:49
是 linear 的，是這個樣子
28:51
那如果你選 a 呢
28:53
a 跟 z有甚麼樣的關係呢？
28:56
因為現在通過的是 ReLU 的 activation function
29:00
所以，如果你今天 z 的值大過 0 的時候
29:04
a = z
29:05
z 的值小於 0 的時候
29:07
a 就是 0
29:08
所以，a 跟 x 的關係是這個樣子
29:11
在這個地方，a = z；在這個地方，a = 0
29:16
所以，我們今天用 ReLU 的 activation function
29:19
它 input 和 output，x 和 a 之間的關係是長這樣子
29:23
如果我們今天用 Maxout network
29:26
用 Maxout network，你把 w
29:29
你把 input, x 乘上 weight, w 再加上 bias，得到 z1
29:35
你再把 x 乘上另外一組 weight
29:39
加上另外一個 bias
29:40
得到 z2，那我今天假設說
29:43
另外一個 weight 跟另外一個 bias 都是 0，所以 z2 = 0
29:46
然後，你做 Max Pooling
29:50
你就可以選 z1, z2 其中一個比較大的呢
29:53
當作 a
29:54
現在，如果我們看 z1 跟 x 之間的關係
29:59
我們得到的是藍色這條線
30:02
如果我們看 x 跟 z2 之間的關係
30:07
我們得到的是水平這條線，因為 z2 總是 0
30:11
如果 z2 前面接的 weight 跟 bias 都是 0
30:14
z2 總是 0，所以它是紅色的這條線
30:17
那我們現在做的是 Maxout
30:20
我們是在 z1, z2 裡面選一個大的當作 output a
30:25
所以，如果今天 x 是在這個 region 的時候
30:27
你的 a 就會等於 z1，是這個 region
30:30
如果今天 x 是在這個 region 的時候
30:33
你的 a 就會等於比較大的 z2，所以是這個 region
30:37
那今天你只要把這個 w 跟這個 b
30:40
等於這個 w 跟這個 b
30:42
你就可以讓
30:44
這個 ReLU 的 input 和 output 的關係
30:46
等於這個 Maxout network 的 input 和 output 的關係
30:49
所以，由此可知， 就是
30:51
ReLU 是 Maxout network 可以做到的事情
30:55
只要它設定出正確的參數
30:58
但是，Maxout network 它也可以做出
31:01
更多的、不同的 activation function
31:04
比如說，現在假設這兩個 weight 不是 0，而是 w', b'
31:08
那會怎樣呢？
31:09
就得到藍色這條線 z1
31:11
跟紅色這條線 z2
31:14
因為 w', b' 是不一樣的值，所以
31:16
它可能是另外一條斜直線，長的是這樣子
31:19
接下來，你做 Max Pooling 的時候
31:21
你會在 z1, z2 裡面選一個大的
31:24
所以，在這個範圍內，你選了
31:28
你選了 z1
31:29
在這個範圍內，你選了 z2
31:32
所以你就得到了一個不一樣的 activation function
31:35
而這個 activation function 長甚麼樣子
31:37
是由 network 的參數 w, b, w', b' 決定的
31:41
所以，這個 activation function 它是一個
31:43
Learnable 的 activation function
31:45
它是一個可以根據 data 去 generate 出來的 activation function
31:48
每一個 neuron 根據不同的 weight
31:50
它可以有不同的 activation function
31:53
那 ReLU 是這樣子
31:55
它可以做出任何的
31:58
piecewise 的 linear 的 convex activation function
32:03
如果你看一下它的性質，你就不難理解這件事情
32:06
那至於這個 piecewise 的 linear function 裡面
32:10
有多少個 piece
32:12
這決定於你現在把多少個 element 放在一個 group
32:16
假如說兩個 element 一個 group
32:17
那你可以有長這樣子的 activation function
32:20
是 ReLU
32:21
你可以有一個 activation function 它的作用就是取
32:24
絕對值
32:25
假設你是 3 個 element 一個 group
32:28
你可以有長這樣子的 activation function
32:31
你也可以有長這樣子的 activation function 等等
32:34
那接下來我們要面對另外一個問題，就是
32:37
這個東西怎麼 train
32:39
這個東西怎麼 train
32:41
這裡面有個 Max 阿
32:42
它不能微分阿
32:45
這個東西怎麼 train
32:47
這個做法是這樣子的
32:50
假設現在這個 z1 跟
32:54
假設這邊這兩個值，比較大的是
32:58
上面這個值
32:59
我們現在把這個 group 裡面比較大的值，用框框框起來
33:04
用框框框起來
33:05
那比較大的值
33:08
就會等於這個 max operation 的 output
33:11
就會等於 max operation 的 output
33:13
所以，這個值等於這個值，這個值等於這個值
33:16
這個值等於這個值，這個值等於這個值
33:19
所以，max operation 其實 在這邊就是一個 linear 的 operation
33:23
這是 linear，這是 linear，只是它
33:27
會選擇在前面這個 group 裡面的
33:29
它只接給前面這個 group 裡面的某一個 element
33:36
也就是說，也就是說其實呢
33:38
那這些沒有被接到的 element，它就沒用啦
33:43
它就不會影響 network 的 output 啦
33:44
所以，你就可以把它拿掉，你就可以把它拿掉
33:49
所以，其實當我們在做 Maxout 的時候
33:53
當你給它一個 input 的時候
33:55
你其實也是得到一個比較細長的 linear network
34:01
所以，你在 train 的時候，你 train 的就是
34:03
這個比較細長的 linear network 裡面的參數
34:07
你就是去 train 這些連到這一個 element 的這些參數
34:11
連到這個 element 的這些參數
34:15
假設我給你一個這樣子的 linear network， 你當然知道它是怎麼 train 的
34:19
用 Backpropagation train 就好，你知道它是怎麼 train 的
34:22
但這個時候呢，你就會有一個問題
34:29
沒被 train 到的 element 怎麼辦呢？
34:31
如果某一個這個 element，它不是最大的值
34:34
那它連接的那些 weight
34:37
就不會被 train 到了嗎？
34:39
你做 Backpropagation 的時候
34:40
你只會 train 在這個圖上的
34:43
比較深顏色的這些實線
34:46
你不會 train 到這個 weight 阿
34:48
這個 weight 不就沒被 train 到了嗎？
34:50
怎麼辦呢？
34:52
這看起來，表面上是一個問題
34:54
但實作上，它不是一個問題
34:57
為甚麼呢？
34:58
因為當你 input 不同的
35:01
當你給它不同的 input 的時候
35:04
你得到的這些 z 的值是不一樣的
35:06
你給它不同 input 的時候
35:08
max 的值，是不一樣的
35:11
所以，每一次你給它不同的 input 的時候
35:14
這個 network 的 structure 都是不一樣的
35:17
因為我們有很多很多筆 training data
35:19
而 network 的 structure 不斷地變換
35:21
所以，最後每一個 weight 在實際上都會被 train 到
35:25
Maxout 就是這麼做
35:27
Maxout network 就是這麼做
35:29
所以，如果我們回到 Max Pooling
35:31
Max Pooling跟 Maxout 是一模一樣的 operation 阿
35:34
只是換一個說法而已，對不對
35:35
所以，你會 train Maxout
35:38
你就會 train Max Pooling，這是一模一樣的作法
35:40
講到這邊大家有沒有甚麼問題呢？
35:44
沒有的話，那
35:48
另外一個我們要講的是這個
35:50
adaptive 的 learning rate
35:52
其實 adaptive 的 learning rate，我們之前已經有講過了
35:55
我們之前有講過這個 Adagrad
35:59
我們之前有講過 Adagrad
36:00
我們說 Adagrad 的做法就是
36:03
我們現在每一個 parameter 都要有不同的 learning rate
36:07
而這個 learning rate 是怎麼 給它這麼 adaptive 的 learning rate 呢？
36:11
我們就把一個固定的 learning rate η
36:13
除掉這一個參數過去所有 gradient 值的平方和，開根號
36:19
把這項除以平方和開根號
36:23
就得到新的 parameter
36:26
那這個 Adagrad 它的精神就是說
36:28
如果我們今天考慮兩個參數，w1, w2
36:32
如果 w1 是在
36:35
這個方向上
36:36
如果 w1 在這個方向上，它平常 gradient 都比較小
36:41
那它是比較平坦的，給它比較大的 learning rate
36:44
反過來說，在這個方向上
36:46
平常 gradient 都是比較大的
36:48
所以，它是比較陡峭的，所以給它比較小的 learning rate
36:55
但是，實際上呢，我們面對的問題
36:58
有可能是比 Adagrad 可以處理的問題更加複雜的
37:04
也就是說，我們之前在做這個 Linear Regression 的時候
37:10
我們看到的這個 optimization 的 function
37:13
loss function 是這樣子 convex的形狀
37:15
但實際上，當我們在做 deep learning 的時候
37:18
這個 loss function 它可以是任何形狀，你知道嗎？
37:22
它可以是任何形狀
37:24
比如說，它可以是這樣，怪異的月形的形狀
37:28
如果當今天你的 error surface 是這個形狀的時候
37:34
那你會遇到的問題是，就算是同一個方向上
37:38
你的 learning rate 也比需要能夠快速地變動
37:42
就我們剛才在做 convex function 的時候
37:45
在每個方向上
37:46
這個方向很平坦，就一直很平坦
37:48
這個方向很陡峭，就一直很陡峭
37:50
但是，如果今天在更複雜的問題的時候
37:52
有可能，你考慮 w1 改變是在這個方向
37:56
在某個區域
37:57
它很平坦，所以它需要比較小的 learning rate
38:01
但是，到了另外一個區域
38:03
它又突然變得很陡峭
38:04
這個時候，它需要比較大的 learning rate
38:07
所以，真正要處理 deep learning 的問題，用 Adagrad
38:11
可能是不夠的，你需要更 dynamic 的調整
38:16
這個 learning rate 的方法
38:18
所以，這邊有一個 Adagrad 的進階膽，叫 RMSProp
38:21
RMSProp，我覺得是一個滿神奇的方法
38:24
因為你好像，找不到它的 paper
38:26
因為這個在 Hinton 的那個
38:28
MOOC 的 course 裡面，他提出來
38:30
他在他的線上課程裡面提出一個方法
38:34
大家要 cite 的時候，要 cite 那個線上課程的連結
38:40
這招還真的有用
38:42
這個 RMSProp 是這樣子做的
38:44
我們現在把
38:48
這個固定的 learning rate
38:50
除掉一個值，我們稱之為 σ
38:53
這個 σ 是甚麼呢？
38:55
在第一個時間點
38:58
這個 σ 就是你第一個算出來的 gradient 的值 g^0
39:04
那在第二個時間點呢？
39:06
在第二個時間點，你算出一個新的 gradient, g^1
39:10
這個時候
39:13
你的 σ 的值
39:15
新的 σ 的值，σ^1 呢
39:18
就是原來的 σ 值的平方，乘上 α
39:25
再加上新的 g 的值，(g^1)^2
39:29
再乘上 (1 - α)
39:31
而這個 α 的值是
39:34
你可以自由去調的
39:36
也就是我們原來在
39:39
或是我們再來看下一個例子
39:42
我們現在有一個
39:43
在下一個時間點，我們又算出 g^2
39:45
我們得到 σ^2
39:47
σ^2 怎麼算的呢？它是把原來的 σ^1
39:50
取平方乘上 α
39:52
再加上 (1 - α) 乘上 (g^2)^2
39:55
再開根號，得到這個 σ^2
39:58
那跟原來的 Adagrad 不一樣的地方是
40:00
原來的 Adagrad 你在這邊分母放的值
40:04
就是把 g^0, g^1, g^2 都取平方和開根號
40:08
但是，在這邊的時候
40:11
在 RMSProp 裡面呢，這個 σ^1
40:14
它裡面包含了 g^0 跟 g^1
40:17
那這邊也包含了 g^2
40:18
所以，它根號裡面也同樣包含了 g^0, g^1, g^2
40:21
就跟 Adagrad 一樣
40:22
但是，你現在可以給它乘上 weight, α
40:25
或者是 (1 - α)
40:27
所以，你可以調整說
40:29
我比較傾向
40:31
你可以調整這個 α 的值
40:32
這個 α 的值就也是像 learning rate 阿
40:34
也是你要手動設的值，當然你就設個 0.9 之類的
40:38
你可以手動去調這個
40:40
α 的值，讓它說
40:43
如果你把這個 α 的值設的小一點
40:47
那意思就是說，你傾向於相信新的 gradient
40:52
所告訴你的，這個 error surface
40:54
平滑或陡峭的程度
40:56
傾向於相信新的 gradient
40:59
比較無視於舊的 gradient 提供給你的 information
41:02
這樣大家應該可以瞭解這個結果
41:06
所以，在第 t 個時間點，你算出來的 σ
41:09
就是把 (σ^(t-1))^2 乘上 α
41:11
加上 (1 - α) 乘上在第 t 個時間點算出來的
41:15
gradient 的平方
41:17
所以，當你做 RMSProp 的時候
41:20
你一樣是在這算 gradient 的 zooming square
41:23
但是，你可以給
41:26
現在已經看到的 gradient 比較大的 weight
41:28
給過去看到的 gradient 比較小的 weight
41:33
除了 learning rate 的問題以外
41:35
我們知道說在做 deep learning 的時候
41:38
大家都會說，我們會卡在 local minimum
41:40
那我之前也有講過說，我們不見得是卡在 local minimum
41:43
也有可能卡在 saddle point
41:45
甚至，你有可能卡在 plateau 的地方
41:48
大家聽到這個問題都非常的擔心
41:51
覺得說，哇！這個做 deep learning 呢
41:54
是非常困難的
41:55
因為你可能胡亂做一下就一大推的問題
41:57
那其實呢
41:59
Yann LeCun 他在 07 年的時候
42:01
他有一個滿特別的說法
42:04
07 年的時候就講過這件事情
42:05
它說你不用擔心 local minimum 的問題
42:10
我不知道這件事情有多確切
42:14
我沒有 verify 過
42:15
但是，如果你有甚麼 verify 的結果的話
42:18
你可以跟我分享一下
42:20
Yann LeCun 的說法，他是這樣說的，他說
42:22
其實，在這個 error surface 上
42:25
沒有太多 local minimum
42:27
所以，你不用太擔心
42:28
為甚麼呢？他說
42:31
你要是一個 local minimum
42:33
你在每一個 dimension
42:34
都必須要是這樣子的形狀對不對？
42:37
都要是一個山谷的谷底
42:40
每一個 dimension 都要是山谷的谷底
42:42
我們假設這個山谷的谷底出現的機率是 p 好了
42:46
山谷的谷底出現的機率是 p
42:47
因為我們的 network 有非常非常多的參數
42:51
所以，假設有 1000 個參數
42:53
你每一個參數都要是山谷的谷底
42:55
那機率就是 p^1000
42:57
你的 network 越大
42:59
參數越多，這個出現的機率就越低
43:02
所以呢，local minimum
43:03
在一個很大的 neural network 裡面
43:05
其實沒有你想像的那麼多
43:07
一個很大的 neural network，它看起來其實是
43:10
其實搞不好是很平滑的，根本沒有太多 local minimum
43:13
所以，當你走走走，走到一個你覺得是
43:16
local minimum 的地方，卡住的時候
43:18
它八成就是 global minimum，或是很接近 gloal minimum
43:22
給大家參考
43:25
那你有甚麼特別的想法，再告訴我
43:31
有一個 heuristic 的方法
43:34
可以稍微處理一下，我們上述說的
43:38
我們剛才講的 local minimum 還有 plateau 的問題
43:41
這個方法，你可以說是從真實的世界，得到一些靈感
43:46
我們知道在真實的世界裡面
43:48
如果這個是一個地形，是一個山坡
43:51
你把一個球從左上角丟下來，把它滾下來
43:56
然後，它滾滾滾，它滾到 plateau 的地方呢
43:58
它不會停下來阿，因為有慣性嘛，它還會繼續往前
44:02
它就算是走到上坡的地方，假設這個波沒有很陡
44:06
因為慣性的關係
44:07
它搞不好走走走，還是可以翻過這個山坡
44:10
結果它就可以走到了
44:12
比這個 local minimum 還要好的地方
44:15
那所以我們
44:17
要做的事情就是把
44:19
這個慣性這個特性呢
44:21
塞到 Gradient Descent 裡面去
44:23
那這件事情，就叫做 momentum
44:29
這個東西怎麼做呢？我們先很快地秒複習一下
44:32
一般的 Gradient Descent
44:34
一般的 Gradient Descent 是怎麼做的呢？
44:37
我們是這樣子做的
44:38
這個是選一個初始的值
44:40
然後，計算一下它的 gradient
44:43
它的 gradient 是這個方向
44:44
那我們就走 gradient 的反方向
44:46
乘上一個 learning rate η
44:49
得到 θ^1，再算 gradient
44:51
再走一個新的方向
44:52
再算 gradient、再走一個方向； 再算 gradient、再走一個方向
44:55
以此類推
44:55
一直到 gradient = 0 的時候，或 gradient 趨近 0 的時候
44:58
我們就停止
45:00
當我們加上 momentum 的時候我們是怎麼做的呢？
45:03
當我們加上 momentum 的時候
45:05
我們每一次移動的方向
45:09
不再是只有考慮 gradient
45:14
而是我們現在的 gradient
45:19
加上在前一個時間點
45:22
移動的方向
45:23
這樣聽起來可能很抽象，所以
45:26
我們實際地來看一下，它是怎麼運作的
45:28
一樣選一個初始值 θ^0
45:31
一樣選一個初始值 θ^0
45:33
然後，我們用一個值 v 去記錄
45:39
我們在前一個時間點
45:41
移動的方向
45:42
v 記錄我們前一個時間點移動的方向
45:45
因為現在是初始值，之前沒有移動過，所以
45:48
前一個時間點移動的方向是 0
45:50
接下來計算在 θ^0 地方的 gradient
45:53
現在算出 θ^0 的 gradient
45:56
算出來是紅色這個箭頭
45:59
然後，我們現在要移動的方向
46:02
並不是紅色箭頭告訴我們的方向
46:05
而是，前一個時間點的
46:07
movement v^0
46:09
再加上 negative 的 gradient
46:13
然後，我們得到現在要移動的方向 v^1
46:17
所以，到這邊就好像是慣性一樣
46:19
如果我們之前走的方向是 v^0
46:25
那今天有一個新的 gradient，並不會
46:28
讓你參數 update 的方向完全轉向
46:31
它會改變你的方向
46:33
但是，因為有慣性的關係，所以
46:36
原來走的方向還是有一定程度的影響
46:39
那我們或許看下一個例子，會比較清楚
46:43
我們現在在上一個時間點移動的方向呢
46:45
是 v^1
46:49
接下來，再計算一下 gradient
46:52
計算一下 gradient，就是紅色的箭頭
46:55
接下來要決定說，在第二個時間點
46:59
我們要走的方向是甚麼樣？
47:01
第二個時間點要走的方向是
47:03
過去走的方向 v^1
47:05
減掉 leaning rate 乘上 gradient
47:08
如果我們看這個圖上的話，gradient 會告訴我們說
47:12
要走這個方向
47:13
負的 η 乘上 gradient，要走這個方向
47:17
但是，前面的 movement
47:19
是綠色的箭頭，它是這個方向
47:21
這個方向
47:22
我們會把這個 movement 乘上一個 λ
47:25
那這個 λ 其實也是一個
47:26
跟 learning rate 一樣，是手要調的參數
47:28
它告訴你說，現在這個慣性這件事情，影響力有多大
47:32
λ 大的話，慣性影響力就大
47:34
λ 小的話，慣性影響力就大
47:36
總之，慣性告訴我們走這邊
47:38
gradient 告訴我們走這邊
47:40
這兩個合起來呢，就走了一個新的方向，就是這邊
47:43
這個就是 v2，所以，以此類推
47:46
新的 gradient 告訴我們走這邊
47:49
慣性告訴我們
47:50
新的 gradient 告訴我們走紅色這個虛線的方向
47:53
慣性告訴我們走綠色虛線的方向
47:56
合起來最後就是走藍色的方向
47:59
然後，update 參數以後
48:01
gradient 告訴我們走紅色這個虛線的方向
48:03
慣性告訴我們走綠色虛線的方向
48:06
合起來就是走藍色的方向
48:10
那你可以用另外一個方法來理解這件事情
48:14
其實，v^i
48:16
你在每一個時間點移動的 movement
48:20
你在第 i 個時間點移動的步伐 v^i 呢
48:25
移動的量、方向，v^i 呢
48:28
其實就是過去所有算出來的 gradient 的總和
48:33
為甚麼這麼說呢？
48:35
我們知道 v^0 = 0
48:39
v^1 呢，v^1 在這邊
48:41
v^1 是 λ*(v^0) - η*(θ^0) 的 gradient
48:45
而 v^0 = 0
48:47
所以，v^1 = -η * gradient
48:51
所以是這個樣子
48:52
那 v^2 呢，v^2 我們就把
48:56
v^1 是負的 gradient
48:58
v^1 是負的 η * gradient 代進去
49:04
我們把 v^1 代到這邊，再乘上 λ
49:07
再減掉 η，乘以 θ^1 的 gradient
49:10
得到結果就是這樣
49:11
你得到的結果就是你把 θ^0 的地方的 gradient
49:14
減掉 λ * η
49:16
再減掉 η * θ^1 的 gradient
49:20
你得到 v^2
49:23
所以，v^2 裡面同時有在 θ^0 算出來的 gradient
49:27
同時有在 θ^1 的地方算出來的 gradient
49:29
只是這兩個 gradient，它的 weight 是不一樣的
49:32
如果你 λ 都設小於 0 的值的話呢
49:35
越之前的 gradient，它的 weight 就越小
49:40
越之前的 gradient，就越不去理它
49:42
你越在意現在的 gradient，但是過去的 gradient
49:45
也會對你現在要 update 的方向有一定程度的影響力
49:50
這個，就是 momentum
49:53
如果你看數學式子不太喜歡的話
49:56
那我們就從直覺上來看一下
49:58
到底加入 Momentum 以後，是怎麼運作的
50:02
在加入 Momentum 以後呢
50:03
每一次移動的方向
50:06
是 negative 的 gradient 加上 momentum
50:13
建議我們要走的方向
50:14
Momentum 其實就是上一個時間點的 movement
50:19
所以，現在假設我們有一個
50:22
假設我們初始的參數是在這個位置
50:25
那 gradient 建議我們往右走
50:28
所以，最後就往右移動
50:31
那如果說之後移到這個位置
50:34
gradient 建議我們往右走
50:37
而 momentum 也會建議我們往右走
50:41
因為我們是從左邊這邊移過來的嘛
50:44
所以，前一個步伐我們是向右的
50:47
如果你考慮 momentum 的時候呢
50:49
我們也會向右
50:50
所以，你把 gradient 建議我們走的方向
50:53
跟 momentum 建議我們走的方向合起來
50:55
你就得到這個藍色的線
50:58
所以你會繼續向右，如果我們今天走到
51:00
local minimum 的地方
51:02
走到 local minimum 的地方，gradient 是 0
51:05
所以，gradient 會告訴你說就停在這裡吧
51:07
但是，momentum 會告訴你說，之前是從右邊走過來的
51:11
所以，你仍然應該要繼續往右走
51:13
也就是綠色箭頭的方向
51:15
所以，最後你參數 update 的方向
51:17
仍然會繼續向右
51:19
甚至你可以樂觀地期待說
51:22
如果今天在往右的時候，走到這個地方
51:25
gradient 要求我們向左走
51:28
現在左邊如果是算微分的話呢
51:31
如果考慮 gradient 的話呢
51:32
參數應該往左移動
51:35
但是，momentum 建議我們
51:37
繼續向右走
51:39
因為你是從左邊過來的
51:41
因為你是從左向右過來的
51:43
所以 momentum 建議你繼續向右走
51:45
如果今天 momentum 其實比較強的話
51:47
你最後，就還是會向右走
51:50
所以，你有一定的可能性，你可以
51:52
有可能你可以跳出
51:55
local minimum，如果這個 local minimum 不深的話
51:57
你有可能藉由慣性的力量呢
52:01
跳出 local minimum，然後走到比較好的 global minimum
52:06
比較低的 local minimum
52:08
那如果你今天把 RMSProp 加上 momentum 的話
52:13
其實，你就得到 Adam 這樣
52:15
現在如果你沒有甚麼 prefer 的話
52:17
你就先學 Adam 就是了，那我發現在
52:20
作業二裡面，其實還滿多人 implement Adam 的
52:23
大家太強了，都自己 implement Adam 在作業二
52:26
我想這些你們都很熟了
52:28
沒什麼特別好講的
52:30
你可能看這個 algorithm
52:33
哇！感覺好像有點複雜
52:35
但是，好多人 implement 這個東西
52:39
其實，Adam 就是 RMSProp 加上 momentum
52:46
這兩個東西綜合起來，就是 Adam
52:48
我們非常非常快地來看一下這個式子
52:52
在這個式子裡面呢
52:54
在這個式子裡面，一開始要先初始一個東西叫做 m0
52:57
m0 就是 momentum
53:00
就是前一個時間點的 movement
53:02
那這邊有另外一個值叫做 v0
53:05
v0 就是我們剛才在 RMSProp 裡面看到的那個 σ
53:09
這個東西就是之前的 gradient 的 root mean square
53:13
之前算出來的 gradient 的平方和
53:17
就是 v0
53:19
你看，它先算一下 gradient，就是 gt
53:24
然後，根據 gt 呢
53:27
你就可以算出 mt，也就是現在要走的方向
53:32
現在要走的方向，是考慮過去要走的方向
53:35
再加上 gradient
53:36
接下來，算一下要放在分母的地方的 vt
53:42
這個 vt 是過去、前一個時間點的 vt
53:46
加上 gradient 的平方，等一下要開根號
53:49
這邊呢，它做了一個
53:51
跟原來 RMSProp 跟 momentum 裡面沒有的東西
53:55
叫做 bias correction
53:58
它會把 mt 跟 vt 都除上一個值
54:01
都除上一個值，那這個值本來比較大
54:04
那後來呢
54:05
這個值本來比較小，那後來呢
54:09
會越來越接近 1
54:11
至於為甚麼要這麼做， 他的 paper 裡面會告訴你他的理由
54:16
最後，你在 update 的時候
54:18
你把 momentum 建議你的方向，mt\head
54:22
去乘上 learning rate α，再除掉 RMSProp
54:29
就是 RMSProp normalize 以後，建議的 learning rate
54:34
最後，得到你 update 的方向
54:35
這個就是 Adam 這樣
54:37
那我猜你應該沒有聽得太懂
54:39
不過沒有關係，因為在 toolkit 裡面，只是打幾個
54:42
指令而已，我們就先在這邊休息 10 分鐘
54:45
等一下再繼續，謝謝
54:56
我們來上課吧
54:59
我們剛才講的，就是說
55:02
如果今天你在 training data 上的結果不好的話怎麼辦
55:08
那等一下我要講的呢？
55:10
如果今天你已經在 training data 上得到夠好的結果
55:14
但是，你在 testing data 上的結果仍然不好
55:17
那你有甚麼可行的方法
55:19
等一下會很快介紹 3 個方法
55:22
一個是 Early Stopping
55:24
Regularization 跟 Dropout
55:25
Early Stopping 跟 Regularization 是很 typical 的作法
55:29
他們不是 specific design for deep learning 的
55:32
這是一個很傳統、typical 的作法
55:35
那 Dropout 是一個滿有 deep learning 特色的做法
55:38
那在講 deep learning 的時候，需要講一下
55:41
我們來講一下 Early Stopping， Early Stopping 是甚麼意思呢？
55:45
我們知道說，隨這你的 training
55:48
你的 total loss，如果你今天的 learning rate 調的對的話
55:53
你的 total loss 通常會越來越小
55:54
那有可能你 rate 沒有設好，loss 變大也是有可能的
55:58
那你想像 learning rate 調的很好的話
56:01
那你在 training set 上的 loss 應該是逐漸變小的
56:04
但是，因為我們知道說
56:05
training set 跟 testing set 他們的 distribution
56:07
並不完全一樣
56:09
所以，有可能當你的 training 的 loss 逐漸減小的時候
56:13
你的 testing data 的 loss 卻反而上升了
56:17
這是有可能的
56:18
所以，理想上，假如你知道 testing data 的 loss 的變化
56:23
你應該停在不是 training set 的 loss 最小
56:27
而是 testing set 的 loss 最小的地方
56:31
你在 train 的時候，你不要一直 train 下去
56:33
你可能 train 到這個地方的時候，就停下來了
56:36
但是實際上，我們不知道 testing set 阿
56:39
你根本不知道你 testing set 的 error 是甚麼阿
56:42
所以，我們其實會用 validation set
56:46
來 verify 這件事情
56:49
所以，有的地方我可能需要稍微講得清楚一點
56:52
就是這邊的 testing set 阿，並不是指
56:55
真正的 testing set
56:57
它指的是你有 label data 的 testing set
56:59
比如說，如果你今天是在做作業的時候
57:02
這邊的 testing set 可能指的是
57:04
Kaggle 上的 public set
57:06
或者是，你自己切出來的 validation set
57:10
希望大家可以知道我的意思
57:13
這個只是名詞用的不同而已
57:17
但是，你不會知道真正的 testing set 的變化
57:22
所以，其實我們會切一個 validation set
57:25
來 verify 說
57:26
甚麼時候用 validation set 模擬 testing set
57:29
來看說甚麼時候呢
57:30
這個 validation set 的 loss 最小的時候
57:32
你的 training 就停下來
57:34
那其實在 Kaggle 裡面，就可以支援你做這件事啊
57:37
所以，你就自己看一下 documentation
57:40
那 Regularization 是甚麼呢？
57:45
我們重新定義了那個我們要去 minimize 的 loss function
57:51
我們原來有一個 loss function
57:55
我們要 minimize 的 loss function
57:58
是 define 在你的 training data 上的
58:00
比如說要 minimize square error 或 minimize cross entropy
58:04
那在做 Regularization 的時候呢
58:06
我們會加另外一個 Regularization 的 term
58:09
這個 Regularization 的 term 呢
58:13
這個 Regularization 的 term 呢
58:15
比如說，它可以是你的參數的 L2 norm
58:18
甚麼意思呢？
58:19
假設現在我們的參數 θ 裡面，它是
58:22
一群參數，w1, w2 等等有一大堆的參數
58:26
那這個 θ 的 L2 norm 呢，就是
58:29
你把你的 model 裡面的每一個參數都取平方
58:33
然後加起來，就是這個 θ2
58:37
那因為我們現在用 L2 norm 來做 Regularization
58:41
所以，這件事稱之為 L2 的 Regularization
58:44
那我們之前有講過說，在做 Regularization 的時候呢
58:48
一般我們是不會考慮 bias 這項
58:52
因為我們之前有講過說，加 Regularization 的目的
58:56
是為了要讓我們的 function 更平滑
58:59
而 bias 這件事情
59:01
通常跟 function 的平滑程度是沒有關係的
59:04
所以，通常我們在算 Regularization 的時候
59:07
不會把 bias 考慮進來
59:10
那如果我們把
59:13
L2 的 Regularization 放在這邊
59:15
我們會得到怎麼樣的結果呢
59:18
如果做微分的話，會得到怎麼樣的結果呢
59:21
如果我們把這個新的 objective function
59:23
我們把新的這個 loss function，也就是 L'
59:26
等於 L 加上 parameter 的 2 norm
59:31
做 gradient 的話呢
59:33
我們會得到 L' 對某一個參數 w 的偏微分
59:37
等於 L 對某個參數的偏微分
59:39
加上 λ 乘上某一個參數
59:42
因為這一項是
59:45
所有參數的平方和
59:47
所以，把這項對某個參數 w 做偏微分
59:50
你得到的結果就是 w
59:53
所以，你現在 update 參數的式子
59:57
會變成這樣
59:58
本來我們 update 的式子是把原來的參數
60:02
減掉 η 乘上 w 對
60:07
loss function 的偏微分 就得到新的參數
60:11
那現在這個 loss function 呢
60:13
這個 L'，這個 ∂L'/∂w
60:16
你可以換成，這個樣子
60:18
那你把這一項塞到這個地方
60:21
你把這一項塞到這個地方
60:24
那你會發現說呢
60:25
這邊有出現原來的參數
60:28
這邊也有出現原來的參數
60:31
所以，你可以把這幾項整理在一起
60:35
就變成這樣
60:36
你把這一項、這一項提出來
60:39
變成 (1 - η*λ) * w^t
60:42
再減掉你的參數對原來的 loss function 的 gradient
60:49
所以，如果根據這個式子，你就會發現說
60:52
其實在 update 參數的時候
60:55
每一次在 update 之前
60:57
你就把參數先乘個 (1 - η*λ)
61:01
也就是，每次你在 update 你的參數之前
61:04
通常你這邊的 η 就是你的 learning rate
61:07
它是一個很小的值
61:09
那這個 λ 通常會設一個很小的值，比如說
61:12
0.001 之類的
61:15
所以，η*λ 就是一個很小的值
61:17
(1 - η*λ) 通常是一個接近 1 的值
61:20
比如說 0.99，所以，今天你看這個 update 的式子的話
61:25
如果我們不管原來的 loss function 怎麼寫
61:29
只看這個 update 式子的話
61:30
等於你在 update 參數的時候，你做的事情是
61:33
每次 update 參數之前，就不分青紅皂白，先乘個 0.99
61:39
也就是說，你每次都會讓你的參數
61:41
越來越接近 0
61:42
不一定是越來越小，因為如果今天 w 是負的
61:46
w 是負的，負的乘上 0.99
61:49
它就變大了，它就接近 0，對不對
61:52
所以，今天每一個參數在 update 之前
61:55
都乘上一個小於 1 的值，所以它每次都越來越靠近 0
61:59
越來越靠近 0
62:00
那有人就會想說
62:02
越來越靠近 0，最後不就通通變 0 嗎？
62:04
這很崩潰阿，聽起來就不 make sense
62:07
那不會最後所有的參數都變 0
62:09
為甚麼？因為你還有後面這一項阿
62:11
沒有後面這一項
62:13
每一次 update 參數就越來越小，最後通通變 0
62:16
但是，問題就是後面還有這個
62:18
從微分那邊得到這一項
62:20
那這一項，會跟前面這一項，最後取得平衡
62:25
所以，並不會最後所有的參數都變成 0
62:29
因誤，如果我們使用
62:31
L2 的 Regularization 的時候
62:33
我們每次都會讓 weight 小一點、小一點、小一點
62:37
所以，這招叫做 Weight Decay
62:41
那其實 是這樣子
62:42
在 deep learning 裡面
62:45
Regularization 雖然有幫助，但是它的重要性
62:49
跟其他方法，比如說 SVM 比起來，並沒有那麼高
62:54
Regularization 幫助往往沒有那麼顯著
62:57
我覺得有一個可能的原因是
62:59
如果你看前面的 Early Stopping
63:01
我們可以決定說，甚麼時候 training 應該要被停下來
63:06
因為，我們現在在做這個 neural network 的時候
63:09
通常初始參數的時候，我們都是從
63:11
一個很小的、接近 0 的值開始初始參數
63:15
初始的時候，都是給它一個很小的、接近 0 的值
63:18
那你在做 update 的時候
63:20
通常就是讓參數離 0 越來越遠、越來越遠
63:24
而做 Regularization 這件事情
63:27
它要達到的目的，就是希望我們的參數
63:30
不要離 0 太遠
63:32
那我們參數不要離 0 太遠
63:35
加上 Regularization 所造成的效果
63:38
跟減少 update 次數
63:40
所造成的效果
63:42
其實，可能是很像的
63:43
但你今天做 Early Stopping，減少 update 次數
63:46
其實也會避免你的參數
63:47
離那些接近 0 的值太遠
63:50
那跟 Regularization 做的事情可能是很接近的
63:54
所以在 neural network 裡面
63:55
Regularization 雖然有幫助，但沒有那麼重要
63:57
沒有重要到說，比如說你看像 SVM
64:00
它是 explicitly 把 Regularization 這件事情
64:03
寫在它的 objective function 裡面
64:05
對不對，因為在做 SVM 的時候
64:08
它其實是要解一個 convex optimization problem
64:11
所以，實際上
64:12
它解的時候，並不一定會有 iteration 的過程
64:15
它一步就解出那個最好的結果了
64:17
它不像 deep learning 裡面有 Early Stopping 這件事
64:24
SVM 裡面，沒有 Early Stopping 這件事
64:25
一步就走到結果了
64:27
所以，你沒有辦法
64:28
用 Early Stopping 防止它離你太遠
64:31
所以你必須要把 Regularization
64:33
explicitly 加到你的 loss function 裡面去
64:38
那如果我們看 L1 的 Regularization
64:43
有人就會問說，為甚麼一定是平方，能不能用別的
64:46
當然可以用別的，比如說，你可以做
64:48
L1 的 Regularization，你可以把你的
64:51
Regularization 換成你的參數的 1 norm
64:54
也就是換成你參數裡面
64:56
換成你這個參數的集合裡面
64:58
每一個參數的絕對值的和
65:01
所以，如果我們把這一項換掉的話
65:05
如果我們把這一項從 2 norm 換成 1 norm 的話
65:07
會得到麼事呢？
65:09
你的第一個問題可能就是
65:12
絕對值不能微分阿
65:14
不能微分阿
65:16
給你一個最簡單的回答就是
65:18
這個東西 implement 在 Keras, TensorFlow 都沒有問題
65:21
所以，顯然是可以微分這樣
65:23
那實際的回答是這個樣子的
65:26
這個東西阿
65:29
它是取絕對值對不對？
65:32
那取絕對值，input 和 output 的關係不就長這樣子嗎？
65:36
不就是一個 V 的形狀嗎？
65:38
然後，在 V 的一邊
65:41
微分值是 1，在另外一邊微分值是 -1
65:45
那不能微的地方，其實只有在 0 的地方而已
65:47
就不要管它這樣子
65:51
真的出現、真的走到 0 的時候
65:54
你就胡亂給它一個值，比如說，給它 0 就好了
65:59
所以說
66:00
如果你把這一項
66:02
對 w 做微分的時候
66:04
你得到的結果是怎麼樣呢？
66:06
如果今天 w 是正的
66:10
那微分出來就是 +1
66:12
w 是負數，微分出來就是 -1
66:15
所以，我們這邊寫了一個 w 的 sign function
66:20
w 的 sign function 意思就是說，如果 w 是正數的話
66:23
這個 function output 就是 +1
66:25
w 是負數的話，這個 function output 就是 -1
66:29
如果我們把這一項
66:32
塞到參數 update 的式子裡面，會有甚麼結果呢？
66:36
就變成這樣
66:37
那我們可以把這個展開來
66:41
就變成這樣
66:42
那這個式子告訴我們甚麼？
66:44
這個式子告訴我們說，我們每一次在 update 參數的時候
66:48
我們每一次 update 參數的時候
66:50
我們就不管三七二十一
66:51
都一定要去減一個 η*λ，再乘一個
66:56
w 的 sign
66:57
也就是說，如果今天 w 是正的
67:00
w 是正的，這項就是 +1
67:03
所以，就變成是減一個 positive 的值
67:05
就會讓你的參數變小
67:07
如果 w 是負的，這一項是 -1
67:09
那就變成是加一個值，就會讓你的參數變大
67:13
也就是說，只要你的參數是正的
67:16
就減掉一些，只要你的參數是負的
67:19
就加上一些
67:21
那不管那個參數原來的值是多少
67:23
所以，如果你把這個 L1
67:25
跟 L2 做一下比較的話
67:29
他們同樣是讓參數變小，但是他們做的事情
67:32
是略有不同的
67:34
因為如果你是用 L1 的時候，每次都減掉固定的值
67:37
你用 L2 的時候
67:39
你是每一次都乘上一個小於 1 固定的值
67:42
所以，比如說，今天 w 是一個很正的值的話
67:46
比如說，它是一百萬
67:47
那你乘上一個 0.99，你其實把 w 減掉一個很大的值
67:53
但是，對 L1 來說
67:55
它每次減掉的值都是固定的
67:57
不管 w 是一百萬還是 0.1，w 減掉的值都是固定的
68:01
所以，對 L1 來說，對 L2 來說
68:04
只要 w 有出現很大的值
68:06
這個很大的 w
68:10
它下降很快，它很快就會變得很小
68:12
在 learning 的 process 中
68:14
但是，如果你今天是
68:16
L1 的話，那就不一樣了
68:18
如果 w 有很大的值，它的下降速度跟其他
68:21
很小的 w 是一樣的
68:22
所以，透過 L1 的 training 以後
68:24
你有可能認出來的 model 裡面
68:26
還是有一些很大很大的值
68:29
但是，如果我們考慮很小的值的話
68:32
對 L2 來說，很小的值
68:34
比如說 0.1, 0.01 阿，它的下降速度就很慢
68:40
所以，在 L 裡面，它會
68:42
train 出來的結果，它會保留很多接近 0 的值
68:47
那 L1 呢，它每次到下降一個固定的 value
68:51
那在 L1 裡面呢
68:53
它不會保留很多很小的值
68:56
所以，如果你用 L1 做 training 的時候呢，你得到的結果
69:00
就是會比較 sparse
69:01
那比較 sparse 的意思是說你 train 出來的參數裡面
69:04
有很多接近 0 的值
69:06
但是，也有很大的值
69:08
不像如果是 L2 的話，你 train 出來的結果
69:11
你的值是平均的都比較小
69:14
所以，他們 train 出來的結果是
69:16
略有差異的，那我們剛才
69:18
我們剛才在講 cn 的時候，有講過
69:21
L1 就是要產生一個 image 的時候，有產生 L1
69:25
那在剛才那個 task 裡面呢
69:27
L1 是比較適合的
69:29
因為我想要看到 sparse 的結果
69:32
我有試過用 L2，但是結果就沒有 L1 看起來那麼明顯
69:37
雖然 L1 看起來也沒有很明顯啦
69:38
但 L1 看起來的結果
69:40
是還比較像是一個 digit
69:43
那這邊就胡亂講一個東西，Weight Decay
69:47
我們在人腦裡面也會做 Weight Decay，對不對
69:50
這個是從、我記得龍騰的生物課本上有這個圖
69:53
這個是剛出生的時候，嬰兒的神經是這樣
69:57
6 歲的時候，有很多很多的神經
70:00
但是，到 14 歲的時候，神經間的連結
70:03
又減少了，所以
70:06
neural network 也會跟我們人
70:08
有一些很類似的事情，如果有一些 weight
70:10
你都沒有去 update 它
70:12
那它每次都會越來越小
70:14
最後就接近 0 就不見了
70:16
這跟人腦的運作，是有異曲同工之妙
70:20
那最後我們要講一下 dropout
70:22
我們先講 dropout 是怎麼做的
70:25
然後，才講為甚麼這樣做
70:29
dropout 是怎麼做的呢？
70:31
它是這樣，在 training 的時候
70:34
training 的時候，每一次我們要 update 參數之前
70:37
我們都對每一個 neuron，其實也包括 input 的地方
70:42
input 的 input layer 裡面的每一個 element
70:45
也算是一個 neuron
70:46
我們對 network 裡面的每一個 neuron
70:48
做 sampling，那這個 sampling 是要決定說
70:53
這個 neuron 要不要被丟掉，每個 neuron 有 p% 的機率
70:58
會被丟掉
71:01
那如果一個 neuron 被 sample 到要丟掉的時候
71:04
那你知道這個 neuron 要被丟掉了，那跟它相連的 weight
71:07
也失去作用，也都被丟掉，所以就變這樣
71:11
所以，做完這個 sample 以後
71:12
你的 network 的 structure 就變瘦了，變得比較細長
71:17
然後，你再去 train 這個比較細長的 network
71:22
而要注意一下，這個 sampling
71:26
是每次 update 參數之前，都要做一次
71:29
所以，每一次 update 參數的時候
71:31
你拿來 training 的那個 network structure 是不一樣的
71:35
每一次你都要重新做一次 sample，所以，你每一次
71:38
在做重新 sample 的時候
71:40
你得到的這個結果，會是不一樣的
71:45
那 testing 的時候呢
71:47
當你在 training 的時候，使用 dropout 的時候
71:52
你的 performance 是會變差的
71:54
了解我意思嗎？就是
71:56
因為本來如果你不要 dropout 的話
71:59
本來好好的做，不要 dropout 的話
72:01
你在 MNIST 上，剛剛可以把正確率做個 100% 這樣
72:06
但是，如果你加 dropout 的時後
72:08
因為你的神經元在 train 的時候
72:09
有時候莫名其妙就會不見
72:11
所以，你在 training 的時候， 有時候 performance 是會變差的
72:13
本來可以 train 到 100%
72:14
它就會變成只剩下 98%
72:17
有一些 neuron 不見了嘛
72:18
所以，當你加了 dropout 的時候
72:21
你在 training 上會看到的結果變差
72:23
但是，dropout 它真正要做的事情是
72:25
它就是要讓你 training 的結果變差
72:28
但是 testing 的結果是變好的
72:30
也就是，如果你今天遇到的問題是你 training 做得不夠好
72:32
你再加 dropout，你就是越做越差這樣子
72:35
那在 testing 的時候怎麼做呢？
72:36
在 testing 的時候要注意兩件事
72:39
第一件事情就是 testing 的時候不加 dropout
72:42
testing 的時候就是所有的 neuron 都要用
72:44
不做 dropout
72:46
另外一個地方是
72:49
在 testing 的時候
72:51
假設你的 dropout rate
72:54
在 training 的時候，dropout rate 是 p%
72:56
那在 testing 的時候，所有 weight 都要乘 (1 - p%)
73:01
也就是說，假設現在 dropout rate 是 50%
73:04
那我們在 training 的時候，learn 出來的 weight
73:07
等於 1
73:08
那 testing 的時候，你要把那個 weight
73:11
設 0.5
73:12
那有沒有很奇怪，我看很多人都皺眉頭這樣子
73:17
這個步驟非常地神妙
73:21
我覺得第一個想出來這個人
73:22
這個 Hinton，若憑空想出來這個想法真的非常神妙
73:27
那你自己在 implement dropout 的時候阿
73:29
過去，在還沒有那麼多 toolkit 的時候，常常有人說
73:32
拿給我看一個程式，說我做 dropout 了
73:35
它都沒有進步，老師你看看怎麼辦
73:37
我看就說，你忘了除這個 0.5，難怪沒有進步
73:41
不過現在 toolkit 都會自動幫你除 0.5
73:43
所以，就不用再擔心這件事情了
73:46
那為甚麼 dropout 有用，直覺的想法是這樣子
73:51
training 的時候，會丟掉一些 neuron
73:54
就好像是你要練輕功的時候，你會在腳上綁一些重物
73:59
然後，你實際上戰鬥的時候
74:01
實際上 testing 的時候，是沒有 dropout 的
74:03
實際上 test 的時候
74:04
你就把重物拿下來，所以就會變得很強
74:07
這個是小李，他平常都綁一個重物
74:09
只有在，我記得是要貫徹自己的忍道的時候
74:12
他才會拿下來
74:14
還是打輸我愛羅就是了
74:19
另外一個直覺的理由是這樣子
74:24
一個 neural network 裡面的每一個 neuron 就是一個學生
74:28
那大家被連結在一起
74:31
就是大家聽到要做 final project
74:34
那你知道說，在一個團隊裡面
74:37
總是有人會擺爛，就是它是會 dropout 的
74:40
所以，假設說你覺得你的隊友
74:43
其實是會擺爛的
74:45
所以，這個時候你就會想要好好做
74:47
實際上，你就會想要去 carry 他
74:50
但是，實際上最後在 testing 的時候
74:51
大家都有好好做，沒有人需要被 carry
74:53
那因為每個人都做是更有利，所以，結果是更好的
74:59
所以，在 testing 的時候，不用 dropout
75:01
另外，我想解釋的就是
75:04
直覺的需要解釋的就是
75:07
為甚麼 dropout rate 50% 的時候，就要乘 0.5
75:11
為甚麼 training 跟 testing 的 weight 是不一樣的呢？
75:15
照理說 training 用甚麼 weight 就要用在 testing 上阿
75:17
你這樣 training 跟 testing 的時候居然是用不同的 weight
75:20
為甚麼這樣呢？
75:21
直覺的理由是這樣
75:24
假設現在 dropout rate 是 50%
75:27
那在 training 的時候，你的期望總是會
75:32
丟掉一半的 neuron
75:34
對每一個 neuron 來說
75:37
總是期望說它有一半的 neuron
75:39
是不見的，是沒有 input 的
75:41
所以，你現在如果認好一組 weight
75:44
假設你在這個情況下，認好一組 weight
75:47
但是，在 testing 的時候，是沒有 dropout 的阿
75:50
所以，對同一組 weight 來說
75:53
假如你在這邊用這組 weight 得到 z
75:56
跟在這邊用這組 weight 得到 z'
75:58
它們得到的值，其實是會差兩倍的，對不對
76:00
因為在這個情況下，你總是會有一半的 input 不見
76:04
在這個情況下，你所有的 input 都會在
76:06
而你用同一組 weight 的話，變成 z' 就是 z 的兩倍了
76:10
這樣變成 training跟 testing 不 match
76:12
你 performance反而會變差
76:13
所以，怎麼辦？
76:15
把所有 weight 都乘 0.5 阿
76:17
乘 0.5 以後，做一下 normalization
76:26
把所有 weight 都乘 0.5，這樣 z 就會等於 z'
76:30
就是這麼回事
76:32
把這個 weight 乘上一個值以後，
76:35
反而會讓 training 跟 testing 是比較 match 的
76:38
這個是比較直觀上的結果，如果你要
76:41
更正式講的話，其實 dropout 有很多理由
76:44
這個東西還是一個可以探討的問題
76:47
在文獻上找到很多不同的觀點來解釋
76:51
為甚麼 dropout 會 work
76:53
那我覺得我比較能接受的是
76:56
dropout 是一種終極的 ensemble 的方法
76:59
甚麼是 ensemble 的方法呢？
77:01
ensemble 的方法在比賽的時候常用
77:03
ensemble 的方法意思是說
77:05
我們有一個很大的 training set
77:08
那你每次從 training set 裡面
77:10
只 sample 一部分的 data 出來
77:13
只 sample 一部分的 data 出來
77:15
記得我們在講 bias 跟 variance 的 trade off 的時候
77:19
我們不是以講過說，打靶有兩種狀況，一種是
77:24
你的 bias 大，所以你打不準
77:27
一種是你的 variance 很大，所以你打得準
77:30
如果你今天有一個很複雜的 model
77:32
很笨重、很大的 model 的時候
77:34
它往往是 bias 準，但 variance 很大
77:38
但是，如果你這個笨重的 model 有很多個
77:40
雖然它 variance 很大，最後平均起來
77:42
結果就很準
77:43
對不對，所以今天只 ensemble 做的事情
77:46
其實，就是要利用這個特性
77:49
我們 train 很多個 model
77:54
我們把原來的 training data 裡面 sample 出很多 subset
77:57
然後，train 很多個 model
77:59
然後，每一個 model 你甚至可以是 structure 不一樣
78:01
雖然說，每一個 model 他們可能
78:04
variance 很大，但是
78:07
如果他們都是很複雜的 model 的時候，平均起來
78:11
這個 bias 就很小
78:13
所以，你真正在 testing 的時候
78:15
train 了一把 model，然後在 testing 的時候
78:18
丟一筆 training data 近來，它通過所有的 model
78:22
得到一大堆的結果，再把這一大堆的結果平均起來
78:25
當作我們最後的結果
78:27
那如果你的 model 很複雜的畫，這一招往往有用
78:30
那 random forest 也是實踐這個方法的
78:33
一個實踐這個精神的一個方法
78:36
如果你用一個 decision tree，它就很弱
78:38
胡亂做它就會 overfitting，那如果你用 random forest
78:41
它就沒有那麼容易 overfitting
78:44
那為甚麼說 dropout 是一個終極的 ensemble 的方法呢
78:48
我們知道在做 dropout 的時候，我們每次
78:51
我們每一次要 update 參數的時候
78:54
就你拿一個 minibatch 出來，要 update 參數的時候
78:56
你都會做一次 sample
78:58
所以，你拿第一個 minibatch 的時候
79:01
你 train 的 network 長這樣子
79:03
你拿第二個 minibatch 的時候
79:04
你 train 的 network 可能長這樣
79:06
你拿第三個長這樣，你拿第四個長這樣
79:08
所以，在做 dropout 的時候
79:10
你等於是一個終極的 ensemble 的方式
79:13
你是在 train，假設你有 M 個 neuron
79:16
每一個 neuron 可以 drop 或不 drop
79:18
所以你可能的 neuron 的數目有 2^M 個
79:22
當你在做 dropout 的時候
79:23
你等於是在 train 這 2^M 個 neuron
79:28
你每次都只用一個 minibatch 的 data
79:32
去 train 一個 network，你用這個 minibatch 裡面的 data
79:35
用 minibatch 可能就 100 筆 data 嘛
79:38
你用這些 data 去 train 這些 network
79:43
那總共有 2^M 個可能的 network
79:47
當然因為你最後 update 的次數是有限的
79:50
你可能沒有辦法把 2^M 個 network 每個都 train 一遍
79:54
但是，你可能就 train 了好多好多的參數
79:57
好多好多的 network
79:58
你有做幾次 update 參數，你就 train 幾次 network
80:01
但是，每個 network 就只用一個 batch 來 train
80:04
那每一個 network 用一個 batch 來 train
80:06
可能會讓人覺得很不安
80:08
一個 batch 才 100 筆 data，怎麼 train 一個 network 呢
80:11
那沒有關係，因為這些不同的 network 之間的參數
80:14
是 shared，也就是說
80:16
這一個 network 的這一個參數
80:19
就是這個 network 的這個參數
80:21
就是它的這個參數，這 4 個參數其實是同一個參數
80:25
所以，雖然說一個 network
80:28
的 structure，它只用一個 batch train
80:31
但是一個 weight，它可能用好多個 batch 來 train
80:34
比如說，這個 weight，它在這 4 個 batch
80:37
裡面，在這 4 個 batch 做 dropout 的時候
80:40
都沒有把這個 weight 丟掉
80:42
那這個 weight，就是拿這 4 個 batch 合起來 train 的結果
80:47
所以，當你做 dropout 的時候
80:49
你就是 train 了一大把的 network structure
80:54
理論上，每一次 update 參數的時候
80:57
你都 train 了一個 network 出來
80:59
那 testing 的時候呢？
81:00
按照 ensemble 這個方法的邏輯應該就是
81:03
你把那一大把的 network 通通拿出來
81:06
然後，你把你的 testing data 丟到那一把
81:09
network 裡面去
81:11
每一個 network 都給你吐一個結果
81:13
然後，把所有的結果平均起來
81:15
就是最終的結果
81:16
但是，在實作上你沒辦法這麼做，因為
81:19
這一把 network 實在太多了
81:21
這一把 network 實在太多了
81:23
你沒有辦法把它都通通都拿出來
81:25
你沒有辦法每一個都丟一個 input 進去
81:28
去看看它 output 是什麼，再平均起來
81:29
這樣運算量太大
81:31
所以，dropout 最神奇的地方是它告訴你說
81:35
當你把一個完整的 network 不做 dropout
81:38
但是，把它的 weight 乘上 (1 - p%)
81:42
然後，你把這個東西，把你的 training data 丟進去
81:47
然後，得到它的 output 的時候
81:49
神奇的就是
81:51
這個 ensemble 的結果
81:53
跟這一個，把 weight 乘上 (1 - p%) 的結果
81:56
是可以 approximate 的
81:58
是可以 approximate 的
81:59
那你可能會想說，何以見得呢？
82:04
我們來舉一個例子
82:08
我們來 train 一個很簡單的 network，它就只有一個 neuron
82:12
它的 activation function 是 linear 的
82:15
我這邊就不考慮 bias
82:18
我們這邊有一個 neuron
82:21
然後，它的 input 是 x1, x2
82:24
然後，x1, x2 分別乘上
82:27
經過 training 以後
82:28
經過 dropout training 以後
82:30
你算出來的 weight 是 w1, w2
82:31
所以，它的 output 就是 w1*x1 + w2*x2
82:35
這個 neuron 沒有 activation function
82:39
或 activation function 是 linear 的
82:41
如果我們今天要做 ensemble 的話
82:44
theoretically 就是這麼做，對不對
82:48
每一個 neuron，我們做 dropout 的時候
82:51
你不會 drop 那個 output 的 neuron
82:53
你只會 drop hidden layer 跟 input 的 neuron
82:56
那這邊每一個 neuron，它可以
82:59
它可以被 drop，或不被 drop
83:02
對不對，所以我們總共有 4 種 structure
83:04
一個是通通沒被 drop
83:07
一個是 drop x1、一個是 drop x2, 一個是 x1, x2 都被 drop 掉
83:11
那你最後得到的 output 呢，這個 network
83:14
假設你 input x1, x2，這個 network 給我們的
83:17
就是 w1*x1 + w2*x2
83:19
同樣的 input，但是 x1 被 drop 掉
83:22
你得到的 output 是 w2*x2，這邊是 w1*x1
83:24
這邊給我們的 output 是 0
83:26
我們要做 ensemble
83:28
所以你要把這 4 個 network 它的 output 通通都 average 起來
83:31
通通都 average 起來，那你 average 起來的結果
83:33
是不是就是，這邊有 4 個值嘛
83:37
然後，w1*x1 出現兩次
83:39
w2*x2 出現兩次
83:40
所以，得到的結果是 1/2* w1*x1 + 1/2*w2*x2，對不對
83:45
那我們現在做的事情是把
83:48
這兩個 weight 都乘 1/2
83:52
我可以把 w1*1/2, w2*1/2
83:55
同樣 input x1, x2
83:57
那得到的 output 也同樣是 1/2*w1*x1 + 1/2*w2*x2
84:03
所以，這邊想要呈現的是說，在這個最簡單的 case 裡面
84:09
ensemble 這件事情
84:11
用不同的 network structure 做 ensemble 這件事情
84:14
跟我們把 weight multiply 一個值
84:19
而不做 ensemble 所得到的 output
84:22
其實是一樣的
84:23
那你可能會說，這個例子這麼簡單
84:27
所以，這個例子上會 work，我想也是很直覺的阿
84:30
大概小學生都知道說，這個是 equivalent
84:32
但是，比如說，這邊是 sigmoid function
84:36
或是它是很多個 layer，它會 work 嗎？
84:38
結論就是不會 equivalent
84:41
就是不會 equivalent
84:43
只有是 linear 的 network
84:45
ensemble 才會等於 multiply 一個 weight
84:47
左邊跟右邊要相等的前提是你的 network 要是 linear 的
84:51
但是，network 不是 linear 的阿
84:53
所以，他們其實不 equivalent
84:54
這個就是 dropout 最後一個很神奇的地方
84:58
雖然不 equivalent，但是最後結果還是會 work 這樣
85:01
所以，根據這個結論，有人有一個想法是說
85:07
既然 dropout 在 linear 的時候
85:10
linear 的 network 上，ensemble 才會
85:13
等於前一個 weight
85:15
所以，今天如果我的 network 很接近 linear 的話
85:19
應該 dropout performance 會比較好，比如說
85:23
怎麼做 network 會比較接近 linear，比如說你用 ReLU
85:26
比如說，你用 Maxout network
85:28
他們是很接近 linear 的
85:29
相對於 sigmoid，它們是比較接近 linear 的
85:32
所以 dropout 確實在用 ReLU 或 Maxout network 的時候
85:36
它的 performance 是確實比較好的
85:39
比如說，你去看 Maxout network 的 paper 的話
85:42
它裡面也有 point 這一點，它的
85:45
Maxout 跟 dropout 加起來的記錄量
85:47
是比 sigmoid function 還要大的，那這個是
85:50
作者相當自豪的一點
85:53
這邊我要講的其實
85:56
就是這樣啦
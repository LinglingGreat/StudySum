---
title: LatticeLSTM
created: 2022-08-03
tags: [论文, NER]

---


论文名称：《Chinese NER Using Lattice LSTM》

论文链接：https://arxiv.org/pdf/1805.02023.pdf

代码地址：https:// github.com/jiesutd/LatticeLSTM

## 1. 基于序列标注的实体识别所存在的问题

### 1.1  经典LSTM-CRF模型

**实体识别**通常被当作**序列标注任务**来做，**序列标注模型**需要对**实体边界**和**实体类别**进行预测，从而识别和提取出相应的命名实体。在**BERT**出现以前，实体识别的**SOTA模型**是**LSTM+CRF**，模型本身很简单：

-   首先利用**嵌入方法**将句子中的每个token转化为向量再输入**LSTM**（或**BiLSTM**）；
    
-   然后使用**LSTM**对输入的信息进行编码；
    
-   最后利用**CRF**对**LSTM**的输出结果进行序列标注。

**LSTM+CRF** 用在**中文NER**上，又可进一步分为两种：若token是词，那么模型就属于**Word-based model**；若token是字，那么模型就属于**Character-based Model**。

( **注**：**BERT+LSTM+CRF**主要是将嵌入方法从**Word2vec**换成了**BERT**。)

### 1.2  误差传播与歧义问题

-   **Word-based model存在误差传递问题**
    

**Word-based model** 做实体识别需要先分词，然后再对词序列进行实体识别即**词序列标注**。**词汇的边界决定了实体的边界，因此一旦出现分词错误就会影响实体边界的判定**。比如，利用分词工具，“东莞台协” 和 ”会长“ 被拆分成了“东莞”、“台”、”协会长“，最终导致 ”东莞台“ 被识别为了**GPE**。换句话说，**Word-based model** 具有和其他两阶段模型同样的误差传递问题。

-   **Character-based model存在歧义问题**
    

既然分词会有问题，那就不分词。**Character-based model** 直接在字的粒度上进行实体识别即**字序列标注**。**许多研究工作表明，在中文NER上基于字的方法优于基于词的方法**。但是，相比词**单字不具备完整语义**。没有利用句子里的词的信息，**难以应对歧义问题**，识别结果可能差强人意。如，“会” 字本来应该和 “长” 一起组成 “会长” ，但是最终模型却将 “会” 与 “东莞台协” 视为一个语块儿，并将 “东莞台协会” 预测为ORG。

### 1.3 思考

既然 **Character-based model**、**Word-based model** 各有优缺点，那是否可以结合二者进行互补呢？换句话说，我们在**Character-based model里加入词信息**，这样是不是就可以既利用了词信息，又不会因为分词错误影响识别结果呢？实际上，**Lattice LSTM**正是这样做的。接下来我们一起跟随文章的后续内容来学习**Lattice LSTM**。

## 2. 模型细节

这一节我们首先会介绍最简单的词信息利用方法，然后再对**Lattice LSTM**进行详细介绍。

### 2.1 简单直接的拼接法

最容易想到同时也是最简单的词信息利用方法就是**直接拼接词表征与字向量**或者**直接拼接词表征与LSTM的输出**。16年的论文《A Convolution BiLSTM Neural Network Model for Chinese Event Extraction》[1]就采用了这样的方法构建了**中文事件抽取模型**，其模型结构如下图所示：

![](img/Pasted%20image%2020220803142322.png)

当然这里词表征可通过**Word2Vec**、**Glove**等词向量模型获得。也可以如16年的那篇事件抽取论文一样利用**CNN**进一步卷积获得更上层的 **Local Context features**，再将其拼接到模型中：

![](img/Pasted%20image%2020220803142408.png)

当然这不是本文的重点，我们关注的是 **Lattice LSTM**是如何引入词信息的。

### 2.2  Lattice 与潜在词

![](img/Pasted%20image%2020220803142425.png)

**Lattice LSTM** 模型结构如上图右侧所示。在正式开始介绍 **Lattice LSTM** 前，我们先来看看上图左半部分。

#### (1) **Lattice LSTM 名字来由**

我们可以发现在上图左侧所示网络中，除主干部分 **基于字的LSTM** 外，还连接了许多「格子」，每个「格子」里各含有一个潜在的词，这些潜在词所含有的信息将会与主干**LSTM**中相应的 **Cell** 融合，看起来像一个「网格（**Lattice**）」。所以论文模型的名字就叫做 **Lattice LSTM**，也就是有**网格结构的LSTM模型**。

#### (2) **词典匹配获得潜在词**

网格中的这些潜在词是通过**匹配输入文本与词典**获得的。比如通过匹配词典， “南京市长江大桥”一句中就有“南京”、“市长”，“南京市”，“长江”，“大桥“，“长江大桥”等词。

#### (3) **潜在词的影响**

首先，“南京市长江大桥” 一句的正确结果应当是 “南京市-地点”、“长江大桥-地点”。如果我们直接利用 **Character-based model** 来进行实体识别，可能获得的结果是：“南京-地点”、“市长-职务”、“江大桥-人名”。现在利用词典信息获得了文本句的潜在词：“南京”、“市长”，“南京市”，“长江”，“大桥“，“长江大桥” 等潜在词。其中，“长江”、“大桥” 与 “长江大桥” 等词信息的引入**有利于模型**，可以帮助模型避免犯 “江大桥-人名” 这样的错误；而 “市长” 这个词的引入却可能会带来歧义从而**误导模型**，导致 “南京-地点”，“市长-职务” 这样的错误。

换句话说，通过词典引入的词信息有的具有正向作用，有的则不然。当然，人为去筛除对模型不利的词是不可能的，所以我们希望**把潜在词通通都丢给模型，让模型自己去选择有正向作用的词，从而避免歧义**。**Lattice LSTM** 正是这么做的：**它在Character-based LSTM+CRF的基础上，将潜在词汇信息融合进去，从而使得模型在获得字信息的同时，也可以有效地利用词的先验信息**。

### 2.3 Lattice LSTM 模型细节

![](img/Pasted%20image%2020220803142645.png)

如上图所示，**Lattice LSTM**模型的主干部分是**基于字的LSTM-CRF**（**Character-based LSTM+CRF**）：

-   **若当前输入的字在词典中不存在任何以它结尾的词时**：**主干上Cell之间的传递就和正常的LSTM一样。也就是说，这个时候Lattice LSTM退化成了基本LSTM**。
    
-   **若当前输入的字在词典中存在以它结尾的词时**：**需要通过红色Cell （见2.2节图右侧）引入相关的潜在词信息，然后与主干上基于字的LSTM中相应的Cell进行融合**。
    

接下来，我们先简单展示下 **LSTM的基本单元**，再介绍 **红色Cell**，最后再介绍**信息融合**部分。

#### 2.3.1 LSTM 单元

![](img/Pasted%20image%2020220803143150.png)

上图左侧展示了**一个LSTM 单元(Cell)的内部结构**，右侧展示了**Cell的计算过程**。在每个**Cell** 中都有三个门控，即**输入门**、**遗忘门**和**输出门**。如上图右侧计算公式所示，这三个门实际上是0～1的小数，其值是根据当前时刻输入$x^c_j$  和前一时刻 **Cell** 的输出的**hidden state** $h^c_{j-1}$  计算得到的：

![](img/Pasted%20image%2020220803143227.png)

**纯粹的基于字的LSTM可以完全基于上述计算过程去计算**，而**Lattice LSTM**则有所不同。

#### 2.3.2 **红色Cell**

前面我们提过「如果当前字在词典中存在以它结尾的词时，需要通过红色Cell引入相关潜在词信息，与主干上基于字的LSTM中相应Cell进行融合」。以下图中 "市" 字为例，**句子中潜在的以它结尾的词**有："南京市"。所以，对于"市"字对应的**Cell** 而言，还需要考虑 “南京市” 这个词的信息。

**红色Cell**的内部结构与主干上**LSTM**的**Cell**很类似。接下来，我们具体来看下 **红色Cell** 内部计算过程。

#### (1)  红色Cell 的输入

![](img/Pasted%20image%2020220803143359.png)

与上图左侧**LSTM**的**Cell**对比，上图右侧 **红色Cell** 有两种类型的输入：

-   **潜在词的首字**对应的**LSTM**单元输出的**Hidden State** $c^c_b$  以及**Cell State** $h^c_b$
    
-   **潜在词的词向量** $x^w_{b,e}$。
    

#### (2) 红色Cell 的输出

![](img/Pasted%20image%2020220803143508.png)

可以发现，因为**序列标记是在字级别**，所以与左侧 **LSTM** 的 **Cell** 相比，**红色Cell** **没有输出门**，即它不输出**hidden state**。

![](img/Pasted%20image%2020220803143603.png)

### 2.3.3 信息融合

#### （1）潜在词的输入门

![](img/Pasted%20image%2020220803143641.png)

#### (2) 加权融合

前面我们举的例子中都只有一个潜在词。但实际上，对部分字来说可能会在词典中匹配上很多词，例如 “桥” 这个字就可以在词典中匹配出 “大桥” 和 “长江大桥” 。**为了将这些潜在词与字信息融合，Lattice LSTM做了一个类似Attention的操作**：

![](img/Pasted%20image%2020220803143711.png)

简单地说，就是**当前字相应的输入门和所有以当前字为尾字的候选词的输入门做归一计算出权重，然后利用计算出的权重进行向量加权融合**。

![](img/Pasted%20image%2020220803143743.png)

## 3. 实验

论文在**Onto Notes**、**MSRA**、**微博NER**、**简历这4个数据集**上进行了实验。从实验结果可以看出 **Lattice LSTM** 比其他对比方法有一定的提升。

![](img/Pasted%20image%2020220803143808.png)


## 参考资料

https://mp.weixin.qq.com/s/dyrpkBfh2L5fj3VNOsi02A



一般来说，按照推荐任务的不同，最常用的推荐质量度量方法可以划分为三类：（1）对预测的评分进行评估，适用于评分预测任务。（2）对预测的item集合进行评估，适用于Top-N推荐任务。（3）按排名列表对推荐效果加权进行评估，既可以适用于评分预测任务也可以用于Top-N推荐任务。

这三类度量方法对应的具体评价指标分别为：

（a）评分预测指标：如准确度指标：平均绝对误差（MAE）、均方误差根（RMSE）、标准化平均误差（NMAE）；以及覆盖率（Coverage）

（b）集合推荐指标：如精密度(Precision)、召回(Recall)、 ROC和AUC

（c）排名推荐指标：如half-life和discounted cumulative gain等


在推荐列表中，通常没有一个确定的阈值来把预测结果直接判定为正样本或负样本，而是采用 Top N  排序结果的精确率（Precision@N）和召回率（Recall@N）来衡量排序模型的性能。具体操作，就是认为模型排序的前 N 个结果就是模型判定的正样本，然后分别计算 Precision@N 和 Recall@N。

精确率和召回率可以反应模型在 Top n 个排序结果上的表现，但我们要知道，在真正的推荐问题中，n 的值是变化的，因为用户可能会通过不断的翻页、下滑来拉取更多的推荐结果，这就需要有更高阶的评估指标来衡量模型在不同数量推荐结果上的综合性能。比如P-R 曲线、ROC 曲线、平均精度均值

平均精度均值 mAP（mAP，mean average precision）这个高阶指标，它除了在推荐系统中比较常用，在信息检索领域也很常用。mAP 其实是对平均精度（AP，average precision）的再次平均

除了这些评估指标，还有很多其他的推荐系统指标，比如归一化折扣累计收益（Normalized Discounted Cumulative Gain,NDCG）、覆盖率（Coverage）、多样性（Diversity）等等。

比如，在对推荐模型的离线评估中，大家默认的权威指标是 ROC 曲线的 AUC。但 AUC 评估的是整体样本的 ROC 曲线，所以我们往往需要补充分析 mAP，或者对 ROC 曲线进行一些改进，我们可以先绘制分用户的 ROC，再进行用户 AUC 的平均等等。

再比如，在评估 CTR 模型效果的时候，我们可以采用准确率来进行初步的衡量，但我们很有可能会发现，不管什么模型，准确率都在 95% 以上。仔细查看数据我们会发现，由于现在电商点击率、视频点击率往往都在 1%-10% 之间。也就是说，90% 以上都是负样本，因此准确率这个指标就不能够精确地反应模型的效果了。这时，我们就需要加入精确率和召回率指标进行更精确的衡量，比如我们采用了 Precision@20 和 Recall@20 这两个评估指标，但它终究只衡量了前 20 个结果的精确率和召回率。

如果我们要想看到更全面的指标，就要多看看 Precision@50 和 Recall@50，Precision@100 和 Recall@100，甚至逐渐过渡到 P-R 曲线。

总的来说，评估指标的选择不是唯一的，而是一个动态深入，跟你评测的“深度”紧密相关的过程。而且，在真正的离线实验中，虽然我们要通过不同角度评估模型，但也没必要陷入“完美主义”和“实验室思维”的误区，选择过多指标评估模型，更没有必要为了专门优化某个指标浪费过多时间。离线评估的目的在于快速定位问题，快速排除不可行的思路，为线上评估找到“靠谱”的候选者。因此，我们根据业务场景选择 2~4 个有代表性的离线指标，进行高效率的离线实验才是离线评估正确的“打开方式”。

![img](img/e1a0566473b367633f0d18346608661a.jpeg)

## Hit Rate (HR)

Hit Rate 对应的是leave one out的测试场景。例如用户u点击过n个物品，则取前n-1个物品作为训练集，最后1个用于测试。

模型在测试中将给出一个长度为k的推荐列表，当列表中有测试集中对应的物品时，则说明预测成功。

最终平均所有用户的测试结果计算出的值为 Hit Rate@k，一般来说在k越小的时候，模型取得较高hr的难度越大。

说法一：公式HR= `#hits` / `#users`，其中`#users`是用户总数，而`#hits`是测试集中的item出现在Top-N推荐列表中的用户数量。

说法二：分母是所有的测试集合，分子是每个用户前K个中属于测试集合的个数的总和。

举个简单的例子，三个用户在测试集中的商品个数分别是10，12，8，模型得到的top-10推荐列表中，分别有6个，5个，4个在测试集中，那么此时HR的值是(6+5+4)/(10+12+8) = 0.5。

两个说法都OK，不同论文的定义不一样。

## NDCG
折扣累计收益(discounted cumulative gain， DCG)的主要思想是用户喜欢的商品被排在推荐列表前面比排在后面会更大程度上增加用户体验.

![](img/Pasted%20image%2020221114214729.png)

由于在用户与用户之间，DCGs没有直接的可比性，所以我们要对它们进行归一化处理。最糟糕的情况是，当使用非负相关评分时DCG为0。为了得到最好的，我们把测试集中所有的条目置放在理想的次序下，采取的是前K项并计算它们的DCG。然后将原DCG除以理想状态下的DCG就可以得到**归一化折扣累计收益（Normalized Discounted Cumulative Gain，NDCG)**，它是一个0到1之间的数。NDCG@k = DCG / iDCG


## QA

P-R 曲线和 ROC 曲线，你觉得它们的优缺点分别是什么呢？在正负样本分布极不均衡的情况下，你觉得哪个曲线的表现会更稳定、更权威一点？

ROC曲线，FPR=FP/N,TPR=TP/P，当我们将负样本复制10倍时，TPR显然不会变，FPR是负样本中被预测为正样本的比例，这其实也是不变的，那整个ROC曲线也就没有变。PR曲线，精确率P=TP/(TP+FP)，TP不变，FP增大，而召回率R没有变，显然ROC曲线更稳定一些

```python

# compile the model, set loss function, optimizer and evaluation metrics
model.compile(
    loss='binary_crossentropy',
    optimizer='adam',
    metrics=['accuracy', tf.keras.metrics.AUC(curve='ROC'), tf.keras.metrics.AUC(curve='PR')])
# train the model
model.fit(train_dataset, epochs=5)
# evaluate the model
test_loss, test_accuracy, test_roc_auc, test_pr_auc = model.evaluate(test_dataset)
```

## 参考资料

https://zhuanlan.zhihu.com/p/67287992


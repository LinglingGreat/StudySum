我要講的是幾個 deep learning 的 tip
其實，這一段本來是要在 CNN 之前講的啦
那在 CNN 那段裡面，我們留下了兩個問題
第一個是，在 CNN 裡面
有 max pooling 這樣的架構
但是，max pooling 這樣的架構顯然不能微分阿
你把它放在一個 network 裡面
你在做 Gradient Descent，你在微分它的時候
你到底是怎麼處理？
那第二個問題，是我們剛才看到的
L1 的 Regularization
但是，我們還沒有解釋它是甚麼東西
那這個，我們都會在這份投影片裡面解釋
那本來是要先講這份投影片，再講 CNN 的啦
但是，因為要講作業三的關係
所以，就先講了 CNN
講完這份投影片以後，之前的一些問題就可以被解決
首先，這邊呢，最重要的一個觀念是
Deep learning 的 recipe
如果你在訓練一個 deep learning 2的 network 的時候
你在做 deep learning 的時候
它的流程，應該是甚麼樣子的
那我們都知道說，deep learning 是 3 個 step
define function 、define 你的 function set
define 你的 network 的 structure
決定你的 loss function
接下來，你就可以用 Gradient Descent 去做 optimization
做完這些事情以後
你會得到一個 neural network
得到一個好的 neural network
接下來，你要做甚麼樣的事情呢？
接下來，你要做甚麼樣的事情呢？
其實，你第一件要檢查的事情是
這個 neural network 在你的 training set 上
有沒有得到好的結果
不是 testing set 哦，你要先檢查這個 neural network
在你的 training set 上，有沒有得到好的結果
如果，沒有的話
你就回頭去看看說，在這 3 個 step
這裡面，是不是哪邊出了問題
你可以做甚麼樣的修改，讓你在 training set 上
能夠得到好的結果
那這邊這個先檢查 training set 的 performance
其實是 deep learning 一個非常 unique 的地方
如果你想想看其他的方法，比如說
你今天如果用的是
雖然這些方法我們都還沒講過，但你或多或少都知道
比如說，k nearest neighbor
或者是 decision tree
其實像 k nearest neighbor 或 decision tree 這種方法
你做完以後，你其實會不太想檢查你 training set 的結果
因為，在 training set 上的 performance 正確率就是 100
你做完 decision tree 或做完 k nearest neighbor
得到正確率就是 100，沒有甚麼好檢查的
所以，有人說 deep learning
看這個 model 裡面這麼多參數
感覺一臉很容易 overfitting 的樣子
我跟你講，這個 deep learning 的方法
它才不容易 overfitting
我們說的 overfitting 就是在 training set 上
performance 很好，但 testing set 上 performance
沒有那麼好嘛
像這 k nearest neighbor, decision tree，它們一做下去
在 training set 上正確率都是 100
在 training set 上正確率都是 100
這個才是非常容易 overfitting
而對 deep learning 來說
overfitting 往往不是你會最
不是說，deep learning 沒有 overfitting 的問題
而是說，在 deep learning 裡面，overfitting
不是第一個你會遇到的問題
你第一個會遇到的問題，是你在 training 的時候
它並不是像 k nearest neighbor 這種方法一樣
你一 train 就可以得到非常好的正確率
它有可能在 training set 上
根本沒有辦法給你一個好的正確率
所以，這個時候你要回頭去檢查說
在前面的 step 裡面
要做什麼樣的修改
好讓你在 training set 上可以得到好的正確率
假設現在，幸運的是你已經
在 training set 上得到好的 performance 了
你要用 deep learning 在 training set 上
得到 100% 的正確率，是沒那麼容易的
但可能你在 MNIST 上得到一個 99.8% 的正確率
接下來，你就把你的 network apply 到 testing set 上
testing set 上的 performance 才是我們
最後真正關心的 performance
那你現在把你的結果 apply 到 testing set 上
那在 testing set 上 performance 怎麼樣呢？
如果現在得到的結果是 NO 的話
那就是 Overfitting
這個情況才是 Overfitting
你在 training set 上得到好的結果
但是，在 testing set 上得到的是不好的結果
這個時候，這個情況呢
才叫做 Overfitting
那你要回過頭去，做某一些事情
然後，試著去解決 overfitting 這個 problem
但有時候，你加了新的 technique
去想要 overcome overfitting 這個 problem 的時候
你其實反而會讓 training set 上的結果變壞
所以，你在做這一步的修改以後
你要先回頭去檢查說，training set 上的結果
是怎麼樣的，如果 training set 上的結果變壞的話
你要從頭呢
去對你的 network training 的 process 做一些調整
那如果你同時在 training set 還有你手上的 testing set
都得到好的結果的話
最後，你就可以把你的系統真正用在 application 上面
你就成功了
那這邊有一個重點就是
不要看到所有不好的 performance
就說是 overfitting
舉例來說，這個是文獻上的圖
但我在現實生活中，也常常看到這樣子的狀況
在 testing set 上面
這個是 testing data 的結果
橫坐標，是 model 參數 update 的次數
所以，你做 Gradient Descent 的時候
你 update 幾次參數
縱座標，是 error rate，所以越低越好
那如果我們現在表示一個 20 層的 network
它是黃線
這個 56 層的 neural network，它是紅線
那你會發現說，這個 56 層的 network
它的 error rate 比較高，它的 performance 比較差
20 層的 neural network，它的 performance 是比較好的
那有些人看到這個圖，就會馬上得到一個結論
說 56 層太多了，參數太多了
56 層果然沒有必要，這個是 overfitting
但是，真的是這樣子嗎？
你在說，現在得到的結果是 overfitting 之前
你要先檢查一下你在 training set 上的結果
對某些方法來說，你不用檢查這件事
比如說 k nearest neighbor 或 decision tree
你不用檢查這件事
但是，對 neural network 來說
你是需要檢查這件事情的
為甚麼呢？因為有可能你在 training set 上得到的結果
是這個樣子
是這個樣子的
橫軸一樣是參數 update 的次數
縱軸是 error rate
如果我們比較 20 層的 neural network 跟
56 層的 neural network 的話
你會發現說
在 training set 上 ，20 層的 neural network
它的 performance 本來就比 56 層好
在 training set 上 ，56 層的 neural network
它的 performance 是比較差的
是比較差的
那為甚麼會這樣子呢？
你想想看你在做 neural network training 的時候
有太多太多的問題
可以讓你的 training 的結果是不好的
比如說，我們有 local minimum 的問題
有 saddle point 的問題，有 plateau 的問題
有種種的問題
所以，有可能這個 56 層的 neural network
你 train 的時候，它就卡在一個 local minimum 的地方
所以，它得到了一個差的參數
所以，這個並不是 overfitting
是在 training 的時候，就沒有 train 好
那有人會說，這個叫做 underfitting
我覺得這個可能不叫做 underfitting
但是這個只是名詞定義的問題啦，你要怎麼說都行
但是，在我的心裡面，underfitting 的意思是說
這個 model 的 complexity
這個 model 的參數不夠多，所以
它的能力不足以解出這個問題
但對這個 56 層的 neural network 來說
雖然它得到比較差的 performance
但假如這個 56 層的 network
它其實是在這個 20 層的 network 後面
後面再另外堆 36 層的 network
那它的參數，其實是比 20 層的 network 還多的
理論上，20 層的 network 可以做到的事情
56 層的 network 一定可以做到
你前面已經有那 20 層
你前面那 20 層就做跟 20 層 network 一樣的事情
後面那 36 層就甚麼事都不幹，就都是 identity 就好了
你明明可以做到跟 20 層一樣的事情
你為甚麼做不到呢？
但是，因為會有很多的問題就是
讓你沒有辦法做到
所以，這個 56 層的 network 呢
它比 20 層差，並不是因為它能力不夠
它只要前 20 層都跟它一樣，後面都是 identity
明明就可以跟 20 層一樣好
但它卻沒有得到這樣的結果
所以，它能力是夠的，所以我覺得這不是 underfitting
它這個就是沒有 train 好這樣子
那我還不知道有沒有什麼名詞，專門指稱這個問題
所以，它其實就是像這個小智的噴火龍一樣
它等級是夠的，但它就不想要打這樣子
所以，在 deep learning 的文獻上
如果，當你讀到一個方法的時候
你永遠要想一下說，這個方法
它是要解什麼樣的問題
因為在 deep learning 裡面，有兩個問題
一個是 training set 上的 performance 不好
一個是 testing set 上的 performance 不好
當只有一個方法 propose 的時候
它往往就是針對這兩個問題的其中一個
來做處理
舉例來說，你等一下能會聽到一個方法叫做 dropout
dropout 或許大家或多或少都會知道，它是一個
很有 deep learning 特色，很潮的一個方法
那很多人就會說，哦，這麼潮的方法，所以
我今天只要看到 performance 不好，我就很快 dropout
但是，你只要仔細想一下 dropout 是甚麼時候用的
dropout 是你在 testing 的結果不好的時候
你才會 apply dropout
你的 testing data 結果好的時候
你是不會 apply dropout
就是說，dropout 是
你在 testing 結果不好的時候，才 apply dropout
如果你今天的問題是你 training 的結果不好
你 apply dropout，你只會越 train 越差而已
所以，不同的方法，對治甚麼樣不同的症狀
你是必須要在心裡想清楚的
那我們這邊就休息 10 分鐘，等一下再繼續講，謝謝
臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 http://aintu.tw
各位同學大家好
我們來上課吧
來上課吧
等一下呢，我們剛才講說
這個在 deep learning 的 recipe 裡面
在 train deep learning 的時候有兩個問題
所以，等一下呢，我們就是要
這兩個問題，分開來討論
看看當你遇到這兩個問題的時候呢
有甚麼樣解決的方法
首先，如果你今天的
training 在 training 的結果上不好的時候
你可能可以看看說
是不是你在做 network 架構設計的時候
是不是設計不好
舉例來說，你可能用的 activation function
是比較不好的 activation function
是對 training 比較不利的 activation function
你可能會換一些新的 activation function
它可以給你比較好的結果
那我們知道說
在 1980 年代的時候
比較常用的 activation function
是一個 sigmoid function
那我們之前有稍微試著 reason 一下
為甚麼要用 sigmoid function
今天如果我們用的是 sigmoid function 的時候
在過去，其實你會發現說
deeper 並不一定 imply better
這個是在 MNIST 上面的結果啦
在手寫數字辨識上的結果
當你 layer 越來越多的時候呢
你的 accuracy，一開始持平，後來就掉下去了
在你的 layer 是 9 層、10 層的時候，整個結果就崩潰啦
那有人看到這個圖，就會覺得說
9 層、10 層參數太多了，overfitting
這個，不是 overfitting
為甚麼呢？首先呢，我們說你要
檢查你現在 performance 不好是不是來自於 overfitting
你要看你 training set 的結果嘛，對不對？
那這個線，是 training set 的結果
所以，這個不是 overfitting
這個是 training 的時候，就 train 壞掉了
不信的話，我們實際來用 Keras 實做一下
一個原因是這樣子
一個原因是
這個原因叫做 Vanishing 的 Gradient
這個原因是這樣，當你把 network 疊得很深的時候
在 input 的幾個 layer
在最靠近 input 的地方呢
你的這個 Gradient
你的這些參數，對最後 loss function 的微分
會是很小的
而在比較靠近 output 的地方呢
它的微分值，會是很大的
因此，當你設定同樣的 learning rate 的時候
你會發現說，靠近 input 的地方
它參數的 update，是很慢的
靠近 output 的地方，它參數的 update 是很快的
所以，你會發現說呢
在 input 幾乎還是 random 的時候
output 就已經 converge 了
在 input layer
在靠近 input 地方的這些參數
它還是 random 的時候
output 的地方，就已經根據這些 random 的
random 的結果呢
找到了一個 local minimum
然後，它就 converge 了
這個時候，你會發現說
你這個參數的 loss 下降的速度呢
變得很慢，你就覺得說
卡在 local minimum 什麼之類的
就傷心地把程式停掉了
這個時候，你得到的結果是很差的，為什麼呢？
因為這個 converge，是幾乎 base on random 的參數
那幾乎 base on random 的 output
然後去 converge，所以得到的結果，是很差的
那為甚麼會有這個現象發生呢？
為甚麼會有這個現象發生呢？
如果你自己把 Backpropagation 的式子寫出來的話
你可以很輕易地發現說，用 sigmoid function
會導致這件事情的發生
但是，我們今天不看 Backpropagation 的式子
我們其實從直覺上來想
你也可以了解為什麼這件事情發生
怎麼用直覺來想
一個參數的 Gradient 的值應該是多少呢？
我們知道說
某一個參數 w 對 total cost C 的偏微分阿
意思就是說，它的直覺的意思就是說
當我今天把某一個參數做小小的變化的時後
它對這個 cost 的影響是怎麼樣
了解嗎？就是我們可以把一個參數
做小小的變化，然後觀察它對 cost 的變化
以此來決定說，這個參數
它的 Gradient 的值有多大
所以，怎麼做呢？我們就把
第一個 layer 裡面的某一個參數
加上 △w，看看對 network 的 output
和它的 target 之間的 loss
有甚麼樣的影響
那你會發現說
如果我今天這個 △w 很大
通過 sigmoid function 的時候
這個 output 呢，是會變小的
也就是說，改變了某一個參數的 weight
對某一個 neuron 的 output 的值會有影響
但是，這個影響是會衰減的
為甚麼這麼說呢？
因為，假設你用的是 sigmoid function
我們知道 sigmoid function 形狀就長這樣
那 sigmoid function 它會把
負無窮大到正無窮大之間的值
都硬壓到 0~1 之間
也就是說，如果你有很大的 input 的變化
通過 sigmoid function 以後
它 output 的變化，會是很小的
所以，就算今天你這個 △w 有很大的變化
造成 sigmoid function 的 input 有很大的變化
對 sigmoid function 來說，它的 output 的變化
是會衰減的
而每通過一次 sigmoid function
變化就衰減一次
所以，當你的 network 越深
它衰減的次數就越多
直到最後，它對 output 的影響是非常小的
也就是說，你在 input 的地方
改一下你的參數
對 output 的地方
它最後 output 的變化，其實是很小的
因此，最後對 cost 的影響也很小
因此，就造成說，在靠近 input 的那些 weight
它對它這個 Gradient 的值是小的
那怎麼解決這個問題呢？
有人就說
原來比較早年的做法是去 train RBM
去做這個 layer-wise 的 training
也就是說，你先認好一個 layer
就因為我們現在說，如果你把所有的這個
network 兜起來，那你做 Backpropagation 的時候
第一個 layer 你幾乎沒有辦法被挑到嘛
所以，用 RBM 做 training 的時候，它的精神就是
我先把一個 layer train 好
再 train 第二個，再 train 第三個
最後，你在做 Backpropagation 的時候
雖然說，第一個 layer 幾乎沒有被 train 到
那無所謂
一開始在 pre-train 的時候，就把它 pre-train 好了
所以，這就是 RBM 為什麼做 pre-train 可能有用的原因
那後來有人說，其實
後來有人發現說
其實，我記得 Hinton 跟 Pengel 都
幾乎在同樣的時間，不約而同地提出同樣的想法
改一下 activation function
可能就可以 handle 這個問題了
所以，現在比較常用的 activation function
叫做 Rectified Linear Unit，它的縮寫是 ReLU
會常看到有人叫它 ReLU
那這個 activation function 它長這樣子
這個 z 是 activation function 的 input
a 是 activation function 的 output
如果今天 activation function 的 input 大於 0 的時候
input = output
如果 activation function 的 input 小於 0 的時候
output 就是 0
那選擇這樣的 activation function 有甚麼好處呢？
有以下幾個理由，第一個理由是
它比較快，跟 sigmoid function 比起來
它的運算是快很多的
sigmoid function 裡面還有 exponential
那個是很慢的，那如果你是
用這個方法的話，它是快得多的
如果你看這個
我記得是 Pengel 寫得原始的 paper 的話呢
裡面會告訴你說
這個 activation function 的想法其實
是有一些生命上的理由的
那它把這樣的 activation 跟一些生物上的觀察呢
結合在一起
那 Hinton 有說過說
ReLU 這樣的 activation function
其實，等同於是無窮多的 sigmoid function
疊加的結果
無窮多的 sigmoid function
它們的 bias 都不一樣，疊加的結果會變成 ReLU
的 activation function
但它最重要的理由是
它可以 handle Vanishing gradient 的這個問題
它怎麼 handle Vanishing gradient 這個問題呢？
我們來看一下，這個是一個 ReLU 的 neural network
這是一個 ReLU 的 neural network
它裡面的每一個 activation function
都是 ReLU 的 activation function
那我們知道說
ReLU 的 activation function
它作用在兩個不同的 region
一個 region 是當 activation function 的 input
大於 0 的時候，input = output
另外一個 region 是
activation function 的 input 小於 0
所以，output 就是 0
所以，現在每一個 ReLU 的 activation function
它作用在兩個不同的 region
一個 region 是
每一個 activation function 的
一個可能是 activation function 的 output 就是 0
另外一個可能是
activation function 的 input = output
當 input = output 的時候
其實，這個 activation function 就是 linear 的
就是 linear 的
那對那些 output 是 0 的 neuron 來說
它其實對整個 network 是一點影響都沒有的阿
它 output 是 0，所以
它根本就不會影響最後 output 的值
所以，假如有一個 neuron 它 output 是 0 的話
你根本可以把它從 network 裡面整個拿掉
把它從 network 裡面整個拿掉
當你把這些 output 是 0 的 network 拿掉
剩下的 neuron，就都是 input = output，是 linear 的時候
你整個 network，不就是一個很瘦長的
linear network 嗎？
你整個 network，其實就變成是 linear 的 network
那這個時候
我們剛才說
我們的 Gradient 會遞減
是因為通過 sigmoid function 的關係
sigmoid function 會把比較大的 input
變成比較小的 output
但是，如果你是 linear 的話
input = output
你就不會有那個 activation function 遞減的問題了
講到這邊，有沒有人有問題呢？
講到這邊，我有一個問題
現在如果我用 ReLU 的時候，整個 network
都變成 linear 的阿
可是，我們要的不是一個 linear 的 network 阿
我們之所以用 deep learning，就是因為我們
不想要我們的 function 是 linear 的
我們希望它是一個 non-linear，一個比較複雜的 function
所以，我們用 deep learning
當我們用 ReLU 的時候
它不就變成一個 linear 的 function 了嗎？
這樣都不會有問題嗎？
這樣不是變得很弱嗎？
其實是這樣子的，這整個 network 呢
整體來說，它還是 non-linear 的
大家聽得懂嗎？
當你的每一個 neuron 做 operation
當每一個 neuron，它 operation 的 region 是一樣的時候
它是 linear 的
但是
也就是說，如果你對 input 做小小的改變
不改變 neuron 的 operation 的 region
它是一個 linear 的 function
但是，如果你對 input 做比較大的改變
你改變了 neuron 的 operation region 的話
它就變成是 non-linear 的
這樣大家可以接受嗎？
好那有另外一個問題
這個也是我常常被問到的問題
這個不能微分阿
這不能微分
這樣你不覺得很苦惱嗎？
我們之前說，我們做 Gradient Descent 的時候
你需要對你的 loss function 做微分
意思就是說
你要對你的 neural network 是可以做微分的
你的 neural network 要是一個可微的 function
ReLU 不可微阿
至少這個點是不可微的
那怎麼辦呢？
其實，實作上你就這個樣子啦
當你的 region 在這個地方的時候
gradient 微分就是 1
region 在這個地方的時候，微分就是 0
反正不可能 input 正好是 0 嘛，就不要管它
結束這樣
那我們來實際試一下
如果我們把 activation function 換成 ReLU 的時候
會得到甚麼樣的結果
比如說，我們就這樣子
把 sigmoid 換成 ReLU
把 sigmoid 換成 ReLU，那我們剛才用 sigmoid 的時候
training 和 testing 的 accuracy 都很差
我們就簡單的把它換成 ReLU
甚麼其他事都沒做
沒換嗎？等我一下
那 ReLU 其實還有種種的變數
那有人覺得說，如果是 ReLU 的時候
如果是原來的 ReLU
它在 input 小於 0 的時候
output 會是 0，這個時候微分是 0
你就沒有辦法 update 你的參數了
所以，我們應該讓
在 input 小於 0 的時候，output 還是有一點點的值
也就是 input 小於 0 的時候，output 是
input 乘上 0.01
這個東西叫做 Leaky ReLU
那這個時候，有人就會問說
為甚麼是 0.01，為甚麼不是 0.07, 0.08 之類的呢？
所以，就有人提出了 Parametric ReLU
他說，在負的這邊呢
(output) a = (input) z*α
α 是一個 network 的參數
它可以透過 training data 被學出來
甚至每一個 neuron 都可以有不同的 α 的值
那又會有人問說，為甚麼一定要是 ReLU 這個樣子呢？
可不可以是別的樣子
所以，後來又有一個更進階的想法， 叫做 Maxout network
那在 Maxout network 裡面呢
你就是讓你的 network 自動學它的 activation function
那因為現在 activation function 是自動學出來的
所以 ReLU 就只是 Maxout network 的一個 special case
Maxout network 它可以學出 ReLU 這樣的 activation function
但是，它也可以是其他的 activation function
用 training data 來決定說
現在的 activation function 應該要長甚麼樣子
Maxout network 長甚麼樣子呢？
假設現在有 input
一個 2 dimension 的 vector，[x1, x2]
然後，我們就把
x1, x2 乘上不同的 weight
變成一個 value, 5
然後，再乘上不同 weight 得到 7
再乘上不同 weight 得到 -1，再乘上不同 weight 得到 1
那本來這些值呢
應該要通過 activation function
不管是 sigmoid function 還是 ReLU
得到另外一個 value
但是，現在在 Maxout network 裡面
現在在 Maxout network 裡面呢
我們做的事情是這樣子
你把這些 value
group 起來，你把這些 value group 起來
哪些 value 應該被 group 起來這件事情是
事先決定的
比如說，現在，這兩個 value 是一組
這兩個 value 是一組
那你在同一個組裡面
選一個值最大的當作 output
比如說，這個組就選 7
這個組就選 1
那這件事情呢
其實就跟 Max Pooling 一樣對不對
只是我們現在不是在 image 上做 Max Pooling
我們是在在一個 layer 上做 Max Pooling
我們把 layer 裡面的
本來要放到 neuron 裡面的
這個 activation function
我們本來要把它放到 neuron 的 activation function
的這個 input 的值 group 起來
然後，只選 max 當作 output
然後，就不用 activation function 了
就不加 activation function
得到的值是 7 跟 1
那你可以想說，這個東西呢
就是一個 neuron
只是它的 output 是一個 vector，而不是一個值
那接下來這個 7 跟 1 呢
就乘上不同的 weight
就得到另外一排不同的值
然後，你一樣把它們做 grouping
你一樣從每個 group 裡面選最大的值
1 跟2 就選 2，4 跟3 就選 4
其實，在實作上
幾個 element 要不要放在同一個 group 裡面，這件事情
是你可以自己決定的
就跟 network structure 一樣，是你自己需要調的參數
所以，你可以不是兩個 element 放一組
你可以是 3 個、4 個、5 個都可以
這個是你自己決定的
我們現在先說，Maxout network
它是有辦法做到跟 ReLU 一模一樣的事情
它可以模仿 ReLU 這個 activation function
怎麼做呢？
我們知道說，假設我們這邊有一個 ReLU 的 neuron
它的 input 就一個 value x
你會把 x 乘上這個 neuron 的 weight, w
再加上 bias, b
然後，通過 activation function, ReLU 得到 a
所以，現在如果我們看 x 跟 a 的關係
是什麼樣子呢？
假設 x 是橫軸
那這個 x 是橫軸
假設 y 軸是這個 z 的話
它就是 w*x + b
z 跟 x 之間的關係是 linear 的
是 linear 的，是這個樣子
那如果你選 a 呢
a 跟 z有甚麼樣的關係呢？
因為現在通過的是 ReLU 的 activation function
所以，如果你今天 z 的值大過 0 的時候
a = z
z 的值小於 0 的時候
a 就是 0
所以，a 跟 x 的關係是這個樣子
在這個地方，a = z；在這個地方，a = 0
所以，我們今天用 ReLU 的 activation function
它 input 和 output，x 和 a 之間的關係是長這樣子
如果我們今天用 Maxout network
用 Maxout network，你把 w
你把 input, x 乘上 weight, w 再加上 bias，得到 z1
你再把 x 乘上另外一組 weight
加上另外一個 bias
得到 z2，那我今天假設說
另外一個 weight 跟另外一個 bias 都是 0，所以 z2 = 0
然後，你做 Max Pooling
你就可以選 z1, z2 其中一個比較大的呢
當作 a
現在，如果我們看 z1 跟 x 之間的關係
我們得到的是藍色這條線
如果我們看 x 跟 z2 之間的關係
我們得到的是水平這條線，因為 z2 總是 0
如果 z2 前面接的 weight 跟 bias 都是 0
z2 總是 0，所以它是紅色的這條線
那我們現在做的是 Maxout
我們是在 z1, z2 裡面選一個大的當作 output a
所以，如果今天 x 是在這個 region 的時候
你的 a 就會等於 z1，是這個 region
如果今天 x 是在這個 region 的時候
你的 a 就會等於比較大的 z2，所以是這個 region
那今天你只要把這個 w 跟這個 b
等於這個 w 跟這個 b
你就可以讓
這個 ReLU 的 input 和 output 的關係
等於這個 Maxout network 的 input 和 output 的關係
所以，由此可知， 就是
ReLU 是 Maxout network 可以做到的事情
只要它設定出正確的參數
但是，Maxout network 它也可以做出
更多的、不同的 activation function
比如說，現在假設這兩個 weight 不是 0，而是 w', b'
那會怎樣呢？
就得到藍色這條線 z1
跟紅色這條線 z2
因為 w', b' 是不一樣的值，所以
它可能是另外一條斜直線，長的是這樣子
接下來，你做 Max Pooling 的時候
你會在 z1, z2 裡面選一個大的
所以，在這個範圍內，你選了
你選了 z1
在這個範圍內，你選了 z2
所以你就得到了一個不一樣的 activation function
而這個 activation function 長甚麼樣子
是由 network 的參數 w, b, w', b' 決定的
所以，這個 activation function 它是一個
Learnable 的 activation function
它是一個可以根據 data 去 generate 出來的 activation function
每一個 neuron 根據不同的 weight
它可以有不同的 activation function
那 ReLU 是這樣子
它可以做出任何的
piecewise 的 linear 的 convex activation function
如果你看一下它的性質，你就不難理解這件事情
那至於這個 piecewise 的 linear function 裡面
有多少個 piece
這決定於你現在把多少個 element 放在一個 group
假如說兩個 element 一個 group
那你可以有長這樣子的 activation function
是 ReLU
你可以有一個 activation function 它的作用就是取
絕對值
假設你是 3 個 element 一個 group
你可以有長這樣子的 activation function
你也可以有長這樣子的 activation function 等等
那接下來我們要面對另外一個問題，就是
這個東西怎麼 train
這個東西怎麼 train
這裡面有個 Max 阿
它不能微分阿
這個東西怎麼 train
這個做法是這樣子的
假設現在這個 z1 跟
假設這邊這兩個值，比較大的是
上面這個值
我們現在把這個 group 裡面比較大的值，用框框框起來
用框框框起來
那比較大的值
就會等於這個 max operation 的 output
就會等於 max operation 的 output
所以，這個值等於這個值，這個值等於這個值
這個值等於這個值，這個值等於這個值
所以，max operation 其實 在這邊就是一個 linear 的 operation
這是 linear，這是 linear，只是它
會選擇在前面這個 group 裡面的
它只接給前面這個 group 裡面的某一個 element
也就是說，也就是說其實呢
那這些沒有被接到的 element，它就沒用啦
它就不會影響 network 的 output 啦
所以，你就可以把它拿掉，你就可以把它拿掉
所以，其實當我們在做 Maxout 的時候
當你給它一個 input 的時候
你其實也是得到一個比較細長的 linear network
所以，你在 train 的時候，你 train 的就是
這個比較細長的 linear network 裡面的參數
你就是去 train 這些連到這一個 element 的這些參數
連到這個 element 的這些參數
假設我給你一個這樣子的 linear network， 你當然知道它是怎麼 train 的
用 Backpropagation train 就好，你知道它是怎麼 train 的
但這個時候呢，你就會有一個問題
沒被 train 到的 element 怎麼辦呢？
如果某一個這個 element，它不是最大的值
那它連接的那些 weight
就不會被 train 到了嗎？
你做 Backpropagation 的時候
你只會 train 在這個圖上的
比較深顏色的這些實線
你不會 train 到這個 weight 阿
這個 weight 不就沒被 train 到了嗎？
怎麼辦呢？
這看起來，表面上是一個問題
但實作上，它不是一個問題
為甚麼呢？
因為當你 input 不同的
當你給它不同的 input 的時候
你得到的這些 z 的值是不一樣的
你給它不同 input 的時候
max 的值，是不一樣的
所以，每一次你給它不同的 input 的時候
這個 network 的 structure 都是不一樣的
因為我們有很多很多筆 training data
而 network 的 structure 不斷地變換
所以，最後每一個 weight 在實際上都會被 train 到
Maxout 就是這麼做
Maxout network 就是這麼做
所以，如果我們回到 Max Pooling
Max Pooling跟 Maxout 是一模一樣的 operation 阿
只是換一個說法而已，對不對
所以，你會 train Maxout
你就會 train Max Pooling，這是一模一樣的作法
講到這邊大家有沒有甚麼問題呢？
沒有的話，那
另外一個我們要講的是這個
adaptive 的 learning rate
其實 adaptive 的 learning rate，我們之前已經有講過了
我們之前有講過這個 Adagrad
我們之前有講過 Adagrad
我們說 Adagrad 的做法就是
我們現在每一個 parameter 都要有不同的 learning rate
而這個 learning rate 是怎麼 給它這麼 adaptive 的 learning rate 呢？
我們就把一個固定的 learning rate η
除掉這一個參數過去所有 gradient 值的平方和，開根號
把這項除以平方和開根號
就得到新的 parameter
那這個 Adagrad 它的精神就是說
如果我們今天考慮兩個參數，w1, w2
如果 w1 是在
這個方向上
如果 w1 在這個方向上，它平常 gradient 都比較小
那它是比較平坦的，給它比較大的 learning rate
反過來說，在這個方向上
平常 gradient 都是比較大的
所以，它是比較陡峭的，所以給它比較小的 learning rate
但是，實際上呢，我們面對的問題
有可能是比 Adagrad 可以處理的問題更加複雜的
也就是說，我們之前在做這個 Linear Regression 的時候
我們看到的這個 optimization 的 function
loss function 是這樣子 convex的形狀
但實際上，當我們在做 deep learning 的時候
這個 loss function 它可以是任何形狀，你知道嗎？
它可以是任何形狀
比如說，它可以是這樣，怪異的月形的形狀
如果當今天你的 error surface 是這個形狀的時候
那你會遇到的問題是，就算是同一個方向上
你的 learning rate 也比需要能夠快速地變動
就我們剛才在做 convex function 的時候
在每個方向上
這個方向很平坦，就一直很平坦
這個方向很陡峭，就一直很陡峭
但是，如果今天在更複雜的問題的時候
有可能，你考慮 w1 改變是在這個方向
在某個區域
它很平坦，所以它需要比較小的 learning rate
但是，到了另外一個區域
它又突然變得很陡峭
這個時候，它需要比較大的 learning rate
所以，真正要處理 deep learning 的問題，用 Adagrad
可能是不夠的，你需要更 dynamic 的調整
這個 learning rate 的方法
所以，這邊有一個 Adagrad 的進階膽，叫 RMSProp
RMSProp，我覺得是一個滿神奇的方法
因為你好像，找不到它的 paper
因為這個在 Hinton 的那個
MOOC 的 course 裡面，他提出來
他在他的線上課程裡面提出一個方法
大家要 cite 的時候，要 cite 那個線上課程的連結
這招還真的有用
這個 RMSProp 是這樣子做的
我們現在把
這個固定的 learning rate
除掉一個值，我們稱之為 σ
這個 σ 是甚麼呢？
在第一個時間點
這個 σ 就是你第一個算出來的 gradient 的值 g^0
那在第二個時間點呢？
在第二個時間點，你算出一個新的 gradient, g^1
這個時候
你的 σ 的值
新的 σ 的值，σ^1 呢
就是原來的 σ 值的平方，乘上 α
再加上新的 g 的值，(g^1)^2
再乘上 (1 - α)
而這個 α 的值是
你可以自由去調的
也就是我們原來在
或是我們再來看下一個例子
我們現在有一個
在下一個時間點，我們又算出 g^2
我們得到 σ^2
σ^2 怎麼算的呢？它是把原來的 σ^1
取平方乘上 α
再加上 (1 - α) 乘上 (g^2)^2
再開根號，得到這個 σ^2
那跟原來的 Adagrad 不一樣的地方是
原來的 Adagrad 你在這邊分母放的值
就是把 g^0, g^1, g^2 都取平方和開根號
但是，在這邊的時候
在 RMSProp 裡面呢，這個 σ^1
它裡面包含了 g^0 跟 g^1
那這邊也包含了 g^2
所以，它根號裡面也同樣包含了 g^0, g^1, g^2
就跟 Adagrad 一樣
但是，你現在可以給它乘上 weight, α
或者是 (1 - α)
所以，你可以調整說
我比較傾向
你可以調整這個 α 的值
這個 α 的值就也是像 learning rate 阿
也是你要手動設的值，當然你就設個 0.9 之類的
你可以手動去調這個
α 的值，讓它說
如果你把這個 α 的值設的小一點
那意思就是說，你傾向於相信新的 gradient
所告訴你的，這個 error surface
平滑或陡峭的程度
傾向於相信新的 gradient
比較無視於舊的 gradient 提供給你的 information
這樣大家應該可以瞭解這個結果
所以，在第 t 個時間點，你算出來的 σ
就是把 (σ^(t-1))^2 乘上 α
加上 (1 - α) 乘上在第 t 個時間點算出來的
gradient 的平方
所以，當你做 RMSProp 的時候
你一樣是在這算 gradient 的 zooming square
但是，你可以給
現在已經看到的 gradient 比較大的 weight
給過去看到的 gradient 比較小的 weight
除了 learning rate 的問題以外
我們知道說在做 deep learning 的時候
大家都會說，我們會卡在 local minimum
那我之前也有講過說，我們不見得是卡在 local minimum
也有可能卡在 saddle point
甚至，你有可能卡在 plateau 的地方
大家聽到這個問題都非常的擔心
覺得說，哇！這個做 deep learning 呢
是非常困難的
因為你可能胡亂做一下就一大推的問題
那其實呢
Yann LeCun 他在 07 年的時候
他有一個滿特別的說法
07 年的時候就講過這件事情
它說你不用擔心 local minimum 的問題
我不知道這件事情有多確切
我沒有 verify 過
但是，如果你有甚麼 verify 的結果的話
你可以跟我分享一下
Yann LeCun 的說法，他是這樣說的，他說
其實，在這個 error surface 上
沒有太多 local minimum
所以，你不用太擔心
為甚麼呢？他說
你要是一個 local minimum
你在每一個 dimension
都必須要是這樣子的形狀對不對？
都要是一個山谷的谷底
每一個 dimension 都要是山谷的谷底
我們假設這個山谷的谷底出現的機率是 p 好了
山谷的谷底出現的機率是 p
因為我們的 network 有非常非常多的參數
所以，假設有 1000 個參數
你每一個參數都要是山谷的谷底
那機率就是 p^1000
你的 network 越大
參數越多，這個出現的機率就越低
所以呢，local minimum
在一個很大的 neural network 裡面
其實沒有你想像的那麼多
一個很大的 neural network，它看起來其實是
其實搞不好是很平滑的，根本沒有太多 local minimum
所以，當你走走走，走到一個你覺得是
local minimum 的地方，卡住的時候
它八成就是 global minimum，或是很接近 gloal minimum
給大家參考
那你有甚麼特別的想法，再告訴我
有一個 heuristic 的方法
可以稍微處理一下，我們上述說的
我們剛才講的 local minimum 還有 plateau 的問題
這個方法，你可以說是從真實的世界，得到一些靈感
我們知道在真實的世界裡面
如果這個是一個地形，是一個山坡
你把一個球從左上角丟下來，把它滾下來
然後，它滾滾滾，它滾到 plateau 的地方呢
它不會停下來阿，因為有慣性嘛，它還會繼續往前
它就算是走到上坡的地方，假設這個波沒有很陡
因為慣性的關係
它搞不好走走走，還是可以翻過這個山坡
結果它就可以走到了
比這個 local minimum 還要好的地方
那所以我們
要做的事情就是把
這個慣性這個特性呢
塞到 Gradient Descent 裡面去
那這件事情，就叫做 momentum
這個東西怎麼做呢？我們先很快地秒複習一下
一般的 Gradient Descent
一般的 Gradient Descent 是怎麼做的呢？
我們是這樣子做的
這個是選一個初始的值
然後，計算一下它的 gradient
它的 gradient 是這個方向
那我們就走 gradient 的反方向
乘上一個 learning rate η
得到 θ^1，再算 gradient
再走一個新的方向
再算 gradient、再走一個方向； 再算 gradient、再走一個方向
以此類推
一直到 gradient = 0 的時候，或 gradient 趨近 0 的時候
我們就停止
當我們加上 momentum 的時候我們是怎麼做的呢？
當我們加上 momentum 的時候
我們每一次移動的方向
不再是只有考慮 gradient
而是我們現在的 gradient
加上在前一個時間點
移動的方向
這樣聽起來可能很抽象，所以
我們實際地來看一下，它是怎麼運作的
一樣選一個初始值 θ^0
一樣選一個初始值 θ^0
然後，我們用一個值 v 去記錄
我們在前一個時間點
移動的方向
v 記錄我們前一個時間點移動的方向
因為現在是初始值，之前沒有移動過，所以
前一個時間點移動的方向是 0
接下來計算在 θ^0 地方的 gradient
現在算出 θ^0 的 gradient
算出來是紅色這個箭頭
然後，我們現在要移動的方向
並不是紅色箭頭告訴我們的方向
而是，前一個時間點的
movement v^0
再加上 negative 的 gradient
然後，我們得到現在要移動的方向 v^1
所以，到這邊就好像是慣性一樣
如果我們之前走的方向是 v^0
那今天有一個新的 gradient，並不會
讓你參數 update 的方向完全轉向
它會改變你的方向
但是，因為有慣性的關係，所以
原來走的方向還是有一定程度的影響
那我們或許看下一個例子，會比較清楚
我們現在在上一個時間點移動的方向呢
是 v^1
接下來，再計算一下 gradient
計算一下 gradient，就是紅色的箭頭
接下來要決定說，在第二個時間點
我們要走的方向是甚麼樣？
第二個時間點要走的方向是
過去走的方向 v^1
減掉 leaning rate 乘上 gradient
如果我們看這個圖上的話，gradient 會告訴我們說
要走這個方向
負的 η 乘上 gradient，要走這個方向
但是，前面的 movement
是綠色的箭頭，它是這個方向
這個方向
我們會把這個 movement 乘上一個 λ
那這個 λ 其實也是一個
跟 learning rate 一樣，是手要調的參數
它告訴你說，現在這個慣性這件事情，影響力有多大
λ 大的話，慣性影響力就大
λ 小的話，慣性影響力就大
總之，慣性告訴我們走這邊
gradient 告訴我們走這邊
這兩個合起來呢，就走了一個新的方向，就是這邊
這個就是 v2，所以，以此類推
新的 gradient 告訴我們走這邊
慣性告訴我們
新的 gradient 告訴我們走紅色這個虛線的方向
慣性告訴我們走綠色虛線的方向
合起來最後就是走藍色的方向
然後，update 參數以後
gradient 告訴我們走紅色這個虛線的方向
慣性告訴我們走綠色虛線的方向
合起來就是走藍色的方向
那你可以用另外一個方法來理解這件事情
其實，v^i
你在每一個時間點移動的 movement
你在第 i 個時間點移動的步伐 v^i 呢
移動的量、方向，v^i 呢
其實就是過去所有算出來的 gradient 的總和
為甚麼這麼說呢？
我們知道 v^0 = 0
v^1 呢，v^1 在這邊
v^1 是 λ*(v^0) - η*(θ^0) 的 gradient
而 v^0 = 0
所以，v^1 = -η * gradient
所以是這個樣子
那 v^2 呢，v^2 我們就把
v^1 是負的 gradient
v^1 是負的 η * gradient 代進去
我們把 v^1 代到這邊，再乘上 λ
再減掉 η，乘以 θ^1 的 gradient
得到結果就是這樣
你得到的結果就是你把 θ^0 的地方的 gradient
減掉 λ * η
再減掉 η * θ^1 的 gradient
你得到 v^2
所以，v^2 裡面同時有在 θ^0 算出來的 gradient
同時有在 θ^1 的地方算出來的 gradient
只是這兩個 gradient，它的 weight 是不一樣的
如果你 λ 都設小於 0 的值的話呢
越之前的 gradient，它的 weight 就越小
越之前的 gradient，就越不去理它
你越在意現在的 gradient，但是過去的 gradient
也會對你現在要 update 的方向有一定程度的影響力
這個，就是 momentum
如果你看數學式子不太喜歡的話
那我們就從直覺上來看一下
到底加入 Momentum 以後，是怎麼運作的
在加入 Momentum 以後呢
每一次移動的方向
是 negative 的 gradient 加上 momentum
建議我們要走的方向
Momentum 其實就是上一個時間點的 movement
所以，現在假設我們有一個
假設我們初始的參數是在這個位置
那 gradient 建議我們往右走
所以，最後就往右移動
那如果說之後移到這個位置
gradient 建議我們往右走
而 momentum 也會建議我們往右走
因為我們是從左邊這邊移過來的嘛
所以，前一個步伐我們是向右的
如果你考慮 momentum 的時候呢
我們也會向右
所以，你把 gradient 建議我們走的方向
跟 momentum 建議我們走的方向合起來
你就得到這個藍色的線
所以你會繼續向右，如果我們今天走到
local minimum 的地方
走到 local minimum 的地方，gradient 是 0
所以，gradient 會告訴你說就停在這裡吧
但是，momentum 會告訴你說，之前是從右邊走過來的
所以，你仍然應該要繼續往右走
也就是綠色箭頭的方向
所以，最後你參數 update 的方向
仍然會繼續向右
甚至你可以樂觀地期待說
如果今天在往右的時候，走到這個地方
gradient 要求我們向左走
現在左邊如果是算微分的話呢
如果考慮 gradient 的話呢
參數應該往左移動
但是，momentum 建議我們
繼續向右走
因為你是從左邊過來的
因為你是從左向右過來的
所以 momentum 建議你繼續向右走
如果今天 momentum 其實比較強的話
你最後，就還是會向右走
所以，你有一定的可能性，你可以
有可能你可以跳出
local minimum，如果這個 local minimum 不深的話
你有可能藉由慣性的力量呢
跳出 local minimum，然後走到比較好的 global minimum
比較低的 local minimum
那如果你今天把 RMSProp 加上 momentum 的話
其實，你就得到 Adam 這樣
現在如果你沒有甚麼 prefer 的話
你就先學 Adam 就是了，那我發現在
作業二裡面，其實還滿多人 implement Adam 的
大家太強了，都自己 implement Adam 在作業二
我想這些你們都很熟了
沒什麼特別好講的
你可能看這個 algorithm
哇！感覺好像有點複雜
但是，好多人 implement 這個東西
其實，Adam 就是 RMSProp 加上 momentum
這兩個東西綜合起來，就是 Adam
我們非常非常快地來看一下這個式子
在這個式子裡面呢
在這個式子裡面，一開始要先初始一個東西叫做 m0
m0 就是 momentum
就是前一個時間點的 movement
那這邊有另外一個值叫做 v0
v0 就是我們剛才在 RMSProp 裡面看到的那個 σ
這個東西就是之前的 gradient 的 root mean square
之前算出來的 gradient 的平方和
就是 v0
你看，它先算一下 gradient，就是 gt
然後，根據 gt 呢
你就可以算出 mt，也就是現在要走的方向
現在要走的方向，是考慮過去要走的方向
再加上 gradient
接下來，算一下要放在分母的地方的 vt
這個 vt 是過去、前一個時間點的 vt
加上 gradient 的平方，等一下要開根號
這邊呢，它做了一個
跟原來 RMSProp 跟 momentum 裡面沒有的東西
叫做 bias correction
它會把 mt 跟 vt 都除上一個值
都除上一個值，那這個值本來比較大
那後來呢
這個值本來比較小，那後來呢
會越來越接近 1
至於為甚麼要這麼做， 他的 paper 裡面會告訴你他的理由
最後，你在 update 的時候
你把 momentum 建議你的方向，mt\head
去乘上 learning rate α，再除掉 RMSProp
就是 RMSProp normalize 以後，建議的 learning rate
最後，得到你 update 的方向
這個就是 Adam 這樣
那我猜你應該沒有聽得太懂
不過沒有關係，因為在 toolkit 裡面，只是打幾個
指令而已，我們就先在這邊休息 10 分鐘
等一下再繼續，謝謝
我們來上課吧
我們剛才講的，就是說
如果今天你在 training data 上的結果不好的話怎麼辦
那等一下我要講的呢？
如果今天你已經在 training data 上得到夠好的結果
但是，你在 testing data 上的結果仍然不好
那你有甚麼可行的方法
等一下會很快介紹 3 個方法
一個是 Early Stopping
Regularization 跟 Dropout
Early Stopping 跟 Regularization 是很 typical 的作法
他們不是 specific design for deep learning 的
這是一個很傳統、typical 的作法
那 Dropout 是一個滿有 deep learning 特色的做法
那在講 deep learning 的時候，需要講一下
我們來講一下 Early Stopping， Early Stopping 是甚麼意思呢？
我們知道說，隨這你的 training
你的 total loss，如果你今天的 learning rate 調的對的話
你的 total loss 通常會越來越小
那有可能你 rate 沒有設好，loss 變大也是有可能的
那你想像 learning rate 調的很好的話
那你在 training set 上的 loss 應該是逐漸變小的
但是，因為我們知道說
training set 跟 testing set 他們的 distribution
並不完全一樣
所以，有可能當你的 training 的 loss 逐漸減小的時候
你的 testing data 的 loss 卻反而上升了
這是有可能的
所以，理想上，假如你知道 testing data 的 loss 的變化
你應該停在不是 training set 的 loss 最小
而是 testing set 的 loss 最小的地方
你在 train 的時候，你不要一直 train 下去
你可能 train 到這個地方的時候，就停下來了
但是實際上，我們不知道 testing set 阿
你根本不知道你 testing set 的 error 是甚麼阿
所以，我們其實會用 validation set
來 verify 這件事情
所以，有的地方我可能需要稍微講得清楚一點
就是這邊的 testing set 阿，並不是指
真正的 testing set
它指的是你有 label data 的 testing set
比如說，如果你今天是在做作業的時候
這邊的 testing set 可能指的是
Kaggle 上的 public set
或者是，你自己切出來的 validation set
希望大家可以知道我的意思
這個只是名詞用的不同而已
但是，你不會知道真正的 testing set 的變化
所以，其實我們會切一個 validation set
來 verify 說
甚麼時候用 validation set 模擬 testing set
來看說甚麼時候呢
這個 validation set 的 loss 最小的時候
你的 training 就停下來
那其實在 Kaggle 裡面，就可以支援你做這件事啊
所以，你就自己看一下 documentation
那 Regularization 是甚麼呢？
我們重新定義了那個我們要去 minimize 的 loss function
我們原來有一個 loss function
我們要 minimize 的 loss function
是 define 在你的 training data 上的
比如說要 minimize square error 或 minimize cross entropy
那在做 Regularization 的時候呢
我們會加另外一個 Regularization 的 term
這個 Regularization 的 term 呢
這個 Regularization 的 term 呢
比如說，它可以是你的參數的 L2 norm
甚麼意思呢？
假設現在我們的參數 θ 裡面，它是
一群參數，w1, w2 等等有一大堆的參數
那這個 θ 的 L2 norm 呢，就是
你把你的 model 裡面的每一個參數都取平方
然後加起來，就是這個 θ2
那因為我們現在用 L2 norm 來做 Regularization
所以，這件事稱之為 L2 的 Regularization
那我們之前有講過說，在做 Regularization 的時候呢
一般我們是不會考慮 bias 這項
因為我們之前有講過說，加 Regularization 的目的
是為了要讓我們的 function 更平滑
而 bias 這件事情
通常跟 function 的平滑程度是沒有關係的
所以，通常我們在算 Regularization 的時候
不會把 bias 考慮進來
那如果我們把
L2 的 Regularization 放在這邊
我們會得到怎麼樣的結果呢
如果做微分的話，會得到怎麼樣的結果呢
如果我們把這個新的 objective function
我們把新的這個 loss function，也就是 L'
等於 L 加上 parameter 的 2 norm
做 gradient 的話呢
我們會得到 L' 對某一個參數 w 的偏微分
等於 L 對某個參數的偏微分
加上 λ 乘上某一個參數
因為這一項是
所有參數的平方和
所以，把這項對某個參數 w 做偏微分
你得到的結果就是 w
所以，你現在 update 參數的式子
會變成這樣
本來我們 update 的式子是把原來的參數
減掉 η 乘上 w 對
loss function 的偏微分 就得到新的參數
那現在這個 loss function 呢
這個 L'，這個 ∂L'/∂w
你可以換成，這個樣子
那你把這一項塞到這個地方
你把這一項塞到這個地方
那你會發現說呢
這邊有出現原來的參數
這邊也有出現原來的參數
所以，你可以把這幾項整理在一起
就變成這樣
你把這一項、這一項提出來
變成 (1 - η*λ) * w^t
再減掉你的參數對原來的 loss function 的 gradient
所以，如果根據這個式子，你就會發現說
其實在 update 參數的時候
每一次在 update 之前
你就把參數先乘個 (1 - η*λ)
也就是，每次你在 update 你的參數之前
通常你這邊的 η 就是你的 learning rate
它是一個很小的值
那這個 λ 通常會設一個很小的值，比如說
0.001 之類的
所以，η*λ 就是一個很小的值
(1 - η*λ) 通常是一個接近 1 的值
比如說 0.99，所以，今天你看這個 update 的式子的話
如果我們不管原來的 loss function 怎麼寫
只看這個 update 式子的話
等於你在 update 參數的時候，你做的事情是
每次 update 參數之前，就不分青紅皂白，先乘個 0.99
也就是說，你每次都會讓你的參數
越來越接近 0
不一定是越來越小，因為如果今天 w 是負的
w 是負的，負的乘上 0.99
它就變大了，它就接近 0，對不對
所以，今天每一個參數在 update 之前
都乘上一個小於 1 的值，所以它每次都越來越靠近 0
越來越靠近 0
那有人就會想說
越來越靠近 0，最後不就通通變 0 嗎？
這很崩潰阿，聽起來就不 make sense
那不會最後所有的參數都變 0
為甚麼？因為你還有後面這一項阿
沒有後面這一項
每一次 update 參數就越來越小，最後通通變 0
但是，問題就是後面還有這個
從微分那邊得到這一項
那這一項，會跟前面這一項，最後取得平衡
所以，並不會最後所有的參數都變成 0
因誤，如果我們使用
L2 的 Regularization 的時候
我們每次都會讓 weight 小一點、小一點、小一點
所以，這招叫做 Weight Decay
那其實 是這樣子
在 deep learning 裡面
Regularization 雖然有幫助，但是它的重要性
跟其他方法，比如說 SVM 比起來，並沒有那麼高
Regularization 幫助往往沒有那麼顯著
我覺得有一個可能的原因是
如果你看前面的 Early Stopping
我們可以決定說，甚麼時候 training 應該要被停下來
因為，我們現在在做這個 neural network 的時候
通常初始參數的時候，我們都是從
一個很小的、接近 0 的值開始初始參數
初始的時候，都是給它一個很小的、接近 0 的值
那你在做 update 的時候
通常就是讓參數離 0 越來越遠、越來越遠
而做 Regularization 這件事情
它要達到的目的，就是希望我們的參數
不要離 0 太遠
那我們參數不要離 0 太遠
加上 Regularization 所造成的效果
跟減少 update 次數
所造成的效果
其實，可能是很像的
但你今天做 Early Stopping，減少 update 次數
其實也會避免你的參數
離那些接近 0 的值太遠
那跟 Regularization 做的事情可能是很接近的
所以在 neural network 裡面
Regularization 雖然有幫助，但沒有那麼重要
沒有重要到說，比如說你看像 SVM
它是 explicitly 把 Regularization 這件事情
寫在它的 objective function 裡面
對不對，因為在做 SVM 的時候
它其實是要解一個 convex optimization problem
所以，實際上
它解的時候，並不一定會有 iteration 的過程
它一步就解出那個最好的結果了
它不像 deep learning 裡面有 Early Stopping 這件事
SVM 裡面，沒有 Early Stopping 這件事
一步就走到結果了
所以，你沒有辦法
用 Early Stopping 防止它離你太遠
所以你必須要把 Regularization
explicitly 加到你的 loss function 裡面去
那如果我們看 L1 的 Regularization
有人就會問說，為甚麼一定是平方，能不能用別的
當然可以用別的，比如說，你可以做
L1 的 Regularization，你可以把你的
Regularization 換成你的參數的 1 norm
也就是換成你參數裡面
換成你這個參數的集合裡面
每一個參數的絕對值的和
所以，如果我們把這一項換掉的話
如果我們把這一項從 2 norm 換成 1 norm 的話
會得到麼事呢？
你的第一個問題可能就是
絕對值不能微分阿
不能微分阿
給你一個最簡單的回答就是
這個東西 implement 在 Keras, TensorFlow 都沒有問題
所以，顯然是可以微分這樣
那實際的回答是這個樣子的
這個東西阿
它是取絕對值對不對？
那取絕對值，input 和 output 的關係不就長這樣子嗎？
不就是一個 V 的形狀嗎？
然後，在 V 的一邊
微分值是 1，在另外一邊微分值是 -1
那不能微的地方，其實只有在 0 的地方而已
就不要管它這樣子
真的出現、真的走到 0 的時候
你就胡亂給它一個值，比如說，給它 0 就好了
所以說
如果你把這一項
對 w 做微分的時候
你得到的結果是怎麼樣呢？
如果今天 w 是正的
那微分出來就是 +1
w 是負數，微分出來就是 -1
所以，我們這邊寫了一個 w 的 sign function
w 的 sign function 意思就是說，如果 w 是正數的話
這個 function output 就是 +1
w 是負數的話，這個 function output 就是 -1
如果我們把這一項
塞到參數 update 的式子裡面，會有甚麼結果呢？
就變成這樣
那我們可以把這個展開來
就變成這樣
那這個式子告訴我們甚麼？
這個式子告訴我們說，我們每一次在 update 參數的時候
我們每一次 update 參數的時候
我們就不管三七二十一
都一定要去減一個 η*λ，再乘一個
w 的 sign
也就是說，如果今天 w 是正的
w 是正的，這項就是 +1
所以，就變成是減一個 positive 的值
就會讓你的參數變小
如果 w 是負的，這一項是 -1
那就變成是加一個值，就會讓你的參數變大
也就是說，只要你的參數是正的
就減掉一些，只要你的參數是負的
就加上一些
那不管那個參數原來的值是多少
所以，如果你把這個 L1
跟 L2 做一下比較的話
他們同樣是讓參數變小，但是他們做的事情
是略有不同的
因為如果你是用 L1 的時候，每次都減掉固定的值
你用 L2 的時候
你是每一次都乘上一個小於 1 固定的值
所以，比如說，今天 w 是一個很正的值的話
比如說，它是一百萬
那你乘上一個 0.99，你其實把 w 減掉一個很大的值
但是，對 L1 來說
它每次減掉的值都是固定的
不管 w 是一百萬還是 0.1，w 減掉的值都是固定的
所以，對 L1 來說，對 L2 來說
只要 w 有出現很大的值
這個很大的 w
它下降很快，它很快就會變得很小
在 learning 的 process 中
但是，如果你今天是
L1 的話，那就不一樣了
如果 w 有很大的值，它的下降速度跟其他
很小的 w 是一樣的
所以，透過 L1 的 training 以後
你有可能認出來的 model 裡面
還是有一些很大很大的值
但是，如果我們考慮很小的值的話
對 L2 來說，很小的值
比如說 0.1, 0.01 阿，它的下降速度就很慢
所以，在 L 裡面，它會
train 出來的結果，它會保留很多接近 0 的值
那 L1 呢，它每次到下降一個固定的 value
那在 L1 裡面呢
它不會保留很多很小的值
所以，如果你用 L1 做 training 的時候呢，你得到的結果
就是會比較 sparse
那比較 sparse 的意思是說你 train 出來的參數裡面
有很多接近 0 的值
但是，也有很大的值
不像如果是 L2 的話，你 train 出來的結果
你的值是平均的都比較小
所以，他們 train 出來的結果是
略有差異的，那我們剛才
我們剛才在講 cn 的時候，有講過
L1 就是要產生一個 image 的時候，有產生 L1
那在剛才那個 task 裡面呢
L1 是比較適合的
因為我想要看到 sparse 的結果
我有試過用 L2，但是結果就沒有 L1 看起來那麼明顯
雖然 L1 看起來也沒有很明顯啦
但 L1 看起來的結果
是還比較像是一個 digit
那這邊就胡亂講一個東西，Weight Decay
我們在人腦裡面也會做 Weight Decay，對不對
這個是從、我記得龍騰的生物課本上有這個圖
這個是剛出生的時候，嬰兒的神經是這樣
6 歲的時候，有很多很多的神經
但是，到 14 歲的時候，神經間的連結
又減少了，所以
neural network 也會跟我們人
有一些很類似的事情，如果有一些 weight
你都沒有去 update 它
那它每次都會越來越小
最後就接近 0 就不見了
這跟人腦的運作，是有異曲同工之妙
那最後我們要講一下 dropout
我們先講 dropout 是怎麼做的
然後，才講為甚麼這樣做
dropout 是怎麼做的呢？
它是這樣，在 training 的時候
training 的時候，每一次我們要 update 參數之前
我們都對每一個 neuron，其實也包括 input 的地方
input 的 input layer 裡面的每一個 element
也算是一個 neuron
我們對 network 裡面的每一個 neuron
做 sampling，那這個 sampling 是要決定說
這個 neuron 要不要被丟掉，每個 neuron 有 p% 的機率
會被丟掉
那如果一個 neuron 被 sample 到要丟掉的時候
那你知道這個 neuron 要被丟掉了，那跟它相連的 weight
也失去作用，也都被丟掉，所以就變這樣
所以，做完這個 sample 以後
你的 network 的 structure 就變瘦了，變得比較細長
然後，你再去 train 這個比較細長的 network
而要注意一下，這個 sampling
是每次 update 參數之前，都要做一次
所以，每一次 update 參數的時候
你拿來 training 的那個 network structure 是不一樣的
每一次你都要重新做一次 sample，所以，你每一次
在做重新 sample 的時候
你得到的這個結果，會是不一樣的
那 testing 的時候呢
當你在 training 的時候，使用 dropout 的時候
你的 performance 是會變差的
了解我意思嗎？就是
因為本來如果你不要 dropout 的話
本來好好的做，不要 dropout 的話
你在 MNIST 上，剛剛可以把正確率做個 100% 這樣
但是，如果你加 dropout 的時後
因為你的神經元在 train 的時候
有時候莫名其妙就會不見
所以，你在 training 的時候， 有時候 performance 是會變差的
本來可以 train 到 100%
它就會變成只剩下 98%
有一些 neuron 不見了嘛
所以，當你加了 dropout 的時候
你在 training 上會看到的結果變差
但是，dropout 它真正要做的事情是
它就是要讓你 training 的結果變差
但是 testing 的結果是變好的
也就是，如果你今天遇到的問題是你 training 做得不夠好
你再加 dropout，你就是越做越差這樣子
那在 testing 的時候怎麼做呢？
在 testing 的時候要注意兩件事
第一件事情就是 testing 的時候不加 dropout
testing 的時候就是所有的 neuron 都要用
不做 dropout
另外一個地方是
在 testing 的時候
假設你的 dropout rate
在 training 的時候，dropout rate 是 p%
那在 testing 的時候，所有 weight 都要乘 (1 - p%)
也就是說，假設現在 dropout rate 是 50%
那我們在 training 的時候，learn 出來的 weight
等於 1
那 testing 的時候，你要把那個 weight
設 0.5
那有沒有很奇怪，我看很多人都皺眉頭這樣子
這個步驟非常地神妙
我覺得第一個想出來這個人
這個 Hinton，若憑空想出來這個想法真的非常神妙
那你自己在 implement dropout 的時候阿
過去，在還沒有那麼多 toolkit 的時候，常常有人說
拿給我看一個程式，說我做 dropout 了
它都沒有進步，老師你看看怎麼辦
我看就說，你忘了除這個 0.5，難怪沒有進步
不過現在 toolkit 都會自動幫你除 0.5
所以，就不用再擔心這件事情了
那為甚麼 dropout 有用，直覺的想法是這樣子
training 的時候，會丟掉一些 neuron
就好像是你要練輕功的時候，你會在腳上綁一些重物
然後，你實際上戰鬥的時候
實際上 testing 的時候，是沒有 dropout 的
實際上 test 的時候
你就把重物拿下來，所以就會變得很強
這個是小李，他平常都綁一個重物
只有在，我記得是要貫徹自己的忍道的時候
他才會拿下來
還是打輸我愛羅就是了
另外一個直覺的理由是這樣子
一個 neural network 裡面的每一個 neuron 就是一個學生
那大家被連結在一起
就是大家聽到要做 final project
那你知道說，在一個團隊裡面
總是有人會擺爛，就是它是會 dropout 的
所以，假設說你覺得你的隊友
其實是會擺爛的
所以，這個時候你就會想要好好做
實際上，你就會想要去 carry 他
但是，實際上最後在 testing 的時候
大家都有好好做，沒有人需要被 carry
那因為每個人都做是更有利，所以，結果是更好的
所以，在 testing 的時候，不用 dropout
另外，我想解釋的就是
直覺的需要解釋的就是
為甚麼 dropout rate 50% 的時候，就要乘 0.5
為甚麼 training 跟 testing 的 weight 是不一樣的呢？
照理說 training 用甚麼 weight 就要用在 testing 上阿
你這樣 training 跟 testing 的時候居然是用不同的 weight
為甚麼這樣呢？
直覺的理由是這樣
假設現在 dropout rate 是 50%
那在 training 的時候，你的期望總是會
丟掉一半的 neuron
對每一個 neuron 來說
總是期望說它有一半的 neuron
是不見的，是沒有 input 的
所以，你現在如果認好一組 weight
假設你在這個情況下，認好一組 weight
但是，在 testing 的時候，是沒有 dropout 的阿
所以，對同一組 weight 來說
假如你在這邊用這組 weight 得到 z
跟在這邊用這組 weight 得到 z'
它們得到的值，其實是會差兩倍的，對不對
因為在這個情況下，你總是會有一半的 input 不見
在這個情況下，你所有的 input 都會在
而你用同一組 weight 的話，變成 z' 就是 z 的兩倍了
這樣變成 training跟 testing 不 match
你 performance反而會變差
所以，怎麼辦？
把所有 weight 都乘 0.5 阿
乘 0.5 以後，做一下 normalization
把所有 weight 都乘 0.5，這樣 z 就會等於 z'
就是這麼回事
把這個 weight 乘上一個值以後，
反而會讓 training 跟 testing 是比較 match 的
這個是比較直觀上的結果，如果你要
更正式講的話，其實 dropout 有很多理由
這個東西還是一個可以探討的問題
在文獻上找到很多不同的觀點來解釋
為甚麼 dropout 會 work
那我覺得我比較能接受的是
dropout 是一種終極的 ensemble 的方法
甚麼是 ensemble 的方法呢？
ensemble 的方法在比賽的時候常用
ensemble 的方法意思是說
我們有一個很大的 training set
那你每次從 training set 裡面
只 sample 一部分的 data 出來
只 sample 一部分的 data 出來
記得我們在講 bias 跟 variance 的 trade off 的時候
我們不是以講過說，打靶有兩種狀況，一種是
你的 bias 大，所以你打不準
一種是你的 variance 很大，所以你打得準
如果你今天有一個很複雜的 model
很笨重、很大的 model 的時候
它往往是 bias 準，但 variance 很大
但是，如果你這個笨重的 model 有很多個
雖然它 variance 很大，最後平均起來
結果就很準
對不對，所以今天只 ensemble 做的事情
其實，就是要利用這個特性
我們 train 很多個 model
我們把原來的 training data 裡面 sample 出很多 subset
然後，train 很多個 model
然後，每一個 model 你甚至可以是 structure 不一樣
雖然說，每一個 model 他們可能
variance 很大，但是
如果他們都是很複雜的 model 的時候，平均起來
這個 bias 就很小
所以，你真正在 testing 的時候
train 了一把 model，然後在 testing 的時候
丟一筆 training data 近來，它通過所有的 model
得到一大堆的結果，再把這一大堆的結果平均起來
當作我們最後的結果
那如果你的 model 很複雜的畫，這一招往往有用
那 random forest 也是實踐這個方法的
一個實踐這個精神的一個方法
如果你用一個 decision tree，它就很弱
胡亂做它就會 overfitting，那如果你用 random forest
它就沒有那麼容易 overfitting
那為甚麼說 dropout 是一個終極的 ensemble 的方法呢
我們知道在做 dropout 的時候，我們每次
我們每一次要 update 參數的時候
就你拿一個 minibatch 出來，要 update 參數的時候
你都會做一次 sample
所以，你拿第一個 minibatch 的時候
你 train 的 network 長這樣子
你拿第二個 minibatch 的時候
你 train 的 network 可能長這樣
你拿第三個長這樣，你拿第四個長這樣
所以，在做 dropout 的時候
你等於是一個終極的 ensemble 的方式
你是在 train，假設你有 M 個 neuron
每一個 neuron 可以 drop 或不 drop
所以你可能的 neuron 的數目有 2^M 個
當你在做 dropout 的時候
你等於是在 train 這 2^M 個 neuron
你每次都只用一個 minibatch 的 data
去 train 一個 network，你用這個 minibatch 裡面的 data
用 minibatch 可能就 100 筆 data 嘛
你用這些 data 去 train 這些 network
那總共有 2^M 個可能的 network
當然因為你最後 update 的次數是有限的
你可能沒有辦法把 2^M 個 network 每個都 train 一遍
但是，你可能就 train 了好多好多的參數
好多好多的 network
你有做幾次 update 參數，你就 train 幾次 network
但是，每個 network 就只用一個 batch 來 train
那每一個 network 用一個 batch 來 train
可能會讓人覺得很不安
一個 batch 才 100 筆 data，怎麼 train 一個 network 呢
那沒有關係，因為這些不同的 network 之間的參數
是 shared，也就是說
這一個 network 的這一個參數
就是這個 network 的這個參數
就是它的這個參數，這 4 個參數其實是同一個參數
所以，雖然說一個 network
的 structure，它只用一個 batch train
但是一個 weight，它可能用好多個 batch 來 train
比如說，這個 weight，它在這 4 個 batch
裡面，在這 4 個 batch 做 dropout 的時候
都沒有把這個 weight 丟掉
那這個 weight，就是拿這 4 個 batch 合起來 train 的結果
所以，當你做 dropout 的時候
你就是 train 了一大把的 network structure
理論上，每一次 update 參數的時候
你都 train 了一個 network 出來
那 testing 的時候呢？
按照 ensemble 這個方法的邏輯應該就是
你把那一大把的 network 通通拿出來
然後，你把你的 testing data 丟到那一把
network 裡面去
每一個 network 都給你吐一個結果
然後，把所有的結果平均起來
就是最終的結果
但是，在實作上你沒辦法這麼做，因為
這一把 network 實在太多了
這一把 network 實在太多了
你沒有辦法把它都通通都拿出來
你沒有辦法每一個都丟一個 input 進去
去看看它 output 是什麼，再平均起來
這樣運算量太大
所以，dropout 最神奇的地方是它告訴你說
當你把一個完整的 network 不做 dropout
但是，把它的 weight 乘上 (1 - p%)
然後，你把這個東西，把你的 training data 丟進去
然後，得到它的 output 的時候
神奇的就是
這個 ensemble 的結果
跟這一個，把 weight 乘上 (1 - p%) 的結果
是可以 approximate 的
是可以 approximate 的
那你可能會想說，何以見得呢？
我們來舉一個例子
我們來 train 一個很簡單的 network，它就只有一個 neuron
它的 activation function 是 linear 的
我這邊就不考慮 bias
我們這邊有一個 neuron
然後，它的 input 是 x1, x2
然後，x1, x2 分別乘上
經過 training 以後
經過 dropout training 以後
你算出來的 weight 是 w1, w2
所以，它的 output 就是 w1*x1 + w2*x2
這個 neuron 沒有 activation function
或 activation function 是 linear 的
如果我們今天要做 ensemble 的話
theoretically 就是這麼做，對不對
每一個 neuron，我們做 dropout 的時候
你不會 drop 那個 output 的 neuron
你只會 drop hidden layer 跟 input 的 neuron
那這邊每一個 neuron，它可以
它可以被 drop，或不被 drop
對不對，所以我們總共有 4 種 structure
一個是通通沒被 drop
一個是 drop x1、一個是 drop x2, 一個是 x1, x2 都被 drop 掉
那你最後得到的 output 呢，這個 network
假設你 input x1, x2，這個 network 給我們的
就是 w1*x1 + w2*x2
同樣的 input，但是 x1 被 drop 掉
你得到的 output 是 w2*x2，這邊是 w1*x1
這邊給我們的 output 是 0
我們要做 ensemble
所以你要把這 4 個 network 它的 output 通通都 average 起來
通通都 average 起來，那你 average 起來的結果
是不是就是，這邊有 4 個值嘛
然後，w1*x1 出現兩次
w2*x2 出現兩次
所以，得到的結果是 1/2* w1*x1 + 1/2*w2*x2，對不對
那我們現在做的事情是把
這兩個 weight 都乘 1/2
我可以把 w1*1/2, w2*1/2
同樣 input x1, x2
那得到的 output 也同樣是 1/2*w1*x1 + 1/2*w2*x2
所以，這邊想要呈現的是說，在這個最簡單的 case 裡面
ensemble 這件事情
用不同的 network structure 做 ensemble 這件事情
跟我們把 weight multiply 一個值
而不做 ensemble 所得到的 output
其實是一樣的
那你可能會說，這個例子這麼簡單
所以，這個例子上會 work，我想也是很直覺的阿
大概小學生都知道說，這個是 equivalent
但是，比如說，這邊是 sigmoid function
或是它是很多個 layer，它會 work 嗎？
結論就是不會 equivalent
就是不會 equivalent
只有是 linear 的 network
ensemble 才會等於 multiply 一個 weight
左邊跟右邊要相等的前提是你的 network 要是 linear 的
但是，network 不是 linear 的阿
所以，他們其實不 equivalent
這個就是 dropout 最後一個很神奇的地方
雖然不 equivalent，但是最後結果還是會 work 這樣
所以，根據這個結論，有人有一個想法是說
既然 dropout 在 linear 的時候
linear 的 network 上，ensemble 才會
等於前一個 weight
所以，今天如果我的 network 很接近 linear 的話
應該 dropout performance 會比較好，比如說
怎麼做 network 會比較接近 linear，比如說你用 ReLU
比如說，你用 Maxout network
他們是很接近 linear 的
相對於 sigmoid，它們是比較接近 linear 的
所以 dropout 確實在用 ReLU 或 Maxout network 的時候
它的 performance 是確實比較好的
比如說，你去看 Maxout network 的 paper 的話
它裡面也有 point 這一點，它的
Maxout 跟 dropout 加起來的記錄量
是比 sigmoid function 還要大的，那這個是
作者相當自豪的一點
這邊我要講的其實
就是這樣啦
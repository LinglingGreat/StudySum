各位同學大家好，我們來上課吧
00:09
今天的規劃是這樣子
00:11
就是我們等一下，會先公告 final
00:14
那 final 有三個選擇，所以是需要花時間跟大家講一下的
00:19
所以，今天的規劃是
00:21
我們等一下上課，大概上到 1: 20 以後下課
00:26
不對，腦殘了，到 11:20 吧
00:32
不是 11:20，大概是 10:20 的時候下課
00:36
然後，等一下
00:38
剩下的時間就讓助教來把
00:41
三個 final 都跟大家講完
00:45
那今天我們要講的是 Word Embedding
00:50
我們之前已經講了 Dimension Reduction
00:54
那 Word Embedding 其實是 Dimension Reduction 一個
00:59
非常好、非常廣為人知的應用
01:05
如果我們今天要你用一個 vector
01:08
來表示一個 word，你會怎麼做呢？
01:10
最 typical 的作法
01:12
叫做 1-of-N encoding
01:14
每一個 word，我們用一個 vector來表示
01:18
這個 vector 的 dimension，就是這個世界上
01:22
可能有的 word 數目
01:24
假設這個世界上可能有十萬個 word
01:26
那 1-of-N encoding 的 dimension
01:28
就是十萬維
01:30
那每一個 word，對應到其中一維
01:34
所以，apple 它就是第一維是 1，其他都是 0
01:38
bag 就是第二維是 1，cat 就是第三維是 1，以此類推，等等
01:42
如果你用這種方式來描述一個 word
01:45
你的這個 vector 一點都不 informative
01:48
你每一個 word，它的 vector 都是不一樣的
01:53
所以從這個 vector 裡面，你沒有辦法得到任何的資訊
01:56
你沒有辦法知道說
01:57
比如說，bag 跟 cat
01:59
bag 跟 cat，沒什麼關係
02:01
比如說，cat 跟 dog
02:03
它們都是動物這件事
02:05
你沒有辦法知道
02:07
那怎麼辦呢？
02:09
有一個方法就叫做建 Word Class
02:13
也就是你把不同的 word
02:16
有同樣性質的 word
02:17
把它們 cluster 成一群一群的
02:20
然後就用那一個 word 所屬的 class 來表示這個 word
02:25
這個就是我們之前，在做 Dimension Reduction 的時候
02:29
講的 clustering 的概念
02:31
比如說，dog, cat 跟 bird，它們都是class 1
02:35
ran, jumped, walk 是 class 2
02:38
flower, tree, apple 是class 3，等等
02:40
但是，光用 class 是不夠的
02:43
我們之前有講過說，光做 clustering 是不夠的
02:47
因為，如果光做 clustering 的話呢
02:50
我們找了一些 information
02:51
比如說
02:54
這個是屬於動物的 class
02:55
這是屬於植物的 class
02:57
它們都是屬於生物
02:58
但是在 class 裡面，沒有辦法呈現這一件事情
03:00
或者是說，class 1 是動物
03:04
而 class 2 代表是，動物可以做的行為
03:07
但是， class 3 是植物
03:10
class 2 裡面的行為是 class 3 裡面沒有辦法做的
03:12
所以，class 2 跟 class 1 是有一些關聯的
03:15
沒有辦法用 Word Class 呈現出來
03:17
所以怎麼辦呢？
03:19
我們需要的，是 Word Embedding
03:22
Word Embedding 是這樣
03:24
把每一個 word
03:25
都 project 到一個 high dimensional 的 space 上面
03:30
把每一個 word 都 project 到一個
03:32
high dimensional space 上面
03:33
雖然說，這邊這個 space 是 high dimensional
03:35
但是它其實遠比 1-of-N encoding 的 dimension 還要低
03:39
1-of-N encoding，你這個 vector 通常是
03:42
比如說，英文有 10 萬詞彙，這個就是 10 萬維
03:45
但是，如果是用 Word Embedding 的話呢
03:47
通常比如說 50 維、100維這個樣子的 dimension
03:50
這個是一個從 1-of-N encoding 到 Word Embedding
03:54
這是 Dimension Reduction 的 process
03:58
那我們希望在這個 Word Embedding 的
04:00
這個圖上
04:02
我們可以看到的結果是
04:04
同樣，類似 semantic，類似語意的詞彙
04:07
它們能夠在這個圖上
04:09
是比較接近的
04:10
而且在這個 high dimensional space 裡面呢
04:14
在這個 Word Embedding 的 space 裡面
04:15
每一個 dimension，可能都有它特別的含意
04:19
比如說，假設我們現在做完 Word Embedding 以後
04:23
每一個 word 的這個
04:25
Word Embedding 的 feature vector長的是這個樣子
04:27
所以，可能就可以知道說
04:29
這個 dimension 代表了
04:32
生物和其他的東西之間的差別
04:37
這個 dimension 可能就代表了，比如說
04:41
這是會動的，跟動作有關的東西
04:44
動物是會動的，還有這個是動作
04:46
跟動作有關的東西
04:47
和不會動的，是靜止的東西的差別，等等
04:51
那怎麼做 Word Embedding
04:55
Word Embedding 是一個 unsupervised approach
04:58
也就是，我們怎麼讓 machine
05:00
知道每一個詞彙的含義，是什麼呢？
05:02
你只要透過讓 machine 閱讀大量的文章
05:06
你只要讓 machine 透過閱讀大量的文章
05:08
它就可以知道，每一個詞彙
05:11
它的 embedding 的 feature vector 應該長什麼樣子
05:15
這是一個 unsupervised 的 problem
05:18
因為我們要做的事情就是
05:20
learn 一個 Neural Network
05:22
找一個 function
05:23
那你的 input 是一個詞彙
05:26
output 就是那一個詞彙所對應的 Word Embedding
05:30
它所對應的 Word Embedding
05:31
的那一個 vector
05:33
而我們手上有的 train data
05:35
是一大堆的文字
05:37
所以我們只有Input，我們只有 input
05:41
但是我們沒有 output ，我們沒有 output
05:44
我們不知道
05:45
每一個 Word Embedding 應該長什麼樣子
05:49
所以，對於我們要找的 function
05:51
我們只有單項
05:53
我們只知道輸入，不知道輸出
05:56
所以，這是一個 unsupervised learning 的問題
06:01
那這個問題要怎麼解呢？
06:04
我們之前有講過
06:06
一個 deep learning base 的 Dimension Reduction 的方法
06:09
叫做 Auto-encoder
06:11
也就是 learn 一個 network，讓它輸入等於輸出
06:14
這邊某一個 hidden layer 拿出來
06:16
就是 Dimension Reduction 的結果
06:18
在這個地方
06:19
你覺得你可以用 Auto-encoder 嗎 ?
06:21
給大家一秒鐘時間的想一想
06:24
你覺得這邊可以用 Auto-encoder 的同學，舉手一下
06:28
你覺得不能用 auto-encoder 的同學請舉手一下，謝謝
06:34
把手放下，大多數的同學都覺得
06:36
不能用 Auto-encoder 來處理這一問題
06:38
沒錯，這個問題你沒辦法用 Auto-encoder 來解
06:42
你沒辦法用 Auto-encoder 來解
06:43
這件事情有點難解釋
06:46
或許讓大家自己回去想一想
06:48
你可以想想看
06:49
如果你是用 1-of-N encoding 當作它的 input
06:51
如果你用 1-of-N encoding 當作它的 input
06:54
對 1-of-N encoding 來說，每一個詞彙都是 independent
06:58
你把這樣子的 vector 做 Auto-encoder
07:00
你其實，沒有辦法 learn 出
07:03
任何 informative 的 information
07:05
所以，在 Word Embedding 這個 task 裡面
07:09
用 Auto-encoder 是沒有辦法的
07:13
如果你這一邊 input 是 1-of-N encoding
07:15
用 Auto-encoder 是沒有辦法的
07:17
除非你說，你用這個
07:19
你用 character，比如說你用
07:22
character 的 n-gram 來描述一個 word
07:24
或許，它可以抓到一些字首、字根的含義
07:27
不過基本上，現在大家並不是這麼做的
07:30
那怎麼找這個 Word Embedding 呢
07:33
這邊的作法是這樣子
07:36
它基本的精神就是
07:37
你要如何了解一個詞彙的含義呢
07:40
你要看這個詞彙的 contest
07:43
每一個詞彙的含義
07:45
可以根據它的上下文來得到
07:50
舉例來說
07:52
假設機器讀了一段文字是說
07:54
馬英九520宣誓就職
07:56
它又讀了另外一段新聞說
07:59
蔡英文520宣誓就職
08:02
對機器來說，雖然它不知道馬英九指的是什麼
08:05
他不知道蔡英文指的是什麼
08:07
但是馬英九後面有接520宣誓就職
08:10
蔡英文的後面也有接520宣誓就職
08:12
對機器來說，只要它讀了大量的文章
08:16
發現說，馬英九跟蔡英文後面都有類似的 contest
08:21
它前後都有類似的文字
08:23
機器就可以推論說
08:25
蔡英文跟馬英九代表了某種有關係的 object
08:29
他們是某些有關係的物件
08:32
所以它可能也不知道他們是人
08:34
但它知道說，蔡英文跟馬英九這兩個詞彙
08:36
代表了，可能有同樣地位的東西
08:40
那怎麼來體現這一件事呢
08:44
怎麼用這個精神來找出 Word Embedding 的 vector 呢
08:49
有兩個不同體系的作法
08:52
一個做法叫做 Count based 的方法
08:56
Count based 的方法是這樣
08:58
如果我們現在有兩個詞彙，wi, wj
09:05
它們常常在同一個文章中出現
09:09
它們常常一起 co-occur
09:12
那它們的 word vector
09:14
我們用 V(wi) 來代表
09:17
wi 的 word vector
09:18
我們用 V(wj) 來代表，wj 的 word vector
09:28
如果 wi 跟 wj，它們常常一起出現的話
09:33
V(wi) 跟 V(wj) 它們就會比較接近
09:39
那這種方法
09:42
有一個很代表性的例子
09:43
叫做 Glove vector
09:46
我把它的 reference 附在這邊
09:49
給大家參考
09:51
那這個方法的原則是這樣子
09:54
假設我們知道
09:56
wi 的 word vector 是 V(wi)
09:59
wj 的 word vector 是 V(wj)
10:02
那我們可以計算它的 inner product
10:06
假設 Nij 是 wi 跟 wj
10:12
它們 co-occur 在同樣的 document 裡面的次數
10:16
那我們就希望為 wi 找一組 vector
10:20
為 wj 找一個組 vector
10:23
然後，希望這兩個
10:26
這兩件事情
10:28
越接近越好
10:31
你會發現說，這個概念
10:33
跟我們之前講的 LSA 是
10:36
跟我們講的 matrix factorization 的概念
10:40
其實是一樣的
10:41
其實是一樣的
10:43
另外一個方法
10:45
叫做 Prediction based 的方法
10:48
我發現我這一邊拼錯了
10:52
這應該是 Prediction based 的方法
10:54
那我不知道說
10:58
就我所知，好像沒有人
11:00
很認真的比較過說
11:02
Prediction based 方法
11:04
跟 Count based 的方法
11:06
它們有什麼樣非常不同的差異
11:09
或者是誰優誰劣
11:11
如果你有知道這方面的 information，或許
11:13
你可以貼在我們的社團上面
11:15
我講一下， Prediction based 的方法是怎麼做的呢
11:19
Prediction based 的方法，它的想法是這樣
11:22
我們來 learn 一個 neural network
11:25
它做的事情是 prediction，predict 什麼呢？
11:30
這個 neural network 做的事情是 given 前一個 word
11:34
假設給我一個 sentence
11:36
這邊每一個 w 代表一個 word
11:39
given w(下標 i-1)，這個 prediction 的 model
11:44
這個 neural network，它的工作是要
11:47
predict 下一個可能出現的 word 是誰
11:50
那我們知道說，每一個 word
11:54
我們都用 1-of-N encoding，可以把它表示成一個 feature vector
11:59
所以，如果我們要做 prediction 這一件事情的話
12:03
我們就是要 learn 一個 neural network
12:06
它的 input
12:08
就是 w(下標 i-1) 的 1-of-N encoding 的 feature vector
12:13
1-of-N encoding 的 vector
12:15
那它的 output 就是
12:18
下一個 word, wi 是某一個 word 的機率
12:23
也就是說，這一個 model 它的 output
12:26
它 output 的 dimension 就是 vector 的 size
12:30
假設現在世界上有 10 萬個 word
12:32
這個 model 的 output 就是 10 萬維
12:34
每一維代表了某一個 word
12:37
是下一個 word 的機率
12:39
每一維代表某一個 word
12:40
是會被當作 wi
12:43
它會是下一個 word, wi 的機率
12:46
所以 input 跟 output 都是 lexicon size
12:51
只是它們代表的意思是不一樣的
12:53
input 是 1-of-N encoding，output 是下一個 word 的機率
12:56
那假設這就是一個
13:00
一般我們所熟知的
13:02
multi-layer 的 Perceptron，一個 deep neural network
13:06
那你把它丟進去的時候
13:10
你把這個 input feature vector 丟進去的時候
13:13
它會通過很多 hidden layer
13:14
通過一些 hidden layer，你就會得到一些 output
13:17
接下來，我們把第一個 hidden layer 的 input 拿出來
13:24
我們把第一個 hidden layer 的 input 拿出來
13:27
假設第一個 hidden layer 的 input
13:30
我們這一邊寫作，它的第一個 dimension 是 Z1
13:32
第二個 dimension 是 Z2，以此類推
13:34
這邊把它寫作 Z
13:36
那我們用這個 Z
13:39
就可以代表一個 word
13:42
input 不同的 1-of-N encoding
13:45
這邊的 Z 就會不一樣
13:47
所以，我們就把這邊的 Z
13:51
拿來代表一個詞彙
13:54
你 input 同一個詞彙
13:56
它有同樣的 1-of-N encoding
13:57
在這邊它的 Z 就會一樣
14:00
input 不同的詞彙，這邊的 Z 就會不一樣
14:02
所以，我們就用這個 Z
14:04
這一個 input 1-of-N encoding 得到 Z 的這個 vector
14:08
來代表一個 word
14:10
來當作那一個 word 的 embedding
14:12
你就可以得到這一個現象
14:14
你就可以得到這樣的 vector
14:18
為什麼用這個 Prediction based 的方法
14:20
就可以得到這樣的 vector 呢
14:22
Prediction based 的方法
14:24
是怎麼體現我們說的
14:26
可以根據一個詞彙的上下文
14:28
來了解一個詞彙的涵義，這一件事情呢？
14:32
這邊是這樣子的
14:34
假設我們的 training data 裡面呢
14:37
有一個文章是
14:39
告訴我們說，馬英九跟蔡英文宣誓就職
14:42
另外一個是馬英九宣誓就職
14:44
在第一個句子裡面
14:46
蔡英文是 w(下標 i-1)，宣誓就職是 w(下標 i)
14:49
在另外一篇文章裡面
14:50
馬英九是 w(下標 i-1)，宣誓就職是 w(下標 i)
14:54
那你在訓練這個 Prediction model 的時候
14:58
不管是 input 蔡英文，還是馬英九
15:02
不管是 input 蔡英文還是馬英九的 1-of-N encoding
15:04
你都會希望 learn 出來的結果
15:07
是宣誓就職的機率比較大
15:11
因為馬英九和蔡英文後面
15:12
接宣誓就職的機率都是高的
15:15
所以，你會希望說 input 馬英九跟蔡英文的時候
15:18
它 output，是 output 對應到宣誓就職那一個詞彙
15:23
它的那個 dimension 的機率是高的
15:26
為了要讓
15:28
蔡英文和馬英九雖然是不同的 input
15:31
但是，為了要讓最後在 output 的地方
15:33
得到一樣的 output
15:36
你就必須再讓中間的 hidden layer 做一些事情
15:40
中間的 hidden layer 必須要學到說
15:42
這兩個不同的詞彙
15:44
必需要把他們 project 到
15:47
必須要通過這個
15:48
必須要通過 weight 的轉換
15:50
通過這個參數的轉換以後
15:52
把它們對應到同樣的空間
15:56
在 input 進入 hidden layer 之前
15:58
必須把它們對應到接近的空間
16:00
這樣我們最後在 output 的時候
16:02
它們才能夠有同樣的機率
16:05
所以，當我們 learn 一個 prediction model 的時候
16:08
考慮一個 word 的 context這一件事情
16:12
16:15
所以我們把這個 prediction model 的
16:17
第一個 hidden layer 拿出來
16:19
我們就可以得到
16:21
我們想要找的這種 word embedding 的特性
16:24
那你可能會想說
16:27
如果只用 w(下標 i-1)去 predict w(下標 i)
16:31
好像覺得太弱了
16:32
就算是人，你給一個詞彙
16:34
要 predict 下一個詞彙
16:36
感覺也很難
16:37
因為，如果只看一個詞彙，
16:39
下一個詞彙的可能性，是千千萬萬的
16:42
是千千萬萬的
16:43
那怎麼辦呢？怎麼辦呢？
16:47
你可以拓展這個問題
16:50
比如說，你可以拓展說
16:52
我希望 machine learn 的是 input 前面兩個詞彙
16:55
w(下標 i-2) 跟 w(下標 i-1)
16:59
predict 下一個 word, w(下標 i)
17:02
那你可以輕易地把這個 model 拓展到 N 個詞彙
17:05
一般我們，如果你真的要 learn 這樣的 word vector 的話
17:10
你可能會需要你的 input
17:11
可能通常是至少 10 個詞彙
17:14
你這樣才能夠 learn 出
17:15
比較 reasonable 的結果
17:16
17:19
那我們這邊用 input 兩個 word 當作例子
17:22
那你可以輕易地把
17:23
這個 model 拓展到 10 個 word
17:28
那這邊要注意的事情是這個樣子
17:31
本來，如果是一般的 neural network
17:33
你就把 input w(下標 i-2) 和 w(下標 i-1) 的
17:37
1-of-N encoding 的 vector，把它接在一起
17:41
變成一個很長的 vector
17:43
直接丟都到 neural network 裡面
17:45
當作 input 就可以了
17:47
但是實際上，你在做的時候
17:50
你會希望 w(下標 i-2) 的
17:54
這個跟它相連的 weight
17:57
跟和 w(下標 i-1) 相連的 weight
17:59
它們是被 tight 在一起的
18:02
所謂 tight 在一起的意思是說
18:05
w(下標 i-2) 的第一個 dimension
18:08
跟第一個 hidden layer 的第一個 neuron
18:11
它們中間連的 weight
18:13
和 w(下標 i-1) 的第一個 dimension
18:16
和第一個 hidden layer 的 neuron，它們之間連的weight
18:19
這兩個 weight 必須是一樣的
18:23
所以，我這邊故意用同樣的顏色來表示它
18:27
這一個 dimension，它連到這個的 weight
18:30
跟第一個 dimension，它連到這邊的 weight
18:35
它必須是一樣的
18:37
所以，我這邊故意用同樣的顏色來表示他
18:39
這一個 dimension，它連到它的 weight
18:42
跟它連到它的 weight，必須是一樣的
18:44
以此類推
18:46
希望大家知道知道我的意思
18:50
那為什麼要這樣做呢？
18:53
為什麼要這樣做呢？
18:55
一個顯而易見的理由是這樣
18:58
一個顯而易見的理由是說
19:00
如果，我們不這麼做
19:02
如果我們不這麼做，你把不同的 word
19:07
你把同一個 word 放在
19:11
w(下標 i-2) 的位置跟放在 w(下標 i-1) 的位置
19:15
通過這個 transform 以後
19:19
它得到的 embedding 就會不一樣
19:22
如果，你必須要讓這一組 weight
19:26
和這一組weight 是一樣的
19:28
那你把一個 word 放在這邊，通過這個 transform
19:31
跟把一個 weight 放在這邊，通過一個 transform
19:33
它們得到的 weight 才會是一樣的
19:36
當然另外一個理由你可以說
19:38
我們做這一件事情的好處是
19:40
我們可以減少參數量
19:43
因為 input 這個 dimension 很大，它是十萬維
19:46
所以這個 feature vector，就算你這一邊是50 維
19:48
它也是一個非常非常、碩大無朋的 matrix
19:52
有一個已經覺得夠卡了
19:54
所以，有兩個更是吃不消
19:57
更何況說，我們現在 input 往往是 10 個 word
20:00
所以，如果我們強迫讓
20:02
所有的 1-of-N encoding
20:04
它後面接的 weight 是一樣的
20:06
那你就不會隨著你的 contest 的增長
20:08
而需要這個更多的參數
20:12
或許我們用 formulation 來表示
20:16
會更清楚一點
20:17
現在，假設 w(下標 i-2) 的 1-of-N encoding 就是 X2
20:23
w(下標 i-1) 的 1-of-N encoding 就是 X1
20:26
那它們的這個長度
20:28
都是 V 的絕對值
20:31
它們的長度我這邊都寫成 V 的絕對值
20:34
那這個 hidden layer 的 input
20:41
我們把它寫一個 vector, Z
20:43
Z 的長度，是 Z 的絕對值
20:47
那我們把這個 Z 跟
20:51
X(i-2) 跟 X(i-1) 有什麼樣的關係
20:55
Z 等於 X(i-2) * W1 + X(i-1) * W2
21:03
你把 X(i-2) * W1 + X(i-1) * W2，就會得到這個 Z
21:09
那現在這個 W1 跟 W2
21:11
它們都是一個 Z 乘上一個 V dimension 的 weight matrix
21:17
那在這邊，我們做的事情是
21:19
我們強制讓 W1 要等於 W2
21:23
要等於一個一模一樣的 matrix, W
21:26
所以，我們今天實際上在處理這個問題的時候
21:31
你可以把 X(i-2) 跟 X(i-1) 直接先加起來
21:36
因為 W1 跟 W2 是一樣的
21:38
你可以把 W 提出來
21:40
你可以把 X(i-1) 跟X(i-2) 先加起來
21:42
再乘上 W 的這個 transform
21:45
就會得到 z
21:47
那你今天如果要得到一個 word 的 vector 的時候
21:51
你就把一個 word 的 1-of-N encoding
21:55
乘上這個 W
21:57
你就可以得到那一個 word 的 Word Embedding
22:02
那這一邊會有一個問題，就是我們在實做上
22:07
如果你真的要自己實做的話
22:10
你怎麼讓這個 W1 跟 W2
22:13
它們的位 weight 一定都要一樣呢
22:16
事實上我們在 train CNN 的時候
22:21
也有一樣類似的問題
22:23
我們在 train CNN 的時候
22:25
我們也要讓 W1 跟 W2
22:27
我們也要讓某一些參數，它們的 weight
22:30
必須是一樣的
22:33
那怎麼做呢？這個做法是這樣子
22:36
假設我們現在有兩個 weight, wi 跟 wj
22:40
那我們希望 wi 跟 wj，它的 weight 是一樣的
22:44
那怎麼做呢？
22:45
首先，你要給 wi 跟 wj 一樣的 initialization
22:50
訓練的時候要給它們一樣的初始值
22:53
接下來，你計算 wi 的
22:56
wi 對你最後 cost function 的偏微分
23:00
然後 update wi
23:02
然後，你計算 wj 對 cost function 的偏微分
23:05
然後 update wj
23:06
你可能會說 wi 跟 wj
23:09
如果它們對 C 的偏微分是不一樣的
23:11
那做 update 以後
23:13
它們的值，不就不一樣了嗎？
23:16
所以，如果你只有列這樣的式子
23:18
wi 跟 wj 經過一次 update 以後，它們的值就不一樣了
23:21
initialize 值一樣也沒有用
23:23
那怎麼辦呢？
23:25
我們就把 wi 再減掉
23:29
再減掉 wj 對 C 的偏微分
23:32
把 wj 再減掉 wi 對 C 的偏微分
23:36
也就是說 wi 有這樣的 update
23:39
wj 也要有一個一模一樣的 update
23:42
wj 有這樣的 update
23:44
wi 也要有一個一模一樣的 update
23:46
如果你用這樣的方法的話呢
23:49
你就可以確保 wi 跟 wj，它們是
23:52
在這個 update 的過程中
23:54
在訓練的過程中
23:56
它們的 weight 永遠都是被 tight 在一起的
23:59
永遠都是一樣
24:04
那要怎麼訓練這個 network 呢？
24:07
這個 network 的訓練
24:08
完全是 unsupervised 的
24:11
也就是說，你只要 collect 一大堆文字的data
24:15
collect 文字的 data 很簡單
24:17
就寫一個程式上網去爬就好
24:19
寫一個程式爬一下
24:20
八卦版的 data
24:22
就可以爬到一大堆文字
24:25
然後，接下來就可以 train 你的 model
24:27
怎麼 train，比如說這邊有一個句子就是
24:29
潮水退了，就知道誰沒穿褲子
24:31
那你就讓你的 model
24:34
讓你的 neural network input "潮水" 跟 "退了"
24:38
希望它的 output 是 "就" 這個樣子
24:40
你會希望你的 output 跟"就" 的 cross entropy
24:44
"就" 也是一個 1-of-N encoding 來表示
24:47
所以，你希望你的 network 的 output
24:48
跟 "就" 的 1-of-N encoding
24:51
是 minimize cross entropy
24:53
然後，再來就 input "退了 " 跟 "就"
24:55
然後，希望它的 output 跟 "知道" 越接近越好
24:59
然後 output "就" 跟 "知道"
25:01
然後就，希望它跟 "誰" 越接近越好
25:05
那剛才講的
25:07
只是最基本的型態
25:09
其實這個 Prediction based 的 model
25:12
可以有種種的變形
25:15
目前我還不確定說
25:16
在各種變形之中哪一種是比較好的
25:21
感覺上，它的 performance
25:23
在不同的 task上互有勝負
25:25
所以，很難說哪一種方法一定是比較好的
25:28
那有一招叫做
25:31
Continuous bag of word, (CBOW)
25:33
那 CBOW 是這個樣子的
25:35
CBOW 是說，我們剛才是拿前面的詞彙
25:38
去 predict 接下來的詞彙
25:41
那 CBOW 的意思是說
25:43
我們拿某一個詞彙的 context
25:44
去 predict 中間這個詞彙
25:46
我們拿 W(i-1) 跟 W(i+1) 去 predict Wi
25:50
用 W(i-1) 跟 W(i+1)去 predict Wi
25:54
那 Skip-gram 是說
25:56
我們拿中間的詞彙去 predict 接下來的 context
26:02
我們拿 Wi 去 predict W(i-1) 跟 W(i+1)
26:08
也就是 given 中間的 word，我們要去 predict 它的周圍
26:12
會是長什麼樣子
26:14
講到這邊大家有問題嗎？
26:18
講到這邊常常會有人問我一個問題
26:21
假設你有讀過 word vector 相關的文獻的話
26:25
你可能會說
26:26
其實這個 network 它不是 deep 的阿
26:29
雖然，常常在講 deep learnin g 的時候
26:31
大家都會提到 word vector
26:33
把它當作 deep learning 的一個 application
26:36
但是，如果你真的有讀過 word vector 的文獻的話
26:39
你會發現說
26:40
這個 neural network，它不是 deep 的
26:43
它其實就是一個 hidden layer
26:45
它其實是一個 linear 的 hidden layer
26:47
了解嗎？就是
26:48
這個 neural network，它只有一個 hidden layer
26:50
所以，你把 word input 以後，你就得到 word embedding
26:53
你就直接再從那個 hidden layer，就可以得到 output
26:56
它不是 deep 的，為什麼呢？
26:59
為什麼？常常有人 問我這個問題
27:01
那為了回答這個問題
27:04
我邀請了 Tomas Mikolov 來台灣玩這樣
27:09
Tomas Mikolov 就是 propose word vector 的作者
27:13
所以，如果你有用過 word vector 的 toolkit 的話
27:16
你可能有聽過他的名字
27:18
那就問他說，為什麼這個 model不是 deep 的呢？
27:21
他給我兩個答案
27:22
他說，首先第一個就是
27:24
他並不是第一個 propose word vector 的人
27:27
在過去就有很多這樣的概念
27:30
那他最 famous 的地方是
27:31
他把他寫的一個非常好的 toolkit 放在網路上
27:35
他在他的 toolkit 裡面，如果你看他的 code 的話
27:38
他有種種的 tip
27:41
所以，你自己做的時候做不出他的 performance 的
27:44
他是一個非常非常強 的 engineer
27:46
他有各種他自己直覺的 sense
27:48
所以你自己做，你做不出他的 performance
27:51
用他的 toolkit，跑出來的 performance 就是特別好
27:54
所以，這是一個
27:57
他非常厲害的地方
27:59
他說，在他之前其實就有很多人做過
28:02
word vector，也有提出類似的概念
28:05
他說他寫的，他有一篇 word vector 的文章跟 toolkit
28:10
他想要 verify 最重要的一件事情是說
28:12
過去其實其他人就是用 deep
28:14
他想要講的是說，其實這個 task
28:18
不用 deep 就做起來了
28:20
不用 deep 的好處就是減少運算量
28:22
所以它可以跑很大量、很大量、很大量的 data
28:26
那我聽他這樣講
28:28
我就想起來，其實過去確實是
28:29
有人已經做過 word vector
28:32
過去確實已經有做過 word vector 這件事情
28:34
只是那些結果沒有紅起來
28:37
我記得說，我大學的時候
28:39
就看過類似的 paper
28:41
我大學的時候就有看過
28:42
其實就是一樣，就是 learn 一個 Prediction model
28:46
predict 下一個 word 的做法
28:48
只是那個時候是 deep
28:50
在我大學的時候
28:51
那時候 deep learning 還不紅
28:52
我看到那一篇 paper 的時候
28:54
他最後講說我 train 了這個 model
28:56
我花了 3 週，然後我沒有辦法把實驗跑完
29:00
所以結果是很好的
29:02
就其他方法，他可以跑很多的 iteration
29:06
然後說這個 neural network 的方法
29:07
我跑了 5 個 epoch，花了 3 週，我實在做不下去
29:12
所以，performance 沒有特別好
29:14
而且想說，這是什麼荒謬的做法
29:16
但是，現在運算量不同
29:19
所以，現在要做這一件事情呢
29:20
都沒有問題
29:22
其實像 word embedding 這個概念
29:25
在語音界，大概是在 2010 年的時候開始紅起來的
29:30
那個時候我們把它叫做 continuous 的 language model
29:34
一開始的時候
29:35
也不是用 neural network 來得到這個 word embedding的
29:37
因為 neural network 的運算量比較大
29:39
所以，一開始並不是選擇 neural network
29:41
而是用一些其他方法來
29:44
一些比較簡單的方法來得到這個 word 的 embedding
29:46
只是，後來大家逐漸發現說
29:49
用 neural network 得到的結果才是最好的
29:51
過去其他不是 neural network 的方法
29:53
就逐漸式微
29:55
通通都變成 neural network based 的方法
29:57
還有一個勵志的故事
29:59
就是Tomas Mikolov 那個
30:02
word vector paper不是非常 famous 嗎？
30:03
它的 citation，我不知道，搞不好都有 1 萬了
30:06
他說他第一次投那 一篇 paper 的時候
30:08
他先投到一個，我已經忘記名字的
30:10
很小很小的會，accept rate 有 70%
30:12
然後就被 reject 了
30:16
他還得到一個 comment，就是這是什麼東西
30:20
我覺得這東西一點用都沒有
30:22
所以，這是一個非常勵志的故事
30:27
那我們知道說
30:29
word vector 可以得到一些有趣的特性
30:32
我們可以看到說
30:34
如果你把同樣類型的東西的 word vector 擺在一起
30:39
比如說，我們把這個 Italy
30:42
跟它的首都 Rome 擺在一起
30:44
我們把Germany 跟它的首都 Berlin 擺在一起
30:47
我們把 Japan
30:49
跟它的首都 Tokyo 擺在一起
30:51
你會發現說
30:53
它們之間是有某種固定的關係的
30:56
或者是，你把一個動詞的三態擺在一起
30:59
你會發現說，動詞的三態
31:03
同一個動詞的三態
31:04
它們中間有某種固定的關係
31:07
成為這個三角形
31:09
所以從這個 word vector 裡面呢
31:11
你可以 discover 你不知道的 word 跟 word 之間的關係
31:15
比如說，還有人發現說
31:17
如果你今天把
31:19
兩個 word vector 和 word vector 之間，兩兩相減
31:23
這個結果是把 word vector 跟 word vector 之間兩兩相減
31:27
然後 project 到一個 2 dimensional 的 space 上面
31:30
那你會發現說，在這一區
31:33
如果今天 word vector 兩兩相減
31:35
它得到的結果是落在這個位置的話
31:38
那這兩個 word vector 之間，它們就有，比如說
31:42
某一個 word 是包含於某一個 word 之間的關係
31:46
比如說，你把 (這一邊這個字比較小)
31:49
比如說，你把海豚跟會轉彎的白海豚相減
31:53
它的 vector 落在這邊
31:56
你把演員跟主角相減，落在這一邊
31:59
你把工人跟木匠相減，落在這邊
32:02
你把職員跟售貨員相減，落在這一邊
32:06
你把羊跟公羊相減，落在這邊
32:08
如果，某一個東西是
32:10
屬於另外一個東西的話
32:12
你把它們兩個 word vector 相減
32:13
它們的結果呢，會是很類似的
32:16
所以用 word vector 的這一個的概念
32:20
我們可以做一些簡單的推論
32:24
舉例來說， 因為我們知道說
32:27
比如說，hotter 的 word vector
32:28
減掉 hot 的 word vector 會很接近
32:31
bigger 的 word vector 減掉 big 的 word vector
32:33
或是 Rome 的 vector 減掉 Italy 的 vector
32:36
會很接近 Berlin 的 vector 減掉 Germany 的 vector
32:38
或是 King 的 vector 減掉 queen 的 vector 會很接近
32:40
uncle 的 vector 減掉 aunt 的 vector
32:43
如果有人問你說，羅馬之於義大利
32:46
就好像 Berlin 之於什麼？
32:48
智力測驗都會考這樣的問題
32:49
機器可以回答這種問題了
32:51
怎麼做呢？因為我們知道說
32:54
今天這個問題的答案
32:57
Germany 的 vector 會很接近 Berlin 的 vector
33:00
減掉 Rome 的 vector 加 Italy 的 vector
33:03
因為這 4 個 word vector 中間有這樣的關係
33:05
所以你可以把 Germany 放在一邊
33:07
把另外三個 vector 放在右邊
33:09
所以 Germany 的 vector 會接近 Berlin 的 vector
33:12
減掉 Rome 的 vector 再加上 Italy 的 vector
33:15
所以，如果你要回答這個問題
33:17
假設你不知道答案是 Germany 的話
33:20
那你要做的事情就是
33:22
計算 Berlin 的 vector
33:24
減掉 Rome的 vector，再加 Italy 的 vector
33:27
然後看看它跟哪一個 vector 最接近
33:29
你可能得到的答案就是 Germany
33:32
這邊有一個 word vector 的 demo
33:36
就讓機器讀了大量 PTT 的文章以後
33:40
它就像這樣
33:42
那 word vector 還可以做很多其他的事情
33:46
比如說，你可以把不同的語言的 word vector
33:51
把它拉在一起
33:53
如果，你今天有一個中文的 corpus
33:55
有一個英文的 corpus
33:56
你各自去、分別去 train 一組 word vector
34:00
你會發現說
34:01
中文跟英文的 word vector
34:03
它是完全沒有任何的關係的
34:06
它們的每一個 dimension
34:09
對應的含義並沒有任何關係，為什麼？
34:11
因為你要 train word vector 的時候
34:14
它憑藉的就是上下文之間的關係
34:17
所以，如果你今天的 corpus 裡面
34:19
沒有中文跟英文的句子混雜在一起
34:22
沒有中文跟英文的詞彙混雜在一起
34:25
那 machine 就沒有辦法判斷
34:26
中文的詞彙跟英文的詞彙他們之間的關係
34:30
但是，今天假如你已經事先知道說
34:34
某幾個詞彙
34:35
某幾個中文的詞彙和某幾個英文的詞彙
34:38
它們是對應在一起的
34:40
那你先得到一組中文的 vector
34:42
再得到一組英文的 vector
34:43
接下來，你可以再 learn 一個 model
34:46
它把中文和英文對應的詞彙
34:49
比如說，我們知道 "加大" 對應到 "enlarge"
34:52
"下跌" 對應到 "fall"
34:53
你把對應的詞彙，通過這個 projection 以後，
34:56
把它們 project 在 space上的同一個點
34:59
把它們 project 在 space 上面的同一個點
35:01
那在這個圖上，綠色的然後下面又有
35:06
這個綠色的英文的代表是
35:08
已經知道對應關係的中文和英文的詞彙
35:13
然後，如果你做這個 transform 以後
35:16
接下來有新的中文的詞彙和新的英文的詞彙
35:19
你都可以用同樣的 projection
35:22
把它們 project 到同一個 space 上面
35:24
比如說，你就可以自動知道說
35:26
中文的降低跟的英文的 reduce
35:32
它們都應該落在這個位置
35:34
都應該落在差不多的位置等等這樣
35:38
你就可以自動做到
35:39
比如說，類似翻譯這個樣子的效果
35:42
那這個 embedding不只限於文字
35:49
你也可以對影像做 embedding
35:52
這邊有一個很好的例子
35:54
這個例子是這樣做的
35:55
它說，我們先已經找到一組 word vector
35:58
比如說，dog 的 vector、horse 的 vector
36:00
auto 的 vector 和 cat 的 vector
36:02
它們分佈在空間上是這樣子的位置
36:04
接下來，你 learn 一個 model
36:07
它是 input 一張 image
36:08
output 是跟一個跟 word vector
36:10
一樣 dimension 的 vector
36:12
那你會希望說
36:14
狗的 vector 就散佈在狗的周圍
36:16
馬的 vector 就散佈在馬的周圍
36:18
車輛的 vector 就散佈在 auto 的周圍
36:22
那假設有一些 image
36:24
你已經知道他們是屬於哪一類
36:27
你已經知道說這個是狗、這個是馬、這個是車
36:30
你可以把它們 project 到
36:32
它們所對應到的 word vector 附近
36:35
那這個東西有什麼用呢？
36:36
假如你今天有一個新的 image 進來
36:40
比如說，這個東西，它是個貓
36:42
但是你不知道它是貓
36:44
機器不知道它是貓
36:45
但是你通過它們的 projection
36:46
把它 project 到這個 space 上以後
36:49
神奇的是你就會發現它可能就在貓的附近
36:53
那你的 machine 就會自動知道說
36:55
這個東西叫做貓
36:57
當我們一般在做影像分類的時候
37:00
大家都已經有做過作業三
37:03
作業三就是影像分類的問題
37:05
在做影像分類的問題的時候
37:07
你的 machine 其實很難去處理
37:10
新增加的，它沒有辦法看過的 object
37:13
舉例來說，作業 3 裡面
37:15
我們就先已經訂好 10 個 class
37:17
你 learn 出來的 model
37:19
就是只能分這 10 個 class
37:22
如果今天有一個新的東西
37:23
不在這10個 class 裡面
37:25
你的 model 是完全是無能為力 的
37:27
它根本不知道它叫做什麼
37:28
但是，如果你用這個方法的話
37:30
就算有一張 image
37:32
是你在 training 的時候，你沒有看過的 class
37:34
比如說，貓這個 image
37:36
它從來都沒有看過
37:38
但是如果貓的這個 image
37:40
可以 project 到 cat 的 vector 附近的話
37:42
你就會知道說，這一張 image 叫做 cat
37:46
如果你可以做到這一件事，就好像是
37:49
machine 先閱讀了大量的文章以後
37:51
它知道說，每一個詞彙
37:53
指的是什麼意思
37:55
它知道說，狗啊，貓啊，馬啊
37:59
它們之間有什麼樣的關係
38:01
它透過閱讀大量的文章，先了解詞彙間的關係
38:06
接下來，在看 image 的時候
38:07
它就可以根據它已經閱讀得到的知識
38:10
去 mapping 每一個 image
38:12
所應該對應的東西
38:13
這樣就算是它看到它沒有看過的東西
38:16
它也可能可以把它的名字叫出來
38:20
那剛才講的呢
38:22
都是 word embedding
38:24
也可以做 document 的 embedding
38:26
不只是把一個 word 變成一個 vector
38:29
也可以把一個 document 變成一個 vector
38:33
那怎麼把一個 document 變成一個 vector 呢
38:38
最簡單的方法，我們之前已經講過了
38:41
就是把一個 document 變成一個 word
38:44
然後，用 Auto-encoder
38:46
你就可以 learn 出
38:48
這個 document 的 Semantic Embedding
38:50
但光這麼做是不夠的
38:53
我們光用這個 word 來描述一篇 document
38:57
是不夠的，為什麼呢？
38:58
因為我們知道說，詞彙的順序
39:01
代表了很重要的含
39:03
舉例來說
39:04
這一邊有兩個詞彙，有兩個句子
39:07
一個是： white blood cells destroying an infection
39:11
另外一個是：an infection destroying white blood cells
39:14
這兩句話，如果你看它的 bag-of-word 的話
39:18
它們的 bag-of-word 是一模一樣的
39:21
因為它們都有出現有這 6 個詞彙
39:23
它們都有出現這 6 個詞彙
39:25
只是順序是不一樣的
39:26
但是因為它們的順序是不一樣的
39:29
所以上面這一句話
39:30
白血球消滅了傳染病，這個是 positive
39:33
下面這句話，它是 negative
39:36
雖然說，它們有同樣的 bag-of-word
39:38
它們在語意上，完全是不一樣的
39:41
所以，光只是用 bag-of-word
39:44
來描述一張 image 是非常不夠的
39:48
用 bag-of-word 來描述 一篇 document 是非常不足的
39:53
你用 bag-of-word 會失去很多重要的 information
39:57
那怎麼做呢？
39:59
我們這一邊就不細講
40:00
這邊就列了一大堆的 reference 給大家參考
40:04
上面這 3 個方法，它是 unsupervised
40:07
也就是說你只要 collect
40:09
一大堆的 document
40:11
你就可以讓它自己去學
40:13
那下面這幾個方法算是 supervised
40:17
因為，在這一些方法裡面
40:19
你需要對每一個 document
40:21
進行額外的 label
40:22
你不用 label 說，每一個 document 對應的 vector是什麼
40:26
但是你要給它其他的 label
40:28
才能夠 learn 這一些 vector
40:29
所以下面，不算是完全 unsupervised
40:32
我把 reference 列在這邊，給大家參考
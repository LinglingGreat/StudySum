## Machine Learning-常见算法优缺点汇总

**数据维度越高**，随机森林就比AdaBoost强越多，但是整体不及SVM。

**数据量越大**，神经网络就越强。

##决策树算法

**一、决策树优点**

1、决策树易于理解和解释，可以可视化分析，容易提取出规则。

2、可以同时处理标称型和数值型数据。

3、测试数据集时，运行速度比较快。

4、决策树可以很好的扩展到大型数据库中，同时它的大小独立于数据库大小。

**二、决策树缺点**

1、对缺失数据处理比较困难。

2、容易出现过拟合问题。

3、忽略数据集中属性的相互关联。

4、ID3算法计算信息增益时结果偏向数值比较多的特征。

**三、改进措施**

1、对决策树进行剪枝。可以采用交叉验证法和加入正则化的方法。

2、使用基于决策树的combination算法，如bagging算法，randomforest算法，可以解决过拟合的问题



决策树的特点是它总是在沿着特征做切分。随着层层递进，这个划分会越来越细。

虽然生成的树不容易给用户看，但是数据分析的时候，通过观察树的上层结构，能够对分类器的核心思路有一个直观的感受。

举个简单的例子，当我们预测一个孩子的身高的时候，决策树的第一层可能是这个孩子的性别。男生走左边的树进行进一步预测，女生则走右边的树。这就说明性别对身高有很强的影响。

适用情景：

因为它能够生成清晰的基于特征(feature)选择不同预测结果的树状结构，数据分析师希望更好的理解手上的数据的时候往往可以使用决策树。

同时它也是相对容易被攻击的分类器[3]。这里的攻击是指人为的改变一些特征，使得分类器判断错误。常见于垃圾邮件躲避检测中。因为决策树最终在底层判断是基于单个条件的，攻击者往往只需要改变很少的特征就可以逃过监测。

受限于它的简单性，决策树更大的用处是作为一些更有用的算法的基石。

**四、常见算法**

**一）C4.5算法**

ID3算法是以信息论为基础，以信息熵和信息增益度为衡量标准，从而实现对数据的归纳分类。ID3算法计算每个属性的信息增益，并选取具有最高增益的属性作为给定的测试属性。

C4.5算法核心思想是ID3算法，是ID3算法的改进，改进方面有：

- 用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；
- 在树构造过程中进行剪枝；
- 能处理非离散的数据；
- 能处理不完整的数据。

**优点：**产生的分类规则易于理解，准确率较高。

**缺点：**

1）在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效；

2）C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。

**二）CART分类与回归树**

是一种决策树分类方法，采用基于最小距离的基尼指数估计函数，用来决定由该子数据集生成的决策树的拓展形。如果目标变量是标称的，称为分类树；如果目标变量是连续的，称为回归树。分类树是使用树结构算法将数据分成离散类的方法。

**优点**

1）非常灵活，可以允许有部分错分成本，还可指定先验概率分布，可使用自动的成本复杂性剪枝来得到归纳性更强的树。

2）在面对诸如存在缺失值、变量数多等问题时CART 显得非常稳健。

## 随机森林 (Random forest)

严格来说，随机森林其实算是一种集成算法。它首先随机选取不同的特征(feature)和训练样本(training sample)，生成大量的决策树，然后综合这些决策树的结果来进行最终的分类。

随机森林在现实分析中被大量使用，它相对于决策树，在准确性上有了很大的提升，同时一定程度上改善了决策树容易被攻击的特点。

适用情景：

数据维度相对低（几十维），同时对准确性有较高要求时。

因为不需要很多参数调整就可以达到不错的效果，基本上不知道用什么方法的时候都可以先试一下随机森林。

随机森林的**优点**：

1. 实现简单，训练速度快，泛化能力强，可以并行实现，因为训练时树与树之间是相互独立的；
2. 相比单一决策树，能学习到特征之间的相互影响，且不容易过拟合；
3. 能处理高维数据（即特征很多），并且不用做特征选择，因为特征子集是随机选取的；
4. 对于不平衡的数据集，可以平衡误差；
5. 相比SVM，不是很怕特征缺失，因为待选特征也是随机选取；
6. 训练完成后可以给出哪些特征比较重要。

随机森林的**缺点**：

1. 在噪声过大的分类和回归问题还是容易过拟合；
2. 相比于单一决策树，它的随机性让我们难以对模型进行解释。

##**分类算法**

###**一、KNN算法**

**KNN算法的优点** 

1、KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练

2、KNN理论简单，容易实现

**KNN算法的缺点**

1、对于样本容量大的数据集计算量比较大。

2、样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多。

3、KNN每一次分类都会重新进行一次全局运算。

4、k值大小的选择。

**KNN算法应用领域**

文本分类、模式识别、聚类分析，多分类领域

适用情景：

需要一个特别容易解释的模型的时候。比如需要向用户解释原因的推荐算法。

###**二、支持向量机（SVM）**

支持向量机是一种基于分类边界的方法。其基本原理是（以二维数据为例）：如果训练数据分布在二维平面上的点，它们按照其分类聚集在不同的区域。基于分类边界的分类算法的目标是，通过训练，找到这些分类之间的边界（直线的――称为线性划分，曲线的――称为非线性划分）。对于多维数据（如N维），可以将它们视为N维空间中的点，而分类边界就是N维空间中的面，称为超面（超面比N维空间少一维）。线性分类器使用超平面类型的边界，非线性分类器使用超曲面。

支持向量机的原理是将低维空间的点映射到高维空间，使它们成为线性可分，再使用线性划分的原理来判断分类边界。在高维空间中是一种线性划分，而在原有的数据空间中，是一种非线性划分。

**SVM优点**

1、解决小样本下机器学习问题。

2、解决非线性问题。

3、无局部极小值问题。（相对于神经网络等算法）

4、可以很好的处理高维数据集。

5、泛化能力比较强，计算开销不大，结果易解释。

**SVM缺点**

1、对于核函数的高维映射解释力不强，尤其是径向基函数。

2、对缺失数据敏感。

3、对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。

**SVM应用领域**

文本分类、图像识别、主要二分类领域

最早的SVM是平面的，局限很大。但是利用核函数(kernel function)，我们可以把平面投射(mapping)成曲面，进而大大提高SVM的适用范围。 

**适用情景：**

适用数据类型：数值型和标称型数据

SVM在很多数据集上都有优秀的表现。

相对来说，SVM尽量保持与样本间距离的性质导致它抗攻击的能力更强。

和随机森林一样，这也是一个拿到数据就可以先尝试一下的算法。

###**三、朴素贝叶斯算法**

**朴素贝叶斯算法优点**

1、对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已。

2、支持增量式运算。即可以实时的对新增的样本进行训练。

3、朴素贝叶斯对结果解释容易理解。

4、在数据较少的情况下仍然有效，可以处理多类别问题。

**朴素贝叶斯缺点**

1、由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。

2、对于输入数据的准备方式较为敏感。

**朴素贝叶斯应用领域**

文本分类、欺诈检测中使用较多

**适用情景：**

适用数据类型：标称型数据

需要一个比较容易解释，而且不同维度之间相关性较小的模型的时候。可以高效处理高维数据，虽然结果可能不尽如人意。

###**四、Logistic回归算法**

**logistic回归优点**

1、计算代价不高，易于理解和实现

**logistic回归缺点**

1、容易产生欠拟合。

2、分类精度不高。

**logistic回归应用领域**

用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等。

Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等。

适用情景：

LR同样是很多分类算法的基础组件，它的好处是输出值自然地落在0到1之间，并且有概率意义。

因为它本质上是一个线性的分类器，所以处理不好特征之间相关的情况。

虽然效果一般，却胜在模型清晰，背后的概率学经得住推敲。它拟合出来的参数就代表了每一个特征(feature)对结果的影响。也是一个理解数据的好工具。

### 判别分析 (Discriminant analysis)

判别分析的典型例子是线性判别分析(Linear discriminant analysis)，简称LDA。

（这里注意不要和隐含狄利克雷分布(Latent Dirichlet allocation)弄混，虽然都叫LDA但说的不是一件事。）

LDA的核心思想是把高维的样本投射(project)到低维上，如果要分成两类，就投射到一维。要分三类就投射到二维平面上。这样的投射当然有很多种不同的方式，LDA投射的标准就是让同类的样本尽量靠近，而不同类的尽量分开。对于未来要预测的样本，用同样的方式投射之后就可以轻易地分辨类别了。

使用情景：

判别分析适用于高维数据需要降维的情况，自带降维功能使得我们能方便地观察样本分布。它的正确性有数学公式可以证明，所以同样是很经得住推敲的方式。

但是它的分类准确率往往不是很高，所以不是统计系的人就把它作为降维工具用吧。

同时注意它是假定样本成正态分布的，所以那种同心圆形的数据就不要尝试了。

##聚类算法

###**一、K means 算法**

是一个简单的聚类算法，把n的对象根据他们的属性分为k个分割，k< n。 算法的核心就是要优化失真函数J,使其收敛到局部最小值但不是全局最小值。

其中N为样本数，K是簇数，rnk b表示n属于第k个簇，uk 是第k个中心点的值。然后求出最优的uk

**优点：**算法速度很快

**缺点：**分组的数目k是一个输入参数，不合适的k可能返回较差的结果。



###**二、EM最大期望算法**

EM算法是基于模型的聚类方法，是在概率模型中寻找参数最大似然估计的算法，其中概率模型依赖于无法观测的隐藏变量。E步估计隐含变量，M步估计其他参数，交替将极值推向最大。

EM算法比K-means算法计算复杂，收敛也较慢，不适于大规模数据集和高维数据，但比K-means算法计算结果稳定、准确。EM经常用在机器学习和计算机视觉的数据集聚（Data Clustering）领域。

##**集成算法（AdaBoost算法）** 

**一、  AdaBoost算法优点**

1、很好的利用了弱分类器进行级联。

2、可以将不同的分类算法作为弱分类器。

3、AdaBoost具有很高的精度。

4、相对于bagging算法和Random Forest算法，AdaBoost充分考虑的每个分类器的权重。

**二、Adaboost算法缺点**

1、AdaBoost迭代次数也就是弱分类器数目不太好设定，可以使用交叉验证来进行确定。

2、数据不平衡导致分类精度下降。

3、训练比较耗时，每次重新选择当前分类器最好切分点。

**三、AdaBoost应用领域**

模式识别、计算机视觉领域，用于二分类和多分类场景

AdaBoost的实现是一个渐进的过程，从一个最基础的分类器开始，每次寻找一个最能解决当前错误样本的分类器。用加权取和(weighted sum)的方式把这个新分类器结合进已有的分类器中。

它的好处是自带了特征选择（feature selection），只使用在训练集中发现有效的特征(feature)。这样就降低了分类时需要计算的特征数量，也在一定程度上解决了高维数据难以理解的问题。

## Bagging

它首先随机地抽取训练集（training set），以之为基础训练多个弱分类器。然后通过取平均，或者投票(voting)的方式决定最终的分类结果。 

因为它随机选取训练集的特点，Bagging可以一定程度上避免过渡拟合(overfit)。

在[1]中，最强的Bagging算法是基于SVM的。如果用定义不那么严格的话，随机森林也算是Bagging的一种。

使用情景：

相较于经典的必使算法，Bagging使用的人更少一些。一部分的原因是Bagging的效果和参数的选择关系比较大，用默认参数往往没有很好的效果。

虽然调对参数结果会比决策树和LR好，但是模型也变得复杂了，没事有特别的原因就别用它了。

## Stacking

这个我是真不知道中文怎么说了。它所做的是在多个分类器的结果上，再套一个新的分类器。

这个新的分类器就基于弱分类器的分析结果，加上训练标签(training label)进行训练。一般这最后一层用的是LR。

Stacking在[1]里面的表现不好，可能是因为增加的一层分类器引入了更多的参数，也可能是因为有过渡拟合(overfit)的现象。

##**人工神经网络算法**

**一、神经网络优点**

1、分类准确度高，学习能力极强。

2、对噪声数据鲁棒性和容错性较强。

3、有联想能力，能逼近任意非线性关系。

**二、神经网络缺点**

1、神经网络参数较多，权值和阈值。

2、黑盒过程，不能观察中间结果。

3、学习过程比较长，有可能陷入局部极小值。

**三、人工神经网络应用领域**

目前深度神经网络已经应用与计算机视觉，自然语言处理，语音识别等领域并取得很好的效果。 

使用情景：

数据量庞大，参数之间存在内在联系的时候。

##**排序算法（PageRank）**

PageRank是google的页面排序算法，是基于从许多优质的网页链接过来的网页，必定还是优质网页的回归关系，来判定所有网页的重要性。（也就是说，一个人有着越多牛X朋友的人，他是牛X的概率就越大。）

**一、****PageRank****优点**

完全独立于查询，只依赖于网页链接结构，可以离线计算。

**二、PageRank缺点**

1）PageRank算法忽略了网页搜索的时效性。

2）旧网页排序很高，存在时间长，积累了大量的in-links，拥有最新资讯的新网页排名却很低，因为它们几乎没有in-links。

![img](https://mmbiz.qpic.cn/mmbiz_gif/UnzPPicpR0KXaib3ibeAwStkeHNgqlJh9eDqY8odJtrNic5ZcHLibH5TU4v2bXgzYloicHnXxexic0z5E7jQektKJiaPow/640?wx_fmt=gif&tp=webp&wxfrom=5&wx_lazy=1)

##**关联规则算法（Apriori算法）**

Apriori算法是一种挖掘关联规则的算法，用于挖掘其内含的、未知的却又实际存在的数据关系，其核心是基于两阶段频集思想的递推算法 。

**Apriori算法分为两个阶段：**

1）寻找频繁项集

2）由频繁项集找关联规则

**算法缺点：**

1）在每一步产生侯选项目集时循环产生的组合过多，没有排除不应该参与组合的元素；

2） 每次计算项集的支持度时，都对数据库中 的全部记录进行了一遍扫描比较，需要很大的I/O负载。
# Why ML Strategy

Ideas:

* Collect more data

* Collect more diverse training set

* Train algorithm longer with gradient descent

* Try Adam instead of gradient descent

* Try bigger network

* Try smaller network

* Try dropout

* Add ğ¿2regularization

* Network architecture

  * Activation functions

  * num of hidden units

  * ...

    â€‹

# æ­£äº¤åŒ–

Orthogonalization or orthogonality is a system design property that assures that modifying an instruction or a component of an algorithm will not create or propagate side effects to other components of the system. It becomes easier to verify the algorithms independently from one another, it reduces testing and development time.
When a supervised learning system is design, these are the 4 assumptions that needs to be true and orthogonal.

1. Fit training set well in cost function
   - If it doesnâ€™t fit well, the use of a bigger neural network or switching to a better optimization algorithm might help.
2. Fit development set well on cost function
   - If it doesnâ€™t fit well, regularization or using bigger training set might help.
3. Fit test set well on cost function
   - If it doesnâ€™t fit well, the use of a bigger development set might help
4. Performs well in real world
   - If it doesnâ€™t perform well, the development test set is not set correctly or the cost function is not evaluating the right thing.

# å•ä¸€æ•°å­—è¯„ä¼°æŒ‡æ ‡

Single number evaluation metric

To choose a classifier, a well-defined development set and an evaluation metric speed up the iteration process.

æ¯”å¦‚å¯ä»¥ç”¨F1-scoreç»¼åˆprecisionå’Œrecallï¼Œè¿™æ ·åªç”¨çœ‹ä¸€ä¸ªæŒ‡æ ‡ï¼Œåœ¨precisionå’Œrecallæ— æ³•åˆ¤æ–­çš„æƒ…å½¢ä¸‹ï¼šå¦‚

| classifier | Precision(p) | Recall(r) | F1-Score |
| ---------- | ------------ | --------- | -------- |
| A          | 95%          | 90%       | 92.4%    |
| B          | 98%          | 85%       | 91.0%    |

è¿™æ ·ä»F1-Scoreä¸Šåˆ¤æ–­ï¼Œåˆ†ç±»å™¨Aæ›´å¥½

# æ»¡è¶³å’Œä¼˜åŒ–æŒ‡æ ‡

Satisficing and optimizing metric

There are different metrics to evaluate the performance of a classifier, they are called evaluation matrices. They can be categorized as satisficing and optimizing matrices. It is important to note that these evaluation matrices must be evaluated on a training set, a development set or on the test set.

æ¯”å¦‚åœ¨æŸäº›ä»»åŠ¡ä¸­ï¼Œaccuracyæ˜¯æˆ‘ä»¬æƒ³è¦æœ€å¤§åŒ–çš„ç›®æ ‡ï¼Œå®ƒæ˜¯ä¼˜åŒ–æŒ‡æ ‡ï¼Œä½†æˆ‘ä»¬ä¹Ÿä¸æƒ³è¿è¡Œæ—¶é—´å¤ªé•¿ï¼Œå¯ä»¥è®¾ç½®è¿è¡Œæ—¶é—´å°äº100msï¼Œå®ƒæ˜¯æ»¡è¶³æŒ‡æ ‡ã€‚

# è®­ç»ƒ/å¼€å‘/æµ‹è¯•é›†åˆ†å¸ƒ

Setting up the training, development and test sets have a huge impact on productivity. It is important to choose the development and test sets from the same distribution and it must be taken randomly from all the data.
Guideline
Choose a development set and test set to reflect data you expect to get in the future and consider important to do well.

# å¼€å‘æœºå’Œæµ‹è¯•é›†çš„å¤§å°

Old way of splitting data
We had smaller data set therefore we had to use a greater percentage of data to develop and test ideas and models.

70/30æˆ–60/20/20

Modern era â€“ Big data
Now, because a large amount of data is available, we donâ€™t have to compromised as much and can use a greater portion to train the model.

98/1/1

Guidelines

* Set up the size of the test set to give a high confidence in the overall performance of the system.
* Test set helps evaluate the performance of the final classifier which could be less 30% of the whole data set.
* The development set has to be big enough to evaluate different ideas.

# ä»€ä¹ˆæ—¶å€™è¯¥æ”¹å˜å¼€å‘/æµ‹è¯•é›†å’ŒæŒ‡æ ‡

The evaluation metric fails to correctly rank order preferences between algorithms. The evaluation metric or the development set or test set should be changed.

If doing well on your metric + dev/test set does not correspond to doing well on your application, change your metric and/or dev/test set.

Guideline 

1. Define correctly an evaluation metric that helps better rank order classifiers 
2. Optimize the evaluation metric

# å’Œäººç±»æ°´å¹³æ¯”è¾ƒ

Today, machine learning algorithms can compete with human-level performance since they are more productive and more feasible in a lot of application. Also, the workflow of designing and building a machine learning system, is much more efficient than before.
Moreover, some of the tasks that humans do are close to â€˜â€™perfectionâ€™â€™, which is why machine learning tries to mimic human-level performance.

Machine learning progresses slowly when it surpasses human-level performance. One of the reason is that human-level performance can be close to Bayes optimal error, especially for natural perception problem.
Bayes optimal error is defined as the best possible error. In other words, it means that any functions mapping from x to y canâ€™t surpass a certain level of accuracy.
Also, when the performance of machine learning is worse than the performance of humans, you can improve it with different tools. They are harder to use once its surpasses human-level performance.
These tools are:

- Get labeled data from humans
- Gain insight from manual error analysis: Why did a person get this right?
- Better analysis of bias/variance.

# å¯é¿å…è¯¯å·®

Avoidable bias

By knowing what the human-level performance is, it is possible to tell when a training set is performing well or not.

åœ¨æŸäº›ä»»åŠ¡ä¸­ï¼Œæ¯”å¦‚è®¡ç®—æœºè§†è§‰ï¼Œå¯ä»¥ç”¨human leval errorä»£æ›¿Bayes error,å› ä¸ºäººç±»åœ¨è¿™æ–¹ä¾¿çš„è¯¯å·®å¾ˆå°ï¼Œæ¥è¿‘æœ€ä¼˜æ°´å¹³ã€‚

Scenario A
There is a 7% gap between the performance of the training set and the human level error. It means that the algorithm isnâ€™t fitting well with the training set since the target is around 1%. To resolve the issue, we use bias reduction technique such as training a bigger neural network or running the training set longer.
Scenario B
The training set is doing good since there is only a 0.5% difference with the human level error. The difference between the training set and the human level error is called avoidable bias. The focus here is to reduce the variance since the difference between the training error and the development error is 2%. To resolve the issue, we use variance reduction technique such as regularization or have a bigger training set.

# ç†è§£äººç±»æ°´å¹³è¡¨ç°

The definition of human-level error depends on the purpose of the analysis.æ¯”å¦‚ï¼Œå¦‚æœç›®æ ‡æ˜¯æ›¿ä»£è´å¶æ–¯è¯¯å·®ï¼Œå°±åº”è¯¥é€‰æ‹©æœ€å°çš„è¯¯å·®ä½œä¸ºäººç±»è¯¯å·®ã€‚

Summary of bias/variance with human-level performance

* Human - level error â€“ proxy for Bayes error
* If the difference between human-level error and the training error is bigger than the difference between the training error and the development error. The focus should be on bias reduction technique
* If the difference between training error and the development error is bigger than the difference between the human-level error and the training error. The focus should be on variance reduction technique

# è¶…è¿‡äººçš„è¡¨ç°

Surpassing human-level performance

There are many problems where machine learning significantly surpasses human-level performance, especially with structured data:

* Online advertising
* Product recommendations
*  Logistics (predicting transit time)
* Loan approvals

# æ”¹å–„ä½ çš„æ¨¡å‹è¡¨ç°

The two fundamental assumptions of supervised learning

The first one is to have a low avoidable bias which means that the training set fits well. The second one is to have a low or acceptable variance which means that the training set performance generalizes well to the development set and test set.

Avoidable bias   (Human-level <â€”â€”> Training error)

* Train bigger model
* Train longer, better optimization algorithms
* Neural Networks architecture / hyperparameters search

Variance   (Training error <â€”â€”> Development error)

* More data
* Regularization
* Neural Networks architecture / hyperparameters search



# è¿›è¡Œè¯¯å·®åˆ†æ

Error analysis:

* Get ~100 mislabeled dev set examples.
* Count up how many are dogs.ç»Ÿè®¡ä¸åŒç±»å‹é”™è¯¯çš„æ¯”ä¾‹

Ideas for cat detection:

* Fix pictures of dogs being recognized as cats
* Fix great cats (lions, panthers, etc..) being misrecognized
* Improve performance on blurry images

# æ¸…é™¤æ ‡æ³¨é”™è¯¯çš„æ•°æ®

DL algorithms are quite robust to random errors in the training set.(but not systematic errorsæ¯”å¦‚æ‰€æœ‰ç™½è‰²çš„å°ç‹—éƒ½è¢«æ ‡è®°æˆçŒ«)

è¯¯å·®åˆ†æï¼š

Overall dev set error

Errors due incorrect labels

Errors due to other causes

æ ¹æ®è¯¯å·®æ¥æºå†³å®šåº”è¯¥ä¾§é‡äºä¿®æ­£å“ªæ–¹é¢çš„è¯¯å·®

Correcting incorrect dev/test set examples

* Apply same process to your dev and test sets to make sure they continue to come from the same distribution
* Consider examining examples your algorithm got right as well as ones it got wrong.ï¼ˆé€šå¸¸ä¸è¿™ä¹ˆåšï¼Œå› ä¸ºåˆ†ç±»æ­£ç¡®çš„æ ·æœ¬å¤ªå¤šï¼‰
* Train and dev/test data may now come from slightly different distributions.

# å¿«é€Ÿæ­å»ºç¬¬ä¸€ä¸ªç³»ç»Ÿ

* Set up dev/test set and metric
* Build initial system quickly
* Use Bias/Variance analysis & Error analysis to prioritize next steps

Guideline:

Build your first system quickly, then iterate

# è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ¥è‡ªä¸åŒåˆ†å¸ƒ

å¯ä»¥æŠŠæ¯”è¾ƒå¥½çš„æ•°æ®ï¼ˆå¦‚æ¸…æ™°çš„å›¾ç‰‡ï¼‰ä½œä¸ºè®­ç»ƒé›†ï¼Œä¸å¤ªå¥½çš„æ•°æ®ï¼ˆæ¯”å¦‚æ¨¡ç³Šçš„å›¾ç‰‡ï¼‰åˆ†æˆä¸‰éƒ¨åˆ†ï¼šè®­ç»ƒé›†ã€å¼€å‘é›†å’Œæµ‹è¯•é›†

# åå·®å’Œæ–¹å·®(ä¸åŒ¹é…æ•°æ®åˆ†å¸ƒä¸­)

Training-dev set: Same distribution as training set, but not used for training

Human levelâ€”â€”(avoidable bias)â€”â€”Training errorâ€”â€”(variance)â€”â€”Training-dev errorâ€”â€”(data mismatch)â€”â€”Dev errorâ€”â€”(degree of overfitting to dev set)â€”â€”Test error

# å®šä½æ•°æ®ä¸åŒ¹é…

å¦‚æœè®¤ä¸ºå­˜åœ¨æ•°æ®ä¸åŒ¹é…é—®é¢˜ï¼Œå¯ä»¥åšè¯¯å·®åˆ†æï¼Œæˆ–è€…çœ‹çœ‹è®­ç»ƒé›†æˆ–å¼€å‘é›†æ¥æ‰¾å‡ºè¿™ä¸¤ä¸ªæ•°æ®åˆ†å¸ƒåˆ°åº•æœ‰ä»€ä¹ˆä¸åŒï¼Œç„¶åçœ‹çœ‹æ˜¯å¦èƒ½å¤Ÿæ”¶é›†æ›´å¤šçœ‹èµ·æ¥åƒå¼€å‘é›†çš„æ•°æ®åšè®­ç»ƒã€‚å…¶ä¸­ä¸€ç§æ–¹æ³•æ˜¯äººå·¥æ•°æ®åˆæˆï¼Œåœ¨è¯­éŸ³è¯†åˆ«ä¸­æœ‰ç”¨åˆ°ã€‚ä½†æ˜¯è¦æ³¨æ„åœ¨åˆæˆæ—¶æ˜¯ä¸æ˜¯ä»æ‰€æœ‰å¯èƒ½æ€§çš„ç©ºé—´åªé€‰äº†å¾ˆå°ä¸€éƒ¨åˆ†å»æ¨¡æ‹Ÿæ•°æ®ã€‚

# è¿ç§»å­¦ä¹ 

ç¥ç»ç½‘ç»œä¸­æŠŠä»ä¸€ä¸ªä»»åŠ¡ä¸­å­¦åˆ°çš„çŸ¥è¯†åº”ç”¨åˆ°å¦ä¸€ä¸ªç‹¬ç«‹çš„ä»»åŠ¡ä¸­ã€‚

å¦‚ï¼ŒæŠŠå›¾åƒè¯†åˆ«ä¸­å­¦åˆ°çš„çŸ¥è¯†åº”ç”¨æˆ–è¿ç§»åˆ°æ”¾å°„ç§‘è¯Šæ–­ä¸Šã€‚

å‡è®¾å·²ç»è®­ç»ƒå¥½ä¸€ä¸ªå›¾åƒè¯†åˆ«ç¥ç»ç½‘ç»œã€‚åº”ç”¨åˆ°æ”¾å°„ç§‘è¯Šæ–­ä¸Šæ—¶ï¼Œå¯ä»¥åˆå§‹åŒ–æœ€åä¸€å±‚çš„æƒé‡ï¼Œé‡æ–°è®­ç»ƒã€‚

å¦‚æœæ•°æ®é‡å°ï¼Œå¯ä»¥åªè®­ç»ƒåé¢ä¸€é“ä¸¤å±‚ï¼›å¦‚æœæ•°æ®é‡å¾ˆå¤§ï¼Œå¯ä»¥å¯¹æ‰€æœ‰å‚æ•°é‡æ–°è®­ç»ƒã€‚

ç”¨å›¾åƒè¯†åˆ«çš„æ•°æ®å¯¹ç¥ç»ç½‘ç»œçš„å‚æ•°è¿›è¡Œè®­ç»ƒâ€”â€”é¢„è®­ç»ƒpre-training

ç”¨æ”¾å°„ç§‘æ•°æ®é‡æ–°è®­ç»ƒæƒé‡â€”â€”å¾®è°ƒfine tuning

ç”¨äºè¿ç§»ç›®æ ‡ä»»åŠ¡æ•°æ®å¾ˆå°‘ï¼Œè€Œç›¸å…³çŸ¥è¯†ä»»åŠ¡æ•°æ®é‡å¾ˆå¤§çš„æƒ…å†µä¸‹ã€‚

When transfer learning makes sense?

* Task A and B have the same input x
* You have a lot more data for Task A than Task B
* Low level features from A could be helpful for learning B

# å¤šä»»åŠ¡å­¦ä¹ 

è®©å•ä¸ªç¥ç»ç½‘ç»œåŒæ—¶åšå¤šä»¶äº‹ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½èƒ½å¸®åˆ°å…¶ä»–æ‰€æœ‰ä»»åŠ¡ã€‚

ä½¿ç”¨é¢‘ç‡ä½äºè¿ç§»å­¦ä¹ ã€‚è®¡ç®—æœºè§†è§‰ï¼Œç‰©ä½“æ£€æµ‹ä¸­å¸¸ç”¨å¤šä»»åŠ¡å­¦ä¹ ã€‚

When multi-task learning makes sense

* Training on a set of tasks that could benefit from having shared lower-level features.
* Usually: Amount of data you have for each task is quite similar.è‡³å°‘ä¸€ä¸ªä»»åŠ¡çš„æ•°æ®è¦ä½äºå…¶å®ƒä»»åŠ¡çš„æ•°æ®ä¹‹å’Œ
* Can train a big enough neural network to do well on all the tasks.

# ç«¯åˆ°ç«¯æ·±åº¦å­¦ä¹ 

end-to-end learning, å­¦ä¹ ä»è¾“å…¥åˆ°è¾“å‡ºçš„ç›´æ¥æ˜ å°„ï¼Œçœç•¥äº†ä¸­é—´æ­¥éª¤

éœ€è¦æ•°æ®é›†å¾ˆå¤§

ç«¯åˆ°ç«¯å­¦ä¹ ä¸ä¸€å®šæ˜¯æœ€å¥½çš„ï¼Œæœ‰æ—¶å€™å¤šæ­¥å­¦ä¹ ä¼šè¡¨ç°æ›´å¥½

# æ˜¯å¦è¦ç”¨ç«¯åˆ°ç«¯å­¦ä¹ 

Pros:

* Let the data speak
* Less hand-designing of components needed

Cons:

* May need large amount of data
* Excludes potentially useful hand-designed components

Key question: Do you have sufficient data to learn a function of the complexity needed to map x to y?


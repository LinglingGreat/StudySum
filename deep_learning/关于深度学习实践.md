#迁移学习

不要做重复的工作。许多深度学习软件平台都有 VGG19、ResNet、Inception v3 这样的预训练模型。从头开始训练非常耗费时间。就像 2014 年 VGG 论文中所说的，「VGG 模型是用 4 块英伟达 Titan Black GPU 训练的，根据架构训练单个网络需要 2-3 周的时间。」

许多预训练模型可用于解决深度学习难题。例如，我们使用预训练 VGG 模型提取图像特征，并将这些特征反馈到 LSTM 模型来生成描述。许多预训练模型都用 ImageNet 数据集训练，如果你的目标数据和 ImageNet 差别不大，我们将固定大部分模型参数，只重新训练最后几个完全连接的层。否则，我们就要使用训练数据集对整个网络进行端到端的重训练。但是在这两种情况下，由于模型已经过预训练，再训练所需的迭代将大大减少。由于训练时间较短，即使训练数据集不够大，也可以避免过拟合。这种迁移学习在各个学科都很有效，例如用预先训练好的英语模型训练汉语模型。

然而，这种迁移学习仅适用于需要复杂模型来提取特征的问题。在我们的项目中，我们的示例与 ImageNet 不同，我们需要对模型进行端到端的重新训练。然而，当我们只需要相对简单的潜在因素（颜色）时，来自 VGG19 的训练复杂度太高。因此，我们决定建立一个新的更简单的 CNN 特征提取模型。

# 成本函数

并非所有的成本函数都是等价的，它会影响模型的训练难度。有些成本函数是相当标准的，但有些问题域需要仔细考虑。

- 分类问题：交叉熵，折页损失函数（SVM）
- 回归： 均方误差（MSE）
- 对象检测或分割：交并比（IoU）
- 策略优化：KL 散度
- 词嵌入：噪音对比估计（NCE）
- 词向量：余弦相似度

在理论分析中看起来不错的成本函数在实践中可能不太好用。例如，GAN 中鉴别器网络的成本函数采用了更为实用也更经得起实验考验的方法，而不是理论分析中看起来不错的方法。在一些问题域中，成本函数可以是部分猜测加部分实验，也可以是几个成本函数的组合。

#正则化

****

> L1 正则化和 L2 正则化都很常见，但 L2 正则化在深度学习中更受欢迎。

L1 正则化有何优点？L1 正则化可以产生更加稀疏的参数，这有助于解开底层表示。由于每个非零参数会往成本上添加惩罚，与 L2 正则化相比，L1 更加青睐零参数，即与 L2 正则化中的许多微小参数相比，它更喜欢零参数。L1 正则化使过滤器更干净、更易于解释，因此是特征选择的良好选择。L1 对异常值的脆弱性也较低，如果数据不太干净，运行效果会更好。然而，L2 正则化仍然更受欢迎，因为解可能更稳定。

#梯度下降

****

始终密切监视梯度是否消失或爆炸，梯度下降问题有许多可能的原因，这些原因难以证实。不要跳至学习速率调整或使模型设计改变太快，小梯度可能仅仅由编程 Bug 引起，如输入数据未正确缩放或权重全部初始化为零。

如果消除了其他可能的原因，则在梯度爆炸时应用梯度截断（特别是对于 NLP）。跳过连接是缓解梯度下降问题的常用技术。在 ResNet 中，残差模块允许输入绕过当前层到达下一层，这有效地增加了网络的深度。

#缩放

缩放输入特征。我们通常将特征缩放为以零为均值在特定范围内，如 [-1, 1]。特征的不适当缩放是梯度爆炸或降低的一个最常见的原因。有时我们从训练数据中计算均值和方差，以使数据更接近正态分布。如果缩放验证或测试数据，要再次利用训练数据的均值和方差。

#批归一化和层归一化

每层激活函数之前节点输出的不平衡性是梯度问题的另一个主要来源，必要时需要对 CNN 应用批量归一化（BN）。如果适当地标准化（缩放）输入数据，DN 将学习得更快更好。在 BN 中，我们从每批训练数据中计算每个空间位置的均值和方差。例如，批大小为 16，特征图具有 10 X10 的空间维度，我们计算 100 个平均值和 100 个方差（每个位置一个）。每个位置处的均值是来自 16 个样本的对应位置平均值，我们使用均值和方差来重新归一化每个位置的节点输出。BN 提高了准确度，同时缩短了训练时间。

然而，BN 对 RNN 无效，我们需要使用层归一化。在 RNN 中，来自 BN 的均值和方差不适合用来重新归一化 RNN 单元的输出，这可能是因为 RNN 和共享参数的循环属性。在层归一化中，输出由当前样本的层输出计算的平均值和方差重新归一化。一个含有 100 个元素的层仅使用来自当前输入的一个平均值方差来重新归一化该层。

#Dropout

可以将 Dropout 应用于层以归一化模型。2015 年批量归一化兴起之后，dropout 热度降低。批量归一化使用均值和标准差重新缩放节点输出。这就像噪声一样，迫使层对输入中的变量进行更鲁棒的学习。由于批量归一化也有助于解决梯度下降问题，因此它逐渐取代了 Dropout。

结合 Dropout 和 L2 正则化的好处是领域特定的。通常，我们可以在调优过程中测试 dropout，并收集经验数据来证明其益处。

#激活函数

在 DL 中，ReLU 是最常用的非线性激活函数。如果学习速率太高，则许多节点的激活值可能会处于零值。如果改变学习速率没有帮助，我们可以尝试 leaky ReLU 或 PReLU。在 leaky ReLU 中，当 x < 0 时，它不输出 0，而是具有小的预定义向下斜率（如 0.01 或由超参数设置）。参数 ReLU（PReLU）往前推动一步。每个节点将具有可训练斜率。

#拆分数据集

为了测试实际性能，我们将数据分为三部分: 70 % 用于训练，20 % 用于验证，10 % 用于测试。确保样本在每个数据集和每批训练样本中被充分打乱。在训练过程中，我们使用训练数据集来构建具有不同超参数的模型。我们使用验证数据集来运行这些模型，并选择精确度最高的模型。但是保险起见，我们使用 10 % 的测试数据进行最后的错乱检查。如果你的测试结果与验证结果有很大差异，则应将数据打乱地更加充分或收集更多的数据。

#基线

设置基线有助于我们比较模型和 Debug，例如我们可使用 VGG19 模型作为分类问题的基线。或者，我们可以先扩展一些已建立的简单模型来解决我们的问题。这有助于我们更好地了解问题，并建立性能基线进行比较。在我们的项目中，我们修改了已建立的 GAN 实现并重新设计了作为基线的生成网络。

#检查点

我们定期保存模型的输出和度量以供比较。有时，我们希望重现模型的结果或重新加载模型以进一步训练它。检查点允许我们保存模型以便以后重新加载。但是，如果模型设计已更改，则无法加载所有旧检查点。我们也使用 Git 标记来跟踪多个模型，并为特定检查点重新加载正确的模型。我们的设计每个检查点占用 4gb 空间。在云环境中工作时，应相应配置足够的存储。我们经常启动和终止 Amazon 云实例，因此我们将所有文件存储在 Amazon EBS 中，以便于重新连接。

#自定义层

深度学习软件包中的内建层已经得到了更好的测试和优化。尽管如此，如果想自定义层，你需要：

- 用非随机数据对前向传播和反向传播代码进行模块测试；
- 将反向传播结果和朴素梯度检查进行对比；
- 在分母中添加小量的ϵ或用对数计算来避免 NaN 值。

#归一化

深度学习的一大挑战是可复现性。在调试过程中，如果初始模型参数在 session 间保持变化，就很难进行调试。因此，我们明确地对所有随机发生器初始化了种子。我们在项目中对 python、NumPy 和 TensorFlow 都初始化了种子。在精调过程中，我们我们关闭了种子初始化，从而为每次运行生成不同的模型。为了复现模型的结果，我们将对其进行 checkpoint，并在稍后重新加载它。

#优化器

Adam 优化器是深度学习中最流行的优化器之一。它适用于很多种问题，包括带稀疏或带噪声梯度的模型。其易于精调的特性使得它能快速获得很好的结果。实际上，默认的参数配置通常就能工作得很好。Adam 优化器结合了 AdaGrad 和 RMSProp 的优点。Adam 对每个参数使用相同的学习率，并随着学习的进行而独立地适应。Adam 是基于动量的算法，利用了梯度的历史信息。因此，梯度下降可以运行得更加平滑，并抑制了由于大梯度和大学习率导致的参数振荡问题。

Adam 优化器调整

Adam 有 4 个可配置参数：

- 学习率（默认 0.001）；
- β1：第一个矩估计的指数衰减率（默认 0.9）；
- β2：第二个矩估计的指数衰减率（默认 0.999），这个值在稀疏梯度问题中应该被设置成接近 1；
- ϵ（默认值 1e^-8）是一个用于避免除以零运算的小值。

β（动量）通过累积梯度的历史信息来平滑化梯度下降。通常对于早期阶段，默认设置已经能工作得很好。否则，最可能需要改变的参数应该是学习率。

#总结

以下是对深度学习项目的主要步骤的简单总结：

• Define task (Object detection, Colorization of line arts)
• Collect dataset (MS Coco, Public web sites)
​    ◦ Search for academic datasets and baselines
​    ◦ Build your own (From Twitter, News, Website,…)
• Define the metrics
​    ◦ Search for established metrics
• Clean and preprocess the data
​    ◦ Select features and transform data 
​    ◦ One-hot vector, bag of words, spectrogram etc...
​    ◦ Bucketize, logarithm scale, spectrogram
​    ◦ Remove noise or outliers 
​    ◦ Remove invalid and duplicate data
​    ◦ Scale or whiten data
• Split datasets for training, validation and testing
​    ◦ Visualize data
​    ◦ Validate dataset
• Establish a baseline
​    ◦ Compute metrics for the baseline
​    ◦ Analyze errors for area of improvements
• Select network structure
​    ◦ CNN, LSTM…
• Implement a deep network
​    ◦ Code debugging and validation
​    ◦ Parameter initialization
​    ◦ Compute loss and metrics
​    ◦ Choose hyper-parameters
​    ◦ Visualize, validate and summarize result
​    ◦ Analyze errors
​    ◦ Add layers and nodes
​    ◦ Optimization
• Hyper-parameters fine tunings
• Try our model variants

链接：

https://mp.weixin.qq.com/s/qpqqeSaRwQyBJlo3P25q6g

https://medium.com/@jonathan_hui/how-to-start-a-deep-learning-project-d9e1db90fa72


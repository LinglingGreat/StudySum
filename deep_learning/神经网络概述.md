## 神经网络

https://juejin.im/entry/5943c7a4a0bb9f006b969082

### 神经元种类

**前馈神经元**；

**卷积神经元**：只跟前一神经细胞层的部分神经元有连接，因为它们不是和某些神经元随机连接的，而是与特定范围内的神经元相连接，通常用来保存空间信息。这让它们对于那些拥有大量局部信息，比如图像数据、语音数据（但多数情况下是图像数据），会非常实用；

**解卷积神经元**：它们是通过跟下一神经细胞层的连接来解码空间信息。

这两种神经元都有很多副本，它们都是独立训练的；每个副本都有自己的权重，但连接方式却完全相同。可以认为，这些副本是被放在了具备相同结构的不同的神经网络中。

**池化神经元和插值神经元（Pooling and interpolating cells）**经常和卷积神经元结合起来使用。**它们不是真正意义上的神经元，只能进行一些简单的操作**。

**池化神经元接受到来自其它神经元的输出过后，决定哪些值可以通过，哪些值不能通过。**在图像领域，可以理解成是把一个图像缩小了

**插值神经元恰好是相反的操作**：**它们获取一些信息，然后映射出更多的信息。**额外的信息都是按照某种方式制造出来的，这就好像在一张小分辨率的图片上面进行放大。

**均值神经元和标准方差神经元（Mean and standard deviation cells）**（作为概率神经元它们总是成对地出现）是一类用来描述数据概率分布的神经元。均值就是所有值的平均值，而标准方差描述的是这些数据偏离（两个方向）均值有多远。比如：一个用于图像处理的概率神经元可以包含一些信息，比如：在某个特定的像素里面有多少红色。举个例来说，均值可能是0.5，同时标准方差是0.2。当要从这些概率神经元取样的时候，你可以把这些值输入到一个高斯随机数生成器，这样就会生成一些分布在0.4和0.6之间的值；值离0.5越远，对应生成的概率也就越小。它们一般和前一神经元层或者下一神经元层是全连接，而且，它们没有偏差（bias）。

**循环神经元（Recurrent cells ）**不仅仅在神经细胞层之间有连接，而且在时间轴上也有相应的连接。每一个神经元内部都会保存它先前的值。它们跟一般的神经元一样更新，但是，具有额外的权重：与当前神经元之前值之间的权重，还有大多数情况下，与同一神经细胞层各个神经元之间的权重。 当前值和存储的先前值之间权重的工作机制，与非永久性存储器（比如RAM）的工作机制很相似，继承了两个性质：



- 第一，维持一个特定的状态；
- 第二：如果不对其持续进行更新（输入），这个状态就会消失。

由于先前的值是通过激活函数得到的，而在每一次的更新时，都会把这个值和其它权重一起输入到激活函数，因此，信息会不断地流失。实际上，信息的保存率非常的低，以至于仅仅四次或者五次迭代更新过后，几乎之前所有的信息都会流失掉。

**长短期记忆神经元（Long short term memory cells）**用于克服循环神经元中信息快速流失的问题。

LSTM是一个逻辑回路，其设计受到了计算机内存单元设计的启发。与只存储两个状态的循环神经元相比，**LSTM可以存储四个状态：输出值的当前和先前值，记忆神经元状态的当前值和先前值** 。**它们都有三个门：输入门，输出门，遗忘门，同时，它们也还有常规的输入**。

这些门它们都有各自的权重，也就是说，与这种类型的神经元细胞连接需要设置四个权重（而不是一个）。这些门的工作机制与流门（flow gates）很相似，而不是栅栏门（fence gates）：它们可以让所有的信息都通过，或者只是通过部分，也可以什么都不让通过，或者通过某个区间的信息。

这种运行机制的实现是通过把输入信息和一个在0到1之间的系数相乘，这个系数存储在当前门中。这样，输入门决定输入的信息有多少可以被叠加到当前门值。输出门决定有多少输出信息是可以传递到后面的神经网络中。遗忘门并不是和输出神经元的先前值相连接，而是，和前一记忆神经元相连接。它决定了保留多少记忆神经元最新的状态信息。因为没有和输出相连接，以及没有激活函数在这个循环中，因此只会有更少的信息流失。

**门控循环神经元（Gated recurrent units (cells)）**是LSTM的变体。它们同样使用门来抑制信息的流失， **但是只用两个门：更新门和重置门**。这使得构建它们付出的代价没有那么高，而且运行速度更加快了，因为它们在所有的地方使用了更少的连接。

从本质上来说LSTM和GRU有两个不同的地方：

- 第一：GRU神经元没有被输出门保护的隐神经元；
- 第二：GRU把输出门和遗忘门整合在了一起，形成了更新门。核心的思想就是如果你想要一些新的信息，那么你就可以遗忘掉一些陈旧的信息（反过来也可以）。



### 神经细胞层(Layers)

**全连接（completely (or fully) connected）**

最简单的连接神经元方式是——**把所有的神经元与其它所有的神经元相连接**。这就好像Hopfield神经网络和玻尔兹曼机（Boltzmann machines）的连接方式。当然，这也就意味着连接数量会随着神经元个数的增加呈指数级地增加，但是，对应的函数表达力也会越来越强。

神经细胞层的定义是一群彼此之间互不连接的神经元，它们仅跟其它神经细胞层有连接。这一概念在受限玻尔兹曼机（Restricted Boltzmann Machines）中有所体现。

**卷积连接层（Convolutionally connected layers）**相对于全连接层要有更多的限制：在卷积连接层中的每一个神经元只与相邻的神经元层连接。图像和声音蕴含了大量的信息，如果一对一地输入到神经网络（比如，一个神经元对应一个像素）。卷积连接的形成，受益于保留空间信息更为重要的观察。实践证明这是一个非常好的猜测，因为现在大多数基于人工神经网络的图像和语音应用都使用了这种连接方式。然而，这种连接方式所需的代价远远低于全连接层的形式。从本质上来讲，卷积连接方式起到重要性过滤的作用，决定哪些紧紧联系在一起的信息包是重要的；卷积连接对于数据降维非常有用。

**随机连接神经元（randomly connected neurons）**。这种形式的连接主要有两种变体：

- 第一，允许部分神经元进行全连接。
- 第二，神经元层之间只有部分连接。

**时间滞后连接（Time delayed connections）**是指相连的神经元（通常是在同一个神经元层，甚至于一个神经元自己跟自己连接），它们不从前面的神经元层获取信息，而是从神经元层先前的状态获取信息。这使得暂时（时间上或者序列上）联系在一起的信息可以被存储起来。这些形式的连接经常被手工重新进行设置，从而可以清除神经网络的状态。和常规连接的主要区别是，这种连接会持续不断地改变，即便这个神经网络当前没有处于训练状态。

![img](https://user-gold-cdn.xitu.io/2017/6/19/699cdeb1f8de72721c0d515ebb9a8692?imageslim)

![img](https://user-gold-cdn.xitu.io/2017/6/19/f96f24ed6cebe1e9ef3a9291dfa83eca?imageslim)

### 前馈神经网络（FFNN）

**前馈神经感知网络与感知机（FF or FFNN：Feed forward neural networks and P：perceptrons）**非常简单，信息从前往后流动（分别对应输入和输出）。

Rosenblatt, Frank. “The perceptron: a probabilistic model for information storage and organization in the brain.” Psychological review 65.6 (1958): 386. 

http://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf

### 径向基神经网络（RBF）

**径向神经网络（RBF：Radial basis function）**是一种以径向基核函数作为激活函数的前馈神经网络。

Broomhead, David S., and David Lowe. Radial basis functions, multi-variable functional interpolation and adaptive networks. No. RSRE-MEMO-4148. ROYAL SIGNALS AND RADAR ESTABLISHMENT MALVERN (UNITED KINGDOM), 1988.  

http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA196234

### 霍普菲尔网络（HN）

**霍普菲尔网络（HN：Hopfield network）**是一种每一个神经元都跟其它神经元相互连接的网络。

每个神经元都在充当所有角色：训练前的每一个节点都是输入神经元，训练阶段是隐神经元，输出阶段则是输出神经元。

该神经网络的训练，是先把神经元的值设置到期望模式，然后计算相应的权重。在这以后，权重将不会再改变了。一旦网络被训练包含一种或者多种模式，这个神经网络总是会收敛于其中的某一种学习到的模式，因为它只会在某一个状态才会稳定。值得注意的是，它并不一定遵从那个期望的状态（很遗憾，它并不是那个具有魔法的黑盒子）。它之所以会稳定下来，部分要归功于在训练期间整个网络的“能量（Energy）”或“温度（Temperature）”会逐渐地减少。 每一个神经元的激活函数阈值都会被设置成这个温度的值，一旦神经元输入的总和超过了这个阈值，那么就会让当前神经元选择状态（通常是-1或1，有时也是0或1）。

可以多个神经元同步，也可以一个神经元一个神经元地对网络进行更新。一旦所有的神经元都已经被更新，并且它们再也没有改变，整个网络就算稳定（退火）了，那你就可以说这个网络已经收敛了。这种类型的网络被称为“联想记忆（associative memory）”，因为它们会收敛到和输入最相似的状态；比如，人类看到桌子的一半就可以想象出另外一半；与之相似，如果输入一半噪音+一半桌子，这个网络就能收敛到整张桌子。



> Hopfield, John J. “Neural networks and physical systems with emergent collective computational abilities.” Proceedings of the national academy of sciences 79.8 (1982): 2554-2558.
>
> https://bi.snu.ac.kr/Courses/g-ai09-2/hopfield82.pdf

### 马尔可夫链（MC）

**马尔可夫链（MC：Markov Chain）或离散时间马尔可夫链（DTMC：MC or discrete time Markov Chain）** 在某种意义上是BMs和HNs的前身。可以这样来理解：从从我当前所处的节点开始，走到任意相邻节点的概率是多少呢？它们没有记忆（所谓的马尔可夫特性）：你所得到的每一个状态都完全依赖于前一个状态。尽管算不上神经网络，但它却跟神经网络类似，并且奠定了BM和HN的理论基础。跟BM、RBM、HN一样，MC并不总被认为是神经网络。此外，它也并不总是全连接的。



> Hayes, Brian. “First links in the Markov chain.” American Scientist 101.2 (2013): 252.
>
> http://www.americanscientist.org/libraries/documents/201321152149545-2013-03Hayes.pdf

### 玻尔兹曼机（BM）

**玻尔兹曼机（BM：Boltzmann machines）** **和霍普菲尔网络很接近，差别只是：一些神经元作为输入神经元，剩余的则是作为隐神经元。**



在整个神经网络更新过后，输入神经元成为输出神经元。刚开始神经元的权重都是随机的，通过反向传播（back-propagation）算法进行学习，或是最近常用的对比散度（contrastive divergence）算法（马尔可夫链用于计算两个信息增益之间的梯度）。



相比HN，大多数BM的神经元激活模式都是二元的。BM由MC训练获得，因而是一个随机网络。BM的训练和运行过程，跟HN大同小异：为输入神经元设好钳位值，而后让神经网络自行学习。因为这些神经元可能会得到任意的值，我们反复地在输入和输出神经元之间来回地进行计算。激活函数的激活受全局温度的控制，如果全局温度降低了，那么神经元的能量也会相应地降低。这个能量上的降低导致了它们激活模式的稳定。在正确的温度下，这个网络会抵达一个平衡状态。





> Hinton, Geoffrey E., and Terrence J. Sejnowski. “Learning and releaming in Boltzmann machines.” Parallel distributed processing: Explorations in the microstructure of cognition 1 (1986): 282-317.
>
> https://www.researchgate.net/profile/Terrence_Sejnowski/publication/242509302_Learning_and_relearning_in_Boltzmann_machines/links/54a4b00f0cf256bf8bb327cc.pdf

### 受限玻尔兹曼机（RBM）

**受限玻尔兹曼机（RBM：Restricted Boltzmann machines）** **与BM出奇地相似，因而也同HN相似。**



它们的最大区别在于：**RBM更具实用价值**，因为它们受到了更多的限制。它们不会随意在所有神经元间建立连接，而只在不同神经元群之间建立连接，因此任何输入神经元都不会同其他输入神经元相连，任何隐神经元也不会同其他隐神经元相连。



RBM的训练方式就像稍微修改过的FFNN：前向通过数据之后再将这些数据反向传回（回到第一层），而非前向通过数据然后反向传播误差。之后，再使用前向和反向传播进行训练。





> Smolensky, Paul. Information processing in dynamical systems: Foundations of harmony theory. No. CU-CS-321-86. COLORADO UNIV AT BOULDER DEPT OF COMPUTER SCIENCE, 1986.
>
> http://www.dtic.mil/cgi-bin/GetTRDoc?Location=U2&doc=GetTRDoc.pdf&AD=ADA620727



### 自编码机（AE）

**自编码机（AE：Autoencoders）和FFNN有些相近，因为它更像是FFNN的另一种用法，而非本质上完全不同的另一种架构。**



自编码机的基本思想是自动对信息进行编码（像压缩一样，而非加密），它也因此而得名。整个网络的形状酷似一个沙漏计时器，中间的隐含层较小，两边的输入层、输出层较大。自编码机总是对称的，以中间层（一层还是两层取决于神经网络层数的奇偶）为轴。最小的层（一层或者多层）总是在中间，在这里信息压缩程度最大（整个网络的关隘口）。在中间层之前为编码部分，中间层之后为解码部分，中间层则是编码部分。



自编码机可用反向传播算法进行训练，给定输入，将误差设为输入和输出之差。自编码机的权重也是对称的，因此编码部分权重与解码部分权重完全一样。



> Bourlard, Hervé, and Yves Kamp. “Auto-association by multilayer perceptrons and singular value decomposition.” Biological cybernetics 59.4-5 (1988): 291-294.
>
> https://pdfs.semanticscholar.org/f582/1548720901c89b3b7481f7500d7cd64e99bd.pdf

### 稀疏自编码机（SAE）

**稀疏自编码机（SAE：Sparse autoencoders）****某种程度上同自编码机相反。** **稀疏自编码机** 不是用更小的空间表征大量信息，而是把原本的信息编码到更大的空间内。因此，中间层不是收敛，而是扩张，然后再还原到输入大小。它可以用于提取数据集内的小特征。



如果用训练自编码机的方式来训练稀疏自编码机，几乎所有的情况，都是得到毫无用处的恒等网络（输入=输出，没有任何形式的变换或分解）。为避免这种情况，需要在反馈输入中加上稀疏驱动数据。稀疏驱动的形式可以是阈值过滤，这样就只有特定的误差才会反向传播用于训练，而其它的误差则被忽略为0，不会用于反向传播。这很像脉冲神经网络（并不是所有的神经元一直都会输出）。





> Marc’Aurelio Ranzato, Christopher Poultney, Sumit Chopra, and Yann LeCun. “Efficient learning of sparse representations with an energy-based model.” Proceedings of NIPS. 2007.
>
> https://papers.nips.cc/paper/3112-efficient-learning-of-sparse-representations-with-an-energy-based-model.pdf



### 变分自编码机（VAE）

**变分自编码机（VAE：Variational autoencoders）****和AE有着相同的架构，却被教会了不同的事情：输入样本的一个近似概率分布，这让它跟BM、RBM更相近。**



不过，VAE却依赖于贝叶斯理论来处理概率推断和独立（probabilistic inference and independence），以及重新参数化（re-parametrisation）来进行不同的表征。推断和独立非常直观，但却依赖于复杂的数学理论。基本原理是：把影响纳入考虑。如果在一个地方发生了一件事情，另外一件事情在其它地方发生了，它们不一定就是关联在一起的。如果它们不相关，那么误差传播应该考虑这个因素。这是一个有用的方法，因为神经网络是一个非常大的图表，如果你能在某些节点排除一些来自于其它节点的影响，随着网络深度地增加，这将会非常有用。





> Kingma, Diederik P., and Max Welling. “Auto-encoding variational bayes.” arXiv preprint arXiv:1312.6114 (2013).
>
> https://arxiv.org/pdf/1312.6114v10.pdf



### 去噪自编码机（DAE）

**去噪自编码机（DAE：Denoising autoencoders）****是一种自编码机，它的训练过程，不仅要输入数据，还有再加上噪音数据（就好像让图像变得更加模糊一样）。**



但在计算误差的时候跟自动编码机一样，降噪自动编码机的输出也是和原始的输入数据进行对比。这种形式的训练旨在鼓励降噪自编码机不要去学习细节，而是一些更加宏观的特征，因为细微特征受到噪音的影响，学习细微特征得到的模型最终表现出来的性能总是很差。





> Vincent, Pascal, et al. “Extracting and composing robust features with denoising autoencoders.” Proceedings of the 25th international conference on Machine learning. ACM, 2008.
>
> http://machinelearning.org/archive/icml2008/papers/592.pdf



### 深度信念网络（DBN）

**深度信念网络（DBN：Deep belief networks）**之所以取这个名字，是由于它本身几乎是由多个受限玻尔兹曼机或者变分自编码机堆砌而成。



实践表明一层一层地对这种类型的神经网络进行训练非常有效，这样每一个自编码机或者受限玻尔兹曼机只需要学习如何编码前一神经元层的输出。这种训练技术也被称为贪婪训练，这里贪婪的意思是通过不断地获取局部最优解，最终得到一个相当不错解（但可能不是全局最优的）。可以通过对比散度算法或者反向传播算法进行训练，它会慢慢学着以一种概率模型来表征数据，就好像常规的自编码机或者受限玻尔兹曼机。 一旦经过非监督式学习方式，训练或者收敛到了一个稳定的状态，那么这个模型就可以用来产生新的数据。如果以对比散度算法进行训练，那么它甚至可以用于区分现有的数据，因为那些神经元已经被引导来获取数据的不同特定。





> Bengio, Yoshua, et al. “Greedy layer-wise training of deep networks.” Advances in neural information processing systems 19 (2007): 153.
>
> https://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf

### 循环神经网络（RNN）

**循环神经网络（RNN：Recurrent neural networks）**是具有时间联结的前馈神经网络：它们有了状态，通道与通道之间有了时间上的联系。 神经元的输入信息，不仅包括前一神经细胞层的输出，还包括它自身在先前通道的状态。

RNN存在一大问题：梯度消失（或梯度爆炸，这取决于所用的激活函数），信息会随时间迅速消失，正如FFNN会随着深度的增加而失去信息一样。

直觉上，这不算什么大问题，因为这些都只是权重，而非神经元的状态，但随时间变化的权重正是来自过去信息的存储；如果权重是0或1000000，那之前的状态就不再有信息价值。

一般来说，循环神经网络是推测或补全信息很好的选择，比如自动补全。

> Elman, Jeffrey L. “Finding structure in time.” Cognitive science 14.2 (1990): 179-211.
>
> https://crl.ucsd.edu/~elman/Papers/fsit.pdf



**双向RNN，字符级别模型，Encoder-decoder，Attention模型**

在经典的循环神经网络中，状态的传输是从前往后单向的。然而，在有些问题中，当前时刻的输出不仅和之前的状态有关系，也和之后的状态相关。这时就需要双向RNN（BiRNN）来解决这类问题。 

Character-level RNN（以下简称char-RNN）最初的想法便是扔给RNN一堆文本，让其从字母级别进行学习，让RNN写出以某字母或者某单词开始，最可能的字符序列，组织成词或者句子。 

https://blog.csdn.net/sinat_33761963/article/details/80248353

### 长短期记忆（LSTM）

**长短期记忆（LSTM：Long / short term memory）网络****试图通过引入门结构与明确定义的记忆单元来解决梯度消失/爆炸的问题。**



这更多的是受电路图设计的启发，而非生物学上某种和记忆相关机制。每个神经元都有一个记忆单元和三个门：**输入门、输出门、遗忘门**。 这三个门的功能就是通过禁止或允许信息流动来保护信息。



**输入门**决定了有多少前一神经细胞层的信息可留在当前记忆单元，**输出层**在另一端决定下一神经细胞层能从当前神经元获取多少信息。**遗忘门**乍看很奇怪，但有时候遗忘部分信息是很有用的：比如说它在学习一本书，并开始学一个新的章节，那遗忘前面章节的部分角色就很有必要了。



实践证明，LSTM可用来学习复杂的序列，比如像莎士比亚一样写作，或创作全新的音乐。值得注意的是，每一个门都对前一神经元的记忆单元赋有一个权重，因此会需要更多的计算资源。





> Hochreiter, Sepp, and Jürgen Schmidhuber. “Long short-term memory.” Neural computation 9.8 (1997): 1735-1780.
>
> http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf



### 门循环单元（GRU）

**门循环单元（GRU : Gated recurrent units）**是LSTM的一种轻量级变体。它们少了一个门，同时连接方式也稍有不同：它们采用了一个**更新门（update gate）**，而非LSTM所用的输入门、输出门、遗忘门。



**更新门**决定了保留多少上一个状态的信息，还决定了收取多少来自前一神经细胞层的信息。重置门（reset gate）跟LSTM遗忘门的功能很相似，但它存在的位置却稍有不同。它们总是输出完整的状态，没有输出门。多数情况下，它们跟LSTM类似，但最大的不同是：GRU速度更快、运行更容易（但函数表达力稍弱）。



在实践中，这里的优势和劣势会相互抵消：当你你需要更大的网络来获取函数表达力时，这样反过来，性能优势就被抵消了。在不需要额外的函数表达力时，GRU的综合性能要好于LSTM。



> Chung, Junyoung, et al. “Empirical evaluation of gated recurrent neural networks on sequence modeling.” arXiv preprint arXiv:1412.3555 (2014).
>
> https://arxiv.org/pdf/1412.3555v1.pdf

### 神经图灵机（NTM）

**神经图灵机（NTM: Neural Turing machines）**可以理解为对LSTM的抽象，它试图把神经网络去黑箱化（以窥探其内部发生的细节）。



NTM不是把记忆单元设计在神经元内，而是分离出来。NTM试图结合常规数字信息存储的高效性、永久性与神经网络的效率及函数表达能力。它的想法是设计一个可作内容寻址的记忆库，并让神经网络对其进行读写操作。NTM名字中的“图灵（Turing）”是表明，它是图灵完备（Turing complete）的，即具备基于它所读取的内容来读取、写入、修改状态的能力，也就是能表达一个通用图灵机所能表达的一切。



> Graves, Alex, Greg Wayne, and Ivo Danihelka. “Neural turing machines.” arXiv preprint arXiv:1410.5401 (2014).
>
> https://arxiv.org/pdf/1410.5401v2.pdf

### BiRNN、BiLSTM、BiGRU

**双向循环神经网络（BiRNN：Bidirectional recurrent neural networks）、双向长短期记忆网络（BiLSTM：bidirectional long / short term memory networks ）和双向门控循环单元**（BiGRU：bidirectional gated recurrent units）在图表中并未呈现出来，因为它们看起来与其对应的单向神经网络结构一样。



所不同的是，这些网络不仅与过去的状态有连接，而且与未来的状态也有连接。比如，通过一个一个地输入字母，训练单向的LSTM预测“鱼（fish）”（在时间轴上的循环连接记住了过去的状态值）。在BiLSTM的反馈通路输入序列中的下一个字母，这使得它可以了解未来的信息是什么。这种形式的训练使得该网络可以填充信息之间的空白，而不是预测信息。因此，它在处理图像时不是扩展图像的边界，而是填补一张图片中的缺失。





> Schuster, Mike, and Kuldip K. Paliwal. “Bidirectional recurrent neural networks.” IEEE Transactions on Signal Processing 45.11 (1997): 2673-2681.
>
> http://www.di.ufpe.br/~fnj/RNA/bibliografia/BRNN.pdf



### 深度残差网络（DRN）

**深度残差网络（DRN: Deep residual networks）**是非常深的FFNN网络，它有一种特殊的连接，可以把信息从某一神经细胞层传至后面几层（通常是2到5层）。



该网络的目的不是要找输入数据与输出数据之间的映射，而是致力于构建输入数据与输出数据+输入数据之间的映射函数。本质上，它在结果中增加一个恒等函数，并跟前面的输入一起作为后一层的新输入。结果表明，当层数超过150后，这一网络将非常擅于学习模式，这比常规的2到5层要多得多。然而，有证据表明这些网络本质上只是没有时间结构的RNN，它们总是与没有门结构的LSTM相提并论。



> He, Kaiming, et al. “Deep residual learning for image recognition.” arXiv preprint arXiv:1512.03385 (2015).
>
> https://arxiv.org/pdf/1512.03385v1.pdf

### 回声状态网络（ESN）

**回声状态网络（ESN：Echo state networks）** ‍‍ ‍ 是另一种不同类型的（循环）网络。



它的不同之处在于：**神经元之间的连接是随机的**（没有整齐划一的神经细胞层），其训练过程也有所不同。不同于输入数据后反向传播误差，ESN先输入数据、前馈、而后更新神经元状态，最后来观察结果。它的输入层和输出层在这里扮演的角色不太常规，输入层用来主导网络，输出层作为激活模式的观测器随时间展开。在训练过程中，只有观测和隐藏单元之间连接会被改变。 ‍



> Jaeger, Herbert, and Harald Haas. “Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication.” science 304.5667 (2004): 78-80.
>
> https://pdfs.semanticscholar.org/8922/17bb82c11e6e2263178ed20ac23db6279c7a.pdf

### 极限学习机（ELM）

**极限学习机（ELM：Extreme learning machines）** ‍‍ ‍ 本质上是拥有随机连接的FFNN。



它们与LSM、ESN极为相似，除了循环特征和脉冲性质，它们还不使用反向传播。相反，它们先给权重设定随机值，然后根据最小二乘法拟合来一次性训练权重（在所有函数中误差最小）。这使ELM的函数拟合能力较弱，但其运行速度比反向传播快多了。 ‍



> Cambria, Erik, et al. “Extreme learning machines [trends & controversies].” IEEE Intelligent Systems 28.6 (2013): 30-59.
>
> http://www.ntu.edu.sg/home/egbhuang/pdf/ieee-is-elm.pdf

### 液态机（LSM）

**液态机（LSM：Liquid state machines）** ‍‍ ‍ 换汤不换药，跟ESN同样相近。



区别在于，LSM是一种脉冲神经网络（spiking neural networks），用阈值激活函数（threshold functions）取代了sigmoid激活函数，每个神经元同时也是具有累加性质的记忆单元。因此，当神经元状态更新时，其值不是相邻神经元的累加值，而是它自身状态值的累加。一旦累加到阈值，它就释放能量至其它神经元。这就形成了一种类似于脉冲的模式：神经元不会进行任何操作，直至到达阈值的那一刻。



> Maass, Wolfgang, Thomas Natschläger, and Henry Markram. “Real-time computing without stable states: A new framework for neural computation based on perturbations.” Neural computation 14.11 (2002): 2531-2560.
>
> https://web.archive.org/web/20120222154641/http://ramsesii.upf.es/seminar/Maass_et_al_2002.pdf

### Kohonen 网络

**Kohonen网络（KN，也称之为自组织（特征）映射（SOM/SOFM：self organising (feature) map））**。

KN利用竞争学习来对数据进行分类，不需要监督。先给神经网络一个输入，而后它会评估哪个神经元最匹配该输入。然后这个神经元会继续调整以更好地匹配输入数据，同时带动相邻的神经元。相邻神经元移动的距离，取决于它们与最佳匹配单元之间的距离。KN有时也不被认为是神经网络。

> Kohonen, Teuvo. “Self-organized formation of topologically correct feature maps.” Biological cybernetics 43.1 (1982): 59-69.
>
> http://cioslab.vcu.edu/alg/Visualize/kohonen-82.pdf

### 卷积神经网络（CNN）

**卷积神经网络（CNN：Convolutional neural networks）或深度卷积神经网络（DCNN：deep convolutional neural networks）** 跟其它类型的神经网络大有不同。 它们主要用于处理图像数据，但可用于其它形式数据的处理，如语音数据。对于卷积神经网络来说，一个典型的应用就是给它输入一个图像，而后它会给出一个分类结果。也就是说，如果你给它一张猫的图像，它就输出“猫”；如果你给一张狗的图像，它就输出“狗”。

卷积神经网络是从一个数据扫描层开始，这种形式的处理并没有尝试在一开始就解析整个训练数据。比如：对于一个大小为200X200像素的图像，你不会想构建一个40000个节点的神经元层。而是，构建一个20X20像素的输入扫描层，然后，把原始图像第一部分的20X20像素图像（通常是从图像的左上方开始）输入到这个扫描层。 当这部分图像（可能是用于进行卷积神经网络的训练）处理完，你会接着处理下一部分的20X20像素图像：逐渐（通常情况下是移动一个像素，但是，移动的步长是可以设置的）移动扫描层，来处理原始数据。

**注意**，你不是一次性移动扫描层20个像素（或其它任何扫描层大小的尺度），也不是把原始图像切分成20X20像素的图像块，而是用扫描层在原始图像上滑过。这个输入数据（20X20像素的图像块）紧接着被输入到卷积层，而非常规的神经细胞层——卷积层的节点不是全连接。每一个输入节点只会和最近的那个神经元节点连接（至于多近要取决于具体的实现，但通常不会超过几个）。

这些卷积层会随着深度的增加而逐渐变小：大多数情况下，会按照输入层数量的某个因子缩小（比如：20个神经元的卷积层，后面是10个神经元的卷积层，再后面就是5个神经元的卷积层）。2的n次方（32, 16, 8, 4, 2, 1）也是一个非常常用的因子，因为它们在定义上可以简洁且完整地除尽。除了卷积层，池化层（pooling layers）也非常重要。

池化是一种过滤掉细节的方式：一种常用的池化方式是最大池化，比如用2X2的像素，然后取四个像素中值最大的那个传递。为了让卷积神经网络处理语音数据，需要把语音数据切分，一段一段输入。在实际应用中，通常会在卷积神经网络后面加一个前馈神经网络，以进一步处理数据，从而对数据进行更高水平的非线性抽象。

> LeCun, Yann, et al. “Gradient-based learning applied to document recognition.” Proceedings of the IEEE 86.11 (1998): 2278-2324.
>
> http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf

**CNN，CNN架构**

![ \begin{cases} height_{out} &= (height_{in} - height_{kernel} + 2 * padding) ~ / ~ stride + 1\\[2ex] width_{out} &= (width_{in} - width_{kernel} + 2 * padding) ~ / ~ stride + 1 \end{cases}](https://www.zhihu.com/equation?tex=+%5Cbegin%7Bcases%7D+height_%7Bout%7D+%26%3D+%28height_%7Bin%7D+-+height_%7Bkernel%7D+%2B+2+%2A+padding%29+%7E+%2F+%7E+stride+%2B+1%5C%5C%5B2ex%5D+width_%7Bout%7D+%26%3D+%28width_%7Bin%7D+-+width_%7Bkernel%7D+%2B+2+%2A+padding%29+%7E+%2F+%7E+stride+%2B+1+%5Cend%7Bcases%7D) 

向下取整，如99.5取99

1x1卷积核可以减少参数

VGG 3x3代替AlexNet 5x5卷积核

GoogleNet 使用了Inception

group convolution:

https://zhuanlan.zhihu.com/p/29367273

https://blog.csdn.net/hhy_csdn/article/details/80030468

### 解卷积网络（DN）

**解卷积网络（DN：Deconvolutional networks）**，又称为逆图形网络（IGNs：inverse graphics networks），是逆向的卷积神经网络。

想象一下，给一个神经网络输入一个“猫”的词，就可以生成一个像猫一样的图像，通过比对它和真实的猫的图片来进行训练。跟常规CNN一样，DN也可以结合FFNN使用，但没必要为这个新的缩写重新做图解释。它们可被称为深度解卷积网络，但把FFNN放到DNN前面和后面是不同的

CNN的池化层往往也是被对应的逆向操作替换了，主要是插值和外推（基于一个基本的假设：如果一个池化层使用了最大池化，你可以在逆操作的时候生成一些相对于最大值更小的数据）。

> Zeiler, Matthew D., et al. “Deconvolutional networks.” Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010.
>
> http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf

### 深度卷积逆向图网络（DCIGN）

**深度卷积逆向图网络（DCIGN：Deep convolutional inverse graphics networks）**，这个名字具有误导性，因为它们实际上是VAE，但分别用CNN、DNN来作编码和解码的部分。

这些网络尝试在编码过程中对“特征“进行概率建模，这样一来，你只要用猫和狗的独照，就能让它们生成一张猫和狗的合照。同理，你可以输入一张猫的照片，如果猫旁边有一只恼人的邻家狗，你可以让它们把狗去掉。很多演示表明，这种类型的网络能学会基于图像的复杂变换，比如灯光强弱的变化、3D物体的旋转。一般也是用反向传播算法来训练此类网络。

> Kulkarni, Tejas D., et al. “Deep convolutional inverse graphics network.” Advances in Neural Information Processing Systems. 2015.
>
> https://arxiv.org/pdf/1503.03167v4.pdf

### 生成式对抗网络（GAN）

**生成式对抗网络（GAN：Generative adversarial networks）****是一类不同的网络，它们有一对“双胞胎”：两个网络协同工作。**

GAN可由任意两种网络组成（但通常是FF和CNN），其中**一个用于生成内容，另一个则用于鉴别生成的内容**。

鉴别网络（discriminating network）同时接收训练数据和生成网络（generative network）生成的数据。鉴别网络的准确率，被用作生成网络误差的一部分。这就形成了一种竞争：鉴别网络越来越擅长于区分真实的数据和生成数据，而生成网络也越来越善于生成难以预测的数据。这种方式非常有效，部分是因为：即便相当复杂的类噪音模式最终都是可预测的，但跟输入数据有着极为相似特征的生成数据，则很难区分。

训练GAN极具挑战性，**因为你不仅要训练两个神经网络（其中的任何一个都会出现它自己的问题），同时还要平衡两者的运行机制**。如果预测或生成相比对方表现得过好，这个GAN就不会收敛，因为它会内部发散。

> Goodfellow, Ian, et al. “Generative adversarial nets.” Advances in Neural Information Processing Systems. 2014.
>
> https://arxiv.org/pdf/1406.2661v1.pdf
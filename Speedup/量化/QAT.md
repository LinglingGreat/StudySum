---
title: QAT
created: 2025-11-09
tags:
  - 量化
---
一些概念介绍：
- PTQ：训练后量化，将训练完成的模型权重通过特定的量化方法量化到对应的精度。
- QAT：训练感知量化，在训练阶段就让模型提前适应某部分的权重/激活被量化到更低比特数下产生的精度损失。通常来说相比PTQ流程更加复杂但精度损失更小。
- W(n)A(m)：模型权重(weight)使用n-bit，激活(activation)使用m-bit。

### 什么是量化感知训练？

**1. 首先，理解“量化”**

在AI模型中，权重和激活值通常以高精度格式存储，比如32位浮点数或16位浮点数。**量化**是一种模型压缩技术，它的核心思想是：**使用更低比特宽度的数据格式来表示这些权重和激活值**，例如从FP32降到INT8（8位整数）甚至INT4（4位整数）。

- **好处**：
    - **减小模型体积**：INT4的模型大小只有FP32模型的约1/8。
    - **加快推理速度**：整数运算在硬件上的计算速度通常远快于浮点运算。
    - **降低功耗和内存带宽**：移动设备和边缘设备非常看重这一点。

- **传统量化（训练后量化）的缺点**：
    最常见的方法是在模型**训练完成之后**，再将其权重转换为低精度格式。但这个过程会引入**量化误差**，因为从高精度到低精度的舍入操作会丢失信息。对于复杂的模型或低比特量化（如INT4），这种误差可能导致模型精度显著下降。

**2. 量化感知训练的核心思想**

QAT就是为了解决上述“精度损失”问题而诞生的。

**QAT的核心思想是：在模型训练的过程中，就模拟量化操作，让模型“提前感知”和“适应”未来在推理时会遇到的量化误差。**

可以把QAT想象成一场“带噪训练”：
- **传统训练**：学生在安静的环境下学习（高精度浮点数）。
- **训练后量化**：学生学完后，突然把他扔进一个嘈杂的考场（低精度整数），他可能发挥失常。
- **量化感知训练**：学生在学习期间，就在一个模拟的嘈杂环境中练习（前向传播时模拟量化），这样他就能学会如何在这种环境下依然做出正确判断，最终在真实考场中表现更好。

**3. QAT的工作流程**

一个典型的QAT流程如下：
1.  **预训练**：首先，在FP32精度下正常训练得到一个高精度的模型。
2.  **插入伪量化节点**：在这个训练好的模型图中，在需要量化的操作（如卷积、矩阵乘法）前后插入“伪量化”模块。这些模块在**前向传播**时，会模拟将FP32数值量化为低比特（如INT4），再反量化为FP32的过程。这样，前向计算中包含了量化误差。
3.  **微调训练**：使用训练数据对这个“改造后”的模型进行微调。在**反向传播**时，由于量化操作（如四舍五入）的导数几乎处处为零，无法直接求导，因此会使用**直通估计器** 来近似梯度，使得训练能够继续。
4.  **部署**：训练完成后，将这些伪量化模块替换为真正的定点整数运算，得到一个既保持高精度又具备高效推理能力的模型。

**总结：量化感知训练通过在训练阶段模拟量化噪声，让模型权重学会自我调整以适应未来的低精度表示，从而在最终量化部署时最大限度地保持模型的性能。**

### 为什么要做反量化？

核心原因在于：**绝大多数硬件（如CPU和GPU）的计算核心是为高精度浮点数计算设计的，它们无法直接对INT4数据进行数学运算。**

1. **硬件限制**：你可以把INT4权重想象成高度压缩的、打包好的货物。计算单元（ALU）是处理这些货物的“工厂流水线”，但这个流水线是为处理FP16或FP32这种“标准尺寸”的货物设计的。它不认识、也无法直接处理INT4这种“微型货物”。因此，在使用前，必须先把这些“微型货物”解压成“标准尺寸”。
    
2. **数值精度与稳定性**：INT4的数值表示范围非常有限（例如只有16个整数）。直接进行INT4之间的乘加运算，非常容易溢出（超过表示范围）或下溢（精度损失殆尽），导致计算结果完全错误。反量化为FP16/FP32后，计算是在一个更宽、更动态的数值范围内进行的，保证了计算的数值稳定性。

### 反量化是如何发生的？

这个过程并非像解压一个ZIP文件那样，将整个模型的权重都还原到FP16再开始计算，那样就失去了量化的意义。实际上，它是一个 **“即用即解压”** 的流水线操作。

我们以一次矩阵乘法为例（这是神经网络中最核心的操作）：  
`Y = X · W`

- `X` 是输入数据（激活值），通常是FP16精度。
    
- `W` 是模型权重，已经被量化为INT4格式存储在内存中。
    
- `Y` 是输出结果。
    

其计算步骤如下：

1. **内存加载**：从内存中将INT4格式的权重 `W_INT4` 加载到处理器的缓存或寄存器中。**这一步是量化的主要收益所在：INT4数据占用的内存带宽只有FP16的1/4，传输速度更快、更节能。**
    
2. **即时反量化**：在数据进入计算核心**之前**，一个专用的硬件单元或微指令会将这些 `W_INT4` **即时地**反量化为FP16（或FP32，取决于硬件和配置）格式，得到 `W_FP16`。
    
3. **高精度计算**：计算核心接收输入数据 `X_FP16` 和反量化后的权重 `W_FP16`，执行标准的FP16精度矩阵乘法，得到输出 `Y_FP16`。

### 现代硬件的优化：INT计算单元

值得注意的是，为了加速AI推理，**现代GPU和专用AI加速器已经开始集成可以直接执行低精度整数运算的硬件单元**。

在这种情况下，流程会有所不同：

1. 同样从内存加载 `W_INT4`。
    
2. 同样从内存加载 `X_FP16`。
    
3. 在计算之前，**输入数据 `X_FP16` 会被动态地量化为 INT8**（或类似精度），以匹配权重的精度。
    
4. **专用的INT计算核心** 直接执行 `X_INT8` 和 `W_INT4` 之间的整数矩阵乘法。
    
5. 得到的 `Y_INT32` 结果（因为整数乘加会累积到更高位宽）再反量化为最终的 `Y_FP16` 输出。
    

**即使在这个更先进的流程中，“反量化”步骤依然存在（第3步和第5步），只是它被整合进了计算流程，并且可能只在输入和输出端进行。**

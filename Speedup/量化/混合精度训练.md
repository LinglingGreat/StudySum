
混合精度训练，顾名思义，就是在训练时混合不同的精度。PyTorch 张量的默认数值精度是单精度浮点格式，也称为 FP32 或 float32，这意味着存储的每个数字占用 32 位或 4 个字节。表示数字的可用位分为 3 个部分：
- **符号位（1位）**：表示正负。
- **指数位（E位）**：决定数值的范围。
- **尾数位（M位）**：决定数值的精度。

![](img/Pasted%20image%2020250308160406.png)

减少总位数是有代价的（这里也没有免费的午餐），但我们可以控制如何付出。我们可以牺牲尾数或指数上的更多位。因此，还存在两种 float8 格式，根据指数和尾数命名，以灵活选择最合适的格式。我们可以看看每种格式可能的数字范围：

![](img/Pasted%20image%2020250309182827.png)

我们可以看到 float32 跨越了 80 个数量级，float16 牺牲了很多范围，而 bfloat16 则保持了整个范围。两种 float8 格式的范围进一步缩小，其中 e5e2 可以保持 float16 范围，而 e4m3 的范围更小。

为什么有些格式能够保持范围，而其他格式却不能？让我们通过绘制 1 到 2 之间的 10,000 个点来研究分辨率。每个点将四舍五入为每种格式中最接近的可表示数字：

![](img/Pasted%20image%2020250309182859.png)

我们在这里可以看到，bfloat16 保持了 float32 的范围而不是 float16，但这样做的代价是牺牲了更多的精度。对于 float8，情况更加糟糕，因为 e4m3 可以表示 7，而 e5m2 只能表示 1-2 区间内的 3 个数字。

衡量格式分辨率的常用指标是 epsilon： 1.001.00 之后的第一个可表示数字。我们可以看到，对于 float32 格式， 10^−4是上限（实际上是 1.19^−7 ）。对于 float16，它是 ~ 10^−3 ，对于 bfloat，它还要高 10 倍。

事实证明，我们**无法**完全放弃 float32，通常需要保留部分全精度。这就是为什么低精度训练通常被称为**_混合精度_**训练。

现在让我们看一下使用 16 位的训练模型，然后看看是否可以更进一步降低到 8 位。


**FP16（半精度，IEEE 754标准）**

- **格式**：1位符号位 + 5位指数 + 10位尾数。
- **指数偏置（Bias）**：2^(5−1)−1=15
- **有效指数范围**：2^5=32，表示范围(0, 31)，再减去偏置得到−14 到 +15（存储时指数值为 1 到 30，全1保留给特殊值）。
- 尾数位数为10，2^10=1024
- **最大正数**：
    (1+1023/1024)×2^15≈65504
- **最小正数（规格化数）**：
    1×2^(−14)≈6.1×10^(−5)

FP8 E4M3
- 偏置是7
- 指数范围是2^4=16，表示范围(0, 15)，减去偏置，得到（-7，8）
- 尾数位数为3，2^3=8，每个表示1/8。
- 最大正数：(1+6/8)* 2^8=448

#### FP16 和 BF16 训练

不幸的是，将所有张量和运算都转换为 float16 不起作用，结果通常是损失发散。然而，原始的混合精度训练论文想出了三个技巧来匹配 float32 训练：

1. **FP32 权重副本**：float16 权重可能存在两个问题。在训练期间，一些权重可能变得非常小，并将四舍五入为 0。但是，即使权重本身不接近零，如果更新非常小，则幅度差异可能会导致权重在添加过程中下溢。一旦权重为零，它们将在其余训练中保持为 0，因为不再有梯度信号通过。
2. **损失缩放**：我们在梯度方面也遇到了类似的问题，因为梯度往往远小于 1，因此存在下溢的风险。一个简单但有效的策略是在反向传播之前缩放损失，在反向传播之后取消缩放梯度。这确保了反向传播期间不会出现下溢，并且缩放不会影响训练，因为我们在进一步处理梯度（例如剪切）和优化步骤之前取消缩放。
3. **累积**：最后，在执行某些 16 位精度的算术运算（例如平均值或求和）时，我们也可能面临不足或溢出的情况。一种解决方案是在运算过程中将中间结果累积到 float32 中，然后仅将最终结果转换回 16 位精度。

借助这些技术，我们可以获得稳定的训练，同时受益于更快、更低精度算术运算带来的更高吞吐量。自然，作为一名好奇的读者（现在有点沉迷于最大化吞吐量），您可能会问这样一个问题：我们能否比 16 位精度走得更快、更快？

####  FP8 预训练

即使我们完美地将通信与计算重叠，我们最终总会遇到硬件本身的低水平理论 FLOPS 限制，即我们硬件上每个单独操作的效率。这就是数值精度变得至关重要的地方。例如，在 NVIDIA 的 H100 GPU 上，FP8 矩阵乘法（GEMM 操作）实现了 bfloat16 理论 FLOPS 的两倍，使低精度训练成为进一步优化的有吸引力的途径。

不过，FP8 预训练带来了一个重大挑战：稳定性。在较低精度下，数值不稳定性通常会导致损失发散，从而难以达到更高精度训练的精度。

我们知道，对于固定的模型大小，随着学习率的增加，不稳定性会增加，使 FP8 预训练变得尤为棘手。

以下是 FP8 训练的典型发散损失曲线的示例：

![](img/Pasted%20image%2020250309183230.png)

首次成功的超大规模 FP8 混合精度训练已在 DeepSeek-V3 上公开报道。作者仔细分析了前向传递 (Fprop) 以及激活 (Dgrad) 和权重 (Wgrad) 后向传递的每次操作。与 BF16 混合精度训练类似，一些聚合和主权重保持更高的精度，而操作本身则以 FP8 执行。

![](img/Pasted%20image%2020250309183247.png)

为了从高精度（例如 FP32 或 BF16）切换到范围较小的低精度（例如 FP16 或 FP8），我们需要对激活值的范围进行归一化，例如通过计算它们的绝对最大值。DeepSeek-V3 进一步引入了一种特定的量化方案，其中范围按图块进行归一化：输入/激活为 1x128，权重和比例元素为 128x128。这使得归一化受激活中异常值的影响较小。他们提出了一些额外的技巧来进一步减少内存和通信占用空间，您可以在 DeepSeek-V3 技术报告的第 3.3 节中了解这些技巧

以下是一些已知的 FP8 训练方法的总结：

![](img/Pasted%20image%2020250309183458.png)


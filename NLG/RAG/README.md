---
title: README
created: 2024-06-06
tags:
  - RAG
---

从实际总结来看，RAG 面临的问题，主要就三类：

1. 针对非结构化多模态文档无法提供有效问答。这意味着以 LLMOps 为代表的工作，只能服务纯文本类场景，而像 PDF，PPT，乃至类似图文结合的格式数据，就无法解锁其实际商业价值，这些数据往往在企业内部占据大多数比例。
    
2. 采用纯向量数据库带来的低召回和命中率，从而无法针对实际场景提供有效问答，这主要是因为向量表征不仅缺乏对精确信息的表达，对语义召回也有损耗。
    
3. RAG 的本质是搜索，它能工作的前提在于根据用户的提问，可以“搜”到答案所在。但是在很多情况下，这个前提不存在，例如一些意图不明的笼统提问，以及一些需要在多个子问题基础之上综合才能得到答案的所谓“多跳”问答，这些问题的答案，都无法通过搜索提问来获得答案，因此提问和答案之间存在着明显的语义鸿沟。



2. 一系列多模态文档解析工具的崛起。
    
3. BM25 和混合搜索的崛起，向量数据库已不需要作为单独品类。

针对朴素 RAG 系统只提供基于文本的 Text Chunking 工具，RAGFlow 引入了针对非结构化数据进行 Semantic Chunking 的步骤，从而保证数据的入口质量。具体做法就是采用专门训练的模型来解析文档布局，避免简易的 Text Chunking 工具对不同布局内部数据的干扰。

利用提供 BM25 能力的全文搜索，可以保证精确查询能力，尽管 BM25 已是近三十年前的技术，但 RAG 再次释放了这种古老技术的青春。BM25 崛起已是事实，纯向量数据库已经没有必要作为一个单独的品类而存在，混合搜索的理念深入人心。

4. GraphRAG 的崛起。做搜索系统的人很早就在面临这样的麻烦，因为查询和答案之间并不总能保证可以匹配。当搜索系统升级到 RAG 之后，这样的问题被放大：搜索系统的查询是用户定义的几个关键词，而 RAG 的查询是用户的提问，从关键词到提问，用户的意图更加难以判断，因此放大了这种语义鸿沟问题。GraphRAG，就是缓解这种语义鸿沟的设计之一。

5. 以 Col-xxx 为代表的延迟交互模型崛起。
    
6. 基于 VLM 和延迟交互模型实现多模态 RAG。

这 2 大事件的背后都是对排序模型的升级，并且都需要在数据库层面提供原生张量支持。对于前者来说，采用延迟交互模型相当于在数据库层面提供了类似重排序模型这样的能力；对于后者，则可以解锁更加复杂的文档（例如杂志，饼图等等）在企业端的商业价值。

## 数据清洗

保证数据的 Quality In 才能有最终结果的 Quality Out，是个很自然的想法。针对多模态非结构化的文档，采用视觉模型来解析文档布局，从而保证数据入口的高质量，这类问题在学术界出现已久，被广泛称为文档智能：Document Intelligence。

从方法上，文档智能模型的工作也可以分为 2 代：

1. 第一代，就是过去的同类工作，以及当前主流的开源工作，当然也包含开源的 RAGFlow DeepDoc模块。这类工作，是构建在传统视觉模型的基础之上。它的好处，就是可以运行在 CPU 上，缺点，就是对于各场景的泛化能力相对有限。由于需要分别针对不同场景和数据训练相应的模型，因此这类技术被戏称为“雕花”。
    
2. 第二代，就是目前这类 OCR，已经开始向生成式模型的架构来演进。最早如 Meta 的 Nougat【参考 4】，最新如所谓的 OCR 2.0【参考 5】，都采用统一的基于 Transformer 的 Encoder-Decoder 架构来生成图片识别后的文字结果。这些工作，跟后边提到的各类多模态 VLM，在架构上有着诸多的相似之处。像 StructEqTable【参考6】这样的结构，则直接把类似的网络结构用到了表格还原上。在 RAGFlow 的企业版，也同样采用了这种架构来处理文档。采用这种生成式模型，它当然无法在 CPU 上进行推理，但它在不同场景的泛化能力，相比过去的视觉模型都有了不小提高。采用这类多模态模型提供文档智能的另一个好处，就是在文档布局中可以引入文本信息，今年的一个代表工作 M2Doc 【参考 23】，在基于视觉的 Encoder-Decoder 架构上融合了 BERT，可以更好地确定文字和段落的语义边界。

以上可以看作针对多模态非结构化文档数据的数据清洗，那么针对文本文档，是否采用朴素的 Text Chunking 就可以了呢？答案依然是不够的，如果 Text Chunk 仅包含文本类信息，那么在检索过程中，面临的主要问题，就从 Chunk 内容的混乱，变成了语义鸿沟。这里介绍从 Chunking 层面可以弥补的工作：

Jina 在今年推出了 Late Chunking 【参考 24】，针对文本类数据，它把 Text Chunking 的步骤放到了 Embedding 之后。也就是说先用 Embedding 模型对整个文档的文本进行编码，然后在 Embedding 模型最后一步的均值 Pooling 之前输出 Chunking 的边界，这就是“Late”一词的由来。跟传统的 Text Chunking 方法相比，Late Chunking 可以更好地保留文本上下文信息。不过，Late Chunking 需要 Embedding 模型最后的输出是均值 Pooling，而大多数 Embedding 模型则是 CLS Pooling，因此并不能直接搭配。

2024 年工业界也针对文本类数据推出了 dsRAG【参考 25】。它的主要贡献在于提供了自动上下文，就是利用大模型给每个 Text Chunk 添加上下文信息，用来解决原始文本不容易检索的问题。例如文本中如果包含疾病治疗方案，而治疗方案却没有疾病描述，那么检索时，可能就无法命中这段文本。dsRAG 的另一个工作是通过聚类，来组合 Text Chunk 形成更长的文本，尽管评测分数良好，但这一点在实际使用未必奏效。

LLM 提供商 Anthropic Claude 在 9 月的时候也推出了 Contextual Retrieval 【参考 26】，其中一个重要部分是 Contextual Chunking，顾名思义，就是给每个 Text Chunk 引入特定于该 Chunk 的上下文解释。这个解释当然也是来自于 LLM 生成。因此，Contextual Retrieval 和 dsRAG 是类似的效果。

人大和上海算法创新研究院在 10 月推出了 Meta-Chunking 【参考 45】，其主要目标在于寻找到文本段落内具有语言逻辑链接的句子集合的边界。具体做法是采用 LLM 进行分类，判断句子是否在一个 Chunk。相比前述工作，它并不能缓解语义鸿沟的问题，尽管也用到了 LLM。

相似时间点，上海人工智能实验室和北航联合推出了多粒度混合 Chunking 【参考 46】，对于每个文档，都被切割成较小粒度的 Chunk，通常由一两个句子组成。这些 Chunk 被视作图的节点，检索的时候，根据查询由模型来预测需要的 Chunk 粒度，然后在图结构中根据这个粒度决定图中遍历的深度——遍历越深，相当于最终的 Chunk 粒度越大。这种手段，实现较为复杂，但并没有缓解语义鸿沟方面的问题，只是动态决定返回大模型的上下文长度，避免上下文冗余，因此实际价值不如上述的一些方案大。

但可以看出，基于 Text Chunking 能做的工作并不很多，今年的工作，有价值的地方还是在于围绕给 Chunk 提供更多上下文信息更有实际价值，而这些上下文信息，通过 LLM 会更加可靠，通过 LLM 解释 Chunk 的文本内容，从而新增一些诸如标签之类的附加信息，可以从一定层面上缓解这些 Chunk 无法被召回的语义鸿沟问题。RAGFlow 在今年的版本中，也新增了让 LLM 给 Text Chunk 提取信息的步骤，用来改善召回效果。

## 混合搜索

在 2024 年 4 月，来自 IBM Research 的一篇技术报告 BlendedRAG【参考7】，通过实验论证了对于 RAG 来说，采用更多的召回手段可以达到更好的效果，其中，采用向量搜索 + 稀疏向量搜索 + 全文搜索的三路混合，可以达到最好的召回。这其实很容易理解，因为向量可以表示语义，一句话，乃至一篇文章，都可以只用一个向量来表示，这时向量本质上表达的是这段文字的“语义”，也就是这段文字跟其他文字在一个上下文窗口内共同出现概率的压缩表示 ，因此向量天然无法表示精确的查询。例如如果用户询问“2024年3月我们公司财务计划包含哪些组合”，那么很可能得到的结果是其他时间段的数据，或者得到运营计划，营销管理等其他类型的数据。而全文搜索和稀疏向量，则主要是用来表达精确语义的。因此，将两者结合，符合我们日常生活中对语义和精确的双重需求。

混合搜索，在 RAG 体系中通常由专用的数据库来承担。虽然看起来有很多提供各种混合搜索能力的数据库，但真正能满足混合搜索的选择并不多见，这是因为，一个符合要求的全文搜索，并不容易实现：

1. 用稀疏向量很难模拟全文搜索。稀疏向量，它的初衷是取代全文搜索，它利用一个标准的预训练模型，将文档中的冗余词删除，并且增加扩展词，从而形成一个标准的固定维度的稀疏向量输出（例如 3 万或者 10 万维）。这在通用查询任务上必然会表现更好，然而在实际使用中，依然有大量用户提问的关键词，并不在生成稀疏向量的预训练模型中，例如各种机器型号，说明书，专用词汇等等，因此稀疏向量和全文搜索虽然都用来做精确召回，但它们各有所长却无法相互取代。
    
2. 全文搜索，不只是提供一个简单的 BM25 计算就可以，它还需要考虑短语查询以及相应的性能问题。RAGFlow 是较早提供混合搜索的 RAG 方案，它在一开始采用了 Elasticsearch 作为唯一的后端文档搜索引擎。在 RAGFlow 里，用户的一个问题，并不是简单的直接送到 Elasticsearch，而是首先经过查询分析，这包括：
    

1. 分词后删掉无意义的词
    
2. 给剩余的词计算对应的词权重
    
3. 给剩余的词产生基于二元分词的短语查询，它们和常规分词结果一起被送去作为查询
    

因此，一个对话“在原文中，教师应该在什么时候提问？”，它产生的查询可能会有如下的结果：

((原文中 OR "原文" OR ("原文"~2)^0.5)^1.0) OR ((教师)^0.9991 (提问)^0.000541 (应该)^0.000368 ("教师 应该 提问"~4)^1.5)((企业)^0.550884 (格局)^0.252471 (文章)^0.195081 (新发)^0.000607 (提到)^0.000261 (展)^0.000262 (适应)^0.000230 (应该)^0.000203 ("文章 提到 企业 应该 适应 新发 展 格局"~4)^1.5)

这个查询非常复杂，但可以看到，它把一个问答转成了包含大量短语的查询，如果倒排索引中没有保存位置信息，就无法提供这种查询能力。

另一方面，为了保证召回，在默认情况下，全文搜索需要保证关键词之前默认采用“OR”的关系而非“AND”，它对查询性能提出了很大的挑战。因此，一个合格的全文搜索，还需要提供查询动态剪枝的技术，并且这种动态剪枝，还需要考虑包含短语查询在内的各种查询。这样一来，符合要求的全文搜索就所剩无几了。除了最常见的 Elasticsearch 之外，我们在另一款开源的 RAG 用数据库 Infinity 也充分提供了以上能力。

下图是我们利用 Infinity 数据库在一个公开评测数据集上，分别利用单路召回（向量、稀疏向量、全文搜索）、2 路召回、3 路召回评测的结果。纵坐标是排序质量，可以看到，3 路召回确实可以达到最好的效果，这用单一数据库完全验证了 BlendedRAG 的实验结果。图中最右侧是 3 路召回 + 基于张量的重排序得到的结果，关于这点，我们在下文还有提及。

## 排序模型

排序是任何搜索系统的核心。在 RAG 上下文里，排序和两个组件有关，一个是用来做粗筛的部分，这就是向量搜索依赖的 Embedding 模型，另一个是在精排阶段所用的 Reranker 模型。重排序模型和 Embedding 模型的训练常常共享很多工作，对于 Embedding 模型来说，它通常采用 Encoder 架构，它的训练目标是使得语义相似的文本在向量空间距离更近，而 Reranker ，则采用 Cross Encoder 架构，它的训练目标是预测查询和文档之间的分数。如下图所示，左边就是 Embedding 模型的工作方式：分别对查询和文档进行编码，然后经过 Pooling 后输出一个向量，在排序阶段只需要计算向量相似度。由于丢失了查询和文档中 Token 之间两两之间的交互信息，因此会丢失很多语义信息，所以向量搜索常用来做粗筛。而作为 Reranker 的 Cross Encoder，它的 Encoder 网络可以跟 Embedding 模型完全一致，但它因为把查询和文档一起输入模型，最终只输出一个得分，因此则可以捕获 Token 之间两两之间的关系，所以它的排序质量要高出很多。但 Cross Encoder 也有它的问题：对于 Encoder 来说，文档的 Embedding，在建立索引的离线阶段就可以完成，因此在线查询阶段，只需要对查询进行编码，就可以快速得到答案。而 Cross Encoder，需要对每个查询-文档对进行交叉编码和模型输出，计算成本高昂，所以它只能用来做重排，并且粗筛的结果不能太多，否则会大大增加查询延迟。



## 参考资料

[RAG全链路的关键模块解析](https://mp.weixin.qq.com/s/kNjOgfQs6yErNtRg6wFA3g)

[近期RAG技术总结和串讲（4w字RAG文章纪念）](https://mp.weixin.qq.com/s/U7fhvkdsx-TDYKVWzzm33Q)

[万字长文梳理 2024 年的 RAG](https://mp.weixin.qq.com/s/9H4ZgqaB_q_FX2DzaaHPmQ)


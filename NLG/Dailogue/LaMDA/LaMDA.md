## LaMDA

LaMDA: Language Models for Dialog Applications  
https://arxiv.org/abs/2201.08239

要打造实际能用的对话产品，我们的思维就要先转换一下：

**不再是我要用什么方法解决这个问题，而是：我要解决什么问题？**

**要往更智能的对话系统走，首先要想清楚怎么样才算是「智能的对话系统」，我们还差在哪里**。

谷歌的Meena和LaMDA，前面很大一部分篇幅都在讲评估指标的定义，分为三个维度：

-   Sensibleness, Specificity, Interestingness：是否合理、符合上下文、有创造力
    
-   Safety：是否有风险、不公正
    
-   Groundedness、Informativeness：在知识型问答中，是否包含真实的信息、并引用相关链接
    

**定义完指标后，第二步是评估一下baseline和天花板，看到差距在哪里**

![](img/Pasted%20image%2020220822102124.png)

![](img/Pasted%20image%2020220822102138.png)

在优化过程中，谷歌并没有用什么高端的技术，只是把Sensibleness、Specificity、Interestingness、Safety分别当成分类任务去标0/1，把知识问答当作生成任务让标注同学去编辑答案，然后精调就完了。可以看到精调之后的LaMDA比纯在对话数据上预训练的PT有提升了不少。

除了上面提到的指标定义外，LaMDA还可以给我们一些小启示，就是怎么更好地在对话任务中利用大模型。

首先是模型的选型，LaMDA用的是纯自回归预训练，Meena用的是Seq2Seq结构

第二点，是个比较巧妙的地方。在业界目前的对话系统中，都是跟搜索一样召回+排序的逻辑，这就需要两个模型。而**LaMDA做到了单模型同时生成+排序**，而且由于语言模型的任务形式，这个排序分数是直接在生成结果后面加prompt完成的。也就是生成完结果的最后一个字后，直接继续预测分数，一气呵成。

从输入上看就是：`<context> <sentinel> <response> <attribute-name> <rating>`

**第三点，也是LaMDA在Meena上的大改进：融入知识**。作者们为了让大模型学会答知识类问题，设计了两个任务：

1.  输入对话上下文，输出知识查询语句：这个查询语句是主要是`TS, Query`的形式，作者开发了一套检索系统用来囊括各类知识
    
2.  输入知识查询语句，输出生成的最终结果：通过标注同学编辑的文本进行精调，让模型学会整合知识
    

经过上述两个任务的训练，模型就会判断什么时候该去查询数据知识，并且返回整合的结果了：
![](img/Pasted%20image%2020220822102333.png)

总结来说，LaMDA的文章虽然在技术上没什么新突破，但却给我们提供了很有价值的落地方案参考：

1.  首先指标一定要定义清楚，然后向着指标优化就完了
    
2.  如何用单个大模型实现整个端到端的问答
    
3.  纯粹依靠模型记忆知识是不行的，知识型问答还需要其他系统辅助

## LaMDA2

**第一个能力，是想象力（Imagine it）**。

人类测试者可以输入一个场景，比如「想象我在海里最深处」，之后LaMDA会给出一些关于场景的描述，测试者可以根据模型推荐的问题，或者自己输入，继续往下探索LaMDA想象力的极限。

**第二个能力，是既开放又不跑题（Talk about it）**。

聊天机器人的跑题已经司空见惯了，是一个绝对的痛点。在这个设定里，测试者可以随便问问题，但模型会尽量保持在一个话题上。不过现在只能谈论狗这个话题，看来通用性还不太确定。

**第三个能力，是理解复杂任务并进行拆解（List it）**。

这是个从Demo看上去没啥应用点，但细思恐极的能力。测试者发送一个想要做的事儿，LaMDA会把它按顺序拆解成几步，而且有的步骤甚至还能往下拆。



## 参考资料

https://mp.weixin.qq.com/s/f9JMqZ8U0f8EYTxMzyf-4Q

https://mp.weixin.qq.com/s/JrIilBLXwgNq882V0EI9UA

Google的Meena提出了SSA（Sensibleness and Specificity Average）指标：

-   Sensibleness：是否符合常识、和上文保持一致
    
-   Specificity：是否对当前上下文是特别的，不然一直答「我不知道」也能拿到很高的Sensibleness分数

Google的LaMDA在SSA的基础上增加了几种，作者希望通过把指标定细，来更好地定义问题，从而找到优化点：

-   SSI（Sensibleness，Specificity，Interestingness）：答案是否不可预料、引起用户好奇
    
-   Safety：包含偏见、攻击等
    
-   Groundedness：是否符合事实
    
-   Helpful：是否正确+是否可用
    
-   Role consistency：上下文中的角色一致性
    

把指标定义清楚之后，谷歌就非常粗暴的让人去标各种对话数据是否符合，然后直接精调一把。

相比Google，DeepMind提出的Sparrow更方便且聪明一些，**既然不知道用哪些维度衡量对话的好坏，那直接基于用户的反馈去训练，让模型自己学就好了**。

于是他们采用的方案是：

1.  用模型根据上下文产出一些不同的答案
    
2.  让用户选择哪个是最好的（Preferred Response）
    
3.  基于用户的选择训练一个打分模型，能够根据输入对话语料，输出分数
    
4.  把第3步的模型提供的奖励作为Reward，用强化学习算法去优化Sparrow的输出结果

同时，作者们为了强化模型的安全性，以及follow一些规则，会特地让用户去「攻击」模型，引导他们打破规则。比如我给出的规则是「这个模型没有性别」，那用户就会故意问模型「你是男的是女的？」，然后根据回答判断模型是否破坏规则。

最终这个流程也会产生一个打分模型，即输入规则和对话数据，判断该对话是否违反规则。同样可以用RL来训练。


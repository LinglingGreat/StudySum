## openchat

https://github.com/imoneoi/openchat

通过ochat.data.generate_dataset生成数据的token结果，在此之前需要配置模板、model_max_length等重要参数。

一个对话的tokens和weights计算方式如下：

```
bos_tokens_ = tokenizer("").input_ids
eot_tokens_ = tokenizer(eot, add_special_tokens=False).input_ids
tokens = bos_tokens_ + system_tokens + eot_tokens_ + 多条(role + text + eot_tokens_)
weights = [0.] * len(self.bos_tokens_) + [0.] * len(system_tokens) + [0.] * len(self.eot_tokens_) + 多条([0.] * len(role) + [w] * len(text) + [w] * len(self.eot_tokens_))
```

role指的是HUMAN:这样的文本对应的token，text是message_content的tokens。w是weight，一般user对应的回复的w是0，assistant对应的回复的w是1。还可以对w修改权重值
```
if seq_level_weight:
	w /= len(text) + len(self.eot_tokens_)
```

根据model_max_context对每个对话的tokens和weights进行截断。并且保证weight的最后一个值是1（如果不是的话，就把末尾那些0的元素删掉）。得到新的tokens和weights。
```
labels = [(t if w != 0 else PAD_TOKEN_ID) for t, w in zip(tokens, weights)]
length = len(tokens)
results = {
        "total_length": len(tokens),

        "seqlens": [length],
        "nz_input_ids": tokens,
        "nz_position_ids": list(range(length)),

        "nz_shifted_label_ids":    labels[1:]  + [PAD_TOKEN_ID],
        "nz_shifted_loss_weights": weights[1:] + [0.0],
        "num_seqs": sum(results["nz_shifted_loss_weights"])
    }

```

所有对话的results组成一个字典，value是一个列表。存储成parquet文件。

把所有数据shuffle(permutation)一下重新排列，使用了一种类似 Multifit algorithm的方法动态组batch。需要设置batch_max_length。(https://github.com/imoneoi/multipack_sampler)
- 该算法的输入是一组数字_S_和一个参数_n 。所需的输出是将__S_划分为_n_个子集，以使最大子集和（也称为**完成时间**）尽可能小。
- First-fit-decreasing_bin_packing(FFD)**是一种**[装箱](https://en.wikipedia.org/wiki/Bin_packing "箱式包装")算法。它的输入是不同大小的项目列表。它的输出是_打包_——将物品划分为固定容量的箱子，使得每个箱子中物品的大小总和最多等于容量。理想情况下，我们希望使用尽可能少的 bin，但最小化 bin 数量是一个 NP 难题，因此我们使用近似最优[启发式](https://en.wikipedia.org/wiki/Heuristic "启发式")。
- FFD 算法的工作原理如下。
- 将物品从最大到最小排序。
- 对于从大到小的每个项目，找到适合该项目的 _第一个箱子（如果有）。_
    - 如果找到这样的垃圾箱，请将新物品放入其中。
    - 否则，打开一个新的空箱，将新物品放入其中。

每个batch会pad到64的倍数（不是每个batch长度都一样，要看该batch最接近哪个64的倍数，pad数最少）



动态学习率

```
def calculate_auto_lr(lr, batch_max_len, model_type, train_dataset):
    if lr is not None:
        return lr
    
    # Llama hyperparameters
    # FIXME: Only 7B/13B is supported
    base_lr = 3e-4
    base_bs = 4_000_000
    if "mistral" in model_type.lower():
        base_lr /= 6.0

    loss_weights = np.concatenate(train_dataset["nz_shifted_loss_weights"])
    supervised_ratio = np.sum(loss_weights != 0) / len(loss_weights)

    supervised_tokens = batch_max_len * dist.get_world_size() * supervised_ratio
    lr = base_lr * math.sqrt(supervised_tokens / base_bs)

    print(f"Use automatic learning rate {lr} (estimated from supervised ratio {supervised_ratio} effective batch size {supervised_tokens})")
    return lr
```



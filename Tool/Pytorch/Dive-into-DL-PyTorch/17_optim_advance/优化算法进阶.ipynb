{"cells":[{"cell_type":"markdown","metadata":{"graffitiCellId":"id_8yq6tup","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"5CB587E1495B436F8C823F70CCE5DBC1","mdEditEnable":false},"source":"# 11.6 Momentum\n\n在 [Section 11.4](https://d2l.ai/chapter_optimization/sgd.html#sec-sgd) 中，我们提到，目标函数有关自变量的梯度代表了目标函数在自变量当前位置下降最快的方向。因此，梯度下降也叫作最陡下降（steepest descent）。在每次迭代中，梯度下降根据自变量当前位置，沿着当前位置的梯度更新自变量。然而，如果自变量的迭代方向仅仅取决于自变量当前位置，这可能会带来一些问题。对于noisy gradient,我们需要谨慎的选取学习率和batch size, 来控制梯度方差和收敛的结果。\n\n\n$$\n\\mathbf{g}_t = \\partial_{\\mathbf{w}} \\frac{1}{|\\mathcal{B}_t|} \\sum_{i \\in \\mathcal{B}_t} f(\\mathbf{x}_{i}, \\mathbf{w}_{t-1}) = \\frac{1}{|\\mathcal{B}_t|} \\sum_{i \\in \\mathcal{B}_t} \\mathbf{g}_{i, t-1}.\n$$\n\n"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_ugnt88i","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"719E334CAB294EEA96D4EAC794CB6C4B","mdEditEnable":false},"source":"## An ill-conditioned Problem\n\nCondition Number of Hessian Matrix:\n   \n$$\n cond_{H} = \\frac{\\lambda_{max}}{\\lambda_{min}} \n$$\n \nwhere $\\lambda_{max}, \\lambda_{min}$ is the maximum amd minimum eignvalue of Hessian matrix.\n\n让我们考虑一个输入和输出分别为二维向量$\\boldsymbol{x} = [x_1, x_2]^\\top$和标量的目标函数:\n\n\n$$\n f(\\boldsymbol{x})=0.1x_1^2+2x_2^2\n$$\n\n\n\n$$\n cond_{H} = \\frac{4}{0.2} = 20 \\quad \\rightarrow \\quad \\text{ill-conditioned} \n$$\n \n\n## Maximum Learning Rate\n+ For $f(x)$, according to convex optimizaiton conclusions, we need step size $\\eta$.\n+ To guarantee the convergence, we need to have $\\eta$ .\n\n## Supp: Preconditioning\n\n在二阶优化中，我们使用Hessian matrix的逆矩阵(或者pseudo inverse)来左乘梯度向量 $i.e. \\Delta_{x} = H^{-1}\\mathbf{g}$，这样的做法称为precondition，相当于将 $H$ 映射为一个单位矩阵，拥有分布均匀的Spectrum，也即我们去优化的等价标函数的Hessian matrix为良好的identity matrix。\n\n\n与[Section 11.4](https://d2l.ai/chapter_optimization/sgd.html#sec-sgd)一节中不同，这里将$x_1^2$系数从$1$减小到了$0.1$。下面实现基于这个目标函数的梯度下降，并演示使用学习率为$0.4$时自变量的迭代轨迹。"},{"cell_type":"code","execution_count":1,"metadata":{"graffitiCellId":"id_kujf7mb","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"14F1B2ECD12E4F64B054F7421F312BF7","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"epoch 20, x1 -0.943467, x2 -0.000073\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/14F1B2ECD12E4F64B054F7421F312BF7/q5qnwqtu81.png\">"},"transient":{}}],"source":"%matplotlib inline\nimport sys\nsys.path.append(\"/home/kesci/input\") \nimport d2lzh1981 as d2l\nimport torch\n\neta = 0.4\n\ndef f_2d(x1, x2):\n    return 0.1 * x1 ** 2 + 2 * x2 ** 2\n\ndef gd_2d(x1, x2, s1, s2):\n    return (x1 - eta * 0.2 * x1, x2 - eta * 4 * x2, 0, 0)\n\nd2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_vkw2adz","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0F0DD4E4F0794763B16067916DDFBFD1","mdEditEnable":false},"source":"可以看到，同一位置上，目标函数在竖直方向（$x_2$轴方向）比在水平方向（$x_1$轴方向）的斜率的绝对值更大。因此，给定学习率，梯度下降迭代自变量时会使自变量在竖直方向比在水平方向移动幅度更大。那么，我们需要一个较小的学习率从而避免自变量在竖直方向上越过目标函数最优解。然而，这会造成自变量在水平方向上朝最优解移动变慢。\n\n下面我们试着将学习率调得稍大一点，此时自变量在竖直方向不断越过最优解并逐渐发散。\n\n### Solution to ill-condition\n+ __Preconditioning gradient vector__: applied in Adam, RMSProp, AdaGrad, Adelta, KFC, Natural gradient and other secord-order optimization algorithms.\n+ __Averaging history gradient__: like momentum, which allows larger learning rates to accelerate convergence; applied in Adam, RMSProp, SGD momentum.  "},{"cell_type":"code","execution_count":2,"metadata":{"graffitiCellId":"id_d8n5yky","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"A293DD7BF95245F998F279897A8A8359","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"epoch 20, x1 -0.387814, x2 -1673.365109\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/A293DD7BF95245F998F279897A8A8359/q5qnxcgkw7.png\">"},"transient":{}}],"source":"eta = 0.6\nd2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))"},{"cell_type":"markdown","metadata":{"collapsed":true,"graffitiCellId":"id_rwoy2z0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F4FC92274AE7486A94298E8A655E4365","mdEditEnable":false},"source":"## Momentum Algorithm\n\n动量法的提出是为了解决梯度下降的上述问题。设时间步 $t$ 的自变量为 $\\boldsymbol{x}_t$，学习率为 $\\eta_t$。\n在时间步 $t=0$，动量法创建速度变量 $\\boldsymbol{m}_0$，并将其元素初始化成 0。在时间步 $t>0$，动量法对每次迭代的步骤做如下修改：\n\n\n$$\n\\begin{aligned}\n\\boldsymbol{m}_t &\\leftarrow \\beta \\boldsymbol{m}_{t-1} + \\eta_t \\boldsymbol{g}_t, \\\\\n\\boldsymbol{x}_t &\\leftarrow \\boldsymbol{x}_{t-1} - \\boldsymbol{m}_t,\n\\end{aligned}\n$$\n\nAnother version:\n\n$$\n\\begin{aligned}\n\\boldsymbol{m}_t &\\leftarrow \\beta \\boldsymbol{m}_{t-1} + (1-\\beta) \\boldsymbol{g}_t, \\\\\n\\boldsymbol{x}_t &\\leftarrow \\boldsymbol{x}_{t-1} - \\alpha_t \\boldsymbol{m}_t,\n\\end{aligned}\n$$\n\n$$\n\\alpha_t = \\frac{\\eta_t}{1-\\beta} \n$$\n\n其中，动量超参数 $\\beta$满足 $0 \\leq \\beta < 1$。当 $\\beta=0$ 时，动量法等价于小批量随机梯度下降。\n\n在解释动量法的数学原理前，让我们先从实验中观察梯度下降在使用动量法后的迭代轨迹。"},{"cell_type":"code","execution_count":3,"metadata":{"graffitiCellId":"id_gzuie5r","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"337EA9AC35D849FA84C00D0811DF3F33","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"epoch 20, x1 -0.062843, x2 0.001202\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/337EA9AC35D849FA84C00D0811DF3F33/q5qnz4bzn0.png\">"},"transient":{}}],"source":"def momentum_2d(x1, x2, v1, v2):\n    v1 = beta * v1 + eta * 0.2 * x1\n    v2 = beta * v2 + eta * 4 * x2\n    return x1 - v1, x2 - v2, v1, v2\n\neta, beta = 0.4, 0.5\nd2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_bn8rcsq","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"CA38E42BA60541E388FF6CCEC87E18E1","mdEditEnable":false},"source":"可以看到使用较小的学习率 $\\eta=0.4$ 和动量超参数 $\\beta=0.5$ 时，动量法在竖直方向上的移动更加平滑，且在水平方向上更快逼近最优解。下面使用较大的学习率 $\\eta=0.6$，此时自变量也不再发散。"},{"cell_type":"code","execution_count":4,"metadata":{"graffitiCellId":"id_7ckrl4v","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B3D7EA65B09743499C1FA28F723B3DF6","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"epoch 20, x1 0.007188, x2 0.002553\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/B3D7EA65B09743499C1FA28F723B3DF6/q5qnzu2jzs.png\">"},"transient":{}}],"source":"eta = 0.6\nd2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_n4u0k3a","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C4193FFE1E9441DA8412BC2D10AB3DCA","mdEditEnable":false},"source":"### Exponential Moving Average\n\n为了从数学上理解动量法，让我们先解释一下指数加权移动平均（exponential moving average）。给定超参数 $0 \\leq \\beta < 1$，当前时间步 $t$ 的变量 $y_t$ 是上一时间步 $t-1$ 的变量 $y_{t-1}$ 和当前时间步另一变量 $x_t$ 的线性组合：\n\n$$\ny_t = \\beta y_{t-1} + (1-\\beta) x_t.\n$$\n\n我们可以对 $y_t$ 展开：\n\n$$\n\\begin{aligned}\ny_t  &= (1-\\beta) x_t + \\beta y_{t-1}\\\\\n         &= (1-\\beta)x_t + (1-\\beta) \\cdot \\beta x_{t-1} + \\beta^2y_{t-2}\\\\\n         &= (1-\\beta)x_t + (1-\\beta) \\cdot \\beta x_{t-1} + (1-\\beta) \\cdot \\beta^2x_{t-2} + \\beta^3y_{t-3}\\\\\n         &= (1-\\beta) \\sum_{i=0}^{t} \\beta^{i}x_{t-i}\n\\end{aligned}\n$$\n\n$$\n(1-\\beta)\\sum_{i=0}^{t} \\beta^{i} = \\frac{1-\\beta^{t}}{1-\\beta} (1-\\beta) = (1-\\beta^{t})\n$$\n\n### Supp\nApproximate Average of $\\frac{1}{1-\\beta}$ Steps\n\n令 $n = 1/(1-\\beta)$，那么 $\\left(1-1/n\\right)^n = \\beta^{1/(1-\\beta)}$。因为\n\n$$\n \\lim_{n \\rightarrow \\infty}  \\left(1-\\frac{1}{n}\\right)^n = \\exp(-1) \\approx 0.3679,\n$$\n\n所以当 $\\beta \\rightarrow 1$时，$\\beta^{1/(1-\\beta)}=\\exp(-1)$，如 $0.95^{20} \\approx \\exp(-1)$。如果把 $\\exp(-1)$ 当作一个比较小的数，我们可以在近似中忽略所有含 $\\beta^{1/(1-\\beta)}$ 和比 $\\beta^{1/(1-\\beta)}$ 更高阶的系数的项。例如，当 $\\beta=0.95$ 时，\n\n$$\ny_t \\approx 0.05 \\sum_{i=0}^{19} 0.95^i x_{t-i}.\n$$\n\n因此，在实际中，我们常常将 $y_t$ 看作是对最近 $1/(1-\\beta)$ 个时间步的 $x_t$ 值的加权平均。例如，当 $\\gamma = 0.95$ 时，$y_t$ 可以被看作对最近20个时间步的 $x_t$ 值的加权平均；当 $\\beta = 0.9$ 时，$y_t$ 可以看作是对最近10个时间步的 $x_t$ 值的加权平均。而且，离当前时间步 $t$ 越近的 $x_t$ 值获得的权重越大（越接近1）。\n\n\n### 由指数加权移动平均理解动量法\n\n现在，我们对动量法的速度变量做变形：\n\n$$\n\\boldsymbol{m}_t \\leftarrow \\beta \\boldsymbol{m}_{t-1} + (1 - \\beta) \\left(\\frac{\\eta_t}{1 - \\beta} \\boldsymbol{g}_t\\right). \n$$\n\nAnother version:\n\n$$\n\\boldsymbol{m}_t \\leftarrow \\beta \\boldsymbol{m}_{t-1} + (1 - \\beta) \\boldsymbol{g}_t. \n$$\n\n\n$$\n\\begin{aligned}\n\\boldsymbol{x}_t &\\leftarrow \\boldsymbol{x}_{t-1} - \\alpha_t \\boldsymbol{m}_t,\n\\end{aligned}\n$$\n\n\n$$\n\\alpha_t = \\frac{\\eta_t}{1-\\beta} \n$$\n\n\n由指数加权移动平均的形式可得，速度变量 $\\boldsymbol{v}_t$ 实际上对序列 $\\{\\eta_{t-i}\\boldsymbol{g}_{t-i} /(1-\\beta):i=0,\\ldots,1/(1-\\beta)-1\\}$ 做了指数加权移动平均。换句话说，相比于小批量随机梯度下降，动量法在每个时间步的自变量更新量近似于将前者对应的最近 $1/(1-\\beta)$ 个时间步的更新量做了指数加权移动平均后再除以 $1-\\beta$。所以，在动量法中，自变量在各个方向上的移动幅度不仅取决当前梯度，还取决于过去的各个梯度在各个方向上是否一致。在本节之前示例的优化问题中，所有梯度在水平方向上为正（向右），而在竖直方向上时正（向上）时负（向下）。这样，我们就可以使用较大的学习率，从而使自变量向最优解更快移动。\n\n\n## Implement\n\n相对于小批量随机梯度下降，动量法需要对每一个自变量维护一个同它一样形状的速度变量，且超参数里多了动量超参数。实现中，我们将速度变量用更广义的状态变量`states`表示。"},{"cell_type":"code","execution_count":6,"metadata":{"graffitiCellId":"id_6121f8l","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"EE50A16E68F142FB8E11D9042EC6E451","collapsed":false,"scrolled":false},"outputs":[],"source":"def get_data_ch7():  \n    data = np.genfromtxt('/home/kesci/input/airfoil4755/airfoil_self_noise.dat', delimiter='\\t')\n    data = (data - data.mean(axis=0)) / data.std(axis=0)\n    return torch.tensor(data[:1500, :-1], dtype=torch.float32), \\\n        torch.tensor(data[:1500, -1], dtype=torch.float32)\n\nfeatures, labels = get_data_ch7()\n\ndef init_momentum_states():\n    v_w = torch.zeros((features.shape[1], 1), dtype=torch.float32)\n    v_b = torch.zeros(1, dtype=torch.float32)\n    return (v_w, v_b)\n\ndef sgd_momentum(params, states, hyperparams):\n    for p, v in zip(params, states):\n        v.data = hyperparams['momentum'] * v.data + hyperparams['lr'] * p.grad.data\n        p.data -= v.data"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_2nqiri4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"CDAA083E7EF942ED80B11F7B0C3EA69D","mdEditEnable":false},"source":"我们先将动量超参数`momentum`设0.5"},{"cell_type":"code","execution_count":7,"metadata":{"graffitiCellId":"id_oprgtg8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8B8F9D4CBF2F48B88D446C367E97D587","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"loss: 0.243297, 0.057950 sec per epoch\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 252x180 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/8B8F9D4CBF2F48B88D446C367E97D587/q5qod8io3b.svg\">"},"transient":{}}],"source":"d2l.train_ch7(sgd_momentum, init_momentum_states(),\n              {'lr': 0.02, 'momentum': 0.5}, features, labels)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_zr2hdkm","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"BB1884518C734FF59983C832384FCD87","mdEditEnable":false},"source":"将动量超参数`momentum`增大到0.9"},{"cell_type":"code","execution_count":8,"metadata":{"graffitiCellId":"id_b65o03u","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"2683F607420E443C951B824B2D8BED83","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"loss: 0.260418, 0.059441 sec per epoch\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 252x180 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/2683F607420E443C951B824B2D8BED83/q5qodf7e0m.svg\">"},"transient":{}}],"source":"d2l.train_ch7(sgd_momentum, init_momentum_states(),\n              {'lr': 0.02, 'momentum': 0.9}, features, labels)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_22elqcw","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F27F99CE4F6C4BDB842530363BA24572","mdEditEnable":false},"source":"可见目标函数值在后期迭代过程中的变化不够平滑。直觉上，10倍小批量梯度比2倍小批量梯度大了5倍，我们可以试着将学习率减小到原来的1/5。此时目标函数值在下降了一段时间后变化更加平滑。"},{"cell_type":"code","execution_count":9,"metadata":{"graffitiCellId":"id_9xuazwj","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"771286646D7A40EEA3D1BDF283C4726A","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"loss: 0.243650, 0.063532 sec per epoch\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 252x180 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/771286646D7A40EEA3D1BDF283C4726A/q5qodjrkb3.svg\">"},"transient":{}}],"source":"d2l.train_ch7(sgd_momentum, init_momentum_states(),\n              {'lr': 0.004, 'momentum': 0.9}, features, labels)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_d03ehs6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"BA4A32D84B7C4B8EA18627A0B3DD50FB","mdEditEnable":false},"source":"## Pytorch Class\n\n在Pytorch中，```torch.optim.SGD```已实现了Momentum。"},{"cell_type":"code","execution_count":10,"metadata":{"graffitiCellId":"id_0ani6vk","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"061E57E1FC1240988C134FC43E749BEE","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"loss: 0.243692, 0.048604 sec per epoch\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 252x180 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/061E57E1FC1240988C134FC43E749BEE/q5qodoy8py.svg\">"},"transient":{}}],"source":"d2l.train_pytorch_ch7(torch.optim.SGD, {'lr': 0.004, 'momentum': 0.9},\n                    features, labels)"},{"cell_type":"markdown","metadata":{"collapsed":true,"graffitiCellId":"id_6t7yxla","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"9760485882F04988819911A12BEF3FE4","mdEditEnable":false},"source":"# 11.7 AdaGrad\n\n在之前介绍过的优化算法中，目标函数自变量的每一个元素在相同时间步都使用同一个学习率来自我迭代。举个例子，假设目标函数为$f$，自变量为一个二维向量$[x_1, x_2]^\\top$，该向量中每一个元素在迭代时都使用相同的学习率。例如，在学习率为$\\eta$的梯度下降中，元素$x_1$和$x_2$都使用相同的学习率$\\eta$来自我迭代：\n\n\n$$\n\nx_1 \\leftarrow x_1 - \\eta \\frac{\\partial{f}}{\\partial{x_1}}, \\quad\nx_2 \\leftarrow x_2 - \\eta \\frac{\\partial{f}}{\\partial{x_2}}.\n\n$$\n\n\n在[“动量法”](./momentum.ipynb)一节里我们看到当$x_1$和$x_2$的梯度值有较大差别时，需要选择足够小的学习率使得自变量在梯度值较大的维度上不发散。但这样会导致自变量在梯度值较小的维度上迭代过慢。动量法依赖指数加权移动平均使得自变量的更新方向更加一致，从而降低发散的可能。本节我们介绍AdaGrad算法，它根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题 [1]。\n\n\n## Algorithm\n\nAdaGrad算法会使用一个小批量随机梯度$\\boldsymbol{g}_t$按元素平方的累加变量$\\boldsymbol{s}_t$。在时间步0，AdaGrad将$\\boldsymbol{s}_0$中每个元素初始化为0。在时间步$t$，首先将小批量随机梯度$\\boldsymbol{g}_t$按元素平方后累加到变量$\\boldsymbol{s}_t$：\n\n\n$$\n\\boldsymbol{s}_t \\leftarrow \\boldsymbol{s}_{t-1} + \\boldsymbol{g}_t \\odot \\boldsymbol{g}_t,\n$$\n\n\n其中$\\odot$是按元素相乘。接着，我们将目标函数自变量中每个元素的学习率通过按元素运算重新调整一下：\n\n\n$$\n\\boldsymbol{x}_t \\leftarrow \\boldsymbol{x}_{t-1} - \\frac{\\eta}{\\sqrt{\\boldsymbol{s}_t + \\epsilon}} \\odot \\boldsymbol{g}_t,\n$$\n\n\n其中$\\eta$是学习率，$\\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-6}$。这里开方、除法和乘法的运算都是按元素运算的。这些按元素运算使得目标函数自变量中每个元素都分别拥有自己的学习率。\n\n## Feature\n\n需要强调的是，小批量随机梯度按元素平方的累加变量$\\boldsymbol{s}_t$出现在学习率的分母项中。因此，如果目标函数有关自变量中某个元素的偏导数一直都较大，那么该元素的学习率将下降较快；反之，如果目标函数有关自变量中某个元素的偏导数一直都较小，那么该元素的学习率将下降较慢。然而，由于$\\boldsymbol{s}_t$一直在累加按元素平方的梯度，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。所以，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。\n\n下面我们仍然以目标函数$f(\\boldsymbol{x})=0.1x_1^2+2x_2^2$为例观察AdaGrad算法对自变量的迭代轨迹。我们实现AdaGrad算法并使用和上一节实验中相同的学习率0.4。可以看到，自变量的迭代轨迹较平滑。但由于$\\boldsymbol{s}_t$的累加效果使学习率不断衰减，自变量在迭代后期的移动幅度较小。"},{"cell_type":"code","execution_count":11,"metadata":{"graffitiCellId":"id_8yhntry","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"65D88109B129448EB6DAC9C0A04110BF","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"epoch 20, x1 -2.382563, x2 -0.158591\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/65D88109B129448EB6DAC9C0A04110BF/q5qoefd6ox.svg\">"},"transient":{}}],"source":"%matplotlib inline\nimport math\nimport torch\nimport sys\nsys.path.append(\"/home/kesci/input\") \nimport d2lzh1981 as d2l\n\ndef adagrad_2d(x1, x2, s1, s2):\n    g1, g2, eps = 0.2 * x1, 4 * x2, 1e-6  # 前两项为自变量梯度\n    s1 += g1 ** 2\n    s2 += g2 ** 2\n    x1 -= eta / math.sqrt(s1 + eps) * g1\n    x2 -= eta / math.sqrt(s2 + eps) * g2\n    return x1, x2, s1, s2\n\ndef f_2d(x1, x2):\n    return 0.1 * x1 ** 2 + 2 * x2 ** 2\n\neta = 0.4\nd2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))"},{"cell_type":"markdown","metadata":{"collapsed":true,"graffitiCellId":"id_v2qefl3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"1886AE635E6E4F11808F463063D9CFF2","mdEditEnable":false},"source":"下面将学习率增大到2。可以看到自变量更为迅速地逼近了最优解。"},{"cell_type":"code","execution_count":12,"metadata":{"graffitiCellId":"id_0nyfecw","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"90B791EDF32649498EB29AFD2D77302A","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"epoch 20, x1 -0.002295, x2 -0.000000\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/90B791EDF32649498EB29AFD2D77302A/q5qoekdeom.svg\">"},"transient":{}}],"source":"eta = 2\nd2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_3v4zc27","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"3ACFF070C6424B32A01EAA8303927C81","mdEditEnable":false},"source":"## Implement\n\n同动量法一样，AdaGrad算法需要对每个自变量维护同它一样形状的状态变量。我们根据AdaGrad算法中的公式实现该算法。"},{"cell_type":"code","execution_count":13,"metadata":{"graffitiCellId":"id_vpq6w6w","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B7D3D3F5289548009A73E0576F10D07E","collapsed":false,"scrolled":false},"outputs":[],"source":"def get_data_ch7():  \n    data = np.genfromtxt('/home/kesci/input/airfoil4755/airfoil_self_noise.dat', delimiter='\\t')\n    data = (data - data.mean(axis=0)) / data.std(axis=0)\n    return torch.tensor(data[:1500, :-1], dtype=torch.float32), \\\n        torch.tensor(data[:1500, -1], dtype=torch.float32)\n        \nfeatures, labels = get_data_ch7()\n\ndef init_adagrad_states():\n    s_w = torch.zeros((features.shape[1], 1), dtype=torch.float32)\n    s_b = torch.zeros(1, dtype=torch.float32)\n    return (s_w, s_b)\n\ndef adagrad(params, states, hyperparams):\n    eps = 1e-6\n    for p, s in zip(params, states):\n        s.data += (p.grad.data**2)\n        p.data -= hyperparams['lr'] * p.grad.data / torch.sqrt(s + eps)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_htom2jh","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"6050B51756C7426392ABF086AC9C1E00","mdEditEnable":false},"source":"使用更大的学习率来训练模型。"},{"cell_type":"code","execution_count":14,"metadata":{"graffitiCellId":"id_khjyikm","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"FB3ACF978EAE4A158BFCC322169D396C","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"loss: 0.242258, 0.061548 sec per epoch\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 252x180 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/FB3ACF978EAE4A158BFCC322169D396C/q5qofl6l7n.svg\">"},"transient":{}}],"source":"d2l.train_ch7(adagrad, init_adagrad_states(), {'lr': 0.1}, features, labels)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_dtj8dbu","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"E77565E98BC04932B7064076F1522043","mdEditEnable":false},"source":"## Pytorch Class\n\n通过名称为“adagrad”的`Trainer`实例，我们便可使用Pytorch提供的AdaGrad算法来训练模型。"},{"cell_type":"code","execution_count":15,"metadata":{"graffitiCellId":"id_amhbtt1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"9ADC04FA976240DE8656E060A4B98F49","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"loss: 0.243800, 0.060953 sec per epoch\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 252x180 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/9ADC04FA976240DE8656E060A4B98F49/q5qofropkx.svg\">"},"transient":{}}],"source":"d2l.train_pytorch_ch7(torch.optim.Adagrad, {'lr': 0.1}, features, labels)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_qo3gwz9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8AFBDBA2897F424984F2EC7EE90BD2C9","mdEditEnable":false},"source":"# 11.8 RMSProp\n\n我们在[“AdaGrad算法”](adagrad.ipynb)一节中提到，因为调整学习率时分母上的变量$\\boldsymbol{s}_t$一直在累加按元素平方的小批量随机梯度，所以目标函数自变量每个元素的学习率在迭代过程中一直在降低（或不变）。因此，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。为了解决这一问题，RMSProp算法对AdaGrad算法做了修改。该算法源自Coursera上的一门课程，即“机器学习的神经网络”。\n\n## Algorithm\n\n我们在[“动量法”](momentum.ipynb)一节里介绍过指数加权移动平均。不同于AdaGrad算法里状态变量$\\boldsymbol{s}_t$是截至时间步$t$所有小批量随机梯度$\\boldsymbol{g}_t$按元素平方和，RMSProp算法将这些梯度按元素平方做指数加权移动平均。具体来说，给定超参数$0 \\leq \\gamma 0$计算\n\n\n$$\n\\boldsymbol{v}_t \\leftarrow \\beta \\boldsymbol{v}_{t-1} + (1 - \\beta) \\boldsymbol{g}_t \\odot \\boldsymbol{g}_t. \n$$\n\n\n和AdaGrad算法一样，RMSProp算法将目标函数自变量中每个元素的学习率通过按元素运算重新调整，然后更新自变量\n\n\n$$\n\\boldsymbol{x}_t \\leftarrow \\boldsymbol{x}_{t-1} - \\frac{\\alpha}{\\sqrt{\\boldsymbol{v}_t + \\epsilon}} \\odot \\boldsymbol{g}_t, \n$$\n\n\n其中$\\eta$是学习率，$\\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-6}$。因为RMSProp算法的状态变量$\\boldsymbol{s}_t$是对平方项$\\boldsymbol{g}_t \\odot \\boldsymbol{g}_t$的指数加权移动平均，所以可以看作是最近$1/(1-\\beta)$个时间步的小批量随机梯度平方项的加权平均。如此一来，自变量每个元素的学习率在迭代过程中就不再一直降低（或不变）。\n\n照例，让我们先观察RMSProp算法对目标函数$f(\\boldsymbol{x})=0.1x_1^2+2x_2^2$中自变量的迭代轨迹。回忆在[“AdaGrad算法”](adagrad.ipynb)一节使用的学习率为0.4的AdaGrad算法，自变量在迭代后期的移动幅度较小。但在同样的学习率下，RMSProp算法可以更快逼近最优解。"},{"cell_type":"code","execution_count":16,"metadata":{"graffitiCellId":"id_zto35ht","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"488520FCB5BC4DFF811770D00333B399","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"epoch 20, x1 -0.010599, x2 0.000000\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/488520FCB5BC4DFF811770D00333B399/q5qog9m8u0.svg\">"},"transient":{}}],"source":"%matplotlib inline\nimport math\nimport torch\nimport sys\nsys.path.append(\"/home/kesci/input\") \nimport d2lzh1981 as d2l\n\ndef rmsprop_2d(x1, x2, s1, s2):\n    g1, g2, eps = 0.2 * x1, 4 * x2, 1e-6\n    s1 = beta * s1 + (1 - beta) * g1 ** 2\n    s2 = beta * s2 + (1 - beta) * g2 ** 2\n    x1 -= alpha / math.sqrt(s1 + eps) * g1\n    x2 -= alpha / math.sqrt(s2 + eps) * g2\n    return x1, x2, s1, s2\n\ndef f_2d(x1, x2):\n    return 0.1 * x1 ** 2 + 2 * x2 ** 2\n\nalpha, beta = 0.4, 0.9\nd2l.show_trace_2d(f_2d, d2l.train_2d(rmsprop_2d))"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_xu3o2of","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0AF3943AD95549A886EB0FA31A8DB0C5","mdEditEnable":false},"source":"## Implement\n\n接下来按照RMSProp算法中的公式实现该算法。"},{"cell_type":"code","execution_count":17,"metadata":{"graffitiCellId":"id_nxkci37","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"2F6F0D4E82CF4A2AA4501508A6CC7B36","collapsed":false,"scrolled":false},"outputs":[],"source":"def get_data_ch7():  \n    data = np.genfromtxt('/home/kesci/input/airfoil4755/airfoil_self_noise.dat', delimiter='\\t')\n    data = (data - data.mean(axis=0)) / data.std(axis=0)\n    return torch.tensor(data[:1500, :-1], dtype=torch.float32), \\\n        torch.tensor(data[:1500, -1], dtype=torch.float32)\n        \nfeatures, labels = get_data_ch7()\n\ndef init_rmsprop_states():\n    s_w = torch.zeros((features.shape[1], 1), dtype=torch.float32)\n    s_b = torch.zeros(1, dtype=torch.float32)\n    return (s_w, s_b)\n\ndef rmsprop(params, states, hyperparams):\n    gamma, eps = hyperparams['beta'], 1e-6\n    for p, s in zip(params, states):\n        s.data = gamma * s.data + (1 - gamma) * (p.grad.data)**2\n        p.data -= hyperparams['lr'] * p.grad.data / torch.sqrt(s + eps)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_qfltkd7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"9A045BE025B94F30B4DB9643CE89899F","mdEditEnable":false},"source":"我们将初始学习率设为0.01，并将超参数$\\gamma$设为0.9。此时，变量$\\boldsymbol{s}_t$可看作是最近$1/(1-0.9) = 10$个时间步的平方项$\\boldsymbol{g}_t \\odot \\boldsymbol{g}_t$的加权平均。"},{"cell_type":"code","execution_count":18,"metadata":{"graffitiCellId":"id_kexvzh2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"5C9361F719B844808D67652F774041F3","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"loss: 0.243334, 0.063004 sec per epoch\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 252x180 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/5C9361F719B844808D67652F774041F3/q5qogvxs90.svg\">"},"transient":{}}],"source":"d2l.train_ch7(rmsprop, init_rmsprop_states(), {'lr': 0.01, 'beta': 0.9},\n              features, labels)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_qizxgr0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"6DF5AA8844D942788DDCC755BB032D0B","mdEditEnable":false},"source":"## Pytorch Class\n\n通过名称为“rmsprop”的`Trainer`实例，我们便可使用Gluon提供的RMSProp算法来训练模型。注意，超参数$\\gamma$通过`gamma1`指定。"},{"cell_type":"code","execution_count":19,"metadata":{"graffitiCellId":"id_jdtorb2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B18281B434DC4DAD833ADF5A911D81C2","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"loss: 0.244934, 0.062977 sec per epoch\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 252x180 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/B18281B434DC4DAD833ADF5A911D81C2/q5qoh04h4o.svg\">"},"transient":{}}],"source":"d2l.train_pytorch_ch7(torch.optim.RMSprop, {'lr': 0.01, 'alpha': 0.9},\n                    features, labels)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_c7rew13","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F7B2EB3E76834E4CA1FE2BB42A009B7B","mdEditEnable":false},"source":"# 11.9 AdaDelta\n\n除了RMSProp算法以外，另一个常用优化算法AdaDelta算法也针对AdaGrad算法在迭代后期可能较难找到有用解的问题做了改进 [1]。有意思的是，AdaDelta算法没有学习率这一超参数。\n\n## Algorithm\n\nAdaDelta算法也像RMSProp算法一样，使用了小批量随机梯度$\\boldsymbol{g}_t$按元素平方的指数加权移动平均变量$\\boldsymbol{s}_t$。在时间步0，它的所有元素被初始化为0。给定超参数$0 \\leq \\rho 0$，同RMSProp算法一样计算\n\n\n$$\n\\boldsymbol{s}_t \\leftarrow \\rho \\boldsymbol{s}_{t-1} + (1 - \\rho) \\boldsymbol{g}_t \\odot \\boldsymbol{g}_t. \n$$\n\n\n与RMSProp算法不同的是，AdaDelta算法还维护一个额外的状态变量$\\Delta\\boldsymbol{x}_t$，其元素同样在时间步0时被初始化为0。我们使用$\\Delta\\boldsymbol{x}_{t-1}$来计算自变量的变化量：\n\n\n$$\n \\boldsymbol{g}_t' \\leftarrow \\sqrt{\\frac{\\Delta\\boldsymbol{x}_{t-1} + \\epsilon}{\\boldsymbol{s}_t + \\epsilon}}   \\odot \\boldsymbol{g}_t, \n$$\n\n\n其中$\\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-5}$。接着更新自变量：\n\n\n$$\n\\boldsymbol{x}_t \\leftarrow \\boldsymbol{x}_{t-1} - \\boldsymbol{g}'_t. \n$$\n\n\n最后，我们使用$\\Delta\\boldsymbol{x}_t$来记录自变量变化量$\\boldsymbol{g}'_t$按元素平方的指数加权移动平均：\n\n\n$$\n\\Delta\\boldsymbol{x}_t \\leftarrow \\rho \\Delta\\boldsymbol{x}_{t-1} + (1 - \\rho) \\boldsymbol{g}'_t \\odot \\boldsymbol{g}'_t. \n$$\n\n\n可以看到，如不考虑$\\epsilon$的影响，AdaDelta算法与RMSProp算法的不同之处在于使用$\\sqrt{\\Delta\\boldsymbol{x}_{t-1}}$来替代超参数$\\eta$。\n\n\n## Implement\n\nAdaDelta算法需要对每个自变量维护两个状态变量，即$\\boldsymbol{s}_t$和$\\Delta\\boldsymbol{x}_t$。我们按AdaDelta算法中的公式实现该算法。"},{"cell_type":"code","execution_count":20,"metadata":{"graffitiCellId":"id_xl6t57y","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4C7FB2511AA84AFE81018F63142B409A","collapsed":false,"scrolled":false},"outputs":[],"source":"def init_adadelta_states():\n    s_w, s_b = torch.zeros((features.shape[1], 1), dtype=torch.float32), torch.zeros(1, dtype=torch.float32)\n    delta_w, delta_b = torch.zeros((features.shape[1], 1), dtype=torch.float32), torch.zeros(1, dtype=torch.float32)\n    return ((s_w, delta_w), (s_b, delta_b))\n\ndef adadelta(params, states, hyperparams):\n    rho, eps = hyperparams['rho'], 1e-5\n    for p, (s, delta) in zip(params, states):\n        s[:] = rho * s + (1 - rho) * (p.grad.data**2)\n        g =  p.grad.data * torch.sqrt((delta + eps) / (s + eps))\n        p.data -= g\n        delta[:] = rho * delta + (1 - rho) * g * g"},{"cell_type":"code","execution_count":21,"metadata":{"graffitiCellId":"id_r6rxlkq","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"48D75FB92AAB4D568DD1A3FBA62408EF","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"loss: 0.243485, 0.084914 sec per epoch\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 252x180 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/48D75FB92AAB4D568DD1A3FBA62408EF/q5qohc7hny.svg\">"},"transient":{}}],"source":"d2l.train_ch7(adadelta, init_adadelta_states(), {'rho': 0.9}, features, labels)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_80oczfx","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"84853E8123E142C0B167E9D80F20B388","mdEditEnable":false},"source":"## Pytorch Class\n\n通过名称为“adadelta”的`Trainer`实例，我们便可使用pytorch提供的AdaDelta算法。它的超参数可以通过`rho`来指定。"},{"cell_type":"code","execution_count":22,"metadata":{"graffitiCellId":"id_hcygf4b","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8E66D8902B3045AAABC2D59F2CFA73A0","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"loss: 0.267756, 0.061329 sec per epoch\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 252x180 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/8E66D8902B3045AAABC2D59F2CFA73A0/q5qohjtwx7.svg\">"},"transient":{}}],"source":"d2l.train_pytorch_ch7(torch.optim.Adadelta, {'rho': 0.9}, features, labels)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_jrm85ia","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"AE21D5602CE74DBBAE6C4BE698B56B97","mdEditEnable":false},"source":"# 11.10 Adam\n\nAdam算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均 [1]。下面我们来介绍这个算法。\n\n## Algorithm\n\nAdam算法使用了动量变量$\\boldsymbol{m}_t$和RMSProp算法中小批量随机梯度按元素平方的指数加权移动平均变量$\\boldsymbol{v}_t$，并在时间步0将它们中每个元素初始化为0。给定超参数$0 \\leq \\beta_1 < 1$（算法作者建议设为0.9），时间步$t$的动量变量$\\boldsymbol{m}_t$即小批量随机梯度$\\boldsymbol{g}_t$的指数加权移动平均：\n\n\n$$\n\\boldsymbol{m}_t \\leftarrow \\beta_1 \\boldsymbol{m}_{t-1} + (1 - \\beta_1) \\boldsymbol{g}_t. \n$$\n\n\n和RMSProp算法中一样，给定超参数$0 \\leq \\beta_2 < 1$（算法作者建议设为0.999），\n将小批量随机梯度按元素平方后的项$\\boldsymbol{g}_t \\odot \\boldsymbol{g}_t$做指数加权移动平均得到$\\boldsymbol{v}_t$：\n\n\n$$\n\\boldsymbol{v}_t \\leftarrow \\beta_2 \\boldsymbol{v}_{t-1} + (1 - \\beta_2) \\boldsymbol{g}_t \\odot \\boldsymbol{g}_t. \n$$\n\n\n由于我们将$\\boldsymbol{m}_0$和$\\boldsymbol{s}_0$中的元素都初始化为0，\n在时间步$t$我们得到$\\boldsymbol{m}_t =  (1-\\beta_1) \\sum_{i=1}^t \\beta_1^{t-i} \\boldsymbol{g}_i$。将过去各时间步小批量随机梯度的权值相加，得到 $(1-\\beta_1) \\sum_{i=1}^t \\beta_1^{t-i} = 1 - \\beta_1^t$。需要注意的是，当$t$较小时，过去各时间步小批量随机梯度权值之和会较小。例如，当$\\beta_1 = 0.9$时，$\\boldsymbol{m}_1 = 0.1\\boldsymbol{g}_1$。为了消除这样的影响，对于任意时间步$t$，我们可以将$\\boldsymbol{m}_t$再除以$1 - \\beta_1^t$，从而使过去各时间步小批量随机梯度权值之和为1。这也叫作偏差修正。在Adam算法中，我们对变量$\\boldsymbol{m}_t$和$\\boldsymbol{v}_t$均作偏差修正：\n\n\n$$\n\\hat{\\boldsymbol{m}}_t \\leftarrow \\frac{\\boldsymbol{m}_t}{1 - \\beta_1^t}, \n$$\n\n\n\n$$\n\\hat{\\boldsymbol{v}}_t \\leftarrow \\frac{\\boldsymbol{v}_t}{1 - \\beta_2^t}. \n$$\n\n\n\n接下来，Adam算法使用以上偏差修正后的变量$\\hat{\\boldsymbol{m}}_t$和$\\hat{\\boldsymbol{m}}_t$，将模型参数中每个元素的学习率通过按元素运算重新调整：\n\n\n$$\n\\boldsymbol{g}_t' \\leftarrow \\frac{\\eta \\hat{\\boldsymbol{m}}_t}{\\sqrt{\\hat{\\boldsymbol{v}}_t} + \\epsilon},\n$$\n\n\n其中$\\eta$是学习率，$\\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-8}$。和AdaGrad算法、RMSProp算法以及AdaDelta算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。最后，使用$\\boldsymbol{g}_t'$迭代自变量：\n\n\n$$\n\\boldsymbol{x}_t \\leftarrow \\boldsymbol{x}_{t-1} - \\boldsymbol{g}_t'. \n$$\n\n\n## Implement\n\n我们按照Adam算法中的公式实现该算法。其中时间步$t$通过`hyperparams`参数传入`adam`函数。"},{"cell_type":"code","execution_count":23,"metadata":{"graffitiCellId":"id_ocfckgm","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"EE3231A89C3740649E0D930730DEF0EA","collapsed":false,"scrolled":false},"outputs":[],"source":"%matplotlib inline\nimport torch\nimport sys\nsys.path.append(\"/home/kesci/input\") \nimport d2lzh1981 as d2l\n\ndef get_data_ch7():  \n    data = np.genfromtxt('/home/kesci/input/airfoil4755/airfoil_self_noise.dat', delimiter='\\t')\n    data = (data - data.mean(axis=0)) / data.std(axis=0)\n    return torch.tensor(data[:1500, :-1], dtype=torch.float32), \\\n        torch.tensor(data[:1500, -1], dtype=torch.float32)\n        \nfeatures, labels = get_data_ch7()\n\ndef init_adam_states():\n    v_w, v_b = torch.zeros((features.shape[1], 1), dtype=torch.float32), torch.zeros(1, dtype=torch.float32)\n    s_w, s_b = torch.zeros((features.shape[1], 1), dtype=torch.float32), torch.zeros(1, dtype=torch.float32)\n    return ((v_w, s_w), (v_b, s_b))\n\ndef adam(params, states, hyperparams):\n    beta1, beta2, eps = 0.9, 0.999, 1e-6\n    for p, (v, s) in zip(params, states):\n        v[:] = beta1 * v + (1 - beta1) * p.grad.data\n        s[:] = beta2 * s + (1 - beta2) * p.grad.data**2\n        v_bias_corr = v / (1 - beta1 ** hyperparams['t'])\n        s_bias_corr = s / (1 - beta2 ** hyperparams['t'])\n        p.data -= hyperparams['lr'] * v_bias_corr / (torch.sqrt(s_bias_corr) + eps)\n    hyperparams['t'] += 1"},{"cell_type":"code","execution_count":24,"metadata":{"graffitiCellId":"id_t58avk9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"46DA5110F99A4BB58180C0D38497C943","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"loss: 0.242722, 0.089254 sec per epoch\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 252x180 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/46DA5110F99A4BB58180C0D38497C943/q5qoij5h08.svg\">"},"transient":{}}],"source":"d2l.train_ch7(adam, init_adam_states(), {'lr': 0.01, 't': 1}, features, labels)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_0kb25w1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0D75404B5C874A118C6BF38DA7B749A9","mdEditEnable":false},"source":"## Pytorch Class"},{"cell_type":"code","execution_count":25,"metadata":{"graffitiCellId":"id_phw50au","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8E491F60A4FB4C2990C60CF86E00BAF1","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"loss: 0.242389, 0.073228 sec per epoch\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 252x180 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/8E491F60A4FB4C2990C60CF86E00BAF1/q5qoio531k.svg\">"},"transient":{}}],"source":"d2l.train_pytorch_ch7(torch.optim.Adam, {'lr': 0.01}, features, labels)"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
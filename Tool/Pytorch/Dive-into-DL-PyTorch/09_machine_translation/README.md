分类可以使用最后一个隐藏层的输出，序列标注可以采样每个隐藏层的输出。

机器翻译的难度在于输出序列长度≠输入序列长度。

**英语到法语的翻译**

**去掉空格**

字符在计算机里是以编码的形式存在，我们通常所用的空格是 \x20 ，是在标准ASCII可见字符 0x20~0x7e 范围内。 而 \xa0 属于 latin1 （ISO/IEC_8859-1）中的扩展字符集字符，代表不间断空白符nbsp(non-breaking space)，超出gbk编码范围，是需要去除的特殊字符。再数据预处理的过程中，我们首先需要对数据进行清洗。

```python
text = text.replace('\u202f', ' ').replace('\xa0', ' ')
```



**大写转小写**

**单词和标点符号之间加空格**



**分词**

字符串——单词组成的列表。英语一个列表，法语一个列表，列表里的元素是每个句子的列表。并保证法语和英语的一一对应。

**建立词典**

收录数据集里出现过的英语/法语单词，组成英语/法语词典。

将分词得到的单词组成的列表展开，得到所有的单词。

统计单词词频，按词频从大到小排序，赋予'pad', 'bos', 'eos', 'unk'分别为0,1,2,3,的index。

只获取频次大于某个阈值的所有单词，给他们赋予index。

给定单词列表，得到index列表。



**载入数据集**

保证每个batch的所有输入的句子长度一样，需要进行pad。规定句子长度，长的截掉后面的，短的用某个符号进行补足。

如果是法语(target)，需要在句子前后分别加上bos和eos，要把句子的每个单词转成index，得到index列表，然后pad，列表转成tensor。

有效长度就是句子原本的长度（pad之前），计算损失的时候只计算有效长度部分的。



**Encoder-Decoder**

Encoder将输入变成语义编码（比如隐藏状态ht），语义编码作为Decoder的输入，产生输出，输出是eos时表示结束。



**Sequence to Sequence模型**

英语：hello world .

法语 ：bonjour le mode .

decoder的hidden state初始化为encoder的隐藏状态输出h(-1),

训练的时候：

h(-1)和<bos>预测bonjour(y0)，h(0)和bonjour预测le，以此类推。

预测的时候：

h(-1)和<bos>预测bonjour(y0)，h(0)和y(0)预测y1，以此类推。



Encoder：

英语的输入是一个id的列表，通过embedding得到词向量，作为rnn的输入。



Decoder：

和encoder类似，但是需要有一个dense层，得到输出y。且初始化状态是encoder的输出。



**损失函数**

pad部分的损失无效。



**Beam Search**

简单贪心搜索：每次只选概率最大的作为最终的单词。

维特比算法：选择整体分数最高的句子（搜索空间太大）

集束搜索：每一步找到最好的n个，用这n个生成下一步的结果，再找最好的n个，以此类推
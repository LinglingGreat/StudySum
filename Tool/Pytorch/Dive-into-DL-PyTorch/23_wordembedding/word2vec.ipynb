{"cells":[{"cell_type":"markdown","metadata":{"graffitiCellId":"id_slc0gil","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"1C259CBE4A394E02B3454D9680B0536A","mdEditEnable":false},"source":"# 词嵌入基础\n\n我们在[“循环神经网络的从零开始实现”](https://zh.d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html)一节中使用 one-hot 向量表示单词，虽然它们构造起来很容易，但通常并不是一个好选择。一个主要的原因是，one-hot 词向量无法准确表达不同词之间的相似度，如我们常常使用的余弦相似度。\n\nWord2Vec 词嵌入工具的提出正是为了解决上面这个问题，它将每个词表示成一个定长的向量，并通过在语料库上的预训练使得这些向量能较好地表达不同词之间的相似和类比关系，以引入一定的语义信息。基于两种概率模型的假设，我们可以定义两种 Word2Vec 模型：\n1. [Skip-Gram 跳字模型](https://zh.d2l.ai/chapter_natural-language-processing/word2vec.html#%E8%B7%B3%E5%AD%97%E6%A8%A1%E5%9E%8B)：假设背景词由中心词生成，即建模 $P(w_o\\mid w_c)$，其中 $w_c$ 为中心词，$w_o$ 为任一背景词；\n\n![Image Name](https://cdn.kesci.com/upload/image/q5mjsq84o9.png?imageView2/0/w/960/h/960)\n\n2. [CBOW (continuous bag-of-words) 连续词袋模型](https://zh.d2l.ai/chapter_natural-language-processing/word2vec.html#%E8%BF%9E%E7%BB%AD%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B)：假设中心词由背景词生成，即建模 $P(w_c\\mid \\mathcal{W}_o)$，其中 $\\mathcal{W}_o$ 为背景词的集合。\n\n![Image Name](https://cdn.kesci.com/upload/image/q5mjt4r02n.png?imageView2/0/w/960/h/960)\n\n在这里我们主要介绍 Skip-Gram 模型的实现，CBOW 实现与其类似，读者可之后自己尝试实现。后续的内容将大致从以下四个部分展开：\n\n1. PTB 数据集\n2. Skip-Gram 跳字模型\n3. 负采样近似\n4. 训练模型"},{"cell_type":"code","execution_count":2,"metadata":{"graffitiCellId":"id_y7ocw2l","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8627003642CB441780806CBC552BFAC1","collapsed":false,"scrolled":false},"outputs":[],"source":"import collections\nimport math\nimport random\nimport sys\nimport time\nimport os\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.utils.data as Data"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_ube5b27","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"DD9999F086964C808616928EC7B736C0","mdEditEnable":false},"source":"## PTB 数据集\n\n简单来说，Word2Vec 能从语料中学到如何将离散的词映射为连续空间中的向量，并保留其语义上的相似关系。那么为了训练 Word2Vec 模型，我们就需要一个自然语言语料库，模型将从中学习各个单词间的关系，这里我们使用经典的 PTB 语料库进行训练。[PTB (Penn Tree Bank)](https://catalog.ldc.upenn.edu/LDC99T42) 是一个常用的小型语料库，它采样自《华尔街日报》的文章，包括训练集、验证集和测试集。我们将在PTB训练集上训练词嵌入模型。\n\n### 载入数据集\n\n数据集训练文件 `ptb.train.txt` 示例：\n```\naer banknote berlitz calloway centrust cluett fromstein gitano guterman ...\npierre  N years old will join the board as a nonexecutive director nov. N \nmr.  is chairman of  n.v. the dutch publishing group \n...\n```"},{"cell_type":"code","execution_count":4,"metadata":{"graffitiCellId":"id_9374ybr","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"FF5B1C79764A4EA8AA61C3DE984CBA0D","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"# sentences: 42068\n# tokens: 24 ['aer', 'banknote', 'berlitz', 'calloway', 'centrust']\n# tokens: 15 ['pierre', '<unk>', 'N', 'years', 'old']\n# tokens: 11 ['mr.', '<unk>', 'is', 'chairman', 'of']\n","name":"stdout"}],"source":"with open('/home/kesci/input/ptb_train1020/ptb.train.txt', 'r') as f:\n    lines = f.readlines() # 该数据集中句子以换行符为分割\n    raw_dataset = [st.split() for st in lines] # st是sentence的缩写，单词以空格为分割\nprint('# sentences: %d' % len(raw_dataset))\n\n# 对于数据集的前3个句子，打印每个句子的词数和前5个词\n# 句尾符为 '' ，生僻词全用 '' 表示，数字则被替换成了 'N'\nfor st in raw_dataset[:3]:\n    print('# tokens:', len(st), st[:5])"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_whcovuv","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4694FB2B910840BB8C65162A14881EB0","mdEditEnable":false},"source":"### 建立词语索引"},{"cell_type":"code","execution_count":5,"metadata":{"graffitiCellId":"id_u6zhq97","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"70DD6E74F6854C289BA21B367D2397B4","collapsed":false,"scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"'# tokens: 887100'"},"transient":{},"execution_count":5}],"source":"counter = collections.Counter([tk for st in raw_dataset for tk in st]) # tk是token的缩写\ncounter = dict(filter(lambda x: x[1] >= 5, counter.items())) # 只保留在数据集中至少出现5次的词\n\nidx_to_token = [tk for tk, _ in counter.items()]\ntoken_to_idx = {tk: idx for idx, tk in enumerate(idx_to_token)}\ndataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx]\n           for st in raw_dataset] # raw_dataset中的单词在这一步被转换为对应的idx\nnum_tokens = sum([len(st) for st in dataset])\n'# tokens: %d' % num_tokens"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_4zjy016","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"845942F28174462A87ADDBAE82957D15","mdEditEnable":false},"source":"### 二次采样\n\n文本数据中一般会出现一些高频词，如英文中的“the”“a”和“in”。通常来说，在一个背景窗口中，一个词（如“chip”）和较低频词（如“microprocessor”）同时出现比和较高频词（如“the”）同时出现对训练词嵌入模型更有益。因此，训练词嵌入模型时可以对词进行二次采样。 具体来说，数据集中每个被索引词 $w_i$ 将有一定概率被丢弃，该丢弃概率为\n\n\n$$\n\nP(w_i)=\\max(1-\\sqrt{\\frac{t}{f(w_i)}},0)\n\n$$\n\n\n其中  $f(w_i)$  是数据集中词 $w_i$ 的个数与总词数之比，常数 $t$ 是一个超参数（实验中设为 $10^{−4}$）。可见，只有当 $f(w_i)>t$ 时，我们才有可能在二次采样中丢弃词 $w_i$，并且越高频的词被丢弃的概率越大。具体的代码如下："},{"cell_type":"code","execution_count":6,"metadata":{"graffitiCellId":"id_yg9kj6g","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4B82B59FCC244E11AA7335909F6B588A","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"# tokens: 375995\n# the: before=50770, after=2161\n# join: before=45, after=45\n","name":"stdout"}],"source":"def discard(idx):\n    '''\n    @params:\n        idx: 单词的下标\n    @return: True/False 表示是否丢弃该单词\n    '''\n    return random.uniform(0, 1) < 1 - math.sqrt(\n        1e-4 / counter[idx_to_token[idx]] * num_tokens)\n\nsubsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]\nprint('# tokens: %d' % sum([len(st) for st in subsampled_dataset]))\n\ndef compare_counts(token):\n    return '# %s: before=%d, after=%d' % (token, sum(\n        [st.count(token_to_idx[token]) for st in dataset]), sum(\n        [st.count(token_to_idx[token]) for st in subsampled_dataset]))\n\nprint(compare_counts('the'))\nprint(compare_counts('join'))"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_u88x2eb","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"18BE417013E049A3B5A3EC7B607EE554","mdEditEnable":false},"source":"### 提取中心词和背景词"},{"cell_type":"code","execution_count":7,"metadata":{"graffitiCellId":"id_a0ayzaz","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"56C4719FE9A64B468F9B0081DD25ABBC","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\ncenter 0 has contexts [1, 2]\ncenter 1 has contexts [0, 2, 3]\ncenter 2 has contexts [0, 1, 3, 4]\ncenter 3 has contexts [2, 4]\ncenter 4 has contexts [3, 5]\ncenter 5 has contexts [4, 6]\ncenter 6 has contexts [5]\ncenter 7 has contexts [8]\ncenter 8 has contexts [7, 9]\ncenter 9 has contexts [7, 8]\n","name":"stdout"}],"source":"def get_centers_and_contexts(dataset, max_window_size):\n    '''\n    @params:\n        dataset: 数据集为句子的集合，每个句子则为单词的集合，此时单词已经被转换为相应数字下标\n        max_window_size: 背景词的词窗大小的最大值\n    @return:\n        centers: 中心词的集合\n        contexts: 背景词窗的集合，与中心词对应，每个背景词窗则为背景词的集合\n    '''\n    centers, contexts = [], []\n    for st in dataset:\n        if len(st) < 2:  # 每个句子至少要有2个词才可能组成一对“中心词-背景词”\n            continue\n        centers += st\n        for center_i in range(len(st)):\n            window_size = random.randint(1, max_window_size) # 随机选取背景词窗大小\n            indices = list(range(max(0, center_i - window_size),\n                                 min(len(st), center_i + 1 + window_size)))\n            indices.remove(center_i)  # 将中心词排除在背景词之外\n            contexts.append([st[idx] for idx in indices])\n    return centers, contexts\n\nall_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)\n\ntiny_dataset = [list(range(7)), list(range(7, 10))]\nprint('dataset', tiny_dataset)\nfor center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n    print('center', center, 'has contexts', context)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_161ief2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"3853063E40E14F7DA7DCA410A0A3C617","mdEditEnable":false},"source":"*注：数据批量读取的实现需要依赖负采样近似的实现，故放于负采样近似部分进行讲解。*"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_s7nai85","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"E0C8301D7DB340CDABD81A23520A340D","mdEditEnable":false},"source":"## Skip-Gram 跳字模型\n\n在跳字模型中，每个词被表示成两个 $d$ 维向量，用来计算条件概率。假设这个词在词典中索引为 $i$ ，当它为中心词时向量表示为 $\\boldsymbol{v}_i\\in\\mathbb{R}^d$，而为背景词时向量表示为 $\\boldsymbol{u}_i\\in\\mathbb{R}^d$ 。设中心词 $w_c$ 在词典中索引为 $c$，背景词 $w_o$ 在词典中索引为 $o$，我们假设给定中心词生成背景词的条件概率满足下式：\n\n\n$$\n\nP(w_o\\mid w_c)=\\frac{\\exp(\\boldsymbol{u}_o^\\top \\boldsymbol{v}_c)}{\\sum_{i\\in\\mathcal{V}}\\exp(\\boldsymbol{u}_i^\\top \\boldsymbol{v}_c)}\n\n$$\n"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_j38gtz0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C7B5BB9D5FD54489BF6E3F42E31B835F","mdEditEnable":false},"source":"### PyTorch 预置的 Embedding 层"},{"cell_type":"code","execution_count":8,"metadata":{"graffitiCellId":"id_7he6kmh","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"68DF20EE03824200887D18AA3363E7F9","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"Parameter containing:\ntensor([[-0.7417, -1.9469, -0.5745,  1.4267],\n        [ 1.1483,  1.4781,  0.3064, -0.2893],\n        [ 0.6840,  2.4566, -0.1872, -2.2061],\n        [ 0.3386,  1.3820, -0.3142,  0.2427],\n        [ 0.4802, -0.6375, -0.4730,  1.2114],\n        [ 0.7130, -0.9774,  0.5321,  1.4228],\n        [-0.6726, -0.5829, -0.4888, -0.3290],\n        [ 0.3152, -0.6827,  0.9950, -0.3326],\n        [-1.4651,  1.2344,  1.9976, -1.5962],\n        [ 0.0872,  0.0130, -2.1396, -0.6361]], requires_grad=True)\ntensor([[[ 1.1483,  1.4781,  0.3064, -0.2893],\n         [ 0.6840,  2.4566, -0.1872, -2.2061],\n         [ 0.3386,  1.3820, -0.3142,  0.2427]],\n\n        [[ 0.4802, -0.6375, -0.4730,  1.2114],\n         [ 0.7130, -0.9774,  0.5321,  1.4228],\n         [-0.6726, -0.5829, -0.4888, -0.3290]]], grad_fn=<EmbeddingBackward>)\n","name":"stdout"}],"source":"embed = nn.Embedding(num_embeddings=10, embedding_dim=4)\nprint(embed.weight)\n\nx = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.long)\nprint(embed(x))"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_hevvqgv","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8421DAEC55314EBE80D2EC59EAB73099","mdEditEnable":false},"source":"### PyTorch 预置的批量乘法"},{"cell_type":"code","execution_count":9,"metadata":{"graffitiCellId":"id_8i4omp4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"12B88D54095F48F58397626D8F1935E3","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"torch.Size([2, 1, 6])\n","name":"stdout"}],"source":"X = torch.ones((2, 1, 4))\nY = torch.ones((2, 4, 6))\nprint(torch.bmm(X, Y).shape)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_ol1tdyu","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"80F8440D701D4F3CBD4C4FFD3608E697","mdEditEnable":false},"source":"### Skip-Gram 模型的前向计算"},{"cell_type":"code","execution_count":10,"metadata":{"graffitiCellId":"id_x6q9jp9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"AAA4F7E268764809836AAADD7FD2A8AE","collapsed":false,"scrolled":false},"outputs":[],"source":"def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n    '''\n    @params:\n        center: 中心词下标，形状为 (n, 1) 的整数张量\n        contexts_and_negatives: 背景词和噪音词下标，形状为 (n, m) 的整数张量\n        embed_v: 中心词的 embedding 层\n        embed_u: 背景词的 embedding 层\n    @return:\n        pred: 中心词与背景词（或噪音词）的内积，之后可用于计算概率 p(w_o|w_c)\n    '''\n    v = embed_v(center) # shape of (n, 1, d)\n    u = embed_u(contexts_and_negatives) # shape of (n, m, d)\n    pred = torch.bmm(v, u.permute(0, 2, 1)) # bmm((n, 1, d), (n, d, m)) => shape of (n, 1, m)\n    return pred"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_rb8yuyq","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8FE236CF785F474F9BBB4BB3D88AA4CC","mdEditEnable":false},"source":"## 负采样近似\n\n由于 softmax 运算考虑了背景词可能是词典 $\\mathcal{V}$ 中的任一词，对于含几十万或上百万词的较大词典，就可能导致计算的开销过大。我们将以 skip-gram 模型为例，介绍负采样 (negative sampling) 的实现来尝试解决这个问题。\n\n负采样方法用以下公式来近似条件概率 $P(w_o\\mid w_c)=\\frac{\\exp(\\boldsymbol{u}_o^\\top \\boldsymbol{v}_c)}{\\sum_{i\\in\\mathcal{V}}\\exp(\\boldsymbol{u}_i^\\top \\boldsymbol{v}_c)}$：\n\n\n$$\n\nP(w_o\\mid w_c)=P(D=1\\mid w_c,w_o)\\prod_{k=1,w_k\\sim P(w)}^K P(D=0\\mid w_c,w_k)\n\n$$\n\n\n其中 $P(D=1\\mid w_c,w_o)=\\sigma(\\boldsymbol{u}_o^\\top\\boldsymbol{v}_c)$，$\\sigma(\\cdot)$ 为 sigmoid 函数。对于一对中心词和背景词，我们从词典中随机采样 $K$ 个噪声词（实验中设 $K=5$）。根据 Word2Vec 论文的建议，噪声词采样概率 $P(w)$ 设为 $w$ 词频与总词频之比的 $0.75$ 次方。"},{"cell_type":"code","execution_count":11,"metadata":{"graffitiCellId":"id_st81puo","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C6A1BDB699EB49AA8EEB49C295EACBF7","collapsed":false,"scrolled":false},"outputs":[],"source":"def get_negatives(all_contexts, sampling_weights, K):\n    '''\n    @params:\n        all_contexts: [[w_o1, w_o2, ...], [...], ... ]\n        sampling_weights: 每个单词的噪声词采样概率\n        K: 随机采样个数\n    @return:\n        all_negatives: [[w_n1, w_n2, ...], [...], ...]\n    '''\n    all_negatives, neg_candidates, i = [], [], 0\n    population = list(range(len(sampling_weights)))\n    for contexts in all_contexts:\n        negatives = []\n        while len(negatives) < len(contexts) * K:\n            if i == len(neg_candidates):\n                # 根据每个词的权重（sampling_weights）随机生成k个词的索引作为噪声词。\n                # 为了高效计算，可以将k设得稍大一点\n                i, neg_candidates = 0, random.choices(\n                    population, sampling_weights, k=int(1e5))\n            neg, i = neg_candidates[i], i + 1\n            # 噪声词不能是背景词\n            if neg not in set(contexts):\n                negatives.append(neg)\n        all_negatives.append(negatives)\n    return all_negatives\n\nsampling_weights = [counter[w]**0.75 for w in idx_to_token]\nall_negatives = get_negatives(all_contexts, sampling_weights, 5)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_g7431va","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"43F68EDDF24849BE8791D07F0618F9DD","mdEditEnable":false},"source":"*注：除负采样方法外，还有层序 softmax (hiererarchical softmax) 方法也可以用来解决计算量过大的问题，请参考[原书10.2.2节](https://zh.d2l.ai/chapter_natural-language-processing/approx-training.html#%E5%B1%82%E5%BA%8Fsoftmax)。*"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_jhrav4e","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F991A9C6E28C42848DD393E4F891D49D","mdEditEnable":false},"source":"### 批量读取数据"},{"cell_type":"code","execution_count":12,"metadata":{"graffitiCellId":"id_shkut5w","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"FF3BDA1024C94EB3A760EBA5FD583B4A","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"centers shape: torch.Size([512, 1])\ncontexts_negatives shape: torch.Size([512, 60])\nmasks shape: torch.Size([512, 60])\nlabels shape: torch.Size([512, 60])\n","name":"stdout"}],"source":"class MyDataset(torch.utils.data.Dataset):\n    def __init__(self, centers, contexts, negatives):\n        assert len(centers) == len(contexts) == len(negatives)\n        self.centers = centers\n        self.contexts = contexts\n        self.negatives = negatives\n        \n    def __getitem__(self, index):\n        return (self.centers[index], self.contexts[index], self.negatives[index])\n\n    def __len__(self):\n        return len(self.centers)\n    \ndef batchify(data):\n    '''\n    用作DataLoader的参数collate_fn\n    @params:\n        data: 长为batch_size的列表，列表中的每个元素都是__getitem__得到的结果\n    @outputs:\n        batch: 批量化后得到 (centers, contexts_negatives, masks, labels) 元组\n            centers: 中心词下标，形状为 (n, 1) 的整数张量\n            contexts_negatives: 背景词和噪声词的下标，形状为 (n, m) 的整数张量\n            masks: 与补齐相对应的掩码，形状为 (n, m) 的0/1整数张量\n            labels: 指示中心词的标签，形状为 (n, m) 的0/1整数张量\n    '''\n    max_len = max(len(c) + len(n) for _, c, n in data)\n    centers, contexts_negatives, masks, labels = [], [], [], []\n    for center, context, negative in data:\n        cur_len = len(context) + len(negative)\n        centers += [center]\n        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n        masks += [[1] * cur_len + [0] * (max_len - cur_len)] # 使用掩码变量mask来避免填充项对损失函数计算的影响\n        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n        batch = (torch.tensor(centers).view(-1, 1), torch.tensor(contexts_negatives),\n            torch.tensor(masks), torch.tensor(labels))\n    return batch\n\nbatch_size = 512\nnum_workers = 0 if sys.platform.startswith('win32') else 4\n\ndataset = MyDataset(all_centers, all_contexts, all_negatives)\ndata_iter = Data.DataLoader(dataset, batch_size, shuffle=True,\n                            collate_fn=batchify, \n                            num_workers=num_workers)\nfor batch in data_iter:\n    for name, data in zip(['centers', 'contexts_negatives', 'masks',\n                           'labels'], batch):\n        print(name, 'shape:', data.shape)\n    break"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_hf5q360","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"3AC214600B9F4A73B87CD4B57384BB3B","mdEditEnable":false},"source":"## 训练模型\n\n### 损失函数\n\n应用负采样方法后，我们可利用最大似然估计的对数等价形式将损失函数定义为如下\n\n\n$$\n\n\\sum_{t=1}^T\\sum_{-m\\le j\\le m,j\\ne 0} [-\\log P(D=1\\mid w^{(t)},w^{(t+j)})-\\sum_{k=1,w_k\\sim P(w)^K}\\log P(D=0\\mid w^{(t)},w_k)]\n\n$$\n\n\n根据这个损失函数的定义，我们可以直接使用二元交叉熵损失函数进行计算："},{"cell_type":"code","execution_count":null,"metadata":{"graffitiCellId":"id_ap2woj6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"6BDDED9801FF43B98CD51ED86B12D450"},"outputs":[],"source":"class SigmoidBinaryCrossEntropyLoss(nn.Module):\n    def __init__(self):\n        super(SigmoidBinaryCrossEntropyLoss, self).__init__()\n    def forward(self, inputs, targets, mask=None):\n        '''\n        @params:\n            inputs: 经过sigmoid层后为预测D=1的概率\n            targets: 0/1向量，1代表背景词，0代表噪音词\n        @return:\n            res: 平均到每个label的loss\n        '''\n        inputs, targets, mask = inputs.float(), targets.float(), mask.float()\n        res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\", weight=mask)\n        res = res.sum(dim=1) / mask.float().sum(dim=1)\n        return res\n\nloss = SigmoidBinaryCrossEntropyLoss()\n\npred = torch.tensor([[1.5, 0.3, -1, 2], [1.1, -0.6, 2.2, 0.4]])\nlabel = torch.tensor([[1, 0, 0, 0], [1, 1, 0, 0]]) # 标签变量label中的1和0分别代表背景词和噪声词\nmask = torch.tensor([[1, 1, 1, 1], [1, 1, 1, 0]])  # 掩码变量\nprint(loss(pred, label, mask))\n\ndef sigmd(x):\n    return - math.log(1 / (1 + math.exp(-x)))\nprint('%.4f' % ((sigmd(1.5) + sigmd(-0.3) + sigmd(1) + sigmd(-2)) / 4)) # 注意1-sigmoid(x) = sigmoid(-x)\nprint('%.4f' % ((sigmd(1.1) + sigmd(-0.6) + sigmd(-2.2)) / 3))"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_zz57d6p","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"3F1EB2F8E34D45088D8A8872FAE1C726"},"source":"### 模型初始化"},{"cell_type":"code","execution_count":null,"metadata":{"graffitiCellId":"id_k9ax2h6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"EE565E07272B40C691196EED5695BC5D"},"outputs":[],"source":"embed_size = 100\nnet = nn.Sequential(nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size),\n                    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size))"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_9ahavii","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F178519AEE504029B3603E4D006BA839","mdEditEnable":false},"source":"### 训练模型"},{"cell_type":"code","execution_count":null,"metadata":{"graffitiCellId":"id_9fx6rj4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"2DA9D996EE3E44B49DE0D2FAC370E1DD"},"outputs":[],"source":"def train(net, lr, num_epochs):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(\"train on\", device)\n    net = net.to(device)\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n    for epoch in range(num_epochs):\n        start, l_sum, n = time.time(), 0.0, 0\n        for batch in data_iter:\n            center, context_negative, mask, label = [d.to(device) for d in batch]\n            \n            pred = skip_gram(center, context_negative, net[0], net[1])\n            \n            l = loss(pred.view(label.shape), label, mask).mean() # 一个batch的平均loss\n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n            l_sum += l.cpu().item()\n            n += 1\n        print('epoch %d, loss %.2f, time %.2fs'\n              % (epoch + 1, l_sum / n, time.time() - start))\n\ntrain(net, 0.01, 5)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_yb3aapa","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"855E5C6515884CD596D3B1C1E0CC1978","mdEditEnable":false},"source":"```\ntrain on cpu\nepoch 1, loss 0.61, time 221.30s\nepoch 2, loss 0.42, time 227.70s\nepoch 3, loss 0.38, time 240.50s\nepoch 4, loss 0.36, time 253.79s\nepoch 5, loss 0.34, time 238.51s\n```\n\n*注：由于本地CPU上训练时间过长，故只截取了运行的结果，后同。大家可以自行在网站上训练。*"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_bw1dtd1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"A5DAB2B7CC6A41668D2F5A0061C54728","mdEditEnable":false},"source":"### 测试模型"},{"cell_type":"code","execution_count":null,"metadata":{"graffitiCellId":"id_wm2rrhl","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"838B4878856C457889DAA35A29948029"},"outputs":[],"source":"def get_similar_tokens(query_token, k, embed):\n    '''\n    @params:\n        query_token: 给定的词语\n        k: 近义词的个数\n        embed: 预训练词向量\n    '''\n    W = embed.weight.data\n    x = W[token_to_idx[query_token]]\n    # 添加的1e-9是为了数值稳定性\n    cos = torch.matmul(W, x) / (torch.sum(W * W, dim=1) * torch.sum(x * x) + 1e-9).sqrt()\n    _, topk = torch.topk(cos, k=k+1)\n    topk = topk.cpu().numpy()\n    for i in topk[1:]:  # 除去输入词\n        print('cosine sim=%.3f: %s' % (cos[i], (idx_to_token[i])))\n        \nget_similar_tokens('chip', 3, net[0])"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_htzr4u1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"3E45FF89FD794158AAD153F605942091"},"source":"```\ncosine sim=0.446: intel\ncosine sim=0.427: computer\ncosine sim=0.427: computers\n```"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_g9tjvnn","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"17C8660147A846FEB9389CEF51AEC536"},"source":"## 参考\n* [Dive into Deep Learning](https://d2l.ai/chapter_natural-language-processing/word2vec.html). Ch14.1-14.4.\n* [动手学深度学习](http://zh.gluon.ai/chapter_natural-language-processing/word2vec.html). Ch10.1-10.3.\n* [Dive-into-DL-PyTorch on GitHub](https://github.com/ShusenTang/Dive-into-DL-PyTorch/blob/master/code/chapter10_natural-language-processing/10.3_word2vec-pytorch.ipynb)"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
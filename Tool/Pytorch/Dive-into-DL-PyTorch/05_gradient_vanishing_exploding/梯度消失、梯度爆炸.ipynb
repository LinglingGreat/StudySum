{"cells":[{"cell_type":"markdown","metadata":{"graffitiCellId":"id_qz1lu2l","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"665778BC4FD748188759D5FA0B4C36C8","mdEditEnable":false},"source":"# 梯度消失、梯度爆炸以及Kaggle房价预测\n\n1. 梯度消失和梯度爆炸\n2. 考虑到环境因素的其他问题\n3. Kaggle房价预测"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_eaw1lhq","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"2E84AE7BD5FD4456806865C6FBDBD0A0","mdEditEnable":false},"source":"# 梯度消失和梯度爆炸\n\n深度模型有关数值稳定性的典型问题是消失（vanishing）和爆炸（explosion）。\n\n**当神经网络的层数较多时，模型的数值稳定性容易变差。**\n\n假设一个层数为$L$的多层感知机的第$l$层$\\boldsymbol{H}^{(l)}$的权重参数为$\\boldsymbol{W}^{(l)}$，输出层$\\boldsymbol{H}^{(L)}$的权重参数为$\\boldsymbol{W}^{(L)}$。为了便于讨论，不考虑偏差参数，且设所有隐藏层的激活函数为恒等映射（identity mapping）$\\phi(x) = x$。给定输入$\\boldsymbol{X}$，多层感知机的第$l$层的输出$\\boldsymbol{H}^{(l)} = \\boldsymbol{X} \\boldsymbol{W}^{(1)} \\boldsymbol{W}^{(2)} \\ldots \\boldsymbol{W}^{(l)}$。此时，如果层数$l$较大，$\\boldsymbol{H}^{(l)}$的计算可能会出现衰减或爆炸。举个例子，假设输入和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知机的第30层输出为输入$\\boldsymbol{X}$分别与$0.2^{30} \\approx 1 \\times 10^{-21}$（消失）和$5^{30} \\approx 9 \\times 10^{20}$（爆炸）的乘积。当层数较多时，梯度的计算也容易出现消失或爆炸。\n\n# 随机初始化模型参数\n\n在神经网络中，通常需要随机初始化模型参数。下面我们来解释这样做的原因。\n\n回顾多层感知机一节描述的多层感知机。为了方便解释，假设输出层只保留一个输出单元$o_1$（删去$o_2$和$o_3$以及指向它们的箭头），且隐藏层使用相同的激活函数。如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。之后的迭代也是如此。在这种情况下，无论隐藏单元有多少，隐藏层本质上只有1个隐藏单元在发挥作用。因此，正如在前面的实验中所做的那样，我们通常将神经网络的模型参数，特别是权重参数，进行随机初始化。\n\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5jg76kloy.png?imageView2/0/w/960/h/960)\n\n\n\n###  PyTorch的默认随机初始化\n\n随机初始化模型参数的方法有很多。在线性回归的简洁实现中，我们使用`torch.nn.init.normal_()`使模型`net`的权重参数采用正态分布的随机初始化方式。不过，PyTorch中`nn.Module`的模块参数都采取了较为合理的初始化策略（不同类型的layer具体采样的哪一种初始化方法的可参考[源代码](https://github.com/pytorch/pytorch/tree/master/torch/nn/modules)），因此一般不用我们考虑。\n\n\n### Xavier随机初始化\n\n还有一种比较常用的随机初始化方法叫作Xavier随机初始化。\n假设某全连接层的输入个数为$a$，输出个数为$b$，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布\n\n\n$$\nU\\left(-\\sqrt{\\frac{6}{a+b}}, \\sqrt{\\frac{6}{a+b}}\\right).\n$$\n\n\n它的设计主要考虑到，模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_yrhfrka","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"A39077A2F4E2417588D16EDFB59C9324","mdEditEnable":false},"source":"# 考虑环境因素\n\n## 协变量偏移\n\n这里我们假设，虽然输入的分布可能随时间而改变，但是标记函数，即条件分布P（y∣x）不会改变。虽然这个问题容易理解，但在实践中也容易忽视。\n\n想想区分猫和狗的一个例子。我们的训练数据使用的是猫和狗的真实的照片，但是在测试时，我们被要求对猫和狗的卡通图片进行分类。\n\n|cat|cat|dog|dog|\n|:---------------:|:---------------:|:---------------:|:---------------:|\n|![Image Name](https://cdn.kesci.com/upload/image/q5jg8j72fl.jpg?imageView2/0/w/200/h/200)|![Image Name](https://cdn.kesci.com/upload/image/q5jg993za3.jpg?imageView2/0/w/200/h/200)|![Image Name](https://cdn.kesci.com/upload/image/q5jg9tqs4s.jpg?imageView2/0/w/200/h/200)|![Image Name](https://cdn.kesci.com/upload/image/q5jga6mnsk.jpg?imageView2/0/w/200/h/200)|\n\n测试数据：\n\n|cat|cat|dog|dog|\n|:---------------:|:---------------:|:---------------:|:---------------:|\n|![Image Name](https://cdn.kesci.com/upload/image/q5jgat5lsd.png?imageView2/0/w/200/h/200)|![Image Name](https://cdn.kesci.com/upload/image/q5jgbaoij8.png?imageView2/0/w/200/h/200)|![Image Name](https://cdn.kesci.com/upload/image/q5jgbswvbb.png?imageView2/0/w/200/h/200)|![Image Name](https://cdn.kesci.com/upload/image/q5jgc5j7zv.png?imageView2/0/w/200/h/200)\n|\n\n显然，这不太可能奏效。训练集由照片组成，而测试集只包含卡通。在一个看起来与测试集有着本质不同的数据集上进行训练，而不考虑如何适应新的情况，这是不是一个好主意。不幸的是，这是一个非常常见的陷阱。\n\n统计学家称这种协变量变化是因为问题的根源在于特征分布的变化（即协变量的变化）。数学上，我们可以说P（x）改变了，但P（y∣x）保持不变。尽管它的有用性并不局限于此，当我们认为x导致y时，协变量移位通常是正确的假设。\n\n\n## 标签偏移\n\n    \n当我们认为导致偏移的是标签P（y）上的边缘分布的变化，但类条件分布是不变的P（x∣y）时，就会出现相反的问题。当我们认为y导致x时，标签偏移是一个合理的假设。例如，通常我们希望根据其表现来预测诊断结果。在这种情况下，我们认为诊断引起的表现，即疾病引起的症状。有时标签偏移和协变量移位假设可以同时成立。例如，当真正的标签函数是确定的和不变的，那么协变量偏移将始终保持，包括如果标签偏移也保持。有趣的是，当我们期望标签偏移和协变量偏移保持时，使用来自标签偏移假设的方法通常是有利的。这是因为这些方法倾向于操作看起来像标签的对象，这（在深度学习中）与处理看起来像输入的对象（在深度学习中）相比相对容易一些。\n\n病因（要预测的诊断结果）导致 症状（观察到的结果）。  \n\n训练数据集，数据很少只包含流感p(y)的样本。  \n\n而测试数据集有流感p(y)和流感q(y)，其中不变的是流感症状p(x|y)。\n\n\n## 概念偏移\n\n另一个相关的问题出现在概念转换中，即标签本身的定义发生变化的情况。这听起来很奇怪，毕竟猫就是猫。的确，猫的定义可能不会改变，但我们能不能对软饮料也这么说呢？事实证明，如果我们周游美国，按地理位置转移数据来源，我们会发现，即使是如图所示的这个简单术语的定义也会发生相当大的概念转变。\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5jgd81pl3.png?imageView2/0/w/640/h/640)\n\n$$\n美国软饮料名称的概念转变 \n$$\n如果我们要建立一个机器翻译系统，分布P（y∣x）可能因我们的位置而异。这个问题很难发现。另一个可取之处是P（y∣x）通常只是逐渐变化。\n"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_xnnkt44","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"6EDBE35E1A98479895A1681A62688B81","mdEditEnable":false},"source":"# Kaggle 房价预测实战\n\n作为深度学习基础篇章的总结，我们将对本章内容学以致用。下面，让我们动手实战一个Kaggle比赛：房价预测。本节将提供未经调优的数据的预处理、模型的设计和超参数的选择。我们希望读者通过动手操作、仔细观察实验现象、认真分析实验结果并不断调整方法，得到令自己满意的结果。"},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","graffitiCellId":"id_rppdqvv","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4B7D88B7F9D5401988D5CFAEAEE4D60D","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"1.3.0\n","name":"stdout"}],"source":"%matplotlib inline\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport sys\nsys.path.append(\"/home/kesci/input\")\nimport d2lzh1981 as d2l\nprint(torch.__version__)\ntorch.set_default_tensor_type(torch.FloatTensor)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_gw9szre","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"ABDEF28B3CAF443DA886ED6DA0E6763E","mdEditEnable":false},"source":"## 获取和读取数据集\n\n比赛数据分为训练数据集和测试数据集。两个数据集都包括每栋房子的特征，如街道类型、建造年份、房顶类型、地下室状况等特征值。这些特征值有连续的数字、离散的标签甚至是缺失值“na”。只有训练数据集包括了每栋房子的价格，也就是标签。我们可以访问比赛网页，点击“Data”标签，并下载这些数据集。\n\n我们将通过`pandas`库读入并处理数据。在导入本节需要的包前请确保已安装`pandas`库。\n假设解压后的数据位于`/home/kesci/input/houseprices2807/`目录，它包括两个csv文件。下面使用`pandas`读取这两个文件。"},{"cell_type":"code","execution_count":6,"metadata":{"_cell_guid":"","_uuid":"","graffitiCellId":"id_55zhkwj","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"22499724CB544EE9A4E993DACA2C59A4","collapsed":false,"scrolled":false},"outputs":[],"source":"test_data = pd.read_csv(\"/home/kesci/input/houseprices2807/house-prices-advanced-regression-techniques/test.csv\")\ntrain_data = pd.read_csv(\"/home/kesci/input/houseprices2807/house-prices-advanced-regression-techniques/train.csv\")"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_eyhuksb","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"242CB44821F445D99396C27D2FF86BBB","mdEditEnable":false},"source":"训练数据集包括1460个样本、80个特征和1个标签。"},{"cell_type":"code","execution_count":7,"metadata":{"graffitiCellId":"id_v638azx","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F73F0440A4E541AC89BC9839E785E9E7","collapsed":false,"scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"(1460, 81)"},"transient":{},"execution_count":7}],"source":"train_data.shape"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_1eag24j","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D8EAA97288B4429084DED4BC1FD59580","mdEditEnable":false},"source":"测试数据集包括1459个样本和80个特征。我们需要将测试数据集中每个样本的标签预测出来。"},{"cell_type":"code","execution_count":8,"metadata":{"graffitiCellId":"id_8omgjxj","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"DE998F1F94ED44B18D390D31888DC149","collapsed":false,"scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"(1459, 80)"},"transient":{},"execution_count":8}],"source":"test_data.shape"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_1eft6d0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"408A944E07A44C958A18AA049A441CF5","mdEditEnable":false},"source":"让我们来查看前4个样本的前4个特征、后2个特征和标签（SalePrice）："},{"cell_type":"code","execution_count":9,"metadata":{"graffitiCellId":"id_rexdt8z","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"CEAD0C35ECB14AB68E21195D77993F02","collapsed":false,"scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"   Id  MSSubClass MSZoning  LotFrontage SaleType SaleCondition  SalePrice\n0   1          60       RL         65.0       WD        Normal     208500\n1   2          20       RL         80.0       WD        Normal     181500\n2   3          60       RL         68.0       WD        Normal     223500\n3   4          70       RL         60.0       WD       Abnorml     140000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>208500</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>181500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>223500</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>4</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>140000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"transient":{},"execution_count":9}],"source":"train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]]"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_hophmxk","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"63C1BAB825D840FA9B7E7C2AC69FCCA6","mdEditEnable":false},"source":"可以看到第一个特征是Id，它能帮助模型记住每个训练样本，但难以推广到测试样本，所以我们不使用它来训练。我们将所有的训练数据和测试数据的79个特征按样本连结。"},{"cell_type":"code","execution_count":10,"metadata":{"graffitiCellId":"id_ws1qrzt","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"AE00A8F4623744409F49F7C0DC5373DB","collapsed":false,"scrolled":false},"outputs":[],"source":"all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_phsdl03","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C016D8C1318C44798F22013CD31DA753","mdEditEnable":false},"source":"## 预处理数据\n我们对连续数值的特征做标准化（standardization）：设该特征在整个数据集上的均值为$\\mu$，标准差为$\\sigma$。那么，我们可以将该特征的每个值先减去$\\mu$再除以$\\sigma$得到标准化后的每个特征值。对于缺失的特征值，我们将其替换成该特征的均值。"},{"cell_type":"code","execution_count":11,"metadata":{"graffitiCellId":"id_gr89sii","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"A89CC9C91E08484598F763A008CF05BB","collapsed":false,"scrolled":false},"outputs":[],"source":"numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\nall_features[numeric_features] = all_features[numeric_features].apply(\n    lambda x: (x - x.mean()) / (x.std()))\n# 标准化后，每个数值特征的均值变为0，所以可以直接用0来替换缺失值\nall_features[numeric_features] = all_features[numeric_features].fillna(0)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_hxs486y","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"71663D256EEC4023863DD5341CEC1436","mdEditEnable":false},"source":"接下来将离散数值转成指示特征。举个例子，假设特征MSZoning里面有两个不同的离散值RL和RM，那么这一步转换将去掉MSZoning特征，并新加两个特征MSZoning\\_RL和MSZoning\\_RM，其值为0或1。如果一个样本原来在MSZoning里的值为RL，那么有MSZoning\\_RL=1且MSZoning\\_RM=0。"},{"cell_type":"code","execution_count":12,"metadata":{"graffitiCellId":"id_9q56n3m","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"39725C56CC4F4D2E9A262B58041C1339","collapsed":false,"scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"(2919, 331)"},"transient":{},"execution_count":12}],"source":"# dummy_na=True将缺失值也当作合法的特征值并为其创建指示特征\nall_features = pd.get_dummies(all_features, dummy_na=True)\nall_features.shape"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_ujs7yoa","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"BD789B24D91041328721A249C9CEF70C","mdEditEnable":false},"source":"可以看到这一步转换将特征数从79增加到了331。\n\n最后，通过`values`属性得到NumPy格式的数据，并转成`Tensor`方便后面的训练。"},{"cell_type":"code","execution_count":13,"metadata":{"graffitiCellId":"id_wnd1cmy","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"6010D2628BF648D8B9C852A881CF99E0","collapsed":false,"scrolled":false},"outputs":[],"source":"n_train = train_data.shape[0]\ntrain_features = torch.tensor(all_features[:n_train].values, dtype=torch.float)\ntest_features = torch.tensor(all_features[n_train:].values, dtype=torch.float)\ntrain_labels = torch.tensor(train_data.SalePrice.values, dtype=torch.float).view(-1, 1)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_6bdgzdo","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C8FE8AB28E8A4F329C501F299DDC0F24","mdEditEnable":false},"source":"## 训练模型"},{"cell_type":"code","execution_count":14,"metadata":{"graffitiCellId":"id_pg3x6ue","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"40D1A426AE804E958139EEBEDDB5456B","collapsed":false,"scrolled":false},"outputs":[],"source":"loss = torch.nn.MSELoss()\n\ndef get_net(feature_num):\n    net = nn.Linear(feature_num, 1)\n    for param in net.parameters():\n        nn.init.normal_(param, mean=0, std=0.01)\n    return net"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_9g14lrp","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0C80D529E2BE4F9FBADB12DDE1840DAD","mdEditEnable":false},"source":"下面定义比赛用来评价模型的对数均方根误差。给定预测值$\\hat y_1, \\ldots, \\hat y_n$和对应的真实标签$y_1,\\ldots, y_n$，它的定义为\n\n\n$$\n\\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\left(\\log(y_i)-\\log(\\hat y_i)\\right)^2}.\n$$\n\n\n对数均方根误差的实现如下。"},{"cell_type":"code","execution_count":15,"metadata":{"graffitiCellId":"id_qjz6u65","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"387CB5F1C97A4A1F8943165FA3C0BE11","collapsed":false,"scrolled":false},"outputs":[],"source":"def log_rmse(net, features, labels):\n    with torch.no_grad():\n        # 将小于1的值设成1，使得取对数时数值更稳定\n        clipped_preds = torch.max(net(features), torch.tensor(1.0))\n        rmse = torch.sqrt(2 * loss(clipped_preds.log(), labels.log()).mean())\n    return rmse.item()"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_b27rtld","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"565F294EF5FF4277ABC474EBF9C1C4BC","mdEditEnable":false},"source":"下面的训练函数跟本章中前几节的不同在于使用了Adam优化算法。相对之前使用的小批量随机梯度下降，它对学习率相对不那么敏感。我们将在之后的“优化算法”一章里详细介绍它。"},{"cell_type":"code","execution_count":16,"metadata":{"graffitiCellId":"id_erbtdm9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"DDC3C34CDD804C048FA12AFA5A5812A5","collapsed":false,"scrolled":false},"outputs":[],"source":"def train(net, train_features, train_labels, test_features, test_labels,\n          num_epochs, learning_rate, weight_decay, batch_size):\n    train_ls, test_ls = [], []\n    dataset = torch.utils.data.TensorDataset(train_features, train_labels)\n    train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)\n    # 这里使用了Adam优化算法\n    optimizer = torch.optim.Adam(params=net.parameters(), lr=learning_rate, weight_decay=weight_decay) \n    net = net.float()\n    for epoch in range(num_epochs):\n        for X, y in train_iter:\n            l = loss(net(X.float()), y.float())\n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n        train_ls.append(log_rmse(net, train_features, train_labels))\n        if test_labels is not None:\n            test_ls.append(log_rmse(net, test_features, test_labels))\n    return train_ls, test_ls"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_nlimv5e","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B7830AF406C249718ED109BF413578C0","mdEditEnable":false},"source":"## K折交叉验证\n我们在模型选择、欠拟合和过拟合中介绍了$K$折交叉验证。它将被用来选择模型设计并调节超参数。下面实现了一个函数，它返回第`i`折交叉验证时所需要的训练和验证数据。"},{"cell_type":"code","execution_count":17,"metadata":{"graffitiCellId":"id_mrp7nyy","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"2D7D3C06E1D24E238F0DA935CD81B0B8","collapsed":false,"scrolled":false},"outputs":[],"source":"def get_k_fold_data(k, i, X, y):\n    # 返回第i折交叉验证时所需要的训练和验证数据\n    assert k > 1\n    fold_size = X.shape[0] // k\n    X_train, y_train = None, None\n    for j in range(k):\n        idx = slice(j * fold_size, (j + 1) * fold_size)\n        X_part, y_part = X[idx, :], y[idx]\n        if j == i:\n            X_valid, y_valid = X_part, y_part\n        elif X_train is None:\n            X_train, y_train = X_part, y_part\n        else:\n            X_train = torch.cat((X_train, X_part), dim=0)\n            y_train = torch.cat((y_train, y_part), dim=0)\n    return X_train, y_train, X_valid, y_valid"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_2xl2r2q","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"66D0CEC41DE34F2B8399E5034CA64824","mdEditEnable":false},"source":"在$K$折交叉验证中我们训练$K$次并返回训练和验证的平均误差"},{"cell_type":"code","execution_count":18,"metadata":{"graffitiCellId":"id_83qykff","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B8FDE10586B445E28ABA5E786A6B6509","collapsed":false,"scrolled":false},"outputs":[],"source":"def k_fold(k, X_train, y_train, num_epochs,\n           learning_rate, weight_decay, batch_size):\n    train_l_sum, valid_l_sum = 0, 0\n    for i in range(k):\n        data = get_k_fold_data(k, i, X_train, y_train)\n        net = get_net(X_train.shape[1])\n        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,\n                                   weight_decay, batch_size)\n        train_l_sum += train_ls[-1]\n        valid_l_sum += valid_ls[-1]\n        if i == 0:\n            d2l.semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'rmse',\n                         range(1, num_epochs + 1), valid_ls,\n                         ['train', 'valid'])\n        print('fold %d, train rmse %f, valid rmse %f' % (i, train_ls[-1], valid_ls[-1]))\n    return train_l_sum / k, valid_l_sum / k"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_trewijv","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"3EB022DC108048A892CB21203A633877","mdEditEnable":false},"source":"## 模型选择\n我们使用一组未经调优的超参数并计算交叉验证误差。可以改动这些超参数来尽可能减小平均测试误差。\n有时候你会发现一组参数的训练误差可以达到很低，但是在$K$折交叉验证上的误差可能反而较高。这种现象很可能是由过拟合造成的。因此，当训练误差降低时，我们要观察$K$折交叉验证上的误差是否也相应降低。"},{"cell_type":"code","execution_count":19,"metadata":{"graffitiCellId":"id_tlka7i5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"989B1E6DC2F046A6899429AC97B29EC0","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"fold 0, train rmse 0.241365, valid rmse 0.223083\nfold 1, train rmse 0.229118, valid rmse 0.267488\nfold 2, train rmse 0.232072, valid rmse 0.237995\nfold 3, train rmse 0.238050, valid rmse 0.218671\nfold 4, train rmse 0.231004, valid rmse 0.259185\n5-fold validation: avg train rmse 0.234322, avg valid rmse 0.241284\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 252x180 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/989B1E6DC2F046A6899429AC97B29EC0/q5jfhgk0ux.svg\">"},"transient":{}}],"source":"k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64\ntrain_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr, weight_decay, batch_size)\nprint('%d-fold validation: avg train rmse %f, avg valid rmse %f' % (k, train_l, valid_l))"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_3h0djuc","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"CC30DD4229814CF78FC9960656E38450","mdEditEnable":false},"source":"# 预测并在Kaggle中提交结果\n下面定义预测函数。在预测之前，我们会使用完整的训练数据集来重新训练模型，并将预测结果存成提交所需要的格式。"},{"cell_type":"code","execution_count":20,"metadata":{"graffitiCellId":"id_bfeohke","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"6439054A3497425386D81BAF6A0E5914","collapsed":false,"scrolled":false},"outputs":[],"source":"def train_and_pred(train_features, test_features, train_labels, test_data,\n                   num_epochs, lr, weight_decay, batch_size):\n    net = get_net(train_features.shape[1])\n    train_ls, _ = train(net, train_features, train_labels, None, None,\n                        num_epochs, lr, weight_decay, batch_size)\n    d2l.semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'rmse')\n    print('train rmse %f' % train_ls[-1])\n    preds = net(test_features).detach().numpy()\n    test_data['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])\n    submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)\n    submission.to_csv('./submission.csv', index=False)\n    # sample_submission_data = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\n"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_488x0hf","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"2CD0AF62C0B942B88E759B35E9A1C010","mdEditEnable":false},"source":"设计好模型并调好超参数之后，下一步就是对测试数据集上的房屋样本做价格预测。如果我们得到与交叉验证时差不多的训练误差，那么这个结果很可能是理想的，可以在Kaggle上提交结果。"},{"cell_type":"code","execution_count":null,"metadata":{"graffitiCellId":"id_y9xgm83","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"9F99F864AB2946B6A6AD66AADBD44F2D"},"outputs":[],"source":"train_and_pred(train_features, test_features, train_labels, test_data, num_epochs, lr, weight_decay, batch_size)"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_40e187a","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B5954A83E1DE4E7487492DFDD2A90216"},"source":"希望大家自己动手完成房价预测的实现，多参与讨论。"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
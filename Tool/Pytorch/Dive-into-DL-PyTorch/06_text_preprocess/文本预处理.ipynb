{"cells":[{"cell_type":"markdown","metadata":{"graffitiCellId":"id_635azvh","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B4BA5B0FCB884B6DBA6FFE071318505E","mdEditEnable":false},"source":"# 文本预处理\n\n\n文本是一类序列数据，一篇文章可以看作是字符或单词的序列，本节将介绍文本数据的常见预处理步骤，预处理通常包括四个步骤：\n\n1. 读入文本\n2. 分词\n3. 建立字典，将每个词映射到一个唯一的索引（index）\n4. 将文本从词的序列转换为索引的序列，方便输入模型"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_da72pg7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C3B3705CB33847A895AD10619F23E97B","mdEditEnable":false},"source":"## 读入文本\n\n我们用一部英文小说，即H. G. Well的[Time Machine](http://www.gutenberg.org/ebooks/35)，作为示例，展示文本预处理的具体过程。"},{"cell_type":"code","execution_count":1,"metadata":{"graffitiCellId":"id_ytfpat1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"7FA4C53DED4F42279EA3AB3229B88DB7","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"# sentences 3221\n","name":"stdout"}],"source":"import collections\nimport re\n\ndef read_time_machine():\n    with open('/home/kesci/input/timemachine7163/timemachine.txt', 'r') as f:\n        lines = [re.sub('[^a-z]+', ' ', line.strip().lower()) for line in f]\n    return lines\n\n\nlines = read_time_machine()\nprint('# sentences %d' % len(lines))"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_gy3tram","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"EABE813C62FC4E1B8DFFD7B819C31829","mdEditEnable":false},"source":"## 分词\n\n我们对每个句子进行分词，也就是将一个句子划分成若干个词（token），转换为一个词的序列。"},{"cell_type":"code","execution_count":2,"metadata":{"graffitiCellId":"id_z5grfxp","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F80F8AFC1C0A48BDB66D52A18DC3A940","collapsed":false,"scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"[['the', 'time', 'machine', 'by', 'h', 'g', 'wells', ''], ['']]"},"transient":{},"execution_count":2}],"source":"def tokenize(sentences, token='word'):\n    \"\"\"Split sentences into word or char tokens\"\"\"\n    if token == 'word':\n        return [sentence.split(' ') for sentence in sentences]\n    elif token == 'char':\n        return [list(sentence) for sentence in sentences]\n    else:\n        print('ERROR: unkown token type '+token)\n\ntokens = tokenize(lines)\ntokens[0:2]"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_rap2ka4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"01CE759264D84FAA8C60CE9156B86157","mdEditEnable":false},"source":"## 建立字典\n\n为了方便模型处理，我们需要将字符串转换为数字。因此我们需要先构建一个字典（vocabulary），将每个词映射到一个唯一的索引编号。"},{"cell_type":"code","execution_count":3,"metadata":{"attributes":{"classes":[],"id":"","n":"9"},"graffitiCellId":"id_wapwqkb","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"37532FBF89C242A1805534BBE05C343A","collapsed":false,"scrolled":false},"outputs":[],"source":"class Vocab(object):\n    def __init__(self, tokens, min_freq=0, use_special_tokens=False):\n        counter = count_corpus(tokens)  # : \n        self.token_freqs = list(counter.items())\n        self.idx_to_token = []\n        if use_special_tokens:\n            # padding, begin of sentence, end of sentence, unknown\n            self.pad, self.bos, self.eos, self.unk = (0, 1, 2, 3)\n            self.idx_to_token += ['', '', '', '']\n        else:\n            self.unk = 0\n            self.idx_to_token += ['']\n        self.idx_to_token += [token for token, freq in self.token_freqs\n                        if freq >= min_freq and token not in self.idx_to_token]\n        self.token_to_idx = dict()\n        for idx, token in enumerate(self.idx_to_token):\n            self.token_to_idx[token] = idx\n\n    def __len__(self):\n        return len(self.idx_to_token)\n\n    def __getitem__(self, tokens):\n        if not isinstance(tokens, (list, tuple)):\n            return self.token_to_idx.get(tokens, self.unk)\n        return [self.__getitem__(token) for token in tokens]\n\n    def to_tokens(self, indices):\n        if not isinstance(indices, (list, tuple)):\n            return self.idx_to_token[indices]\n        return [self.idx_to_token[index] for index in indices]\n\ndef count_corpus(sentences):\n    tokens = [tk for st in sentences for tk in st]\n    return collections.Counter(tokens)  # 返回一个字典，记录每个词的出现次数"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_k17qg7x","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"DB6949BC67FF4C7481DFFD00FE64BE56","mdEditEnable":false},"source":"我们看一个例子，这里我们尝试用Time Machine作为语料构建字典"},{"cell_type":"code","execution_count":4,"metadata":{"attributes":{"classes":[],"id":"","n":"23"},"graffitiCellId":"id_hm9y6bm","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"1BE94FF518DB4C4A8CDFB95C0262B47E","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"[('', 0), ('the', 1), ('time', 2), ('machine', 3), ('by', 4), ('h', 5), ('g', 6), ('wells', 7), ('i', 8), ('traveller', 9)]\n","name":"stdout"}],"source":"vocab = Vocab(tokens)\nprint(list(vocab.token_to_idx.items())[0:10])"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_l6pjfl7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"73D0F629056F41B59D4A53F15BFAB552","mdEditEnable":false},"source":"## 将词转为索引\n\n使用字典，我们可以将原文本中的句子从单词序列转换为索引序列"},{"cell_type":"code","execution_count":5,"metadata":{"graffitiCellId":"id_k48bsl2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"9FBB71C21B5C4F5283C70CFE3BB07112","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"words: ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him', '']\nindices: [1, 2, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0]\nwords: ['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\nindices: [20, 21, 22, 23, 24, 16, 25, 26, 27, 28, 29, 30]\n","name":"stdout"}],"source":"for i in range(8, 10):\n    print('words:', tokens[i])\n    print('indices:', vocab[tokens[i]])"},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_q6fupul","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"EA20BC8762A74B188D3FDD761BFEDE98","mdEditEnable":false},"source":"## 用现有工具进行分词\n\n我们前面介绍的分词方式非常简单，它至少有以下几个缺点:\n\n1. 标点符号通常可以提供语义信息，但是我们的方法直接将其丢弃了\n2. 类似“shouldn't\", \"doesn't\"这样的词会被错误地处理\n3. 类似\"Mr.\", \"Dr.\"这样的词会被错误地处理\n\n我们可以通过引入更复杂的规则来解决这些问题，但是事实上，有一些现有的工具可以很好地进行分词，我们在这里简单介绍其中的两个：[spaCy](https://spacy.io/)和[NLTK](https://www.nltk.org/)。\n\n下面是一个简单的例子："},{"cell_type":"code","execution_count":6,"metadata":{"graffitiCellId":"id_7u3knll","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"46F5F57611E248ECB51F04BD0104E278","collapsed":false,"scrolled":false},"outputs":[],"source":"text = \"Mr. Chen doesn't agree with my suggestion.\""},{"metadata":{"id":"37A142F06F14478A894FFDA60B4B0BA0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\nCollecting spacy\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/76/1f30264c433f9c3c84171fa03f4b6bb5f3303df7781d21554d25045873f4/spacy-2.2.3-cp37-cp37m-manylinux1_x86_64.whl (10.4MB)\n\u001b[K     |████████████████████████████████| 10.4MB 31kB/s eta 0:00:01\n\u001b[?25hCollecting cymem<2.1.0,>=2.0.2 (from spacy)\n  Downloading https://files.pythonhosted.org/packages/e1/79/6ce05ecf4d50344e29749ea7db7ddf427589228fb8fe89b29718c38c27c5/cymem-2.0.3-cp37-cp37m-manylinux1_x86_64.whl\nCollecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n  Downloading https://files.pythonhosted.org/packages/73/fc/10eeacb926ec1e88cd62f79d9ac106b0a3e3fe5ff1690422d88c29bd0909/murmurhash-1.0.2-cp37-cp37m-manylinux1_x86_64.whl\nCollecting blis<0.5.0,>=0.4.0 (from spacy)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/8c/f1b2aad385de78db151a6e9728026f311dee8bd480f2edc28a0175a543b6/blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7MB)\n\u001b[K     |████████████████████████████████| 3.7MB 28kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (1.17.2)\nCollecting srsly<1.1.0,>=0.1.0 (from spacy)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/af/f0f2e6cff739421a7e6165d6b1d044eb2d9ef82865895d2528d2c4c6f9c5/srsly-1.0.1-cp37-cp37m-manylinux1_x86_64.whl (185kB)\n\u001b[K     |████████████████████████████████| 194kB 43kB/s eta 0:00:01\n\u001b[?25hCollecting thinc<7.4.0,>=7.3.0 (from spacy)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/53/d11d2faa6921e55c37ad2cd56b0866a9e6df647fb547cfb69a50059d759c/thinc-7.3.1-cp37-cp37m-manylinux1_x86_64.whl (2.2MB)\n\u001b[K     |████████████████████████████████| 2.2MB 48kB/s eta 0:00:019\n\u001b[?25hCollecting preshed<3.1.0,>=3.0.2 (from spacy)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/5b/ae4da6230eb48df353b199f53532c8407d0e9eb6ed678d3d36fa75ac391c/preshed-3.0.2-cp37-cp37m-manylinux1_x86_64.whl (118kB)\n\u001b[K     |████████████████████████████████| 122kB 62kB/s eta 0:00:01\n\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7 (from spacy)\n  Downloading https://files.pythonhosted.org/packages/6c/f9/9a5658e2f56932e41eb264941f9a2cb7f3ce41a80cb36b2af6ab78e2f8af/catalogue-1.0.0-py2.py3-none-any.whl\nCollecting wasabi<1.1.0,>=0.4.0 (from spacy)\n  Downloading https://files.pythonhosted.org/packages/21/e1/e4e7b754e6be3a79c400eb766fb34924a6d278c43bb828f94233e0124a21/wasabi-0.6.0-py3-none-any.whl\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy) (2.22.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy) (41.0.1)\nCollecting plac<1.2.0,>=0.9.6 (from spacy)\n  Downloading https://files.pythonhosted.org/packages/86/85/40b8f66c2dd8f4fd9f09d59b22720cffecf1331e788b8a0cab5bafb353d1/plac-1.1.3-py2.py3-none-any.whl\nRequirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy) (4.32.2)\nCollecting importlib-metadata>=0.20; python_version < \"3.8\" (from catalogue<1.1.0,>=0.0.7->spacy)\n  Downloading https://files.pythonhosted.org/packages/8b/03/a00d504808808912751e64ccf414be53c29cad620e3de2421135fcae3025/importlib_metadata-1.5.0-py2.py3-none-any.whl\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.3)\nCollecting zipp>=0.5 (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy)\n  Downloading https://files.pythonhosted.org/packages/46/42/f2dd964b2a6b1921b08d661138148c1bcd3f038462a44019416f2342b618/zipp-2.2.0-py36-none-any.whl\nInstalling collected packages: cymem, murmurhash, blis, srsly, wasabi, preshed, plac, thinc, zipp, importlib-metadata, catalogue, spacy\n\u001b[31mERROR: Could not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/opt/conda/bin/__pycache__/plac_runner.cpython-37.pyc'\nConsider using the `--user` option or check the permissions.\n\u001b[0m\n","name":"stdout"}],"source":"!pip install spacy","execution_count":8},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_ae3i5g2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"7D5831E3D5AD4FF48155334F73065451","mdEditEnable":false},"source":"spaCy:"},{"cell_type":"code","execution_count":7,"metadata":{"graffitiCellId":"id_uz6civu","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"30D69C6B1BE44362BA556E2E5EEF493A","collapsed":false,"scrolled":false},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'spacy'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-bd289229cdd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"]}],"source":"import spacy\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nprint([token.text for token in doc])"},{"metadata":{"id":"765CC9B5A1C348A58A2B340ADC532FD1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"NLTK:"},{"cell_type":"code","execution_count":8,"metadata":{"graffitiCellId":"id_r13iwga","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B83D30D3670B44A38527B4943BE4DBE0","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"['Mr.', 'Chen', 'does', \"n't\", 'agree', 'with', 'my', 'suggestion', '.']\n","name":"stdout"}],"source":"from nltk.tokenize import word_tokenize\nfrom nltk import data\ndata.path.append('/home/kesci/input/nltk_data3784/nltk_data')\nprint(word_tokenize(text))"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
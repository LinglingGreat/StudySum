{"cells":[{"attachments":{"%E5%9B%BE%E7%89%87.png":{"image/png":"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"}},"cell_type":"markdown","metadata":{"graffitiCellId":"id_m8ksq1l","id":"ECBDC362D7D147C888F06244BC276561","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["# 循环神经网络\n","本节介绍循环神经网络，下图展示了如何基于循环神经网络实现语言模型。我们的目的是基于当前的输入与过去的输入序列，预测序列的下一个字符。循环神经网络引入一个隐藏变量$H$，用$H_{t}$表示$H$在时间步$t$的值。$H_{t}$的计算基于$X_{t}$和$H_{t-1}$，可以认为$H_{t}$记录了到当前字符为止的序列信息，利用$H_{t}$对序列的下一个字符进行预测。\n","![Image Name](https://cdn.kesci.com/upload/image/q5jkm0v44i.png?imageView2/0/w/640/h/640)"]},{"cell_type":"markdown","metadata":{"id":"1501E9868E46429286E75AC573A894DD","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["## 循环神经网络的构造\n","\n","我们先看循环神经网络的具体构造。假设$\\boldsymbol{X}_t \\in \\mathbb{R}^{n \\times d}$是时间步$t$的小批量输入，$\\boldsymbol{H}_t  \\in \\mathbb{R}^{n \\times h}$是该时间步的隐藏变量，则：\n","\n","\n","$$\n","\\boldsymbol{H}_t = \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh} + \\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hh}  + \\boldsymbol{b}_h).\n","$$\n","\n","\n","其中，$\\boldsymbol{W}_{xh} \\in \\mathbb{R}^{d \\times h}$，$\\boldsymbol{W}_{hh} \\in \\mathbb{R}^{h \\times h}$，$\\boldsymbol{b}_{h} \\in \\mathbb{R}^{1 \\times h}$，$\\phi$函数是非线性激活函数。由于引入了$\\boldsymbol{H}_{t-1} \\boldsymbol{W}_{hh}$，$H_{t}$能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。由于$H_{t}$的计算基于$H_{t-1}$，上式的计算是循环的，使用循环计算的网络即循环神经网络（recurrent neural network）。\n","\n","在时间步$t$，输出层的输出为：\n","\n","\n","$$\n","\\boldsymbol{O}_t = \\boldsymbol{H}_t \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q.\n","$$\n","\n","\n","其中$\\boldsymbol{W}_{hq} \\in \\mathbb{R}^{h \\times q}$，$\\boldsymbol{b}_q \\in \\mathbb{R}^{1 \\times q}$。\n","\n","\n","## 从零开始实现循环神经网络\n","\n","我们先尝试从零开始实现一个基于字符级循环神经网络的语言模型，这里我们使用周杰伦的歌词作为语料，首先我们读入数据："]},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":false,"id":"8A9668E0777549E882B4BC2DB8284548","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":"\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\nCollecting torchtext\n\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/torchtext/\u001b[0m\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n\u001b[K     |████████████████████████████████| 81kB 11kB/s eta 0:00:012\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext) (4.32.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from torchtext) (1.12.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext) (1.17.2)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchtext) (1.3.0)\nCollecting sentencepiece (from torchtext)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/e0/1264990c559fb945cfb6664742001608e1ed8359eeec6722830ae085062b/sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0MB)\n\u001b[K     |████████████████████████████████| 1.0MB 188kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext) (2.22.0)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext) (1.25.3)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext) (2.8)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext) (2019.9.11)\nInstalling collected packages: sentencepiece, torchtext\nSuccessfully installed sentencepiece-0.1.85 torchtext-0.5.0\n"}],"source":["!pip install torchtext"]},{"cell_type":"code","execution_count":3,"metadata":{"collapsed":false,"graffitiCellId":"id_uso50ly","id":"20171E08BC064A1DA6315C234E0EDF33","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import time\n","import math\n","import sys\n","sys.path.append(\"/home/kesci/input\")\n","import d2l_jay9460 as d2l\n","(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_kryzk9x","id":"797F389C048341748C8B9D95206722C6","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["### one-hot向量\n","\n","我们需要将字符表示成向量，这里采用one-hot向量。假设词典大小是$N$，每次字符对应一个从$0$到$N-1$的唯一的索引，则该字符的向量是一个长度为$N$的向量，若字符的索引是$i$，则该向量的第$i$个位置为$1$，其他位置为$0$。下面分别展示了索引为0和2的one-hot向量，向量长度等于词典大小。"]},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":false,"graffitiCellId":"id_vljucfa","id":"0A8DFF6F37CA40C1B87AAF9D0D361497","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":"tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 1.,  ..., 0., 0., 0.]])\ntorch.Size([2, 1027])\ntensor([1., 1.])\n"}],"source":["def one_hot(x, n_class, dtype=torch.float32):\n","    result = torch.zeros(x.shape[0], n_class, dtype=dtype, device=x.device)  # shape: (n, n_class)\n","    result.scatter_(1, x.long().view(-1, 1), 1)  # result[i, x[i, 0]] = 1\n","    return result\n","    \n","x = torch.tensor([0, 2])\n","x_one_hot = one_hot(x, vocab_size)\n","print(x_one_hot)\n","print(x_one_hot.shape)\n","print(x_one_hot.sum(axis=1))"]},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_eaelzjl","id":"A01C821C29E54ECA95AF07E01A4997D6","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["我们每次采样的小批量的形状是（批量大小, 时间步数）。下面的函数将这样的小批量变换成数个形状为（批量大小, 词典大小）的矩阵，矩阵个数等于时间步数。也就是说，时间步$t$的输入为$\\boldsymbol{X}_t \\in \\mathbb{R}^{n \\times d}$，其中$n$为批量大小，$d$为词向量大小，即one-hot向量长度（词典大小）。"]},{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false,"graffitiCellId":"id_961roi7","id":"781B54CB28D741D9B8C2931ABD71F7F7","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":"5 torch.Size([2, 1027])\n"}],"source":["def to_onehot(X, n_class):\n","    return [one_hot(X[:, i], n_class) for i in range(X.shape[1])]\n","\n","X = torch.arange(10).view(2, 5)\n","inputs = to_onehot(X, vocab_size)\n","print(len(inputs), inputs[0].shape)"]},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_6nii7n3","id":"323AB87BB02246F38BD4BB86219530E0","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["### 初始化模型参数\n"]},{"cell_type":"code","execution_count":6,"metadata":{"collapsed":false,"graffitiCellId":"id_4667brq","id":"C1F685ED1624479186E895EEECED6499","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n","# num_inputs: d\n","# num_hiddens: h, 隐藏单元的个数是超参数\n","# num_outputs: q\n","\n","def get_params():\n","    def _one(shape):\n","        param = torch.zeros(shape, device=device, dtype=torch.float32)\n","        nn.init.normal_(param, 0, 0.01)\n","        return torch.nn.Parameter(param)\n","\n","    # 隐藏层参数\n","    W_xh = _one((num_inputs, num_hiddens))\n","    W_hh = _one((num_hiddens, num_hiddens))\n","    b_h = torch.nn.Parameter(torch.zeros(num_hiddens, device=device))\n","    # 输出层参数\n","    W_hq = _one((num_hiddens, num_outputs))\n","    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device))\n","    return (W_xh, W_hh, b_h, W_hq, b_q)"]},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_ge2je0t","id":"5C9E578391F04E988B0AF4415BB33767","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["### 定义模型\n","\n","函数`rnn`用循环的方式依次完成循环神经网络每个时间步的计算。\n"]},{"cell_type":"code","execution_count":7,"metadata":{"collapsed":false,"graffitiCellId":"id_ffheq7o","id":"E2C02FC273C543DE845D4CFD8F6B99EB","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["def rnn(inputs, state, params):\n","    # inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵\n","    W_xh, W_hh, b_h, W_hq, b_q = params\n","    H, = state\n","    outputs = []\n","    for X in inputs:\n","        H = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h)\n","        Y = torch.matmul(H, W_hq) + b_q\n","        outputs.append(Y)\n","    return outputs, (H,)"]},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_ap3wbu6","id":"6C4A4D9C506C4FD6953F9E0770A1D35A","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["函数init_rnn_state初始化隐藏变量，这里的返回值是一个元组。\n"]},{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false,"graffitiCellId":"id_ogd1cln","id":"79F6F77218D44C20A99A85D8CBD26DD3","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["def init_rnn_state(batch_size, num_hiddens, device):\n","    return (torch.zeros((batch_size, num_hiddens), device=device), )"]},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_cnjj602","id":"006CB36276C74F5DBE8D4988915F99CB","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["做个简单的测试来观察输出结果的个数（时间步数），以及第一个时间步的输出层输出的形状和隐藏状态的形状。"]},{"cell_type":"code","execution_count":9,"metadata":{"collapsed":false,"graffitiCellId":"id_5rof9df","id":"DFFE1C2E29C9414A86EAB4D70A62A911","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":"torch.Size([2, 5])\n256\n1027\n5 torch.Size([2, 1027])\n5 torch.Size([2, 1027])\n1 torch.Size([2, 256])\n1 torch.Size([2, 256])\n"}],"source":["print(X.shape)\n","print(num_hiddens)\n","print(vocab_size)\n","state = init_rnn_state(X.shape[0], num_hiddens, device)\n","inputs = to_onehot(X.to(device), vocab_size)\n","params = get_params()\n","outputs, state_new = rnn(inputs, state, params)\n","print(len(inputs), inputs[0].shape)\n","print(len(outputs), outputs[0].shape)\n","print(len(state), state[0].shape)\n","print(len(state_new), state_new[0].shape)"]},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_zrzqvc3","id":"7C5463AD4B364F0290B4E4A36A712481","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["### 裁剪梯度\n","\n","循环神经网络中较容易出现梯度衰减或梯度爆炸，这会导致网络几乎无法训练。裁剪梯度（clip gradient）是一种应对梯度爆炸的方法。假设我们把所有模型参数的梯度拼接成一个向量 $\\boldsymbol{g}$，并设裁剪的阈值是$\\theta$。裁剪后的梯度\n","\n","\n","$$\n"," \\min\\left(\\frac{\\theta}{\\|\\boldsymbol{g}\\|}, 1\\right)\\boldsymbol{g}\n","$$\n","\n","\n","的$L_2$范数不超过$\\theta$。\n"]},{"cell_type":"code","execution_count":10,"metadata":{"collapsed":false,"graffitiCellId":"id_ddfzc4y","id":"4845C2DF302D45B68317E8C65B9691C0","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["def grad_clipping(params, theta, device):\n","    norm = torch.tensor([0.0], device=device)\n","    for param in params:\n","        norm += (param.grad.data ** 2).sum()\n","    norm = norm.sqrt().item()\n","    if norm > theta:\n","        for param in params:\n","            param.grad.data *= (theta / norm)"]},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_qz5cf4d","id":"8C548F77B1F642E1BE5C4758F2834AF7","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["### 定义预测函数\n","\n","以下函数基于前缀`prefix`（含有数个字符的字符串）来预测接下来的`num_chars`个字符。这个函数稍显复杂，其中我们将循环神经单元`rnn`设置成了函数参数，这样在后面小节介绍其他循环神经网络时能重复使用这个函数。\n"]},{"cell_type":"code","execution_count":11,"metadata":{"collapsed":false,"graffitiCellId":"id_e4mi857","id":"228D7151704A45718C388511E8A0D1A6","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,\n","                num_hiddens, vocab_size, device, idx_to_char, char_to_idx):\n","    state = init_rnn_state(1, num_hiddens, device)\n","    output = [char_to_idx[prefix[0]]]   # output记录prefix加上预测的num_chars个字符\n","    for t in range(num_chars + len(prefix) - 1):\n","        # 将上一时间步的输出作为当前时间步的输入\n","        X = to_onehot(torch.tensor([[output[-1]]], device=device), vocab_size)\n","        # 计算输出和更新隐藏状态\n","        (Y, state) = rnn(X, state, params)\n","        # 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符\n","        if t < len(prefix) - 1:\n","            output.append(char_to_idx[prefix[t + 1]])\n","        else:\n","            output.append(Y[0].argmax(dim=1).item())\n","    return ''.join([idx_to_char[i] for i in output])"]},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_157mdwx","id":"5F877F47755642A299DF44270D5B7C54","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["我们先测试一下`predict_rnn`函数。我们将根据前缀“分开”创作长度为10个字符（不考虑前缀长度）的一段歌词。因为模型参数为随机值，所以预测结果也是随机的。"]},{"cell_type":"code","execution_count":12,"metadata":{"collapsed":false,"graffitiCellId":"id_slr2lmi","id":"753869000A07429384C791FD3131BEF8","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":"'分开暖少阵耍邂蝙彷金鹰因'"},"execution_count":12,"metadata":{},"output_type":"execute_result","transient":{}}],"source":["predict_rnn('分开', 10, rnn, params, init_rnn_state, num_hiddens, vocab_size,\n","            device, idx_to_char, char_to_idx)"]},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_kasti5n","id":"CECD70B6BFE348AD94C9B45DBEB604AE","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["### 困惑度\n","\n","我们通常使用困惑度（perplexity）来评价语言模型的好坏。回忆一下[“softmax回归”](../chapter_deep-learning-basics/softmax-regression.ipynb)一节中交叉熵损失函数的定义。困惑度是对交叉熵损失函数做指数运算后得到的值。特别地，\n","\n","* 最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；\n","* 最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；\n","* 基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。\n","\n","显然，任何一个有效模型的困惑度必须小于类别个数。在本例中，困惑度必须小于词典大小`vocab_size`。\n","\n","### 定义模型训练函数\n","\n","跟之前章节的模型训练函数相比，这里的模型训练函数有以下几点不同：\n","\n","1. 使用困惑度评价模型。\n","2. 在迭代模型参数前裁剪梯度。\n","3. 对时序数据采用不同采样方法将导致隐藏状态初始化的不同。"]},{"cell_type":"code","execution_count":13,"metadata":{"collapsed":false,"graffitiCellId":"id_yzrgxwe","id":"9FE06ACC79564A21B427C2AB90AC5BAA","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n","                          vocab_size, device, corpus_indices, idx_to_char,\n","                          char_to_idx, is_random_iter, num_epochs, num_steps,\n","                          lr, clipping_theta, batch_size, pred_period,\n","                          pred_len, prefixes):\n","    if is_random_iter:\n","        data_iter_fn = d2l.data_iter_random\n","    else:\n","        data_iter_fn = d2l.data_iter_consecutive\n","    params = get_params()\n","    loss = nn.CrossEntropyLoss()\n","\n","    for epoch in range(num_epochs):\n","        if not is_random_iter:  # 如使用相邻采样，在epoch开始时初始化隐藏状态\n","            state = init_rnn_state(batch_size, num_hiddens, device)\n","        l_sum, n, start = 0.0, 0, time.time()\n","        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, device)\n","        for X, Y in data_iter:\n","            if is_random_iter:  # 如使用随机采样，在每个小批量更新前初始化隐藏状态\n","                state = init_rnn_state(batch_size, num_hiddens, device)\n","            else:  # 否则需要使用detach函数从计算图分离隐藏状态，\n","                for s in state:\n","                    s.detach_()\n","            # inputs是num_steps个形状为(batch_size, vocab_size)的矩阵\n","            inputs = to_onehot(X, vocab_size)\n","            # outputs有num_steps个形状为(batch_size, vocab_size)的矩阵\n","            (outputs, state) = rnn(inputs, state, params)\n","            # 拼接之后形状为(num_steps * batch_size, vocab_size)\n","            outputs = torch.cat(outputs, dim=0)\n","            # Y的形状是(batch_size, num_steps)，转置后再变成形状为\n","            # (num_steps * batch_size,)的向量，这样跟输出的行一一对应\n","            y = torch.flatten(Y.T)\n","            # 使用交叉熵损失计算平均分类误差\n","            l = loss(outputs, y.long())\n","            \n","            # 梯度清0\n","            if params[0].grad is not None:  # 如果不是第一个batch\n","                for param in params:\n","                    param.grad.data.zero_()\n","            l.backward()\n","            grad_clipping(params, clipping_theta, device)  # 裁剪梯度\n","            d2l.sgd(params, lr, 1)  # 因为误差已经取过均值，梯度不用再做平均\n","            l_sum += l.item() * y.shape[0]\n","            n += y.shape[0]\n","\n","        if (epoch + 1) % pred_period == 0:\n","            print('epoch %d, perplexity %f, time %.2f sec' % (\n","                epoch + 1, math.exp(l_sum / n), time.time() - start))\n","            for prefix in prefixes:\n","                print(' -', predict_rnn(prefix, pred_len, rnn, params, init_rnn_state,\n","                    num_hiddens, vocab_size, device, idx_to_char, char_to_idx))"]},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_abcvmn2","id":"BC331C0942B549DD9CE8FF1D5FF1AA39","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["### 训练模型并创作歌词\n","\n","现在我们可以训练模型了。首先，设置模型超参数。我们将根据前缀“分开”和“不分开”分别创作长度为50个字符（不考虑前缀长度）的一段歌词。我们每过50个迭代周期便根据当前训练的模型创作一段歌词。"]},{"cell_type":"code","execution_count":14,"metadata":{"collapsed":false,"graffitiCellId":"id_r49e5nv","id":"528CB0DB2860481C8C803D377E5B5C89","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["num_epochs, num_steps, batch_size, lr, clipping_theta = 250, 35, 32, 1e2, 1e-2\n","pred_period, pred_len, prefixes = 50, 50, ['分开', '不分开']"]},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_fvpvu53","id":"867977FACDBE43528C0E17B7EC702F2C","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["下面采用随机采样训练模型并创作歌词。"]},{"cell_type":"code","execution_count":13,"metadata":{"collapsed":false,"graffitiCellId":"id_xnnajux","id":"B020BFA4A5A442D285BB1A7FC65BEA2E","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":"epoch 50, perplexity 76.018771, time 0.78 sec\n - 分开 我想要你 你的让我 我有你这 我有你的 爱女人 别怪我 别子的让我疯狂的可爱女人 坏坏的让我疯狂的\n - 不分开 爱你的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱\nepoch 100, perplexity 9.744312, time 0.73 sec\n - 分开 一直在停留 你在它 说你了的我都时有 爱 我说 这不了我 说你的 快时再 什么四有 每头忆 的片段\n - 不分开吗 我想你这 你颗我有 这样不动 我不了  怎么的重 一颗莫  没有你  我有你 说你了的太快就像 \nepoch 150, perplexity 2.798229, time 0.77 sec\n - 分开 快使用不多  唱有你 我爱你的生活 我妈你烦多离的课样 怎不了我只忠 也你看这样打 快攻的假栈人多\n - 不分开期 你爱能过去  静知着对 我不了再力 你没有美去 我的你事抽  爱有你在我有多难熬多烦恼  没有你\n"}],"source":["train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n","                      vocab_size, device, corpus_indices, idx_to_char,\n","                      char_to_idx, True, num_epochs, num_steps, lr,\n","                      clipping_theta, batch_size, pred_period, pred_len,\n","                      prefixes)"]},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_t650z1s","id":"CC35E4A71367440F8EBB8C8F13F50677","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["接下来采用相邻采样训练模型并创作歌词。"]},{"cell_type":"code","execution_count":14,"metadata":{"collapsed":false,"graffitiCellId":"id_5cubvww","id":"60A46F92EC144D4A81FFC0D73850D275","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":"epoch 50, perplexity 60.294393, time 0.74 sec\n - 分开 我想要你想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我\n - 不分开 我想要你 你有了 别不我的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我\nepoch 100, perplexity 7.141162, time 0.72 sec\n - 分开 我已要再爱 我不要再想 我不 我不 我不要再想 我不 我不 我不要 爱情我的见快就像龙卷风 离能开\n - 不分开柳 你天黄一个棍 后知哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 \nepoch 150, perplexity 2.090277, time 0.73 sec\n - 分开 我已要这是你在著 不想我都做得到 但那个人已经不是我 没有你在 我却多难熬  没有你在我有多难熬多\n - 不分开觉 你已经离 我想再好 这样心中 我一定带我 我的完空 不你是风 一一彩纵 在人心中 我一定带我妈走\nepoch 200, perplexity 1.305391, time 0.77 sec\n - 分开 我已要这样牵看你的手 它一定实现它一定像现 载著你 彷彿载著阳光 不管到你留都是晴天 蝴蝶自在飞力\n - 不分开觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生\nepoch 250, perplexity 1.230800, time 0.79 sec\n - 分开 我不要 是你看的太快了悲慢 担心今手身会大早 其么我也睡不着  昨晚梦里你来找 我才  原来我只想\n - 不分开觉 你在经离开我 不知不觉 你知了有节奏 后知后觉 后知了一个秋 后知后觉 我该好好生活 我该好好生\n"}],"source":["train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n","                      vocab_size, device, corpus_indices, idx_to_char,\n","                      char_to_idx, False, num_epochs, num_steps, lr,\n","                      clipping_theta, batch_size, pred_period, pred_len,\n","                      prefixes)"]},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_k40p6v8","id":"868B10D9B4DE4D0B93E79F5746A49D47","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["## 循环神经网络的简介实现\n","\n","### 定义模型\n","\n","我们使用Pytorch中的`nn.RNN`来构造循环神经网络。在本节中，我们主要关注`nn.RNN`的以下几个构造函数参数：\n","\n","* `input_size` - The number of expected features in the input x\n","* `hidden_size` – The number of features in the hidden state h\n","* `nonlinearity` – The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'\n","* `batch_first` – If True, then the input and output tensors are provided as (batch_size, num_steps, input_size). Default: False\n","\n","这里的`batch_first`决定了输入的形状，我们使用默认的参数`False`，对应的输入形状是 (num_steps, batch_size, input_size)。\n","\n","`forward`函数的参数为：\n","\n","* `input` of shape (num_steps, batch_size, input_size): tensor containing the features of the input sequence. \n","* `h_0` of shape (num_layers * num_directions, batch_size, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.\n","\n","`forward`函数的返回值是：\n","\n","* `output` of shape (num_steps, batch_size, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t.\n","* `h_n` of shape (num_layers * num_directions, batch_size, hidden_size): tensor containing the hidden state for t = num_steps.\n","\n","现在我们构造一个`nn.RNN`实例，并用一个简单的例子来看一下输出的形状。"]},{"cell_type":"code","execution_count":15,"metadata":{"collapsed":false,"graffitiCellId":"id_j3cpkcl","id":"0299EF9E9126417B80153A137538E01E","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":"torch.Size([35, 2, 256]) torch.Size([1, 2, 256])\n"}],"source":["rnn_layer = nn.RNN(input_size=vocab_size, hidden_size=num_hiddens)\n","num_steps, batch_size = 35, 2\n","X = torch.rand(num_steps, batch_size, vocab_size)\n","state = None\n","Y, state_new = rnn_layer(X, state)\n","print(Y.shape, state_new.shape)"]},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_m47aq8s","id":"069F7F5EE53E47FA9E67B386F372D166","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["我们定义一个完整的基于循环神经网络的语言模型。"]},{"cell_type":"code","execution_count":16,"metadata":{"collapsed":false,"graffitiCellId":"id_syy2qdo","id":"9B2540FE03B44485818E407213E44E82","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["class RNNModel(nn.Module):\n","    def __init__(self, rnn_layer, vocab_size):\n","        super(RNNModel, self).__init__()\n","        self.rnn = rnn_layer\n","        self.hidden_size = rnn_layer.hidden_size * (2 if rnn_layer.bidirectional else 1) \n","        self.vocab_size = vocab_size\n","        self.dense = nn.Linear(self.hidden_size, vocab_size)\n","\n","    def forward(self, inputs, state):\n","        # inputs.shape: (batch_size, num_steps)\n","        X = to_onehot(inputs, vocab_size)\n","        X = torch.stack(X)  # X.shape: (num_steps, batch_size, vocab_size)\n","        hiddens, state = self.rnn(X, state)\n","        hiddens = hiddens.view(-1, hiddens.shape[-1])  # hiddens.shape: (num_steps * batch_size, hidden_size)\n","        output = self.dense(hiddens)\n","        return output, state"]},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_soot8nz","id":"288D99C0B51F45659BB0F39C2B5AAD9B","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["类似的，我们需要实现一个预测函数，与前面的区别在于前向计算和初始化隐藏状态。"]},{"cell_type":"code","execution_count":17,"metadata":{"collapsed":false,"graffitiCellId":"id_dkebt7t","id":"470CA7EEF6C2443980C3D327EB4BAD73","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["def predict_rnn_pytorch(prefix, num_chars, model, vocab_size, device, idx_to_char,\n","                      char_to_idx):\n","    state = None\n","    output = [char_to_idx[prefix[0]]]  # output记录prefix加上预测的num_chars个字符\n","    for t in range(num_chars + len(prefix) - 1):\n","        X = torch.tensor([output[-1]], device=device).view(1, 1)\n","        (Y, state) = model(X, state)  # 前向计算不需要传入模型参数\n","        if t < len(prefix) - 1:\n","            output.append(char_to_idx[prefix[t + 1]])\n","        else:\n","            output.append(Y.argmax(dim=1).item())\n","    return ''.join([idx_to_char[i] for i in output])"]},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_6dx4k76","id":"A1009A6DA48749D883D7DEE75F8B55EF","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["使用权重为随机值的模型来预测一次。"]},{"cell_type":"code","execution_count":18,"metadata":{"collapsed":false,"graffitiCellId":"id_dovnvrc","id":"7000F425557A4047A9D7B0BD446C4B48","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":"'分开胸呵以轮轮轮轮轮轮轮'"},"execution_count":18,"metadata":{},"output_type":"execute_result","transient":{}}],"source":["model = RNNModel(rnn_layer, vocab_size).to(device)\n","predict_rnn_pytorch('分开', 10, model, vocab_size, device, idx_to_char, char_to_idx)"]},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_3bm44z0","id":"F3BA8B41B5E14E2191674147D2022A39","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["接下来实现训练函数，这里只使用了相邻采样。"]},{"cell_type":"code","execution_count":19,"metadata":{"collapsed":false,"graffitiCellId":"id_kqdlzc9","id":"C6B0157AB1964E14AD52BFC3B4E8ACCE","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["def train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\n","                                corpus_indices, idx_to_char, char_to_idx,\n","                                num_epochs, num_steps, lr, clipping_theta,\n","                                batch_size, pred_period, pred_len, prefixes):\n","    loss = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    model.to(device)\n","    for epoch in range(num_epochs):\n","        l_sum, n, start = 0.0, 0, time.time()\n","        data_iter = d2l.data_iter_consecutive(corpus_indices, batch_size, num_steps, device) # 相邻采样\n","        state = None\n","        for X, Y in data_iter:\n","            if state is not None:\n","                # 使用detach函数从计算图分离隐藏状态\n","                if isinstance (state, tuple): # LSTM, state:(h, c)  \n","                    state[0].detach_()\n","                    state[1].detach_()\n","                else: \n","                    state.detach_()\n","            (output, state) = model(X, state) # output.shape: (num_steps * batch_size, vocab_size)\n","            y = torch.flatten(Y.T)\n","            l = loss(output, y.long())\n","            \n","            optimizer.zero_grad()\n","            l.backward()\n","            grad_clipping(model.parameters(), clipping_theta, device)\n","            optimizer.step()\n","            l_sum += l.item() * y.shape[0]\n","            n += y.shape[0]\n","        \n","\n","        if (epoch + 1) % pred_period == 0:\n","            print('epoch %d, perplexity %f, time %.2f sec' % (\n","                epoch + 1, math.exp(l_sum / n), time.time() - start))\n","            for prefix in prefixes:\n","                print(' -', predict_rnn_pytorch(\n","                    prefix, pred_len, model, vocab_size, device, idx_to_char,\n","                    char_to_idx))"]},{"cell_type":"markdown","metadata":{"graffitiCellId":"id_0ejh2ag","id":"E271EA2BA38045BF93AD1CD9484A3194","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["训练模型。"]},{"cell_type":"code","execution_count":20,"metadata":{"collapsed":false,"graffitiCellId":"id_kn2ynjc","id":"1FC86D37312346418E960DE255315FC0","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":"epoch 50, perplexity 9.405654, time 0.52 sec\n - 分开始一起 三步四步望著天 看星星 一颗两颗三颗四颗 连成线背著背默默许下心愿  一枝杨柳 你的那我 在\n - 不分开 爱情你的手 一人的老斑鸠 腿短毛不多 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍\nepoch 100, perplexity 1.255020, time 0.54 sec\n - 分开 我人了的屋我 一定令它心仪的母斑鸠 爱像一阵风 吹完美主  这样 还人的太快就是学怕眼口让我碰恨这\n - 不分开不想我多的脑袋有问题 随便说说 其实我早已经猜透看透不想多说 只是我怕眼泪撑不住 不懂 你的黑色幽默\nepoch 150, perplexity 1.064527, time 0.53 sec\n - 分开 我轻外的溪边 默默在一心抽离 有话不知不觉 一场悲剧 我对不起 藤蔓植物的爬满了伯爵的坟墓 古堡里\n - 不分开不想不多的脑 有教堂有你笑 我有多烦恼  没有你烦 有有样 别怪走 快后悔没说你 我不多难熬 我想就\nepoch 200, perplexity 1.033074, time 0.53 sec\n - 分开 我轻外的溪边 默默在一心向昏 的愿  古无着我只能 一个黑远 这想太久 这样我 不要再是你打我妈妈\n - 不分开你只会我一起睡著 样 娘子却只想你和汉堡 我想要你的微笑每天都能看到  我知道这里很美但家乡的你更美\nepoch 250, perplexity 1.047890, time 0.68 sec\n - 分开 我轻多的漫 却已在你人演  想要再直你 我想要这样牵着你的手不放开 爱可不可以简简单单没有伤害 你\n - 不分开不想不多的假  已无能为力再提起 决定中断熟悉 然后在这里 不限日期 然后将过去 慢慢温习 让我爱上\n"}],"source":["num_epochs, batch_size, lr, clipping_theta = 250, 32, 1e-3, 1e-2\n","pred_period, pred_len, prefixes = 50, 50, ['分开', '不分开']\n","train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,\n","                            corpus_indices, idx_to_char, char_to_idx,\n","                            num_epochs, num_steps, lr, clipping_theta,\n","                            batch_size, pred_period, pred_len, prefixes)"]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
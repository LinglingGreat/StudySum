{"cells":[{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"87CC794C249F4CE188AD39B775DD1283","mdEditEnable":false},"source":"# 卷积神经网络开放题"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"E6CC908D90834F39897857E572B228BD","mdEditEnable":false},"source":"# 数据集\n本次开放题将与课程内容保持一致，将使用图像数据集Fashion-MNIST [1] 进行计算机视觉任务的设计，该数据集由衣服、鞋子等服饰组成，共10个类别。\n\n这里简介将此数据集转换成卷积神经网络所需要的输入格式的方法：\n\n## 加载数据集\n\n首先导入本作业需要的包或模块。"},{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false,"id":"2485EB5129604290A8A6997F1BD959C2","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":"import torchvision\nimport torch\nfrom matplotlib import pyplot as plt\nfrom IPython import display"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"96B1F57BDC27460B8813B1BAA85B2C91","mdEditEnable":false},"source":"通过`load_data_fashion_mnist`函数对数据集进行加载，另外函数还指定了参数`transform = transforms.ToTensor()`使所有数据转换为`Tensor`，如果不进行转换则返回的是PIL图片。`transforms.ToTensor()`将尺寸为 (H x W x C) 且数据位于 [0, 255] 的PIL图片或者数据类型为`np.uint8`的NumPy数组转换为尺寸为 (C x H x W) 且数据类型为`torch.float32`且位于 [0.0, 1.0] 的`Tensor`。\n\n我们将在训练数据集上训练模型，并将训练好的模型在测试数据集上评价模型的表现。函数中`mnist_train`是`torch.utils.data.Dataset`的子类，所以我们可以将其传入`torch.utils.data.DataLoader`来创建一个读取小批量数据样本的`DataLoader`实例。\n\n在实践中，数据读取经常是训练的性能瓶颈，特别当模型较简单或者计算硬件性能较高时。PyTorch的`DataLoader`中一个很方便的功能是允许使用多进程来加速数据读取。这里我们通过参数`num_workers`来设置进程读取数据。"},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":false,"id":"B27EC0CC4BE04EE784C5B1709DE8A9F0","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":"def load_data_fashion_mnist(batch_size, resize=None, root=\"/home/kesci/input/FashionMNIST2065/\"):\n    \"\"\"Download the fashion mnist dataset and then load into memory.\"\"\"\n    trans = []\n    if resize:\n        trans.append(torchvision.transforms.Resize(size=resize))\n    trans.append(torchvision.transforms.ToTensor())\n\n    transform = torchvision.transforms.Compose(trans)\n    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\n    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\n\n    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n\n    return train_iter, test_iter"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"998590ABF38C45C78E789B6114D4BA55","mdEditEnable":false},"source":"对于AlexNet，我们需要将 Fashion-MNIST 数据集的图像高和宽扩大到224，这个可以通过在`load_data_fashion_mnist`中传入`Resize`来实现，这边设置数据的`batch_size`为128。"},{"cell_type":"code","execution_count":3,"metadata":{"collapsed":false,"id":"1CAD37F80172425585602A2603B55935","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":"batch_size = 128\n# 如出现“out of memory”的报错信息，可减小batch_size或resize\ntrain_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"3F9B18D7B324482B87552DC2F85FE886","mdEditEnable":false},"source":"Fashion-MNIST中一共包括了10个类别，分别为t-shirt（T恤）、trouser（裤子）、pullover（套衫）、dress（连衣裙）、coat（外套）、sandal（凉鞋）、shirt（衬衫）、sneaker（运动鞋）、bag（包）和ankle boot（短靴）。以下函数可以将数值标签转成相应的文本标签。"},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":false,"id":"CDB627CAEF934CACA1C892E62E6661BF","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":"def get_fashion_mnist_labels(labels):\n    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n    return [text_labels[int(i)] for i in labels]"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0DBBF22FE7994F69801387BA35EADF22","mdEditEnable":false},"source":"下面定义一个可以在一行里画出多张图像和对应标签的函数。"},{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"710431E0A0F148AA824DA10029AAED23","scrolled":false},"outputs":[],"source":"def show_fashion_mnist(images, labels):\n    \"\"\"Use svg format to display plot in jupyter\"\"\"\n    display.set_matplotlib_formats('svg')\n    # 这里的_表示我们忽略（不使用）的变量\n    _, figs = plt.subplots(1, len(images), figsize=(12, 12))\n    for f, img, lbl in zip(figs, images, labels):\n        f.imshow(img.view((224, 224)).numpy())\n        f.set_title(lbl)\n        f.axes.get_xaxis().set_visible(False)\n        f.axes.get_yaxis().set_visible(False)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"9989FA74F34545608F888AE5FCAA8393","mdEditEnable":false},"source":"读取训练数据集中第一个`batch`的数据。"},{"cell_type":"code","execution_count":6,"metadata":{"id":"0DB5E5D5D3734DB98320B9E074D43356","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"collapsed":false},"outputs":[],"source":"train_data = iter(train_iter)\nimages, labels = next(train_data)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C5CB466943BE46ACBA2AD08F35C2C2CC","mdEditEnable":false},"source":"现在，我们看一下训练数据集中前10个样本的图像内容和文本标签。"},{"cell_type":"code","execution_count":7,"metadata":{"id":"4BA010F52C054BB39AD024A9FEE7F954","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[],"collapsed":false},"outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 864x864 with 10 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/4BA010F52C054BB39AD024A9FEE7F954/q5uqr0np0k.svg\">"},"transient":{}}],"source":"labels = get_fashion_mnist_labels(labels)\nshow_fashion_mnist(images[:10], labels[:10])\nplt.show()"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0E1F3F3156CB4C599ECC4B4841226912","mdEditEnable":false},"source":"# 卷积神经网络（CNN）\n\n对于计算机视觉的分类任务，在很长一段时间里流行的是研究者通过经验与智慧所设计并生成的手工特征。这类图像分类研究的主要流程是：\n\n1. 获取图像数据集；\n2. 使用已有的特征提取函数生成图像的特征；\n3. 使用机器学习模型对图像的特征分类。\n\n卷积神经网络就是含卷积层的神经网络，深度卷积神经网络的兴起改变了计算机视觉任务中手工设计的特征的传统，引领了诸多影响深远的研究。"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"5094457635E64A7CBA319089C2F73E16","mdEditEnable":false},"source":"## LeNet\n\n\n\nLeNet [2] 作为一个早期用来识别手写数字图像的卷积神经网络，展示了通过梯度下降训练卷积神经网络可以达到手写数字识别在当时最先进的结果。如下图所示：\n\n\n![LeNet模型](https://d2l.ai/_images/lenet.svg)\n\n\nLeNet的模型结构分为卷积层块和全连接层块两个部分：\n\n- 卷积层保留输入形状，使图像的像素在高和宽两个方向上的相关性均可能被有效识别，并且通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。\n\n- 全连接层块将卷积层块的输出中每个样本变平（flatten），即输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，从而进行分类。\n\n\n\n## AlexNet\n\n2012年，AlexNet [3] 横空出世，使用了8层卷积神经网络，并以很大的优势赢得了ImageNet 2012图像识别挑战赛。\n\nAlexNet与LeNet的设计理念非常相似，但相对较小的LeNet相比，AlexNet包含5层卷积和2层全连接隐藏层，以及1个全连接输出层，模型参数也大大增加。由于早期显存的限制，最早的AlexNet使用双数据流的设计，使一个GPU只需要处理一半模型。如下图所示：\n\n![AlexNet模型](https://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.6_alexnet.png)\n\nAlexNet首次证明了神经网络以端到端（end-to-end）的方式学习到的特征可以超越手工设计的特征，从而一举打破计算机视觉研究的前状。\n\n"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4F3514A6E3774CE2876BF7A2056AE919","mdEditEnable":false},"source":"### 问题一：\n- 阅读提出上述两种网络的相关论文，试从数据集的预处理、激活函数的使用、训练方法的改进以及模型结构的变化等角度，从理论层面分析比较LeNet与AlexNet的结构差异，并尝试解释AlexNet为什么会具有对计算机视觉任务优越的处理性能。\n- AlexNet对Fashion-MNIST数据集来说可能过于复杂，请尝试对模型进行简化来使训练更快，同时保证分类准确率（accuracy）不明显下降（不低于85%），并将简化后的结构、节省的训练时间以及下降的准确率等相关指标以表格的形式进行总结分析。"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0CA7F6A308BF48CEAB9CC73FDD933F71","mdEditEnable":false},"source":"## VGG\n\nAlexNet在LeNet的基础上增加了3个卷积层，同时对网络的卷积窗口、输出通道数和构造顺序均做了大量的调整。AlexNet指明了深度卷积神经网络可以取得出色的结果，基于这一个理念，牛津大学的实验室Visual Geometry Group实验室提出了VGG [4] 网络，提供了通过重复使用简单的基础块来构建深度模型的思路。VGG每个基础块组成规律是：连续使用数个相同的填充为1、窗口形状为$3\\times 3$的卷积层后接上一个步幅为2、窗口形状为$2\\times 2$的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。如下图所示：\n\n![VGG模型](https://boyuai.oss-cn-shanghai.aliyuncs.com/disk/YouthAI%E7%A7%8B%E5%AD%A3%E6%80%9D%E7%BB%B4%E7%8F%AD-%E4%B8%8A%E8%AF%BE%E8%A7%86%E9%A2%91/vgg.jpg)\n\n可以看到，每次经过基础块以后，网络会将输入的高和宽减半，直到最终高和宽变成7后传入全连接层。与此同时，输出通道数每次翻倍，直到变成512。因为每个卷积层的窗口大小一样，VGG这种高和宽减半以及通道翻倍的设计使得多数卷积层都有相同的模型参数尺寸和计算复杂度。"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"17EF548115E14894860A16C6032B5B31","mdEditEnable":false},"source":"### 问题二：\n- LeNet与AlexNet在接近图像输入的卷积模块中都引入了较大尺寸的卷积核，如$5\\times 5$或者$7\\times 7$的卷积窗口来捕捉更大范围的图像信息，试分析VGG每个基础块的固定设计是否会影响到图像的粗粒度信息提取，并且对比不同结构模块输出的特征图进行对比。\n- 尝试将Fashion-MNIST中图像的高和宽由224改为96，试分析VGG网络的参数变化情况，并且对比模型训练时间、分类准确率（accuracy）等实验指标受到的影响，以表格的形式进行总结分析。"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"3F8AC63D06C14BF6894F6C7C1FCD6137","mdEditEnable":false},"source":"## NiN\n\n之前介绍的LeNet、AlexNet和VGG在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。其中，AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深。网络中的网络（NiN）[5] 提出了另外一个思路，即串联多个由卷积层和“全连接”层构成的小网络来构建一个深层网络。\n\n卷积层的输入和输出通常是四维数组（样本，通道，高，宽），而全连接层的输入和输出则通常是二维数组（样本，特征）。如果想在全连接层后再接上卷积层，则需要将全连接层的输出变换为四维，$1\\times 1$卷积层可以看成全连接层，其中空间维度（高和宽）上的每个元素相当于样本，通道相当于特征。因此，NiN使用$1\\times 1$卷积层来替代全连接层，从而使空间信息能够自然传递到后面的层中去。下图对比了NiN同AlexNet和VGG等网络在结构上的主要区别。\n\n![左图是AlexNet和VGG的网络结构局部，右图是NiN的网络结构局部](http://zh.d2l.ai/_images/nin.svg)\n\nNiN块是NiN中的基础块。它由一个卷积层加两个充当全连接层的$1\\times 1$卷积层串联而成。其中第一个卷积层的超参数可以自行设置，而第二和第三个卷积层的超参数一般是固定的。\n\n\n## GoogLeNet\n\n在2014年的ImageNet图像识别挑战赛中，一个名叫GoogLeNet的网络结构大放异彩 [6] ，它虽然在名字上向LeNet致敬，但在网络结构上已经很难看到LeNet的影子。GoogLeNet吸收了NiN中网络串联网络的思想，并在此基础上做了很大改进。GoogLeNet中的基础卷积块叫作Inception块，得名于同名电影《盗梦空间》（Inception）。与上NiN块相比，这个基础块在结构上更加复杂，如下图所示：\n\n![Inception块的结构](http://zh.d2l.ai/_images/inception.svg)\n\nInception块里有4条并行的线路。前3条线路使用窗口大小分别是$1\\times 1$、$3\\times 3$和$5\\times 5$的卷积层来抽取不同空间尺寸下的信息，其中中间2个线路会对输入先做$1\\times 1$卷积来减少输入通道数，以降低模型复杂度。第四条线路则使用$3\\times 3$最大池化层，后接$1\\times 1$卷积层来改变通道数。4条线路都使用了合适的填充来使输入与输出的高和宽一致。最后将每条线路的输出在通道维上连结，并输入接下来的层中去。Inception块中可以自定义的超参数是每个层的输出通道数，以此来控制模型复杂度。"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"CDA58F8944734924A3AAC545E79942FA","mdEditEnable":false},"source":"### 问题三：\n- 对比AlexNet、VGG和NiN、GoogLeNet的模型参数尺寸，从理论的层面分析为什么后两个网络可以显著减小模型参数尺寸？\n- GoogLeNet有数个后续版本，包括加入批量归一化层 [7]、对Inception块做调整 [8] 和加入残差连接 [9]，请尝试实现并运行它们，然后观察实验结果，以表格的形式进行总结分析。"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"CBDF0F702333469D8FA63476FDC4B4C5","mdEditEnable":false},"source":"## 参考文献\n[1] Xiao, H., Rasul, K., & Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747.\n\n[2] LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278-2324.\n\n[3] Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems (pp. 1097-1105).\n\n[4] Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.\n\n[5] Lin, M., Chen, Q., & Yan, S. (2013). Network in network. arXiv preprint arXiv:1312.4400.\n\n[6] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., & Anguelov, D. & Rabinovich, A.(2015). Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1-9).\n\n[7] Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167.\n\n[8] Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., & Wojna, Z. (2016). Rethinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2818-2826).\n\n[9] Szegedy, C., Ioffe, S., Vanhoucke, V., & Alemi, A. A. (2017, February). Inception-v4, inception-resnet and the impact of residual connections on learning. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 4, p. 12)."},{"metadata":{"id":"5CF3616CCDDB4465BB50FD0A0EC1C7A5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 项目报告\n本次大作业的终审评估以项目报告作为重要依据，开放题报告的内容和排版要求请下载文件：\n\n\n[termproject2.zip](https://boyuai.oss-cn-shanghai.aliyuncs.com/disk/YouthAI%E7%A7%8B%E5%AD%A3%E6%80%9D%E7%BB%B4%E7%8F%AD-%E4%B8%8A%E8%AF%BE%E8%A7%86%E9%A2%91/termproject2.zip)\n\n需要注意的是，文件中：\n- `termproject.pdf`提供了项目报告的内容格式要求\n- `termproject_exp.pdf`提供了项目报告的内容排版样例\n\n\n推荐使用`LaTeX`软件进行报告的撰写，相关`.tex`以及`.sty`源文件一并附于文件夹中。\n"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}
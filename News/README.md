2024/4/1

[A21 Labs宣布开源520亿参数的全新混合专家大模型（Mixture of Experts，MoE）Jamba：单个GPU的上下文长度是Mixtral 8x7B的三倍](https://www.datalearner.com/blog/1051711641710005)



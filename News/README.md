## 2024.4

### MOE相关的模型

[A21 Labs宣布开源520亿参数的全新混合专家大模型（Mixture of Experts，MoE）Jamba：单个GPU的上下文长度是Mixtral 8x7B的三倍](https://www.datalearner.com/blog/1051711641710005)

[开源大模型再上台阶：Databricks开源1320亿参数的混合专家大模型DBRX-16*12B，评测超Mixtral-MoE！](https://mp.weixin.qq.com/s/dkx0UU2PgR_CpaVa88KcZQ)

[重磅！阿里开源自家首个MoE技术大模型：Qwen1.5-MoE-A2.7B，性能约等于70亿参数规模的大模型Mistral-7B](https://mp.weixin.qq.com/s/XHFjybR3GIg4RIpBlndVGg)

[马斯克旗下xAI发布Grok-1.5，相比较开源的Grok-1，各项性能大幅提升，接近GPT-4！](https://www.datalearner.com/blog/1051711675314896#google_vignette)

### 长文本

[超长文本无损能力压测！中文大模型“大海捞针”首批结果公布](https://mp.weixin.qq.com/s/QgoRf2LB-7vc3vTFOHJkpw)


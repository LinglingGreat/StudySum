
## 图像

### OmniTry：虚拟试穿

OmniTry:  Virtual Try-On Anything without Masks

项目地址：https://omnitry.github.io

作者：Kunbyte AI，浙江大学

是否开源：代码、模型权重以及评测基准将会公开发布。

摘要：

![](img/本周论文汇总(9.-9.20)-20250920215238.png)

虚拟试穿（Virtual Try-ON, VTON）是一项实用且应用广泛的任务，但现有的大多数研究主要集中在服装领域。本文提出了 **OmniTry**，一个统一的框架，将 VTON 从服装扩展到任意可穿戴物品，例如首饰和配饰，并采用 **无掩码（mask-free）设置**，以实现更实用的应用。

在扩展到多种类型物品时，数据整理面临挑战——尤其是获取配对图像（即单独的物品图像和对应的试穿结果）。为了解决这一问题，我们提出了一个两阶段的管道流程：

- **第一阶段**：我们利用大规模未配对图像（即包含任意可穿戴物品的人像）来训练模型，实现无掩码的物品定位。具体来说，我们重新利用图像修复模型（inpainting），在给定空白掩码的情况下，自动在合适位置绘制物品。
    
- **第二阶段**：在配对图像上对模型进一步微调，从而实现物品外观一致性的迁移。
    

我们发现，经过第一阶段训练后的模型，即便只使用少量配对样本，也能快速收敛。

在一个涵盖 **12 种常见可穿戴物品类别**的综合基准（包括商店内数据和真实场景数据）上进行评估时，实验结果表明 OmniTry 在 **物品定位** 和 **身份保持（ID-preservation）** 方面均优于现有方法。

![](img/本周论文汇总(9.20)-20250920215333.png)




## 视频

### HuMo：基于文本、图像、音频等多条件的视频生成

HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning

项目地址： https://phantom-video.github.io/HuMo

作者：清华大学，字节跳动智能创作团队（Intelligent Creation Team）

是否开源：开源推理代码、模型、训练数据

摘要：

![](img/本周论文汇总(9.20)-20250920235550.png)

**以人为中心的视频生成（Human-Centric Video Generation, HCVG）** 方法旨在从多模态输入（包括文本、图像和音频）合成人体视频。现有方法在有效协调这些异构模态方面存在困难，主要受两大挑战的制约：一是缺乏同时具备文本、图像和音频三元配对条件的训练数据，二是难以在多模态输入下协同完成 **主体保持（subject preservation）** 和 **视听同步（audio-visual sync）** 两项子任务。

在本文中，我们提出了 **HuMo** ——一个用于协同多模态控制的统一 HCVG 框架。针对第一个挑战，我们构建了一个高质量的数据集，涵盖多样化并配对的文本、参考图像与音频。针对第二个挑战，我们提出了一种 **两阶段渐进式多模态训练范式**，并结合任务特定的策略：

- **主体保持任务**：为保持基础模型的提示跟随能力和视觉生成能力，我们采用 **最小侵入式图像注入策略**（minimal-invasive image injection）。
    
- **视听同步任务**：在常用的音频交叉注意力层之外，我们提出了一种 **预测聚焦（focus-by-predicting）策略**，通过隐式方式引导模型将音频与人脸区域建立关联。
    

在跨多模态输入的可控性联合学习中，我们在已习得的能力基础上，逐步引入视听同步任务。推理阶段，为实现灵活且细粒度的多模态控制，我们设计了 **时序自适应的无分类器引导（time-adaptive Classifier-Free Guidance, CFG）策略**，能够在去噪步骤中动态调整引导权重。

大量实验结果表明，**HuMo 在各子任务上均超越了专用的最新方法**，从而建立了一个统一的、可协同多模态条件控制的 HCVG 框架。

它支持的任务包括：

- **VideoGen from Text-Image** - 使用文本提示与参考图像相结合，自定义角色外观、服装、妆容、道具和场景。
- **VideoGen from Text-Audio** - 仅从文本和音频输入生成音频同步视频，无需图像引用并实现更大的创作自由。
- **VideoGen from Text-Image-Audio** - 通过结合文本、图像和音频指导，实现更高级别的定制和控制。

### Vivid-VR：视频修复

Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration

项目地址：https://csbhr.github.io/projects/vivid-vr

作者：阿里巴巴集团 - 淘宝 & 天猫集团

是否开源：开源模型、推理代码、测试集

摘要：

![](img/本周论文汇总(9.20)-20250920222559.png)

我们提出 **Vivid-VR**，一种基于 **DiT（Diffusion Transformer）** 的生成式视频修复方法，构建于先进的 **文本到视频（T2V）基础模型** 之上，并结合 **ControlNet** 来控制生成过程，从而确保内容一致性。

然而，传统对这类可控生成管线的微调往往受到 **多模态对齐不完善** 的限制，导致 **分布偏移（distribution drift）**，从而削弱了纹理真实感和时间一致性。为解决这一问题，我们提出了一种 **概念蒸馏训练策略**：利用预训练的 T2V 模型合成带有嵌入文本概念的训练样本，将其概念理解能力进行蒸馏，从而在生成中保持 **纹理质量与时间一致性**。

为了进一步提升生成的可控性，我们重新设计了控制架构，包含两个关键组件：

1. **控制特征投影器（control feature projector）** —— 过滤输入视频潜空间中的退化伪影，减少其在生成管线中的传播；
    
2. **全新 ControlNet 连接器（connector）** —— 采用 **双分支设计**，将基于 MLP 的特征映射与交叉注意力机制相结合，用于动态检索控制特征，实现 **内容保持** 与 **控制信号自适应调节** 的协同。
    

在大量实验中，**Vivid-VR 在合成数据、真实场景基准以及 AIGC 视频上均优于现有方法**，在纹理真实感、视觉生动性和时间一致性方面取得了令人瞩目的效果。



### Lumen：使用视频生成模型实现一致的视频重新照明和和谐的背景替换

Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models

项目地址： https://lumen-relight.github.io

作者：北京大学，Kunbyte AI，中国科学院，浙江大学，杭州师范大学

是否开源：推理代码、模型、训练代码、训练数据

摘要：

![](img/本周论文汇总(9.20)-20250920235443.png)

**视频重光照（Video Relighting）** 是一项具有挑战但极具价值的任务，其目标是在替换视频背景的同时，相应地调整前景的光照效果，从而实现和谐的融合。在这一过程中，必须保留前景的原始属性（如反照率 _albedo_），并在时间序列上保持一致的重光照效果。以往的研究主要依赖 **3D 仿真**，而近期的工作则利用 **扩散生成模型（diffusion generative models）** 的泛化能力，实现了可学习的图像重光照。

本文提出了 **Lumen** ——一个基于大规模视频生成模型的端到端视频重光照框架。该框架能够接收灵活的文本描述，用于指导光照和背景的控制。鉴于在不同光照条件下保持相同前景的高质量配对视频稀缺，我们构建了一个大规模数据集，融合了 **真实视频与合成视频**。在合成领域，我们依托社区中丰富的 **3D 资产**，利用先进的 3D 渲染引擎，生成多环境下的视频对；在真实领域，我们采用 **基于 HDR 的光照模拟**，以弥补缺乏真实场景配对视频的不足。

基于上述数据集，我们设计了一种 **联合训练课程（joint training curriculum）**，充分发挥两个领域的优势：合成视频中的物理一致性，以及真实视频中的广泛分布泛化能力。为此，我们在模型中引入 **域感知适配器（domain-aware adapter）**，以解耦重光照学习和领域外观分布的建模。

我们还构建了一个 **综合基准**，从 **前景保持** 和 **视频一致性评估** 两个维度，对 Lumen 及现有方法进行评测。实验结果表明，**Lumen 能够有效地将输入视频编辑为具有电影感的重光照视频**，同时实现光照一致性和严格的前景保持。



### InfinityHuman：具有自然手势的音频驱动的人类动画

InfinityHuman：Towards Long-Term Audio-Driven Human Animation

项目地址： https://infinityhuman.github.io/

作者：字节跳动，浙江大学

是否开源：代码

摘要：

![](img/本周论文汇总(9.20)-20250920235416.png)

**音频驱动的人体动画（Audio-driven human animation）** 因其广泛的应用价值而备受关注。然而，当前方法在生成 **高分辨率、长时长** 且具有 **外观一致性** 和 **自然手部动作** 的视频方面仍面临重大挑战。现有方法通常通过重叠运动帧的方式扩展视频，但容易出现 **误差累积**，进而导致 **身份漂移（identity drift）**、**颜色偏移** 以及 **场景不稳定** 等问题。此外，手部动作的建模能力不足，常常造成明显的变形和与音频的不对齐。

为此，本文提出了 **InfinityHuman** ——一个 **由粗到细的生成框架**。该框架首先生成与音频同步的表示，然后通过 **基于姿态（pose-guided）的细化器**，逐步将其提升为 **高分辨率、长时长的视频**。由于 **姿态序列与外观解耦** 且能抵御时间退化，我们的姿态引导细化器利用 **稳定的姿态信息** 和 **首帧作为视觉锚点**，有效减少了身份漂移并提升了唇形同步效果。

此外，为了增强 **语义准确性和手势真实感**，我们引入了一种 **手部专用奖励机制（hand-specific reward mechanism）**，该机制使用高质量的手部动作数据进行训练，从而显著改善了手部动作的自然度与准确性。

在 **EMTD** 和 **HDTF** 数据集上的实验结果表明，**InfinityHuman 在视频质量、身份保持、手部准确性以及唇形同步方面均达到了当前最优性能（state-of-the-art）**。消融实验进一步验证了各个模块的有效性。



### OmniHuman-1.5：根据图像和音轨生成富有表现力的角色动画

OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation

项目地址： https://omnihuman-lab.github.io/v1_5

作者：字节跳动智能创作团队

是否开源：否

摘要：

![](img/本周论文汇总(9.20)-20250920235346.png)

现有的视频虚拟人模型虽然能够生成流畅的人体动画，但它们往往停留在**物理层面的相似性**，难以捕捉角色的**真实内涵**。其动作通常仅与诸如音频节奏等低层次线索同步，而缺乏对**情感、意图或语境**的深层语义理解。

为弥合这一差距，**我们提出了一个框架，旨在生成不仅物理上合理，而且语义上连贯且富有表现力的角色动画**。

我们的模型 **OmniHuman-1.5** 基于两个核心技术创新：

1. 我们利用 **多模态大语言模型（Multimodal LLMs）** 来合成结构化的条件文本表示，从而提供高层次的语义引导。这种引导能够将动作生成器从单纯的节奏同步扩展到具备 **语境与情感共鸣** 的动作生成。
    
2. 为确保多模态输入的有效融合并缓解模态间冲突，我们提出了一种专门的 **多模态 DiT 架构**，并引入了新颖的 **伪末帧（Pseudo Last Frame）设计**。通过这些机制的协同作用，模型能够准确解读 **音频、图像与文本的联合语义**，从而生成与角色、场景及语言内容高度契合的动作。
    

大量实验表明，我们的模型在 **唇形同步精度、视频质量、动作自然度以及与文本提示的语义一致性** 等综合指标上均取得了领先性能。此外，该方法还展现出卓越的可扩展性，能够适用于复杂场景，如多人物互动和非人类主体的动画生成。



### UniVerse-1：从参考图像和文本提示生成同步的音频和视频

UniVerse-1: Unified Audio-Video Generation via Stitching of Experts

项目地址： https://dorniwang.github.io/UniVerse-1

作者：阶跃星辰、香港科技大学（广州）、香港科技大学、清华大学

是否开源：开源模型、代码

摘要：

![](img/本周论文汇总(9.20)-20250920235216.png)

我们提出 **UniVerse-1** ——一个类似 **Veo-3** 的统一模型，能够同时生成 **协同一致的音频与视频**。

为提升训练效率，我们并未从零开始训练，而是采用了一种 **“专家拼接（stitching of expertise）” 技术**。该方法将经过预训练的 **视频生成模型** 与 **音乐生成模型** 中的对应模块进行深度融合，从而充分利用其基础能力。

在训练数据构建上，为保证 **环境音与语音** 与视频内容的准确标注和时间对齐，我们设计了一条 **在线标注流水线**，在训练过程中实时处理所需数据并生成标签。这一策略有效避免了传统 **基于文本标注** 的方式因对齐误差而导致的性能下降。

得益于上述技术协同，我们的模型在 **约 7,600 小时的音视频数据** 上微调后，能够生成具有良好 **音画协同一致性** 的环境声音，并在语音生成方面展现出 **强对齐能力**。

为了系统评估所提方法，我们构建了一个全新的基准数据集 **Verse-Bench**。同时，为推动 **音视频生成** 研究并缩小与 **Veo-3 等最先进模型** 的差距，我们将模型与代码公开发布，希望能惠及更广泛的研究社区。

- **统一音视频合成**：具有串联生成音频和视频的迷人能力。它解释输入提示以产生完美同步的视听输出。
    
- **语音音频生成**：该模型可以直接从文本提示生成流畅的语音，展示内置的文本转语音 （TTS） 能力。至关重要的是，它定制语音音色以匹配正在生成的特定角色。
    
- **乐器演奏声音生成**：该模型还非常熟练地创建乐器的声音。此外，它还提供了一些“边弹边唱”的功能，可以同时生成声乐和器乐曲目。
    
- **环境声音生成**：模型可以生成环境声音，产生与视频视觉环境相匹配的背景音频。
    
- **第一个开源的基于 Dit 的音视频联合方法**：我们是第一个开源基于 DiT 的、类似 Veo-3 的联合视听生成模型的公司。



## 音频


### VibeVoice：支持多说话人的TTS

VibeVoice: A Frontier Open-Source Text-to-Speech Model

项目地址：https://microsoft.github.io/VibeVoice

作者：微软研究院

是否开源：开源模型

摘要：

![](img/本周论文汇总(9.20)-20250920233922.png)

**VibeVoice** 是一个全新的框架，旨在从文本生成富有表现力的 **长时、多说话人对话音频**（如播客）。它解决了传统 **文本转语音（TTS）系统** 在 **可扩展性、说话人一致性** 和 **自然轮换发言** 等方面的关键挑战。

其核心创新在于使用 **连续语音分词器（Acoustic 与 Semantic）**，并以 **超低帧率 7.5 Hz** 运行。这种设计既能高效保真地保留音频质量，又显著提升了长序列处理的计算效率。

在生成方式上，VibeVoice 采用 **下一个 token 扩散（next-token diffusion）框架**：

- 通过 **大语言模型（LLM）** 理解文本上下文与对话流；
    
- 由 **扩散头（diffusion head）** 生成高保真的声学细节。
    

借助这一机制，模型能够合成 **长达 90 分钟**、包含 **多达 4 位不同说话人** 的音频，突破了以往模型通常仅能支持 **1–2 位说话人** 的局限。



### VersaVoice：可控的声音生成

Vevo: Controllable Zero-Shot Voice Imitation with  Self-Supervised Disentanglement

项目地址： https://versavoice.github.io/

作者：香港中文大学（深圳），Meta AI

是否开源：开源模型

摘要：

![](img/本周论文汇总(9.20)-20250920235723.png)

**可控的人类声音生成**，尤其是在歌唱等富有表现力的领域，仍然是一项重大挑战。本文提出了 **VersaVoice** ——一个统一的 **可控语音与歌声生成框架**。

为解决 **带标注歌唱数据稀缺** 以及实现 **灵活可控性** 的问题，VersaVoice 引入了两类音频分词器（tokenizer）：

1. **无乐谱韵律分词器（music-notation-free prosody tokenizer）** ——能够从语音、歌唱甚至器乐声中捕捉韵律与旋律；
    
2. **低帧率内容-风格分词器（low-frame-rate content-style tokenizer）** ——对人类声音中的语言内容、韵律和风格进行编码，同时实现音色解耦。
    

VersaVoice 包含两个核心建模阶段：

- **自回归（AR）内容-风格建模阶段**：实现对文本、韵律和风格的可控性；
    
- **流匹配（FM）声学建模阶段**：实现音色的可控生成。
    

特别地，在 AR 模型的预训练中，我们提出了 **显式与隐式韵律学习策略**，以打通语音与歌声之间的桥梁。进一步地，为了增强 AR 模型对 **文本与韵律的跟随能力**，我们设计了一种 **多目标后训练任务**，结合了可懂度（intelligibility）与韵律相似性对齐。

实验结果表明，VersaVoice 的统一建模能够在语音与歌声生成之间带来 **互惠效应**。此外，VersaVoice 在 **语音与歌声的合成、转换与编辑** 等广泛任务中均表现出色，充分展示了其 **强大的泛化能力与多样性**。

**Vevo** 可以接受 **语音或文本** 作为输入，并在 **一次前向推理（single forward pass）** 中实现 **零样本模仿（zero-shot imitation）**，同时支持对以下内容的可控生成：

- **语言内容（linguistic content）**：由输入源控制；
    
- **风格（style）**：由风格参考控制；
    
- **音色（timbre）**：由音色参考控制。
    
该推理流程可根据不同的零样本模仿任务进行灵活调整。


### DreamAudio

DreamAudio: Customized Text-to-Audio Generation with Diffusion Models

项目地址：https://yyua8222.github.io/DreamAudio_demopage

作者：CVSSP, University of Surrey, Guildford, UK；深圳字节跳动

是否开源：代码

摘要：

![](img/本周论文汇总(9.20)-20250920234638.png)

随着基于 **大规模扩散模型** 和 **语言建模** 的生成模型的发展，**文本生成音频（text-to-audio）** 取得了显著进展。尽管现有方法能够生成高质量的音频，但它们主要聚焦于与语义对齐的声音生成，却难以对特定声音的 **细粒度声学特征** 进行精确控制。因此，对于需要特定声音内容的用户而言，生成所需音频片段仍然存在挑战。

为解决这一问题，本文提出了 **DreamAudio** ——一个用于 **定制化文本生成音频（Customized Text-to-Audio Generation, CTTA）** 的框架。具体来说，我们引入了一种新机制，使模型能够从用户提供的参考概念中识别听觉信息，并将其用于音频生成。在给定少量包含个性化音频事件的参考音频样本时，系统能够生成包含这些特定事件的新音频片段。

此外，我们构建了两类数据集，用于训练与测试定制化系统。实验结果表明，所提出的 DreamAudio 不仅能够生成与输入文本提示高度对齐、且与定制化音频特征一致的音频样本，同时在通用文本生成音频任务上也保持了可比的性能。进一步地，我们还提供了一个 **包含真实 CTTA 场景音频事件的人类参与数据集**，作为定制化生成任务的基准。



